{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu3.fnal.gov      Thu Oct 24 20:29:37 2024  555.42.06\n",
      "[0] Tesla P100-PCIE-12GB | 54Â°C,  95 % |   762 / 12288 MB | aherrera(306M) ckapsiak(454M)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams_RR\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "V2_MERGED = True\n",
    "\n",
    "SIGNAL_FILEPATHS = [\n",
    "    # Test sig files #\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE/GluGluToHH/nominal/*',\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE_merged_v2/GluGluToHH/nominal/*',\n",
    "    # ggF HH # \n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\",\n",
    "    # VBF HH #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [\n",
    "    # ttH (i.e. the main bkg to reduce) #\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    # # Other single H samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\",\n",
    "    # # Prompt-Prompt samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\",\n",
    "    # # Prompt-Fake samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\",\n",
    "    # Fake-Fake samples #\n",
    "    # ADD HERE #\n",
    "]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v9'\n",
    "CRITERION = \"NLLLoss\"\n",
    "N_PARTICLES, N_PARTICLE_FIELDS = 6, 7\n",
    "MOD_VALS = (5, 5)\n",
    "# VARS = 'base_vars'\n",
    "# VARS = 'extra_vars'\n",
    "# CURRENT_TIME = '2024-08-30_14-35-01'\n",
    "# VARS = 'extra_vars+'\n",
    "# CURRENT_TIME = '2024-10-09_20-47-24'\n",
    "VARS = 'extra_vars+max+opt_space'\n",
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# CURRENT_TIME = '2024-10-21_12-09-38'\n",
    "# VARS = 'extra_vars+max+moved_vars_to_RNN'\n",
    "# CURRENT_TIME = '2024-10-21_00-35-33'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 6, 9\n",
    "# VARS = 'extra_vars_no_dijet_mass'\n",
    "# VARS = 'no_bad_vars'\n",
    "# VARS = 'simplified_bad_vars'\n",
    "# VARS = 'extra_vars_and_bools'\n",
    "# VARS = 'extra_vars_in_RNN'\n",
    "# VARS = f'extra_vars_mod{MOD_VALS[0]}-{MOD_VALS[1]}'\n",
    "# VARS = 'extra_vars_lead_lep_only'\n",
    "# CURRENT_TIME = '2024-10-02_18-03-26'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 3, 6\n",
    "# VARS = 'extra_vars_no_lep'\n",
    "# CURRENT_TIME = '2024-10-04_12-49-31'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 2, 6\n",
    "# OUTPUT_DIRPATH = CURRENT_DIRPATH + f\"/model_outputs/{VERSION}/{VARS}/\"\n",
    "OUTPUT_DIRPATH = CURRENT_DIRPATH + f\"/model_outputs/{VERSION}/{VARS}/\" + CURRENT_TIME\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = True\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413735, 6, 7)\n",
      "Data HLF: (413735, 15)\n",
      "n signal = 136530, n bkg = 277205\n",
      "Data list test: (103521, 6, 7)\n",
      "Data HLF test: (103521, 15)\n",
      "n signal = 34224, n bkg = 69297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413918, 6, 7)\n",
      "Data HLF: (413918, 15)\n",
      "n signal = 136466, n bkg = 277452\n",
      "Data list test: (103338, 6, 7)\n",
      "Data HLF test: (103338, 15)\n",
      "n signal = 34288, n bkg = 69050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413265, 6, 7)\n",
      "Data HLF: (413265, 15)\n",
      "n signal = 136638, n bkg = 276627\n",
      "Data list test: (103991, 6, 7)\n",
      "Data HLF test: (103991, 15)\n",
      "n signal = 34116, n bkg = 69875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413725, 6, 7)\n",
      "Data HLF: (413725, 15)\n",
      "n signal = 136671, n bkg = 277054\n",
      "Data list test: (103531, 6, 7)\n",
      "Data HLF test: (103531, 15)\n",
      "n signal = 34083, n bkg = 69448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (414381, 6, 7)\n",
      "Data HLF: (414381, 15)\n",
      "n signal = 136711, n bkg = 277670\n",
      "Data list test: (102875, 6, 7)\n",
      "Data HLF test: (102875, 15)\n",
      "n signal = 34043, n bkg = 68832\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    data_df_dict, data_test_df_dict, \n",
    "    data_list_dict, data_hlf_dict, label_dict, \n",
    "    data_list_test_dict, data_hlf_test_dict, label_test_dict, \n",
    "    high_level_fields_dict, input_hlf_vars_dict, hlf_vars_columns_dict,\n",
    "    data_aux_dict, data_test_aux_dict\n",
    ") = process_data(\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED, mod_vals=MOD_VALS, k_fold_test=True\n",
    ")\n",
    "# skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_weights(event_weights, labels):\n",
    "    sum_of_bkg = np.sum(event_weights[labels==0])\n",
    "    sum_of_sig = np.sum(event_weights[labels==1])\n",
    "\n",
    "    sig_scale_factor = sum_of_bkg / sum_of_sig\n",
    "\n",
    "    weights = np.where(labels==0, event_weights, event_weights*sig_scale_factor)\n",
    "    mean_weight = np.mean(weights)\n",
    "    abs_weights = np.abs(weights)\n",
    "    scaled_weights = abs_weights / mean_weight\n",
    "\n",
    "    return scaled_weights\n",
    "\n",
    "def plot_train_val_losses(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None, n_folds=5\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dotted']\n",
    "        linestyles = linestyles * ((2*len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:2*len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx if fold_idx%2 == 0 else fold_idx+1]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx+1 if fold_idx%2 == 0 else fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='std', \n",
    "    labels=None, yscale='linear', run2=True, sort=None, run3=None,\n",
    "    mask=None, n_folds=5\n",
    "):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d, AUC = %.4f\" % (fold_idx, float(auc(IN_info['fprs'][fold_idx],IN_info['base_tpr']))), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "    elif method == 'round_robin_sum':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['all_labels']), dtype=bool)\n",
    "\n",
    "        base_tpr = np.linspace(0, 1, 5000)\n",
    "        all_labels = np.flatten(np.array(IN_info['all_labels'])[mask])\n",
    "        all_preds = np.flatten(np.exp(IN_info['all_preds'])[mask][fold_idx, :, 1])\n",
    "\n",
    "        fpr, tpr, threshold = roc_curve(all_labels, all_preds)\n",
    "        fpr = np.interp(base_tpr, tpr, fpr)\n",
    "        threshold = np.interp(base_tpr, tpr, threshold)\n",
    "        fpr[0] = 0.0\n",
    "\n",
    "        all_auc = float(auc(fpr, base_tpr))\n",
    "\n",
    "        plt.plot(\n",
    "            fpr, base_tpr,\n",
    "            label=\"Run3 NN - sum over folds, AUC = %.4f\" % all_auc, linestyle='solid',\n",
    "            alpha=1\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for i in index_arr:\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_fprs'])[0], dtype=bool)\n",
    "            plt.plot(\n",
    "                np.array(IN_info[i]['mean_fprs'])[mask], np.array(IN_info[i]['base_tpr'])[mask], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i], alpha=0.5\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if run3 is not None:\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(run3['mean_fprs'])[0], dtype=bool)\n",
    "        plt.plot(\n",
    "            np.array(run3['mean_fprs'])[mask], np.array(run3['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (run3['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, n_bins=50, all_sig=False, all_bkg=False,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        # for cut in np.linspace(0, 1, 10, endpoint=False):\n",
    "        #     print(f\"output score > {cut:.2f}\")\n",
    "        #     print('='*60)\n",
    "        #     print(f\"num sig > {cut:.2f} = {len(sig_np[sig_np > cut])}\")\n",
    "        #     print(f\"num bkg > {cut:.2f} = {len(bkg_np[bkg_np > cut])}\")\n",
    "        #     print('-'*60)\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'] if weights[fold_idx]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'] if weights[fold_idx]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[fold_idx]['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights[fold_idx]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=1, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=[\n",
    "                f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal', \n",
    "                f'{\"ttH\" if not all_bkg else \"all\"} background'\n",
    "            ]\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'] if weights[i]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'] if weights[i]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[i]['sig'] is not None else False),\n",
    "                alpha=0.7, density=(False if weights[i]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[i] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[i], linestyles[i]]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def s_over_root_b(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, lines_fold=None, lines=None, lines_labels=None, \n",
    "        lines_colors=None, only_fold=None, no_fold=False, n_bins=50,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/âb', alpha=0.8 \n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "    elif method == 'arr':\n",
    "        if mask is None or np.all(mask):\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "        if only_fold is None:\n",
    "            sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb - avg. over folds', \n",
    "                alpha=0.5, \n",
    "            )\n",
    "            if lines is not None:\n",
    "                for i in range(len(lines)):\n",
    "                    plt.vlines(\n",
    "                        lines[i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb - avg.'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[i]\n",
    "                    )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'])\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "                label='s/âb'+(' - '+labels[i] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[i], \n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 150., 2000, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    # 'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # 'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Integer(0, 10, name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($\\Sigma E_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t0}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t1}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma$ ln($p_{T}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'ln($M_{jj}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, fold, var_name, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label_dict[f'fold_{fold}'] == 1\n",
    "    sig_test_mask = label_test_dict[f'fold_{fold}'] == 1\n",
    "    bkg_mask = label_dict[f'fold_{fold}'] == 0\n",
    "    bkg_test_mask = label_test_dict[f'fold_{fold}'] == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_train_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            # index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold):\n",
    "    sig_train_mask = (label_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux_dict[f'fold_{fold}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux_dict[f'fold_{fold}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux_dict[f'fold_{fold}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux_dict[f'fold_{fold}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(\n",
    "    output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, \n",
    "    plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True\n",
    "):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir_ = os.path.join(output_dir, \"fold\")\n",
    "        if not os.path.exists(output_dir_):\n",
    "            os.makedirs(output_dir_)\n",
    "        plt.savefig(f'{output_dir_}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir_}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_input_vars_after_score_cut(\n",
    "    IN_info, score_cut, destdir, fold, plot_prefix, plot_postfix='', method='std', \n",
    "    weights={'sig': None, 'bkg': None}, all_sig=False, all_bkg=False,\n",
    "    mask=None\n",
    "):\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "        bkg_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            sig_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=sig_var.loc[sig_mask], \n",
    "                weight=weights['sig'][sig_mask] if weights['sig'] is not None else np.ones(np.sum(sig_mask))\n",
    "            )\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            bkg_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=bkg_var.loc[bkg_mask], \n",
    "                weight=weights['bkg'][bkg_mask] if weights['bkg'] is not None else np.ones(np.sum(bkg_mask))\n",
    "            )\n",
    "            make_input_plot(\n",
    "                destdir, var_name, [sig_hist, bkg_hist], fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore{score_cut}', labels=['HH signal', 'ttH background'], density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['all_preds'][fold])[0], dtype=bool)\n",
    "        \n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut if score_cut is list else [score_cut]:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['all_preds'][fold]\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['all_labels'][fold]) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['all_preds'][fold]\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['all_labels'][fold]) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used method 'std'. You used {method}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New configuration: {'hidden_layers': 6, 'initial_nodes': 424, 'dropout': 0.7735715996842537, 'gru_layers': 9, 'gru_size': 316, 'dropout_g': 0.35209971949050295, 'learning_rate': 0.00015493103643906704, 'batch_size': 4000, 'L2_reg': 1.8735112038624322e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0005417766 Acc: 57.4223213196\n",
      "validation Loss: 0.0005436922 Acc: 33.0767288208\n",
      "training Loss: 0.0005393866 Acc: 56.5990295410\n",
      "validation Loss: 0.0005428292 Acc: 33.0767288208\n",
      "training Loss: 0.0005379574 Acc: 43.7855758667\n",
      "validation Loss: 0.0005418593 Acc: 33.0767288208\n",
      "training Loss: 0.0005356746 Acc: 35.2441787720\n",
      "validation Loss: 0.0005412253 Acc: 33.0767288208\n",
      "training Loss: 0.0005314845 Acc: 33.4994010925\n",
      "validation Loss: 0.0005390025 Acc: 33.0767288208\n",
      "training Loss: 0.0005206102 Acc: 33.0256690979\n",
      "validation Loss: 0.0005245972 Acc: 33.0767288208\n",
      "training Loss: 0.0005057476 Acc: 32.8755111694\n",
      "validation Loss: 0.0004863397 Acc: 33.0767288208\n",
      "training Loss: 0.0004986894 Acc: 32.9700775146\n",
      "validation Loss: 0.0004777886 Acc: 33.0767288208\n",
      "training Loss: 0.0004924336 Acc: 32.9800453186\n",
      "validation Loss: 0.0004570441 Acc: 33.0767288208\n",
      "training Loss: 0.0004899238 Acc: 32.9800453186\n",
      "validation Loss: 0.0004519364 Acc: 33.0767288208\n",
      "training Loss: 0.0004860091 Acc: 32.9800453186\n",
      "validation Loss: 0.0004448809 Acc: 33.0767288208\n",
      "training Loss: 0.0004828726 Acc: 49.2135658264\n",
      "validation Loss: 0.0004432032 Acc: 84.6773910522\n",
      "training Loss: 0.0004799877 Acc: 68.2230148315\n",
      "validation Loss: 0.0004297097 Acc: 84.8647079468\n",
      "training Loss: 0.0004763063 Acc: 75.5628585815\n",
      "validation Loss: 0.0004210810 Acc: 84.8586654663\n",
      "training Loss: 0.0004738090 Acc: 77.2867889404\n",
      "validation Loss: 0.0004199522 Acc: 84.8393249512\n",
      "training Loss: 0.0004725630 Acc: 77.5191192627\n",
      "validation Loss: 0.0004131714 Acc: 84.9287567139\n",
      "training Loss: 0.0004706183 Acc: 77.7774429321\n",
      "validation Loss: 0.0004069057 Acc: 84.9964294434\n",
      "training Loss: 0.0004684684 Acc: 77.9816741943\n",
      "validation Loss: 0.0004107467 Acc: 84.9190902710\n",
      "training Loss: 0.0004670804 Acc: 78.2067642212\n",
      "validation Loss: 0.0004062199 Acc: 84.9275512695\n",
      "training Loss: 0.0004634353 Acc: 78.4451370239\n",
      "validation Loss: 0.0004020099 Acc: 84.9456787109\n",
      "training Loss: 0.0004623016 Acc: 78.5674972534\n",
      "validation Loss: 0.0004083927 Acc: 84.7220993042\n",
      "training Loss: 0.0004621219 Acc: 78.6436386108\n",
      "validation Loss: 0.0003945489 Acc: 85.0314788818\n",
      "training Loss: 0.0004592783 Acc: 78.8007431030\n",
      "validation Loss: 0.0003975365 Acc: 84.9710540771\n",
      "training Loss: 0.0004595744 Acc: 78.8738555908\n",
      "validation Loss: 0.0003980763 Acc: 84.9311752319\n",
      "training Loss: 0.0004577008 Acc: 78.8859405518\n",
      "validation Loss: 0.0003927107 Acc: 84.9565505981\n",
      "training Loss: 0.0004565639 Acc: 79.0234069824\n",
      "validation Loss: 0.0003931706 Acc: 84.9964294434\n",
      "training Loss: 0.0004544554 Acc: 79.2231140137\n",
      "validation Loss: 0.0003870407 Acc: 85.0798187256\n",
      "training Loss: 0.0004558626 Acc: 79.2028732300\n",
      "validation Loss: 0.0003896843 Acc: 85.0616912842\n",
      "training Loss: 0.0004543126 Acc: 79.3028717041\n",
      "validation Loss: 0.0003840938 Acc: 85.1329956055\n",
      "training Loss: 0.0004523869 Acc: 79.3738708496\n",
      "validation Loss: 0.0003886281 Acc: 85.0544433594\n",
      "Best val acc: 85.132996\n",
      "----------\n",
      "training Loss: 0.0005430834 Acc: 33.0681838989\n",
      "validation Loss: 0.0005440984 Acc: 32.8046455383\n",
      "training Loss: 0.0005393410 Acc: 33.0138244629\n",
      "validation Loss: 0.0005444795 Acc: 32.8046455383\n",
      "training Loss: 0.0005377110 Acc: 33.0105056763\n",
      "validation Loss: 0.0005440809 Acc: 32.8046455383\n",
      "training Loss: 0.0005363196 Acc: 33.0105056763\n",
      "validation Loss: 0.0005449299 Acc: 32.8046455383\n",
      "training Loss: 0.0005338654 Acc: 33.0105056763\n",
      "validation Loss: 0.0005462886 Acc: 32.8046455383\n",
      "training Loss: 0.0005299591 Acc: 33.0105056763\n",
      "validation Loss: 0.0005471132 Acc: 32.8046455383\n",
      "training Loss: 0.0005248963 Acc: 33.0105056763\n",
      "validation Loss: 0.0005444534 Acc: 32.8046455383\n",
      "training Loss: 0.0005177657 Acc: 33.0105056763\n",
      "validation Loss: 0.0005373961 Acc: 32.8046455383\n",
      "training Loss: 0.0005107563 Acc: 33.0105056763\n",
      "validation Loss: 0.0005277253 Acc: 32.8046455383\n",
      "training Loss: 0.0005055437 Acc: 33.0105056763\n",
      "validation Loss: 0.0005192285 Acc: 32.8046455383\n",
      "training Loss: 0.0005022300 Acc: 33.0105056763\n",
      "validation Loss: 0.0005055085 Acc: 32.8046455383\n",
      "training Loss: 0.0004997906 Acc: 37.5265007019\n",
      "validation Loss: 0.0004992615 Acc: 32.8046455383\n",
      "training Loss: 0.0004970527 Acc: 50.8319892883\n",
      "validation Loss: 0.0004875158 Acc: 32.8046455383\n",
      "training Loss: 0.0004958770 Acc: 52.4156379700\n",
      "validation Loss: 0.0004902965 Acc: 32.9266510010\n",
      "training Loss: 0.0004928573 Acc: 55.7810440063\n",
      "validation Loss: 0.0004762300 Acc: 84.8509368896\n",
      "training Loss: 0.0004917102 Acc: 59.5405502319\n",
      "validation Loss: 0.0004668220 Acc: 84.7845001221\n",
      "training Loss: 0.0004898608 Acc: 63.2610969543\n",
      "validation Loss: 0.0004705434 Acc: 84.6334991455\n",
      "training Loss: 0.0004887700 Acc: 66.9348373413\n",
      "validation Loss: 0.0004639629 Acc: 84.7482604980\n",
      "training Loss: 0.0004872617 Acc: 69.5111389160\n",
      "validation Loss: 0.0004600455 Acc: 84.6781921387\n",
      "training Loss: 0.0004843592 Acc: 72.4265747070\n",
      "validation Loss: 0.0004506358 Acc: 84.7748336792\n",
      "training Loss: 0.0004840110 Acc: 75.2453689575\n",
      "validation Loss: 0.0004469032 Acc: 84.8279800415\n",
      "training Loss: 0.0004837191 Acc: 76.1540679932\n",
      "validation Loss: 0.0004456811 Acc: 84.8074493408\n",
      "training Loss: 0.0004821888 Acc: 76.4560623169\n",
      "validation Loss: 0.0004456233 Acc: 84.6407470703\n",
      "training Loss: 0.0004815506 Acc: 76.7027893066\n",
      "validation Loss: 0.0004299583 Acc: 84.8859634399\n",
      "training Loss: 0.0004798209 Acc: 76.8462295532\n",
      "validation Loss: 0.0004350387 Acc: 84.7808761597\n",
      "training Loss: 0.0004799758 Acc: 76.9096527100\n",
      "validation Loss: 0.0004391172 Acc: 84.5501480103\n",
      "training Loss: 0.0004788111 Acc: 76.9205245972\n",
      "validation Loss: 0.0004398210 Acc: 84.5066680908\n",
      "training Loss: 0.0004784895 Acc: 77.0404129028\n",
      "validation Loss: 0.0004311255 Acc: 84.7808761597\n",
      "training Loss: 0.0004768468 Acc: 77.1249694824\n",
      "validation Loss: 0.0004323753 Acc: 84.6649093628\n",
      "training Loss: 0.0004769359 Acc: 77.2430496216\n",
      "validation Loss: 0.0004310076 Acc: 84.7047729492\n",
      "Best val acc: 84.885963\n",
      "----------\n",
      "training Loss: 0.0005893816 Acc: 66.8799057007\n",
      "validation Loss: 0.0005846852 Acc: 67.1639251709\n",
      "training Loss: 0.0005763135 Acc: 66.8789978027\n",
      "validation Loss: 0.0005777424 Acc: 67.1639251709\n",
      "training Loss: 0.0005685131 Acc: 66.8735580444\n",
      "validation Loss: 0.0005715735 Acc: 67.1639251709\n",
      "training Loss: 0.0005627758 Acc: 66.8421020508\n",
      "validation Loss: 0.0005662557 Acc: 67.1639251709\n",
      "training Loss: 0.0005581553 Acc: 66.6454925537\n",
      "validation Loss: 0.0005621719 Acc: 67.1639251709\n",
      "training Loss: 0.0005537010 Acc: 65.9873199463\n",
      "validation Loss: 0.0005576789 Acc: 67.1639251709\n",
      "training Loss: 0.0005503063 Acc: 64.7369079590\n",
      "validation Loss: 0.0005547468 Acc: 67.1639251709\n",
      "training Loss: 0.0005474661 Acc: 63.7163810730\n",
      "validation Loss: 0.0005519890 Acc: 67.1639251709\n",
      "training Loss: 0.0005452629 Acc: 61.6114959717\n",
      "validation Loss: 0.0005496033 Acc: 67.1639251709\n",
      "training Loss: 0.0005431556 Acc: 60.2748832703\n",
      "validation Loss: 0.0005480918 Acc: 67.1639251709\n",
      "training Loss: 0.0005418857 Acc: 59.6542167664\n",
      "validation Loss: 0.0005470263 Acc: 67.1639251709\n",
      "training Loss: 0.0005405200 Acc: 58.9458351135\n",
      "validation Loss: 0.0005457697 Acc: 67.1639251709\n",
      "training Loss: 0.0005393751 Acc: 57.7377700806\n",
      "validation Loss: 0.0005453413 Acc: 32.8360748291\n",
      "training Loss: 0.0005383111 Acc: 56.8497200012\n",
      "validation Loss: 0.0005444407 Acc: 32.8360748291\n",
      "training Loss: 0.0005372358 Acc: 55.4692497253\n",
      "validation Loss: 0.0005435503 Acc: 32.8360748291\n",
      "training Loss: 0.0005358030 Acc: 54.7560272217\n",
      "validation Loss: 0.0005423918 Acc: 32.8360748291\n",
      "training Loss: 0.0005341822 Acc: 54.2639083862\n",
      "validation Loss: 0.0005401370 Acc: 32.8360748291\n",
      "training Loss: 0.0005310390 Acc: 54.2572555542\n",
      "validation Loss: 0.0005335128 Acc: 32.8360748291\n",
      "training Loss: 0.0005249264 Acc: 49.9491844177\n",
      "validation Loss: 0.0005190544 Acc: 32.8360748291\n",
      "training Loss: 0.0005153972 Acc: 37.3147354126\n",
      "validation Loss: 0.0004971643 Acc: 32.8360748291\n",
      "training Loss: 0.0005080801 Acc: 38.1344299316\n",
      "validation Loss: 0.0004667520 Acc: 32.8360748291\n",
      "training Loss: 0.0005036355 Acc: 38.9229660034\n",
      "validation Loss: 0.0004666200 Acc: 32.8360748291\n",
      "training Loss: 0.0005006963 Acc: 39.7635307312\n",
      "validation Loss: 0.0004557739 Acc: 32.8360748291\n",
      "training Loss: 0.0004971108 Acc: 40.5278701782\n",
      "validation Loss: 0.0004540834 Acc: 32.8360748291\n",
      "training Loss: 0.0004953562 Acc: 41.2870674133\n",
      "validation Loss: 0.0004407052 Acc: 84.4361343384\n",
      "training Loss: 0.0004930662 Acc: 41.9116668701\n",
      "validation Loss: 0.0004393447 Acc: 84.4857406616\n",
      "training Loss: 0.0004895693 Acc: 42.7310562134\n",
      "validation Loss: 0.0004265052 Acc: 84.0816421509\n",
      "training Loss: 0.0004883573 Acc: 43.5208015442\n",
      "validation Loss: 0.0004269944 Acc: 84.3635406494\n",
      "training Loss: 0.0004864940 Acc: 43.8958663940\n",
      "validation Loss: 0.0004296596 Acc: 84.5365524292\n",
      "training Loss: 0.0004853596 Acc: 43.8021011353\n",
      "validation Loss: 0.0004283235 Acc: 84.6006774902\n",
      "Best val acc: 84.600677\n",
      "----------\n",
      "training Loss: 0.0005398693 Acc: 33.6165313721\n",
      "validation Loss: 0.0005444934 Acc: 33.1101570129\n",
      "training Loss: 0.0005325038 Acc: 36.0613937378\n",
      "validation Loss: 0.0005427121 Acc: 33.1101570129\n",
      "training Loss: 0.0005151835 Acc: 42.0810928345\n",
      "validation Loss: 0.0005210976 Acc: 38.3793563843\n",
      "training Loss: 0.0005027212 Acc: 46.3121604919\n",
      "validation Loss: 0.0004916243 Acc: 48.1165008545\n",
      "training Loss: 0.0004938964 Acc: 49.0621757507\n",
      "validation Loss: 0.0004812077 Acc: 52.5131416321\n",
      "training Loss: 0.0004864918 Acc: 50.9471855164\n",
      "validation Loss: 0.0004741281 Acc: 55.0534744263\n",
      "training Loss: 0.0004808719 Acc: 52.4705390930\n",
      "validation Loss: 0.0004425370 Acc: 66.1961441040\n",
      "training Loss: 0.0004751968 Acc: 54.1111221313\n",
      "validation Loss: 0.0004207469 Acc: 68.5974960327\n",
      "training Loss: 0.0004703944 Acc: 55.0839920044\n",
      "validation Loss: 0.0004217013 Acc: 66.6843872070\n",
      "training Loss: 0.0004664532 Acc: 56.1798286438\n",
      "validation Loss: 0.0004039044 Acc: 70.9746780396\n",
      "training Loss: 0.0004607979 Acc: 57.1593437195\n",
      "validation Loss: 0.0003999396 Acc: 71.2623062134\n",
      "training Loss: 0.0004591690 Acc: 57.6741790771\n",
      "validation Loss: 0.0003954430 Acc: 71.1088256836\n",
      "training Loss: 0.0004538970 Acc: 58.4216537476\n",
      "validation Loss: 0.0003913570 Acc: 71.4762191772\n",
      "training Loss: 0.0004513085 Acc: 58.8999328613\n",
      "validation Loss: 0.0003790548 Acc: 73.2841873169\n",
      "training Loss: 0.0004497965 Acc: 59.3168754578\n",
      "validation Loss: 0.0003869026 Acc: 70.3124008179\n",
      "training Loss: 0.0004445551 Acc: 60.0296058655\n",
      "validation Loss: 0.0003679560 Acc: 74.6099472046\n",
      "training Loss: 0.0004435052 Acc: 60.4311408997\n",
      "validation Loss: 0.0003732644 Acc: 72.6424560547\n",
      "training Loss: 0.0004416570 Acc: 60.9097213745\n",
      "validation Loss: 0.0003802449 Acc: 70.7027587891\n",
      "training Loss: 0.0004397862 Acc: 61.2209167480\n",
      "validation Loss: 0.0003692831 Acc: 72.9095382690\n",
      "training Loss: 0.0004370882 Acc: 61.7457237244\n",
      "validation Loss: 0.0003593089 Acc: 74.8540649414\n",
      "training Loss: 0.0004355378 Acc: 61.9339523315\n",
      "validation Loss: 0.0003757023 Acc: 70.5964050293\n",
      "training Loss: 0.0004359889 Acc: 61.9076652527\n",
      "validation Loss: 0.0003731828 Acc: 71.0411453247\n",
      "training Loss: 0.0004330111 Acc: 62.4098129272\n",
      "validation Loss: 0.0003616243 Acc: 73.4352493286\n",
      "training Loss: 0.0004301736 Acc: 62.7530326843\n",
      "validation Loss: 0.0003748405 Acc: 70.4707183838\n",
      "training Loss: 0.0004302408 Acc: 62.8974533081\n",
      "validation Loss: 0.0003486062 Acc: 76.1979522705\n",
      "training Loss: 0.0004292219 Acc: 63.2594108582\n",
      "validation Loss: 0.0003587457 Acc: 73.5899429321\n",
      "training Loss: 0.0004292686 Acc: 63.1723937988\n",
      "validation Loss: 0.0003540083 Acc: 74.6268615723\n",
      "training Loss: 0.0004266280 Acc: 63.2896232605\n",
      "validation Loss: 0.0003632694 Acc: 72.3330688477\n",
      "training Loss: 0.0004289609 Acc: 63.1560783386\n",
      "validation Loss: 0.0003533193 Acc: 74.5918121338\n",
      "training Loss: 0.0004248753 Acc: 63.5023231506\n",
      "validation Loss: 0.0003527505 Acc: 74.5180969238\n",
      "Best val acc: 76.197952\n",
      "----------\n",
      "training Loss: 0.0005332372 Acc: 33.3893432617\n",
      "validation Loss: 0.0005374188 Acc: 33.0163955688\n",
      "training Loss: 0.0005320414 Acc: 33.1006584167\n",
      "validation Loss: 0.0005372460 Acc: 33.0163955688\n",
      "training Loss: 0.0005316157 Acc: 33.0201148987\n",
      "validation Loss: 0.0005370692 Acc: 33.0163955688\n",
      "training Loss: 0.0005314780 Acc: 32.9993019104\n",
      "validation Loss: 0.0005373860 Acc: 33.0163955688\n",
      "training Loss: 0.0005313161 Acc: 32.9971885681\n",
      "validation Loss: 0.0005370284 Acc: 33.0163955688\n",
      "training Loss: 0.0005312038 Acc: 32.9833145142\n",
      "validation Loss: 0.0005372751 Acc: 33.0163955688\n",
      "training Loss: 0.0005311504 Acc: 32.9775810242\n",
      "validation Loss: 0.0005370940 Acc: 33.0163955688\n",
      "training Loss: 0.0005309831 Acc: 32.9760742188\n",
      "validation Loss: 0.0005368760 Acc: 33.0163955688\n",
      "training Loss: 0.0005307459 Acc: 32.9796943665\n",
      "validation Loss: 0.0005369729 Acc: 33.0163955688\n",
      "training Loss: 0.0005304328 Acc: 32.9631042480\n",
      "validation Loss: 0.0005371921 Acc: 33.0163955688\n",
      "training Loss: 0.0005297464 Acc: 32.9591827393\n",
      "validation Loss: 0.0005366693 Acc: 33.0163955688\n",
      "training Loss: 0.0005285227 Acc: 32.9422874451\n",
      "validation Loss: 0.0005362773 Acc: 33.0163955688\n",
      "training Loss: 0.0005259294 Acc: 32.9172515869\n",
      "validation Loss: 0.0005330896 Acc: 33.0163955688\n",
      "training Loss: 0.0005219249 Acc: 33.0804481506\n",
      "validation Loss: 0.0005253316 Acc: 33.0163955688\n",
      "training Loss: 0.0005165672 Acc: 34.4146690369\n",
      "validation Loss: 0.0005134139 Acc: 33.0163955688\n",
      "training Loss: 0.0005112578 Acc: 37.8098030090\n",
      "validation Loss: 0.0004988607 Acc: 33.0163955688\n",
      "training Loss: 0.0005073502 Acc: 43.4450263977\n",
      "validation Loss: 0.0004890783 Acc: 33.0163955688\n",
      "training Loss: 0.0005030521 Acc: 48.2319984436\n",
      "validation Loss: 0.0004759299 Acc: 33.0163955688\n",
      "training Loss: 0.0004999070 Acc: 49.4283638000\n",
      "validation Loss: 0.0004654299 Acc: 33.0163955688\n",
      "training Loss: 0.0004978811 Acc: 50.6027107239\n",
      "validation Loss: 0.0004643536 Acc: 33.0163955688\n",
      "training Loss: 0.0004941791 Acc: 51.2352790833\n",
      "validation Loss: 0.0004538497 Acc: 84.6061019897\n",
      "training Loss: 0.0004939085 Acc: 51.5999832153\n",
      "validation Loss: 0.0004472237 Acc: 84.4504470825\n",
      "training Loss: 0.0004914375 Acc: 51.8440208435\n",
      "validation Loss: 0.0004474165 Acc: 84.6857376099\n",
      "training Loss: 0.0004901106 Acc: 51.7405548096\n",
      "validation Loss: 0.0004383046 Acc: 84.4649276733\n",
      "training Loss: 0.0004878676 Acc: 51.8385925293\n",
      "validation Loss: 0.0004348509 Acc: 84.4359664917\n",
      "training Loss: 0.0004866041 Acc: 51.8159675598\n",
      "validation Loss: 0.0004304696 Acc: 84.3961486816\n",
      "training Loss: 0.0004854072 Acc: 51.8675498962\n",
      "validation Loss: 0.0004279940 Acc: 84.3141021729\n",
      "training Loss: 0.0004834088 Acc: 51.8385925293\n",
      "validation Loss: 0.0004340657 Acc: 84.7424468994\n",
      "training Loss: 0.0004831477 Acc: 51.8648376465\n",
      "validation Loss: 0.0004221148 Acc: 84.1017379761\n",
      "training Loss: 0.0004814368 Acc: 51.8446235657\n",
      "validation Loss: 0.0004245922 Acc: 84.5711059570\n",
      "Best val acc: 84.742447\n",
      "----------\n",
      "Average best_acc across k-fold: 83.11199951171875\n",
      "New configuration: {'hidden_layers': 5, 'initial_nodes': 408, 'dropout': 0.43717968341380115, 'gru_layers': 5, 'gru_size': 420, 'dropout_g': 0.31028258277136894, 'learning_rate': 0.003914600869268913, 'batch_size': 4000, 'L2_reg': 9.060562467991656e-05}\n",
      "training Loss: 0.0003393948 Acc: 78.1161270142\n",
      "validation Loss: 0.0002454294 Acc: 85.6236495972\n",
      "training Loss: 0.0002638439 Acc: 85.9278869629\n",
      "validation Loss: 0.0002285883 Acc: 86.3197402954\n",
      "training Loss: 0.0002577107 Acc: 86.0022125244\n",
      "validation Loss: 0.0002276593 Acc: 87.5222015381\n",
      "training Loss: 0.0002510063 Acc: 86.4025268555\n",
      "validation Loss: 0.0002239989 Acc: 87.1850280762\n",
      "training Loss: 0.0002484451 Acc: 86.3853073120\n",
      "validation Loss: 0.0002250393 Acc: 86.3874206543\n",
      "training Loss: 0.0002483576 Acc: 86.5197525024\n",
      "validation Loss: 0.0002195704 Acc: 86.5191497803\n",
      "training Loss: 0.0002458510 Acc: 86.3798675537\n",
      "validation Loss: 0.0002253836 Acc: 85.8109664917\n",
      "training Loss: 0.0002448155 Acc: 86.4007110596\n",
      "validation Loss: 0.0002213617 Acc: 86.4333419800\n",
      "training Loss: 0.0002435748 Acc: 86.3523712158\n",
      "validation Loss: 0.0002168436 Acc: 86.7511825562\n",
      "training Loss: 0.0002439487 Acc: 86.3417968750\n",
      "validation Loss: 0.0002252323 Acc: 88.0708618164\n",
      "training Loss: 0.0002441989 Acc: 86.3182296753\n",
      "validation Loss: 0.0002180509 Acc: 87.7083129883\n",
      "training Loss: 0.0002427751 Acc: 86.5608367920\n",
      "validation Loss: 0.0002255501 Acc: 88.0140686035\n",
      "training Loss: 0.0002423016 Acc: 86.3001022339\n",
      "validation Loss: 0.0002166813 Acc: 86.4019241333\n",
      "training Loss: 0.0002412789 Acc: 86.4469375610\n",
      "validation Loss: 0.0002167219 Acc: 87.8098297119\n",
      "training Loss: 0.0002418697 Acc: 86.4272994995\n",
      "validation Loss: 0.0002190917 Acc: 86.5964889526\n",
      "training Loss: 0.0002422240 Acc: 86.4061508179\n",
      "validation Loss: 0.0002180136 Acc: 86.5626525879\n",
      "training Loss: 0.0002402523 Acc: 86.2692871094\n",
      "validation Loss: 0.0002216968 Acc: 87.7566528320\n",
      "training Loss: 0.0002359233 Acc: 86.5949783325\n",
      "validation Loss: 0.0002152124 Acc: 86.6387863159\n",
      "training Loss: 0.0002347414 Acc: 86.5155181885\n",
      "validation Loss: 0.0002151594 Acc: 88.0599899292\n",
      "training Loss: 0.0002343262 Acc: 86.7726287842\n",
      "validation Loss: 0.0002126273 Acc: 87.1463623047\n",
      "training Loss: 0.0002344791 Acc: 86.8665924072\n",
      "validation Loss: 0.0002129207 Acc: 87.3082962036\n",
      "training Loss: 0.0002341650 Acc: 86.8306350708\n",
      "validation Loss: 0.0002119078 Acc: 87.6055908203\n",
      "training Loss: 0.0002337075 Acc: 86.7542037964\n",
      "validation Loss: 0.0002137514 Acc: 87.3977279663\n",
      "training Loss: 0.0002340761 Acc: 86.9623641968\n",
      "validation Loss: 0.0002140595 Acc: 86.3016128540\n",
      "training Loss: 0.0002324432 Acc: 86.7922668457\n",
      "validation Loss: 0.0002109389 Acc: 87.3771820068\n",
      "training Loss: 0.0002334424 Acc: 87.0889587402\n",
      "validation Loss: 0.0002111619 Acc: 86.9820022583\n",
      "training Loss: 0.0002319637 Acc: 86.9270172119\n",
      "validation Loss: 0.0002132842 Acc: 88.3923263550\n",
      "training Loss: 0.0002322509 Acc: 86.8236923218\n",
      "validation Loss: 0.0002122079 Acc: 88.0454864502\n",
      "training Loss: 0.0002323740 Acc: 86.9463500977\n",
      "validation Loss: 0.0002110220 Acc: 87.6466827393\n",
      "training Loss: 0.0002295291 Acc: 87.2073898315\n",
      "validation Loss: 0.0002093281 Acc: 87.4025650024\n",
      "Best val acc: 88.392326\n",
      "----------\n",
      "training Loss: 0.0003755950 Acc: 71.2134704590\n",
      "validation Loss: 0.0002670397 Acc: 85.7653656006\n",
      "training Loss: 0.0002760594 Acc: 86.0225753784\n",
      "validation Loss: 0.0002413782 Acc: 87.4106063843\n",
      "training Loss: 0.0002632871 Acc: 86.2481689453\n",
      "validation Loss: 0.0002368408 Acc: 85.0744094849\n",
      "training Loss: 0.0002571371 Acc: 86.6461944580\n",
      "validation Loss: 0.0002326159 Acc: 88.4941482544\n",
      "training Loss: 0.0002546411 Acc: 87.0366668701\n",
      "validation Loss: 0.0002276551 Acc: 87.6618652344\n",
      "training Loss: 0.0002514688 Acc: 86.9729461670\n",
      "validation Loss: 0.0002284148 Acc: 88.8855285645\n",
      "training Loss: 0.0002498567 Acc: 86.9841232300\n",
      "validation Loss: 0.0002244123 Acc: 86.5227584839\n",
      "training Loss: 0.0002492685 Acc: 86.8941268921\n",
      "validation Loss: 0.0002254934 Acc: 86.7534790039\n",
      "training Loss: 0.0002466890 Acc: 87.1003875732\n",
      "validation Loss: 0.0002240696 Acc: 87.0337219238\n",
      "training Loss: 0.0002466169 Acc: 87.0276107788\n",
      "validation Loss: 0.0002224447 Acc: 88.0761947632\n",
      "training Loss: 0.0002476107 Acc: 87.0723037720\n",
      "validation Loss: 0.0002231574 Acc: 87.5301971436\n",
      "training Loss: 0.0002448133 Acc: 87.2876281738\n",
      "validation Loss: 0.0002230198 Acc: 87.1955871582\n",
      "training Loss: 0.0002445241 Acc: 87.2172622681\n",
      "validation Loss: 0.0002211659 Acc: 88.7695693970\n",
      "training Loss: 0.0002448065 Acc: 87.4618759155\n",
      "validation Loss: 0.0002205063 Acc: 88.2513504028\n",
      "training Loss: 0.0002465990 Acc: 87.4884490967\n",
      "validation Loss: 0.0002252813 Acc: 87.4444274902\n",
      "training Loss: 0.0002460290 Acc: 87.5651550293\n",
      "validation Loss: 0.0002212404 Acc: 88.2404785156\n",
      "training Loss: 0.0002446633 Acc: 87.5518722534\n",
      "validation Loss: 0.0002230864 Acc: 88.9869995117\n",
      "training Loss: 0.0002436361 Acc: 87.5896148682\n",
      "validation Loss: 0.0002229502 Acc: 88.9217681885\n",
      "training Loss: 0.0002389522 Acc: 87.8345336914\n",
      "validation Loss: 0.0002168440 Acc: 88.3854370117\n",
      "training Loss: 0.0002382854 Acc: 87.8647308350\n",
      "validation Loss: 0.0002189173 Acc: 88.0761947632\n",
      "training Loss: 0.0002381538 Acc: 87.8885879517\n",
      "validation Loss: 0.0002172606 Acc: 88.2948379517\n",
      "training Loss: 0.0002374391 Acc: 87.9251327515\n",
      "validation Loss: 0.0002203759 Acc: 88.9133148193\n",
      "training Loss: 0.0002371586 Acc: 88.0616302490\n",
      "validation Loss: 0.0002172951 Acc: 87.8599700928\n",
      "training Loss: 0.0002357910 Acc: 88.1573638916\n",
      "validation Loss: 0.0002145570 Acc: 88.7176208496\n",
      "training Loss: 0.0002343158 Acc: 88.0694808960\n",
      "validation Loss: 0.0002147791 Acc: 88.9954528809\n",
      "training Loss: 0.0002340633 Acc: 88.1486053467\n",
      "validation Loss: 0.0002140421 Acc: 88.8130493164\n",
      "training Loss: 0.0002337964 Acc: 88.1344146729\n",
      "validation Loss: 0.0002134852 Acc: 88.2791366577\n",
      "training Loss: 0.0002332673 Acc: 88.2675933838\n",
      "validation Loss: 0.0002126921 Acc: 88.8372116089\n",
      "training Loss: 0.0002327943 Acc: 88.2005462646\n",
      "validation Loss: 0.0002138501 Acc: 89.0498123169\n",
      "training Loss: 0.0002324084 Acc: 88.2198791504\n",
      "validation Loss: 0.0002146327 Acc: 89.1706085205\n",
      "Best val acc: 89.170609\n",
      "----------\n",
      "training Loss: 0.0003718363 Acc: 78.5391921997\n",
      "validation Loss: 0.0002562560 Acc: 86.7554702759\n",
      "training Loss: 0.0002730822 Acc: 85.9808502197\n",
      "validation Loss: 0.0002406621 Acc: 86.5497894287\n",
      "training Loss: 0.0002609795 Acc: 86.2833175659\n",
      "validation Loss: 0.0002349894 Acc: 85.7742614746\n",
      "training Loss: 0.0002526983 Acc: 86.5794372559\n",
      "validation Loss: 0.0002395707 Acc: 84.7410278320\n",
      "training Loss: 0.0002508399 Acc: 86.7176589966\n",
      "validation Loss: 0.0002258309 Acc: 87.4668807983\n",
      "training Loss: 0.0002493678 Acc: 86.8250350952\n",
      "validation Loss: 0.0002293003 Acc: 86.2533721924\n",
      "training Loss: 0.0002474524 Acc: 86.9623565674\n",
      "validation Loss: 0.0002285374 Acc: 86.9720382690\n",
      "training Loss: 0.0002465532 Acc: 87.0240631104\n",
      "validation Loss: 0.0002251336 Acc: 87.6773986816\n",
      "training Loss: 0.0002443110 Acc: 87.1402130127\n",
      "validation Loss: 0.0002186689 Acc: 88.5714950562\n",
      "training Loss: 0.0002429353 Acc: 87.4801864624\n",
      "validation Loss: 0.0002198613 Acc: 88.1722335815\n",
      "training Loss: 0.0002447896 Acc: 87.3979187012\n",
      "validation Loss: 0.0002185740 Acc: 88.1686096191\n",
      "training Loss: 0.0002421153 Acc: 87.6525955200\n",
      "validation Loss: 0.0002176581 Acc: 86.4396896362\n",
      "training Loss: 0.0002419591 Acc: 87.4632492065\n",
      "validation Loss: 0.0002157328 Acc: 88.3053207397\n",
      "training Loss: 0.0002411594 Acc: 87.5769805908\n",
      "validation Loss: 0.0002204011 Acc: 89.2115249634\n",
      "training Loss: 0.0002404872 Acc: 87.6447296143\n",
      "validation Loss: 0.0002146958 Acc: 88.4493026733\n",
      "training Loss: 0.0002398406 Acc: 87.6299133301\n",
      "validation Loss: 0.0002147810 Acc: 88.9973754883\n",
      "training Loss: 0.0002403601 Acc: 87.6389846802\n",
      "validation Loss: 0.0002205091 Acc: 89.2502365112\n",
      "training Loss: 0.0002400016 Acc: 87.5697174072\n",
      "validation Loss: 0.0002177498 Acc: 88.0923843384\n",
      "training Loss: 0.0002399883 Acc: 87.5893783569\n",
      "validation Loss: 0.0002166095 Acc: 87.6556167603\n",
      "training Loss: 0.0002361544 Acc: 87.8383102417\n",
      "validation Loss: 0.0002128139 Acc: 87.8915481567\n",
      "training Loss: 0.0002336844 Acc: 87.8570632935\n",
      "validation Loss: 0.0002114403 Acc: 88.8727569580\n",
      "training Loss: 0.0002336237 Acc: 87.9151382446\n",
      "validation Loss: 0.0002153187 Acc: 87.9060668945\n",
      "training Loss: 0.0002341612 Acc: 87.9369125366\n",
      "validation Loss: 0.0002113765 Acc: 88.4263153076\n",
      "training Loss: 0.0002331218 Acc: 87.8268203735\n",
      "validation Loss: 0.0002108192 Acc: 88.3718643188\n",
      "training Loss: 0.0002334550 Acc: 87.9287490845\n",
      "validation Loss: 0.0002132669 Acc: 87.3120117188\n",
      "training Loss: 0.0002329802 Acc: 87.9354019165\n",
      "validation Loss: 0.0002133204 Acc: 88.8872756958\n",
      "training Loss: 0.0002339541 Acc: 88.0152587891\n",
      "validation Loss: 0.0002107840 Acc: 88.5533523560\n",
      "training Loss: 0.0002328930 Acc: 88.0715179443\n",
      "validation Loss: 0.0002131714 Acc: 87.2611999512\n",
      "training Loss: 0.0002331542 Acc: 87.9934768677\n",
      "validation Loss: 0.0002113030 Acc: 87.4705047607\n",
      "training Loss: 0.0002324332 Acc: 88.0067901611\n",
      "validation Loss: 0.0002139160 Acc: 86.7373199463\n",
      "Best val acc: 89.250237\n",
      "----------\n",
      "training Loss: 0.0003612800 Acc: 73.5452270508\n",
      "validation Loss: 0.0002562714 Acc: 85.0879211426\n",
      "training Loss: 0.0002745338 Acc: 86.0861663818\n",
      "validation Loss: 0.0002372464 Acc: 85.3223724365\n",
      "training Loss: 0.0002624477 Acc: 86.6103668213\n",
      "validation Loss: 0.0002455427 Acc: 88.5116882324\n",
      "training Loss: 0.0002564780 Acc: 86.7430038452\n",
      "validation Loss: 0.0002285565 Acc: 86.4632263184\n",
      "training Loss: 0.0002528049 Acc: 86.9399948120\n",
      "validation Loss: 0.0002222155 Acc: 87.7660217285\n",
      "training Loss: 0.0002508138 Acc: 87.1665954590\n",
      "validation Loss: 0.0002215928 Acc: 88.0198135376\n",
      "training Loss: 0.0002460885 Acc: 87.3901748657\n",
      "validation Loss: 0.0002260340 Acc: 88.0137710571\n",
      "training Loss: 0.0002468832 Acc: 87.3282318115\n",
      "validation Loss: 0.0002260547 Acc: 88.3594131470\n",
      "training Loss: 0.0002465683 Acc: 87.3046646118\n",
      "validation Loss: 0.0002181649 Acc: 88.2035140991\n",
      "training Loss: 0.0002456570 Acc: 87.5007553101\n",
      "validation Loss: 0.0002219413 Acc: 88.1708831787\n",
      "training Loss: 0.0002442949 Acc: 87.6037826538\n",
      "validation Loss: 0.0002191154 Acc: 88.1890106201\n",
      "training Loss: 0.0002442430 Acc: 87.5306625366\n",
      "validation Loss: 0.0002227024 Acc: 87.8046951294\n",
      "training Loss: 0.0002435875 Acc: 87.4641952515\n",
      "validation Loss: 0.0002188932 Acc: 88.6434173584\n",
      "training Loss: 0.0002397563 Acc: 87.8240356445\n",
      "validation Loss: 0.0002141160 Acc: 88.2240600586\n",
      "training Loss: 0.0002380916 Acc: 87.8043975830\n",
      "validation Loss: 0.0002148335 Acc: 87.8167800903\n",
      "training Loss: 0.0002377121 Acc: 87.8844604492\n",
      "validation Loss: 0.0002134620 Acc: 88.4282989502\n",
      "training Loss: 0.0002380469 Acc: 87.9119567871\n",
      "validation Loss: 0.0002173835 Acc: 86.4789352417\n",
      "training Loss: 0.0002373378 Acc: 87.9022903442\n",
      "validation Loss: 0.0002127486 Acc: 88.2917327881\n",
      "training Loss: 0.0002369683 Acc: 88.0013885498\n",
      "validation Loss: 0.0002125960 Acc: 88.7824020386\n",
      "training Loss: 0.0002364498 Acc: 87.9170913696\n",
      "validation Loss: 0.0002123962 Acc: 88.3775405884\n",
      "training Loss: 0.0002383238 Acc: 87.8938293457\n",
      "validation Loss: 0.0002133637 Acc: 88.1322097778\n",
      "training Loss: 0.0002372309 Acc: 87.9409637451\n",
      "validation Loss: 0.0002129760 Acc: 87.3756713867\n",
      "training Loss: 0.0002368818 Acc: 87.9219284058\n",
      "validation Loss: 0.0002139707 Acc: 88.0415725708\n",
      "training Loss: 0.0002364899 Acc: 88.0146789551\n",
      "validation Loss: 0.0002128987 Acc: 88.1116638184\n",
      "training Loss: 0.0002329219 Acc: 88.1086425781\n",
      "validation Loss: 0.0002103613 Acc: 88.5261917114\n",
      "training Loss: 0.0002329847 Acc: 88.1050186157\n",
      "validation Loss: 0.0002123104 Acc: 87.6125411987\n",
      "training Loss: 0.0002334391 Acc: 88.1415786743\n",
      "validation Loss: 0.0002091299 Acc: 88.0294876099\n",
      "training Loss: 0.0002318791 Acc: 88.1992874146\n",
      "validation Loss: 0.0002084873 Acc: 88.2458114624\n",
      "training Loss: 0.0002315654 Acc: 88.1820602417\n",
      "validation Loss: 0.0002092791 Acc: 88.7014312744\n",
      "training Loss: 0.0002314393 Acc: 88.2204360962\n",
      "validation Loss: 0.0002087558 Acc: 89.1377105713\n",
      "Best val acc: 89.137711\n",
      "----------\n",
      "training Loss: 0.0003793014 Acc: 80.2916412354\n",
      "validation Loss: 0.0002656635 Acc: 87.3161392212\n",
      "training Loss: 0.0002705137 Acc: 86.3832778931\n",
      "validation Loss: 0.0002369853 Acc: 87.6769180298\n",
      "training Loss: 0.0002601466 Acc: 86.2991104126\n",
      "validation Loss: 0.0002350128 Acc: 87.7649993896\n",
      "training Loss: 0.0002543624 Acc: 86.6095123291\n",
      "validation Loss: 0.0002295767 Acc: 86.1638336182\n",
      "training Loss: 0.0002512448 Acc: 86.7763290405\n",
      "validation Loss: 0.0002311074 Acc: 86.1891708374\n",
      "training Loss: 0.0002486291 Acc: 87.1286621094\n",
      "validation Loss: 0.0002227187 Acc: 86.6187210083\n",
      "training Loss: 0.0002467141 Acc: 87.0749740601\n",
      "validation Loss: 0.0002286525 Acc: 86.7840270996\n",
      "training Loss: 0.0002475827 Acc: 87.3015136719\n",
      "validation Loss: 0.0002216782 Acc: 87.2606353760\n",
      "training Loss: 0.0002460451 Acc: 87.3024215698\n",
      "validation Loss: 0.0002297296 Acc: 86.8166046143\n",
      "training Loss: 0.0002441240 Acc: 87.4022674561\n",
      "validation Loss: 0.0002246419 Acc: 87.1399765015\n",
      "training Loss: 0.0002437390 Acc: 87.4131240845\n",
      "validation Loss: 0.0002192422 Acc: 87.1399765015\n",
      "training Loss: 0.0002424425 Acc: 87.5235290527\n",
      "validation Loss: 0.0002222590 Acc: 88.2959060669\n",
      "training Loss: 0.0002438395 Acc: 87.5181045532\n",
      "validation Loss: 0.0002218991 Acc: 88.9317855835\n",
      "training Loss: 0.0002414817 Acc: 87.6535491943\n",
      "validation Loss: 0.0002198533 Acc: 88.5565338135\n",
      "training Loss: 0.0002418306 Acc: 87.4831085205\n",
      "validation Loss: 0.0002206524 Acc: 88.9607467651\n",
      "training Loss: 0.0002373493 Acc: 87.8921585083\n",
      "validation Loss: 0.0002160364 Acc: 89.1200180054\n",
      "training Loss: 0.0002363346 Acc: 87.8900451660\n",
      "validation Loss: 0.0002142360 Acc: 88.5070648193\n",
      "training Loss: 0.0002357866 Acc: 87.9458465576\n",
      "validation Loss: 0.0002131008 Acc: 89.0693435669\n",
      "training Loss: 0.0002347616 Acc: 87.9087448120\n",
      "validation Loss: 0.0002152928 Acc: 88.6675415039\n",
      "training Loss: 0.0002354286 Acc: 87.9564056396\n",
      "validation Loss: 0.0002163813 Acc: 88.7507934570\n",
      "training Loss: 0.0002355028 Acc: 87.8604812622\n",
      "validation Loss: 0.0002162005 Acc: 88.2741851807\n",
      "training Loss: 0.0002348141 Acc: 87.9859695435\n",
      "validation Loss: 0.0002140515 Acc: 88.5818710327\n",
      "training Loss: 0.0002315724 Acc: 88.0381546021\n",
      "validation Loss: 0.0002118708 Acc: 89.4071884155\n",
      "training Loss: 0.0002312094 Acc: 88.0948638916\n",
      "validation Loss: 0.0002112447 Acc: 89.3058319092\n",
      "training Loss: 0.0002303026 Acc: 88.2164382935\n",
      "validation Loss: 0.0002123832 Acc: 88.6928787231\n",
      "training Loss: 0.0002307250 Acc: 88.1111602783\n",
      "validation Loss: 0.0002099837 Acc: 89.3830566406\n",
      "training Loss: 0.0002301035 Acc: 88.1950149536\n",
      "validation Loss: 0.0002097568 Acc: 88.5022354126\n",
      "training Loss: 0.0002306314 Acc: 88.2146224976\n",
      "validation Loss: 0.0002086879 Acc: 88.3924331665\n",
      "training Loss: 0.0002298224 Acc: 88.3268432617\n",
      "validation Loss: 0.0002097446 Acc: 89.1719055176\n",
      "training Loss: 0.0002289782 Acc: 88.2396621704\n",
      "validation Loss: 0.0002087604 Acc: 87.7903366089\n",
      "Best val acc: 89.407188\n",
      "----------\n",
      "Average best_acc across k-fold: 89.07160949707031\n",
      "New configuration: {'hidden_layers': 2, 'initial_nodes': 436, 'dropout': 0.4315111602936024, 'gru_layers': 8, 'gru_size': 265, 'dropout_g': 0.6142027818058747, 'learning_rate': 0.007630106835260422, 'batch_size': 4001, 'L2_reg': 3.446459897310035e-05}\n",
      "training Loss: 0.0003673511 Acc: 75.7426223755\n",
      "validation Loss: 0.0002749972 Acc: 85.1668319702\n",
      "training Loss: 0.0002758023 Acc: 84.1399078369\n",
      "validation Loss: 0.0002620654 Acc: 83.9111938477\n",
      "training Loss: 0.0002597907 Acc: 85.5215301514\n",
      "validation Loss: 0.0002460964 Acc: 85.4991683960\n",
      "training Loss: 0.0002478163 Acc: 86.3650588989\n",
      "validation Loss: 0.0002329459 Acc: 86.7342605591\n",
      "training Loss: 0.0002426389 Acc: 86.8113021851\n",
      "validation Loss: 0.0002291142 Acc: 88.0237350464\n",
      "training Loss: 0.0002376581 Acc: 86.9693145752\n",
      "validation Loss: 0.0002373987 Acc: 88.6920318604\n",
      "training Loss: 0.0002369687 Acc: 87.1681137085\n",
      "validation Loss: 0.0002256017 Acc: 88.2098388672\n",
      "training Loss: 0.0002357317 Acc: 87.2064819336\n",
      "validation Loss: 0.0002237157 Acc: 88.5446014404\n",
      "training Loss: 0.0002331373 Acc: 87.3490829468\n",
      "validation Loss: 0.0002222173 Acc: 88.0188980103\n",
      "training Loss: 0.0002314993 Acc: 87.3385086060\n",
      "validation Loss: 0.0002238048 Acc: 88.4708786011\n",
      "training Loss: 0.0002316768 Acc: 87.4067916870\n",
      "validation Loss: 0.0002215718 Acc: 87.2515029907\n",
      "training Loss: 0.0002307519 Acc: 87.4463729858\n",
      "validation Loss: 0.0002251109 Acc: 88.0889892578\n",
      "training Loss: 0.0002312781 Acc: 87.4200820923\n",
      "validation Loss: 0.0002212369 Acc: 88.9083557129\n",
      "training Loss: 0.0002306420 Acc: 87.3862457275\n",
      "validation Loss: 0.0002207958 Acc: 88.0285644531\n",
      "training Loss: 0.0002305564 Acc: 87.4605712891\n",
      "validation Loss: 0.0002206768 Acc: 88.2533493042\n",
      "training Loss: 0.0002300855 Acc: 87.4847412109\n",
      "validation Loss: 0.0002280004 Acc: 87.9548492432\n"
     ]
    }
   ],
   "source": [
    "# CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# OUTPUT_DIRPATH = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME)\n",
    "# os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json')\n",
    "    best_conf = optimize_hyperparams_RR( # NEED TO FIX THIS FUNC STILL !!!\n",
    "        data_list_dict, data_hlf_dict, label_dict, {f'fold_{fold}': training_weights(data_aux_dict[f'fold_{fold}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold}']) for fold in range(len(data_list_dict))},\n",
    "        config_file, NUM_EPOCHS=30, SEED=SEED\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "    # with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    try:\n",
    "        with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "            best_conf = json.load(f)\n",
    "            print(best_conf)\n",
    "    except:\n",
    "        config_file = os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json')\n",
    "        with open(config_file, 'r') as f:\n",
    "            best_conf = json.load(f)\n",
    "\n",
    "fom = []\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx in range(len(data_hlf_dict)):\n",
    "    weight = training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}'])\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], weight,\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    model_file = OUTPUT_DIRPATH + '_ttH_Killer_IN_model_'+ f'{fold_idx}.torch'\n",
    "    state_file = OUTPUT_DIRPATH + '_ttH_Killer_IN_performance_'+ f'{fold_idx}.torch'\n",
    "    \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], \n",
    "        dnn_input=np.shape(data_hlf_dict[f'fold_{fold_idx}'])[-1], rnn_input=np.shape(data_list_dict[f'fold_{fold_idx}'])[-1],\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(train_data_list, train_data_hlf, train_label, train_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(val_data_list, val_data_hlf, val_label, val_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        NUM_EPOCHS, model, optimizer, scheduler, \n",
    "        state_file, model_file, data_loader=data_loader, \n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "# weight_test_dict = {\n",
    "#     f'fold_{fold_idx}': copy.deepcopy(training_weights(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_test_dict[f'fold_{fold_idx}'])) for fold_idx in range(len(data_test_aux_dict))\n",
    "# }  # DO NOT USE SCALED FOR TRAINING!!\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, \n",
    "        save=True, dict_lists=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "    NNtable.add_row(\n",
    "        [\n",
    "            IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "            \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "        ]\n",
    "    )\n",
    "print(NNtable)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = OUTPUT_DIRPATH + f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}/' + CURRENT_TIME\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [\n",
    "            (data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) \n",
    "            for i in range(len(data_test_aux_dict))\n",
    "        ]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "    print(plot_type)\n",
    "\n",
    "    weights_plot = {\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1) & mask_arr[fold_idx]],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0) & mask_arr[fold_idx]],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    density_weights_plot = {\n",
    "        fold_idx: {'sig': None, 'bkg': None} for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "\n",
    "    plot_train_val_losses(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME,\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    "    )\n",
    "    plot_roc(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], run2=False,\n",
    "        mask=None\n",
    "    )\n",
    "    # print(f\"num bkg: {np.sum(label_test==0)}\")\n",
    "    # print(f\"num sig: {np.sum(label_test==1)}\")\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, n_bins=25, \n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], n_bins=25, weights=density_weights_plot,\n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, n_bins=25,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    #### NEED TO ADD PLOTS THAT SHOW ROC, OUTPUT SCORE, AND S/âB ADDED ACROSS THE FOLDS FOR THE OVERALL/COMPLETE EVAL METRIC TO SHOW TO OTHERS ####\n",
    "\n",
    "    # for fold_idx in range(len(data_test_aux_dict)):\n",
    "    #     for score_cut in [0.2, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    #         plot_input_vars_after_score_cut(\n",
    "    #             IN_perf, score_cut, plot_destdir, fold_idx, method='round_robin', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #             mask=mask_arr[fold_idx]\n",
    "    #         )\n",
    "    #     plot_input_vars_after_score_cut(\n",
    "    #         IN_perf, [0.2, 0.6, 0.9], plot_destdir, fold_idx, method='round_robin', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #         mask=mask_arr[fold_idx]\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimized cut-boundaries for ttH score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(IN_perf, weights, method='arr', bins=50, mask=None, n_folds=5):\n",
    "    if method == 'round_robin':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "    elif method == 'arr':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "                    \n",
    "        sig_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "\n",
    "        cut_boundaries = []\n",
    "        cut_s_over_root_bs = []\n",
    "        prev_s_over_root_b = 0\n",
    "        sig_weights = []\n",
    "        bkg_weights = []\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                cut_boundaries.append(bins - i)\n",
    "                cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        cut_boundaries.append(0)\n",
    "        cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "        cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold, cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = OUTPUT_DIRPATH + f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}/' + CURRENT_TIME\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [(data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "\n",
    "    weights_plot = {\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1) & mask_arr[fold_idx]],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0) & mask_arr[fold_idx]],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    \n",
    "    (\n",
    "        cut_boundaries_fold, cut_s_over_root_bs_fold, \n",
    "        sig_weights_fold, bkg_weights_fold\n",
    "    ) = optimize_cut_boundaries(\n",
    "        IN_perf, weights_plot, method='round_robin',\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    fold_labels = [\n",
    "        [\n",
    "            f\"s/âb={cut_s_over_root_bs_fold[fold_idx][cut_idx]:.04f}, s={sig_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{sig_weights_fold[fold_idx][cut_idx]['w2']:.04f}, b={bkg_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{bkg_weights_fold[fold_idx][cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[fold_idx]))\n",
    "        ] for fold_idx in range(len(weight_test_dict))\n",
    "    ]\n",
    "    fold_colors = [copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries_fold[i]) // len(cmap_petroff10)) + 1)) for i in range(len(weight_test_dict))]\n",
    "    for fold_idx in range(len(weight_test_dict)):\n",
    "        s_over_root_b(\n",
    "            IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_{plot_type}_fold{fold_idx}', \n",
    "            labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, method='round_robin',\n",
    "            lines_fold=cut_boundaries_fold, lines_labels=fold_labels, only_fold=fold_idx, lines_colors=fold_colors,\n",
    "            mask=mask_arr\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for fold_idx in range(len(data_list_dict)):\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "        training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            train_data_list, train_data_hlf, train_label, train_weight,\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, only_fold_idx=fold_idx,\n",
    "            # dict_lists=True\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            val_data_list, val_data_hlf, val_label, val_weight,\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, only_fold_idx=fold_idx,\n",
    "            # dict_lists=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'r') as f:\n",
    "    IN_perf_dict = json.load(f)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold ']\n",
    "val_weights_arr = []\n",
    "for fold_idx, (train_IN_dict, val_IN_dict) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'])):\n",
    "    all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight,\n",
    "        train_indices, val_indices\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "        training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "        all_indices,\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    weights_plot = [\n",
    "        {\n",
    "            'sig': _weight[_label == 1],\n",
    "            'bkg': _weight[_label == 0],\n",
    "        } for _weight, _label in [(train_weight, train_label), (val_weight, val_label)]\n",
    "    ]\n",
    "    density_weights_plot = [\n",
    "        {'sig': None, 'bkg': None} for _ in ['train', 'val']\n",
    "    ]\n",
    "\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_weighted_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights_plot\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_density_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=density_weights_plot\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights_plot\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Vars Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict[\"fold_0\"]:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        train_mask = np.ones_like(all_indices, dtype=bool)\n",
    "        train_mask[val_indices] = np.zeros_like(val_indices, dtype=bool)\n",
    "        val_mask = np.logical_not(train_mask)\n",
    "\n",
    "        sig_mask = (label_dict[f'fold_{fold_idx}'] == 1)\n",
    "        sig_test_mask = (label_test_dict[f'fold_{fold_idx}'] == 1)\n",
    "        bkg_mask = (label_dict[f'fold_{fold_idx}'] == 0)\n",
    "        bkg_test_mask = (label_test_dict[f'fold_{fold_idx}'] == 0)\n",
    "        \n",
    "        sig_train_mask = train_mask & sig_mask\n",
    "        sig_val_mask = val_mask & sig_mask\n",
    "        bkg_train_mask = train_mask & bkg_mask\n",
    "        bkg_val_mask = val_mask & bkg_mask\n",
    "\n",
    "        sig_train_np = data_df_dict[f'fold_{fold_idx}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "        sig_val_np = data_df_dict[f'fold_{fold_idx}'].loc[sig_val_mask, var_name].to_numpy()\n",
    "        sig_test_np = data_test_df_dict[f'fold_{fold_idx}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "        bkg_train_np = data_df_dict[f'fold_{fold_idx}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "        bkg_val_np = data_df_dict[f'fold_{fold_idx}'].loc[bkg_val_mask, var_name].to_numpy()\n",
    "        bkg_test_np = data_test_df_dict[f'fold_{fold_idx}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    # sig_train_np = data_df.loc[sig_mask, var_name].to_numpy()\n",
    "    # sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "    # bkg_train_np = data_df.loc[bkg_mask, var_name].to_numpy()\n",
    "    # bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "    # sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    # sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    # bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    # bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    # pre_std_hists[var_name] = [\n",
    "    #     copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "    #     copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    # ]\n",
    "    # make_input_plot(output_dir_pre_std, var_name, pre_std_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### post-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        train_mask = np.ones_like(all_indices, dtype=bool)\n",
    "        train_mask[val_indices] = np.zeros_like(val_indices, dtype=bool)\n",
    "        val_mask = np.logical_not(train_mask)\n",
    "\n",
    "        data, data_test = None, None\n",
    "        if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "            data, data_test = data_list_dict[f'fold_{fold_idx}'], data_list_test_dict[f'fold_{fold_idx}']\n",
    "        else:\n",
    "            data, data_test = data_hlf_dict[f'fold_{fold_idx}'], data_hlf_test_dict[f'fold_{fold_idx}']\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, fold_idx, var_name,\n",
    "            train_index=train_mask, val_index=val_mask\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    # sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    # sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    # sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    # bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    # bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    # post_std_hists[var_name] = [\n",
    "    #     copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "    #     copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    # ]\n",
    "    # make_input_plot(output_dir_post_std, var_name, post_std_hists[var_name], labels=label_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian smearing on test set (for feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    mask_arr = data_list_index_map(var_name, particle_list_to_smear, np.ones(len(particle_list_to_smear), dtype=bool), n_pFields=N_PARTICLE_FIELDS)\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear = np.where(mask_arr, particle_list_to_smear*rng.normal(), particle_list_to_smear)\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear = np.where(mask_arr, particle_list_to_smear+rng.normal(), particle_list_to_smear)\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns_dict['fold_0'][var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate smeared variable test-data on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "\n",
    "    weight_test_dict = {\n",
    "        f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "\n",
    "    gauss_data_list_dict, gauss_data_hlf_dict = None, None\n",
    "    if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "        gauss_data_list_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_test_dict))\n",
    "        }\n",
    "        gauss_data_hlf_dict = data_hlf_test_dict\n",
    "    else:\n",
    "        gauss_data_list_dict = data_list_test_dict\n",
    "        gauss_data_hlf_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_test_dict))\n",
    "        }\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list_dict, gauss_data_hlf_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "IN_perf_smear_list = []\n",
    "label_arr = []\n",
    "for var_name, IN_perf_smear in IN_perf_smear_dict.items():\n",
    "    IN_perf_smear_list.append(IN_perf_smear)\n",
    "    label_arr.append(var_name)\n",
    "sort = np.argsort([IN_perf_smear['mean_area'] for IN_perf_smear in IN_perf_smear_list])\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_all', \n",
    "    method='IN_arr', labels=label_arr, yscale='log', run2=False, sort=sort\n",
    ")\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5', \n",
    "    method='IN_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False\n",
    ")\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5_and_orig', \n",
    "    method='IN_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False, run3=IN_perf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "\n",
    "    gauss_data_list_dict, gauss_data_hlf_dict = None, None\n",
    "    if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "        gauss_data_list_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_dict))\n",
    "        }\n",
    "        gauss_data_hlf_dict = data_hlf_dict\n",
    "\n",
    "        gauss_data_list_test_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_test_dict))\n",
    "        }\n",
    "        gauss_data_hlf_test_dict = data_hlf_test_dict\n",
    "    else:\n",
    "        gauss_data_list_dict = data_list_dict\n",
    "        gauss_data_hlf_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_dict))\n",
    "        }\n",
    "\n",
    "        gauss_data_list_test_dict = data_list_test_dict\n",
    "        gauss_data_hlf_test_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_test_dict))\n",
    "        }\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        train_mask = np.ones_like(all_indices, dtype=bool)\n",
    "        train_mask[val_indices] = np.zeros_like(val_indices, dtype=bool)\n",
    "        val_mask = np.logical_not(train_mask)\n",
    "\n",
    "        data, data_test = None, None\n",
    "        if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "            data, data_test = gauss_data_list_dict[f'fold_{fold_idx}'], gauss_data_list_test_dict[f'fold_{fold_idx}']\n",
    "        else:\n",
    "            data, data_test = gauss_data_hlf_dict[f'fold_{fold_idx}'], gauss_data_hlf_test_dict[f'fold_{fold_idx}']\n",
    "        \n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, fold_idx, var_name,\n",
    "            train_index=train_mask, val_index=val_mask\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_gauss_smear, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    # sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    # sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    # sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    # bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    # bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    # gauss_hists[var_name] = [\n",
    "    #     copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "    #     copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    # ]\n",
    "    # make_input_plot(output_dir_gauss_smear, var_name, gauss_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "\n",
    "weight_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_aux_dict))\n",
    "}\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y, w2 in [('train', data_list_dict, data_hlf_dict, label_dict, weight_dict), ('test', data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, w2,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'r') as f:\n",
    "    IN_full_eval_dict = json.load(f)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {fold_idx: {'mass': [], 'dijet_mass': []} for fold_idx in range(len(data_aux_dict))}\n",
    "for var_name in hist_dict[0].keys():\n",
    "    for fold_idx in range(len(data_aux_dict)):\n",
    "        for i, score_cut in enumerate(score_cuts):\n",
    "            sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold_idx)\n",
    "            sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "            sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "            bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "            bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "            hist_dict[fold_idx][var_name].extend(\n",
    "                [\n",
    "                    copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                    copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "                ]\n",
    "            )\n",
    "        for mod_factor, label_mod in enumerate(['sig_train', 'sig_test', 'bkg_train', 'bkg_test']):\n",
    "            plot_list = []\n",
    "            label_list = []\n",
    "            for i in range(len(hist_dict[fold_idx][var_name])):\n",
    "                if (i - mod_factor) % 4 == 0:\n",
    "                    plot_list.append(hist_dict[fold_idx][var_name][i])\n",
    "                    label_list.append(label_arr[i])\n",
    "            make_input_plot(\n",
    "                plot_destdir, var_name, plot_list, labels=label_list, density=True, \n",
    "                plot_prefix=CURRENT_TIME+'_', plot_postfix='_'+label_mod, alpha=0.5,\n",
    "                linestyle=False, fold_idx=fold_idx\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset size plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_dirs = glob.glob(OUTPUT_DIRPATH[:-1]+'_mod*') + [OUTPUT_DIRPATH[:-1]]\n",
    "final_train_losses_arr, final_val_losses_arr = [], []\n",
    "mod_values_arr = []\n",
    "\n",
    "for train_size_dir in train_size_dirs:\n",
    "    if len(glob.glob(train_size_dir + '/*IN_perf.json')) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        mod_values_arr.append([\n",
    "            float(\n",
    "                train_size_dir[\n",
    "                    train_size_dir.find('_mod')+4 : train_size_dir.find('-')\n",
    "                ]\n",
    "            ),\n",
    "            float(\n",
    "                train_size_dir[train_size_dir.find('-')+1:]\n",
    "            )\n",
    "        ])\n",
    "    except:\n",
    "         mod_values_arr.append([2, 2])\n",
    "    IN_perf_path = glob.glob(f'{train_size_dir}/*IN_perf.json')[0]\n",
    "    with open(IN_perf_path, 'r') as f:\n",
    "        IN_perf = json.load(f)\n",
    "    final_train_losses_arr.append([train_losses[-7 if len(train_losses) < NUM_EPOCHS else -1] for train_losses in IN_perf['train_losses_arr']])\n",
    "    final_val_losses_arr.append([val_losses[-7 if len(val_losses) < NUM_EPOCHS else -1] for val_losses in IN_perf['val_losses_arr']])\n",
    "\n",
    "final_train_losses_arr = np.array(final_train_losses_arr)\n",
    "final_val_losses_arr = np.array(final_val_losses_arr)\n",
    "mod_values_arr = np.array(mod_values_arr)\n",
    "dataset_sizes = (len(label) + len(label_test)) / mod_values_arr\n",
    "sorted_indices = np.argsort(dataset_sizes[:, 0])\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_train_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_train_losses_arr, axis=1)-np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_train_losses_arr, axis=1)+np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[0], alpha=0.5, label='Train data'\n",
    ")\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_val_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_val_losses_arr, axis=1)-np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_val_losses_arr, axis=1)+np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[1], alpha=0.5, label='Val data'\n",
    ")\n",
    "plt.xlabel('Size of train dataset')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.pdf')\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
