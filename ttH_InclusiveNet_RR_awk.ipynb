{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu3.fnal.gov      Thu Dec 12 17:46:23 2024  555.42.06\n",
      "[0] Tesla P100-PCIE-12GB | 41Â°C,   0 % |     2 / 12288 MB |\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams_RR\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "# V2_MERGED = True\n",
    "\n",
    "SIGNAL_FILEPATHS = [\n",
    "    # Test sig files #\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE/GluGluToHH/nominal/*',\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE_merged_v2/GluGluToHH/nominal/*',\n",
    "    # ggF HH # \n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\",\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GluGluToHH/nominal/*\",\n",
    "    # VBF HH #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [\n",
    "    # ttH (i.e. the main bkg to reduce) #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/ttHToGG/nominal/*\",\n",
    "    # # Other single H samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\",\n",
    "    # # Prompt-Prompt samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\",\n",
    "    # # Prompt-Fake samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\",\n",
    "    # Fake-Fake samples #\n",
    "    # ADD HERE #\n",
    "]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v11'\n",
    "CRITERION = \"NLLLoss\"\n",
    "N_PARTICLES, N_PARTICLE_FIELDS = 6, 7\n",
    "MOD_VALS = (5, 5)\n",
    "# VARS = 'base_vars'\n",
    "# VARS = 'extra_vars'\n",
    "# CURRENT_TIME = '2024-08-30_14-35-01'\n",
    "# VARS = 'extra_vars+'\n",
    "# CURRENT_TIME = '2024-10-09_20-47-24'\n",
    "VARS = 'extra_vars+max'\n",
    "# CURRENT_TIME = '2024-10-29_00-47-20'\n",
    "# CURRENT_TIME = '2024-10-21_12-09-38'\n",
    "# CURRENT_TIME = '2024-11-20_18-34-50'\n",
    "# VARS = 'extra_vars+max+opt_space'\n",
    "# CURRENT_TIME = '2024-10-24_20-29-37'\n",
    "# VARS = 'extra_vars+max+moved_vars_to_RNN'\n",
    "# CURRENT_TIME = '2024-10-21_00-35-33'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 6, 9\n",
    "# VARS = 'extra_vars_no_dijet_mass'\n",
    "# VARS = 'no_bad_vars'\n",
    "# VARS = 'simplified_bad_vars'\n",
    "# VARS = 'extra_vars_and_bools'\n",
    "# VARS = 'extra_vars_in_RNN'\n",
    "# VARS = f'extra_vars_mod{MOD_VALS[0]}-{MOD_VALS[1]}'\n",
    "# VARS = 'extra_vars_lead_lep_only'\n",
    "# CURRENT_TIME = '2024-10-02_18-03-26'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 3, 6\n",
    "# VARS = 'extra_vars_no_lep'\n",
    "# CURRENT_TIME = '2024-10-04_12-49-31'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 2, 6\n",
    "if 'CURRENT_TIME' in globals():\n",
    "    OUTPUT_DIRPATH = os.path.join(CURRENT_DIRPATH, f\"model_outputs/{VERSION}/{VARS}\", CURRENT_TIME)\n",
    "else:\n",
    "    OUTPUT_DIRPATH = os.path.join(CURRENT_DIRPATH, f\"model_outputs/{VERSION}/{VARS}\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413735, 6, 7)\n",
      "Data HLF: (413735, 15)\n",
      "n signal = 136530, n bkg = 277205\n",
      "Data list test: (103521, 6, 7)\n",
      "Data HLF test: (103521, 15)\n",
      "n signal = 34224, n bkg = 69297\n",
      "Data list: (413918, 6, 7)\n",
      "Data HLF: (413918, 15)\n",
      "n signal = 136466, n bkg = 277452\n",
      "Data list test: (103338, 6, 7)\n",
      "Data HLF test: (103338, 15)\n",
      "n signal = 34288, n bkg = 69050\n",
      "Data list: (413265, 6, 7)\n",
      "Data HLF: (413265, 15)\n",
      "n signal = 136638, n bkg = 276627\n",
      "Data list test: (103991, 6, 7)\n",
      "Data HLF test: (103991, 15)\n",
      "n signal = 34116, n bkg = 69875\n",
      "Data list: (413725, 6, 7)\n",
      "Data HLF: (413725, 15)\n",
      "n signal = 136671, n bkg = 277054\n",
      "Data list test: (103531, 6, 7)\n",
      "Data HLF test: (103531, 15)\n",
      "n signal = 34083, n bkg = 69448\n",
      "Data list: (414381, 6, 7)\n",
      "Data HLF: (414381, 15)\n",
      "n signal = 136711, n bkg = 277670\n",
      "Data list test: (102875, 6, 7)\n",
      "Data HLF test: (102875, 15)\n",
      "n signal = 34043, n bkg = 68832\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    data_df_dict, data_test_df_dict, \n",
    "    data_list_dict, data_hlf_dict, label_dict, \n",
    "    data_list_test_dict, data_hlf_test_dict, label_test_dict, \n",
    "    high_level_fields_dict, input_hlf_vars_dict, hlf_vars_columns_dict,\n",
    "    data_aux_dict, data_test_aux_dict\n",
    ") = process_data(\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, \n",
    "    seed=SEED, mod_vals=MOD_VALS, k_fold_test=True,\n",
    "    save_std=False if 'CURRENT_TIME' in globals() else True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_weights(event_weights, labels):\n",
    "    sum_of_bkg = np.sum(event_weights[labels==0])\n",
    "    sum_of_sig = np.sum(event_weights[labels==1])\n",
    "\n",
    "    sig_scale_factor = sum_of_bkg / sum_of_sig\n",
    "\n",
    "    weights = np.where(labels==0, event_weights, event_weights*sig_scale_factor)\n",
    "    mean_weight = np.mean(weights)\n",
    "    abs_weights = np.abs(weights)\n",
    "    scaled_weights = abs_weights / mean_weight\n",
    "\n",
    "    return scaled_weights\n",
    "\n",
    "def plot_train_val_losses(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None, n_folds=5\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dotted']\n",
    "        linestyles = linestyles * ((2*len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:2*len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx if fold_idx%2 == 0 else fold_idx+1]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx+1 if fold_idx%2 == 0 else fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='std', \n",
    "    labels=None, yscale='linear', run2=True, sort=None, run3=None,\n",
    "    mask=None, n_folds=5\n",
    "):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d, AUC = %.4f\" % (fold_idx, float(auc(IN_info['fprs'][fold_idx],IN_info['base_tpr']))), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "    elif method == 'round_robin_sum':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "\n",
    "        base_tpr = np.linspace(0, 1, 5000)\n",
    "        flat_preds = np.concatenate([\n",
    "            np.exp(IN_info['all_preds'][i])[mask[i]][:, 1] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "        flat_labels = np.concatenate([\n",
    "            np.array(IN_info['all_labels'][i])[mask[i]] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "\n",
    "        fpr, tpr, threshold = roc_curve(flat_labels, flat_preds)\n",
    "        fpr = np.interp(base_tpr, tpr, fpr)\n",
    "        threshold = np.interp(base_tpr, tpr, threshold)\n",
    "        fpr[0] = 0.0\n",
    "\n",
    "        all_auc = float(auc(fpr, base_tpr))\n",
    "\n",
    "        plt.plot(\n",
    "            fpr, base_tpr,\n",
    "            label=\"Run3 NN - sum over folds, AUC = %.4f\" % all_auc, linestyle='solid',\n",
    "            alpha=1\n",
    "        )\n",
    "    elif method == 'round_robin_sum_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for IN_idx in index_arr:\n",
    "            if mask is None:\n",
    "                mask = [np.ones(np.shape(IN_info[IN_idx]['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "\n",
    "            base_tpr = np.linspace(0, 1, 5000)\n",
    "            flat_preds = np.concatenate([\n",
    "                np.exp(IN_info[IN_idx]['all_preds'][i])[mask[i]][:, 1] for i in range(len(IN_info[IN_idx]['all_preds']))\n",
    "            ], axis=None)\n",
    "            flat_labels = np.concatenate([\n",
    "                np.array(IN_info[IN_idx]['all_labels'][i])[mask[i]] for i in range(len(IN_info[IN_idx]['all_preds']))\n",
    "            ], axis=None)\n",
    "\n",
    "            fpr, tpr, threshold = roc_curve(flat_labels, flat_preds)\n",
    "            fpr = np.interp(base_tpr, tpr, fpr)\n",
    "            threshold = np.interp(base_tpr, tpr, threshold)\n",
    "            fpr[0] = 0.0\n",
    "\n",
    "            all_auc = float(auc(fpr, base_tpr))\n",
    "\n",
    "            plt.plot(\n",
    "                fpr, base_tpr,\n",
    "                label=(labels[IN_idx]+', ' if labels is not None else '') + \"AUC = %.4f\" % all_auc, \n",
    "                linestyle=linestyles[IN_idx], alpha=0.8\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for i in index_arr:\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_fprs'])[0], dtype=bool)\n",
    "            plt.plot(\n",
    "                np.array(IN_info[i]['mean_fprs'])[mask], np.array(IN_info[i]['base_tpr'])[mask], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i], alpha=0.5\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if run3 is not None:\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(run3['mean_fprs'])[0], dtype=bool)\n",
    "        plt.plot(\n",
    "            np.array(run3['mean_fprs'])[mask], np.array(run3['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (run3['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, n_bins=50, all_sig=False, all_bkg=False,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        # for cut in np.linspace(0, 1, 10, endpoint=False):\n",
    "        #     print(f\"output score > {cut:.2f}\")\n",
    "        #     print('='*60)\n",
    "        #     print(f\"num sig > {cut:.2f} = {len(sig_np[sig_np > cut])}\")\n",
    "        #     print(f\"num bkg > {cut:.2f} = {len(bkg_np[bkg_np > cut])}\")\n",
    "        #     print('-'*60)\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'] if weights[fold_idx]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'] if weights[fold_idx]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[fold_idx]['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights[fold_idx]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "    elif method == 'round_robin_sum':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "\n",
    "        flat_preds = np.concatenate([\n",
    "            np.exp(IN_info['all_preds'][i])[mask[i]][:, 1] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "        flat_labels = np.concatenate([\n",
    "            np.array(IN_info['all_labels'][i])[mask[i]] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "        sig_preds = flat_preds[flat_labels == 1]\n",
    "        bkg_preds = flat_preds[flat_labels == 0]\n",
    "\n",
    "        if weights[0]['sig'] is not None:\n",
    "            sig_weight = np.concatenate([\n",
    "                weight['sig'] for weight in weights.values()\n",
    "            ], axis=None)\n",
    "            bkg_weight = np.concatenate([\n",
    "                weight['bkg'] for weight in weights.values()\n",
    "            ], axis=None)\n",
    "            full_weights = {'sig': sig_weight, 'bkg': bkg_weight}\n",
    "        else:\n",
    "            full_weights = {'sig': None, 'bkg': None}\n",
    "\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_preds, weight=full_weights['sig'] if full_weights['sig'] is not None else np.ones_like(sig_preds))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_preds, weight=full_weights['bkg'] if full_weights['bkg'] is not None else np.ones_like(bkg_preds))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if full_weights['sig'] is not None else False),\n",
    "            alpha=0.5, density=(False if full_weights['sig'] is not None else True), histtype='step',\n",
    "            label=[\n",
    "                f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal - sum over folds', \n",
    "                f'{\"ttH\" if not all_bkg else \"all\"} background - sum over folds'\n",
    "            ], linestyle=['solid', 'solid']\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=1, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=[\n",
    "                f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal', \n",
    "                f'{\"ttH\" if not all_bkg else \"all\"} background'\n",
    "            ]\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'] if weights[i]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'] if weights[i]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[i]['sig'] is not None else False),\n",
    "                alpha=0.7, density=(False if weights[i]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[i] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[i], linestyles[i]]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.ylabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def s_over_root_b(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, lines_fold=None, lines=None, lines_labels=None, \n",
    "        lines_colors=None, only_fold=None, no_fold=False, n_bins=50,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/âb', alpha=0.8 \n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "    elif method == 'round_robin_sum':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "\n",
    "        flat_preds = np.concatenate([\n",
    "            np.exp(IN_info['all_preds'][i])[mask[i]][:, 1] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "        flat_labels = np.concatenate([\n",
    "            np.array(IN_info['all_labels'][i])[mask[i]] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "        sig_preds = flat_preds[flat_labels == 1]\n",
    "        bkg_preds = flat_preds[flat_labels == 0]\n",
    "        \n",
    "        sig_weight = np.concatenate([\n",
    "            weight['sig'] for weight in weights.values()\n",
    "        ], axis=None)\n",
    "        bkg_weight = np.concatenate([\n",
    "            weight['bkg'] for weight in weights.values()\n",
    "        ], axis=None)\n",
    "        full_weights = {'sig': sig_weight, 'bkg': bkg_weight}\n",
    "\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_preds, weight=full_weights['sig'] if full_weights['sig'] is not None else np.ones_like(sig_preds))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_preds, weight=full_weights['bkg'] if full_weights['bkg'] is not None else np.ones_like(bkg_preds))\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/âb - sum over folds', \n",
    "            alpha=0.5, linestyle='solid', \n",
    "        )\n",
    "        if lines is not None:\n",
    "            for i in range(len(lines)):\n",
    "                plt.vlines(\n",
    "                    lines[i], 0, np.max(s_over_root_b_points), \n",
    "                    label='s/âb'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                    alpha=0.5, colors=lines_colors[i]\n",
    "                )\n",
    "    elif method == 'arr':\n",
    "        if mask is None or np.all(mask):\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "        if only_fold is None:\n",
    "            sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb - avg. over folds', \n",
    "                alpha=0.5, \n",
    "            )\n",
    "            if lines is not None:\n",
    "                for i in range(len(lines)):\n",
    "                    plt.vlines(\n",
    "                        lines[i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb - avg.'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[i]\n",
    "                    )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'])\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "                label='s/âb'+(' - '+labels[i] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[i], \n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.ylabel(\"s/âb\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 150., 2000, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    # 'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # 'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Integer(0, 10, name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($\\Sigma E_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t0}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t1}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma$ ln($p_{T}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'ln($M_{jj}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, fold, var_name, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label_dict[f'fold_{fold}'] == 1\n",
    "    sig_test_mask = label_test_dict[f'fold_{fold}'] == 1\n",
    "    bkg_mask = label_dict[f'fold_{fold}'] == 0\n",
    "    bkg_test_mask = label_test_dict[f'fold_{fold}'] == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_train_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            # index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold):\n",
    "    sig_train_mask = (label_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux_dict[f'fold_{fold}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux_dict[f'fold_{fold}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux_dict[f'fold_{fold}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux_dict[f'fold_{fold}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(\n",
    "    output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, \n",
    "    plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True\n",
    "):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir_ = os.path.join(output_dir, \"fold\")\n",
    "        if not os.path.exists(output_dir_):\n",
    "            os.makedirs(output_dir_)\n",
    "        plt.savefig(f'{output_dir_}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir_}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_input_vars_after_score_cut(\n",
    "    IN_info, score_cut, destdir, fold, plot_prefix, plot_postfix='', method='std', \n",
    "    weights={'sig': None, 'bkg': None}, all_sig=False, all_bkg=False,\n",
    "    mask=None\n",
    "):\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "        bkg_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            sig_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=sig_var.loc[sig_mask], \n",
    "                weight=weights['sig'][sig_mask] if weights['sig'] is not None else np.ones(np.sum(sig_mask))\n",
    "            )\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            bkg_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=bkg_var.loc[bkg_mask], \n",
    "                weight=weights['bkg'][bkg_mask] if weights['bkg'] is not None else np.ones(np.sum(bkg_mask))\n",
    "            )\n",
    "            make_input_plot(\n",
    "                destdir, var_name, [sig_hist, bkg_hist], fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore{score_cut}', labels=['HH signal', 'ttH background'], density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['all_preds'][fold])[0], dtype=bool)\n",
    "        \n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut if score_cut is list else [score_cut]:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['all_preds'][fold]\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['all_labels'][fold]) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['all_preds'][fold]\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['all_labels'][fold]) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used method 'std'. You used {method}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0002870304 Acc: 82.1223144531\n",
      "validation Loss: 0.0002377816 Acc: 85.8460083008\n",
      "training Loss: 0.0002321645 Acc: 86.7146224976\n",
      "validation Loss: 0.0002279729 Acc: 85.6139755249\n",
      "training Loss: 0.0002269668 Acc: 87.1566314697\n",
      "validation Loss: 0.0002242643 Acc: 88.2400512695\n",
      "training Loss: 0.0002239582 Acc: 87.3146438599\n",
      "validation Loss: 0.0002207683 Acc: 88.4599990845\n",
      "training Loss: 0.0002214513 Acc: 87.4986419678\n",
      "validation Loss: 0.0002158271 Acc: 87.8702545166\n",
      "training Loss: 0.0002194790 Acc: 87.7315750122\n",
      "validation Loss: 0.0002164409 Acc: 87.2889633179\n",
      "training Loss: 0.0002171299 Acc: 87.8062057495\n",
      "validation Loss: 0.0002131615 Acc: 88.8866043091\n",
      "training Loss: 0.0002152171 Acc: 87.8817367554\n",
      "validation Loss: 0.0002183025 Acc: 89.1319274902\n",
      "training Loss: 0.0002154183 Acc: 87.9071121216\n",
      "validation Loss: 0.0002112638 Acc: 87.6140518188\n",
      "training Loss: 0.0002139954 Acc: 87.9197998047\n",
      "validation Loss: 0.0002099540 Acc: 88.3874893188\n",
      "training Loss: 0.0002136199 Acc: 88.0566635132\n",
      "validation Loss: 0.0002109984 Acc: 87.7167739868\n",
      "training Loss: 0.0002128037 Acc: 88.0621032715\n",
      "validation Loss: 0.0002091319 Acc: 88.8177185059\n",
      "training Loss: 0.0002109875 Acc: 88.2077255249\n",
      "validation Loss: 0.0002101315 Acc: 88.8189315796\n",
      "training Loss: 0.0002109880 Acc: 88.2158813477\n",
      "validation Loss: 0.0002098887 Acc: 86.6279144287\n",
      "training Loss: 0.0002107924 Acc: 88.2228317261\n",
      "validation Loss: 0.0002064634 Acc: 88.1361236572\n",
      "training Loss: 0.0002106580 Acc: 88.2566757202\n",
      "validation Loss: 0.0002063689 Acc: 89.4388885498\n",
      "training Loss: 0.0002087286 Acc: 88.3811492920\n",
      "validation Loss: 0.0002084965 Acc: 87.6249237061\n",
      "training Loss: 0.0002090702 Acc: 88.3031997681\n",
      "validation Loss: 0.0002065924 Acc: 87.0545120239\n",
      "training Loss: 0.0002086532 Acc: 88.2874908447\n",
      "validation Loss: 0.0002045399 Acc: 88.2908096313\n",
      "training Loss: 0.0002083808 Acc: 88.3533554077\n",
      "validation Loss: 0.0002057421 Acc: 88.6896209717\n",
      "training Loss: 0.0002079864 Acc: 88.3908157349\n",
      "validation Loss: 0.0002068761 Acc: 88.9276962280\n",
      "training Loss: 0.0002081960 Acc: 88.3340148926\n",
      "validation Loss: 0.0002034423 Acc: 88.8636398315\n",
      "training Loss: 0.0002080080 Acc: 88.4065246582\n",
      "validation Loss: 0.0002017406 Acc: 88.6134796143\n",
      "training Loss: 0.0002079730 Acc: 88.4168014526\n",
      "validation Loss: 0.0002045015 Acc: 87.5282440186\n",
      "training Loss: 0.0002072141 Acc: 88.4479141235\n",
      "validation Loss: 0.0002027543 Acc: 87.9947280884\n",
      "training Loss: 0.0002077806 Acc: 88.3820571899\n",
      "validation Loss: 0.0002008130 Acc: 88.7826690674\n",
      "training Loss: 0.0002059600 Acc: 88.4593963623\n",
      "validation Loss: 0.0001994550 Acc: 88.8745193481\n",
      "training Loss: 0.0002061912 Acc: 88.4566802979\n",
      "validation Loss: 0.0002018130 Acc: 88.1228332520\n",
      "training Loss: 0.0002058685 Acc: 88.4597015381\n",
      "validation Loss: 0.0002025390 Acc: 88.8225555420\n",
      "training Loss: 0.0002064055 Acc: 88.4569778442\n",
      "validation Loss: 0.0002022391 Acc: 89.2987060547\n",
      "training Loss: 0.0002069538 Acc: 88.4905166626\n",
      "validation Loss: 0.0002010913 Acc: 89.3180389404\n",
      "training Loss: 0.0002018679 Acc: 88.7548751831\n",
      "validation Loss: 0.0001980159 Acc: 89.0086593628\n",
      "training Loss: 0.0002003334 Acc: 88.7727050781\n",
      "validation Loss: 0.0001981555 Acc: 88.0926208496\n",
      "training Loss: 0.0002005873 Acc: 88.7995910645\n",
      "validation Loss: 0.0001975707 Acc: 89.6092910767\n",
      "training Loss: 0.0002010394 Acc: 88.7684707642\n",
      "validation Loss: 0.0001968351 Acc: 89.1234741211\n",
      "training Loss: 0.0002010868 Acc: 88.7325210571\n",
      "validation Loss: 0.0001984635 Acc: 89.2455291748\n",
      "training Loss: 0.0001998969 Acc: 88.8098678589\n",
      "validation Loss: 0.0001973522 Acc: 89.0751342773\n",
      "training Loss: 0.0002008017 Acc: 88.7364501953\n",
      "validation Loss: 0.0001974427 Acc: 88.6026077271\n",
      "training Loss: 0.0002002819 Acc: 88.7808609009\n",
      "validation Loss: 0.0001976663 Acc: 88.4587936401\n",
      "training Loss: 0.0001975246 Acc: 88.9255752563\n",
      "validation Loss: 0.0001948792 Acc: 88.9216537476\n",
      "training Loss: 0.0001969902 Acc: 88.9730148315\n",
      "validation Loss: 0.0001952940 Acc: 88.9711990356\n",
      "training Loss: 0.0001968951 Acc: 88.9751281738\n",
      "validation Loss: 0.0001945542 Acc: 89.2068557739\n",
      "training Loss: 0.0001966422 Acc: 88.8905334473\n",
      "validation Loss: 0.0001954616 Acc: 88.9035263062\n",
      "training Loss: 0.0001960332 Acc: 88.9696884155\n",
      "validation Loss: 0.0001954826 Acc: 89.4171371460\n",
      "training Loss: 0.0001964050 Acc: 88.9681777954\n",
      "validation Loss: 0.0001947740 Acc: 89.2020263672\n",
      "training Loss: 0.0001962141 Acc: 89.0080566406\n",
      "validation Loss: 0.0001945657 Acc: 88.6243591309\n",
      "training Loss: 0.0001948312 Acc: 89.1074600220\n",
      "validation Loss: 0.0001929717 Acc: 88.9240646362\n",
      "training Loss: 0.0001943709 Acc: 89.1183319092\n",
      "validation Loss: 0.0001934526 Acc: 88.9470291138\n",
      "training Loss: 0.0001941399 Acc: 89.0654602051\n",
      "validation Loss: 0.0001938214 Acc: 88.9844894409\n",
      "training Loss: 0.0001941870 Acc: 89.0772476196\n",
      "validation Loss: 0.0001924189 Acc: 89.1283035278\n",
      "training Loss: 0.0001939537 Acc: 89.1289062500\n",
      "validation Loss: 0.0001933283 Acc: 88.8998947144\n",
      "training Loss: 0.0001941961 Acc: 89.0340423584\n",
      "validation Loss: 0.0001933766 Acc: 89.0666732788\n",
      "training Loss: 0.0001935916 Acc: 89.0754318237\n",
      "validation Loss: 0.0001928340 Acc: 88.9107742310\n",
      "training Loss: 0.0001941323 Acc: 89.0977935791\n",
      "validation Loss: 0.0001926108 Acc: 88.9482345581\n",
      "training Loss: 0.0001925970 Acc: 89.1536865234\n",
      "validation Loss: 0.0001925487 Acc: 89.0461273193\n",
      "training Loss: 0.0001925061 Acc: 89.1699981689\n",
      "validation Loss: 0.0001927127 Acc: 88.9192352295\n",
      "training Loss: 0.0001929523 Acc: 89.1225662231\n",
      "validation Loss: 0.0001924366 Acc: 89.2962875366\n",
      "Early stopped.\n",
      "Best val acc: 89.609291\n",
      "----------\n",
      "training Loss: 0.0002931257 Acc: 81.9124603271\n",
      "validation Loss: 0.0002413511 Acc: 84.7422180176\n",
      "training Loss: 0.0002323263 Acc: 86.7011566162\n",
      "validation Loss: 0.0002287232 Acc: 87.0156021118\n",
      "training Loss: 0.0002263317 Acc: 87.2196807861\n",
      "validation Loss: 0.0002302594 Acc: 87.0747909546\n",
      "training Loss: 0.0002235955 Acc: 87.4615707397\n",
      "validation Loss: 0.0002221480 Acc: 88.3165817261\n",
      "training Loss: 0.0002216001 Acc: 87.6297836304\n",
      "validation Loss: 0.0002199022 Acc: 87.2632369995\n",
      "training Loss: 0.0002196579 Acc: 87.7475585938\n",
      "validation Loss: 0.0002195260 Acc: 87.9614410400\n",
      "training Loss: 0.0002187850 Acc: 87.6614913940\n",
      "validation Loss: 0.0002214889 Acc: 88.6644744873\n",
      "training Loss: 0.0002178989 Acc: 87.7327651978\n",
      "validation Loss: 0.0002190085 Acc: 86.9648666382\n",
      "training Loss: 0.0002165667 Acc: 87.7774581909\n",
      "validation Loss: 0.0002157119 Acc: 87.8116531372\n",
      "training Loss: 0.0002162883 Acc: 87.8689651489\n",
      "validation Loss: 0.0002182687 Acc: 87.9904327393\n",
      "training Loss: 0.0002157865 Acc: 87.8620147705\n",
      "validation Loss: 0.0002130030 Acc: 88.4820709229\n",
      "training Loss: 0.0002150415 Acc: 87.8888931274\n",
      "validation Loss: 0.0002149153 Acc: 87.5845565796\n",
      "training Loss: 0.0002132626 Acc: 88.0580062866\n",
      "validation Loss: 0.0002153080 Acc: 88.9374694824\n",
      "training Loss: 0.0002134485 Acc: 88.0815658569\n",
      "validation Loss: 0.0002127512 Acc: 87.3610839844\n",
      "training Loss: 0.0002132301 Acc: 88.1410598755\n",
      "validation Loss: 0.0002150246 Acc: 87.2729034424\n",
      "training Loss: 0.0002128711 Acc: 88.1407546997\n",
      "validation Loss: 0.0002106085 Acc: 88.6258163452\n",
      "training Loss: 0.0002114091 Acc: 88.1730651855\n",
      "validation Loss: 0.0002092888 Acc: 88.4772415161\n",
      "training Loss: 0.0002110688 Acc: 88.1936035156\n",
      "validation Loss: 0.0002097310 Acc: 88.2803421021\n",
      "training Loss: 0.0002113210 Acc: 88.2056808472\n",
      "validation Loss: 0.0002115903 Acc: 87.6449508667\n",
      "training Loss: 0.0002096872 Acc: 88.3065490723\n",
      "validation Loss: 0.0002083980 Acc: 88.6258163452\n",
      "training Loss: 0.0002097345 Acc: 88.2231979370\n",
      "validation Loss: 0.0002087256 Acc: 88.2779235840\n",
      "training Loss: 0.0002097759 Acc: 88.3545684814\n",
      "validation Loss: 0.0002091925 Acc: 87.4528884888\n",
      "training Loss: 0.0002089681 Acc: 88.3708724976\n",
      "validation Loss: 0.0002088436 Acc: 87.4106063843\n",
      "training Loss: 0.0002094223 Acc: 88.3663406372\n",
      "validation Loss: 0.0002063589 Acc: 88.8311691284\n",
      "training Loss: 0.0002090787 Acc: 88.3464126587\n",
      "validation Loss: 0.0002091518 Acc: 88.0013046265\n",
      "training Loss: 0.0002083695 Acc: 88.3880844116\n",
      "validation Loss: 0.0002111030 Acc: 89.4085769653\n",
      "training Loss: 0.0002086934 Acc: 88.3935241699\n",
      "validation Loss: 0.0002064538 Acc: 88.2948379517\n",
      "training Loss: 0.0002080502 Acc: 88.4022827148\n",
      "validation Loss: 0.0002075296 Acc: 88.1051864624\n",
      "training Loss: 0.0002043003 Acc: 88.6299819946\n",
      "validation Loss: 0.0002056647 Acc: 89.3167724609\n",
      "training Loss: 0.0002036633 Acc: 88.6118621826\n",
      "validation Loss: 0.0002043497 Acc: 88.9217681885\n",
      "training Loss: 0.0002023730 Acc: 88.7389984131\n",
      "validation Loss: 0.0002029106 Acc: 89.1730270386\n",
      "training Loss: 0.0002028783 Acc: 88.6598815918\n",
      "validation Loss: 0.0002063117 Acc: 89.7238540649\n",
      "training Loss: 0.0002030486 Acc: 88.6819229126\n",
      "validation Loss: 0.0002050050 Acc: 89.4472351074\n",
      "training Loss: 0.0002022984 Acc: 88.6924972534\n",
      "validation Loss: 0.0002023484 Acc: 89.2648315430\n",
      "training Loss: 0.0002022273 Acc: 88.6879653931\n",
      "validation Loss: 0.0002036435 Acc: 89.4122009277\n",
      "training Loss: 0.0002020684 Acc: 88.7245101929\n",
      "validation Loss: 0.0002055063 Acc: 89.4423980713\n",
      "training Loss: 0.0002020492 Acc: 88.7338714600\n",
      "validation Loss: 0.0002042367 Acc: 88.9797515869\n",
      "training Loss: 0.0002015523 Acc: 88.7220916748\n",
      "validation Loss: 0.0002032868 Acc: 89.1017532349\n",
      "training Loss: 0.0001993158 Acc: 88.8492279053\n",
      "validation Loss: 0.0001993172 Acc: 88.8275451660\n",
      "training Loss: 0.0001985806 Acc: 88.9020767212\n",
      "validation Loss: 0.0001998011 Acc: 89.0111618042\n",
      "training Loss: 0.0001980870 Acc: 88.9087219238\n",
      "validation Loss: 0.0001996542 Acc: 88.8432540894\n",
      "training Loss: 0.0001985913 Acc: 88.9174804688\n",
      "validation Loss: 0.0001996222 Acc: 88.9108963013\n",
      "training Loss: 0.0001981094 Acc: 88.9413375854\n",
      "validation Loss: 0.0002009742 Acc: 89.4617309570\n",
      "training Loss: 0.0001965596 Acc: 89.0307235718\n",
      "validation Loss: 0.0001989315 Acc: 89.1452407837\n",
      "training Loss: 0.0001963727 Acc: 88.9377136230\n",
      "validation Loss: 0.0001983151 Acc: 89.1971817017\n",
      "training Loss: 0.0001961899 Acc: 89.0518646240\n",
      "validation Loss: 0.0001994024 Acc: 89.2938232422\n",
      "training Loss: 0.0001962389 Acc: 89.0597152710\n",
      "validation Loss: 0.0001984248 Acc: 89.0763854980\n",
      "training Loss: 0.0001960815 Acc: 89.0730056763\n",
      "validation Loss: 0.0001994622 Acc: 89.1452407837\n",
      "training Loss: 0.0001958549 Acc: 89.0382766724\n",
      "validation Loss: 0.0001983639 Acc: 88.9072723389\n",
      "training Loss: 0.0001948334 Acc: 89.0908203125\n",
      "validation Loss: 0.0001982387 Acc: 89.2575836182\n",
      "training Loss: 0.0001949406 Acc: 89.0878067017\n",
      "validation Loss: 0.0001984029 Acc: 89.4218673706\n",
      "training Loss: 0.0001947088 Acc: 89.1379318237\n",
      "validation Loss: 0.0001981007 Acc: 89.4387741089\n",
      "training Loss: 0.0001946783 Acc: 89.1155853271\n",
      "validation Loss: 0.0001975053 Acc: 89.0957183838\n",
      "training Loss: 0.0001944124 Acc: 89.1053161621\n",
      "validation Loss: 0.0001975611 Acc: 89.2938232422\n",
      "training Loss: 0.0001941269 Acc: 89.0675735474\n",
      "validation Loss: 0.0001976268 Acc: 89.3288497925\n",
      "training Loss: 0.0001943935 Acc: 89.1385345459\n",
      "validation Loss: 0.0001978309 Acc: 89.1766510010\n",
      "training Loss: 0.0001940780 Acc: 89.1077346802\n",
      "validation Loss: 0.0001981295 Acc: 89.0401458740\n",
      "training Loss: 0.0001933481 Acc: 89.1714553833\n",
      "validation Loss: 0.0001978638 Acc: 89.1911468506\n",
      "training Loss: 0.0001937872 Acc: 89.1469955444\n",
      "validation Loss: 0.0001973444 Acc: 89.0872573853\n",
      "training Loss: 0.0001938893 Acc: 89.1472930908\n",
      "validation Loss: 0.0001976818 Acc: 89.1935577393\n",
      "training Loss: 0.0001941717 Acc: 89.1361236572\n",
      "validation Loss: 0.0001972603 Acc: 89.2467117310\n",
      "training Loss: 0.0001939049 Acc: 89.1690368652\n",
      "validation Loss: 0.0001977157 Acc: 89.2732849121\n",
      "training Loss: 0.0001937910 Acc: 89.1557540894\n",
      "validation Loss: 0.0001973636 Acc: 89.3071060181\n",
      "training Loss: 0.0001934906 Acc: 89.1149826050\n",
      "validation Loss: 0.0001974691 Acc: 89.2950286865\n",
      "training Loss: 0.0001935506 Acc: 89.1599807739\n",
      "validation Loss: 0.0001974980 Acc: 89.3324737549\n",
      "training Loss: 0.0001933789 Acc: 89.1448822021\n",
      "validation Loss: 0.0001972822 Acc: 89.2056427002\n",
      "training Loss: 0.0001936241 Acc: 89.1318969727\n",
      "validation Loss: 0.0001970146 Acc: 89.2104721069\n",
      "training Loss: 0.0001935155 Acc: 89.1451797485\n",
      "validation Loss: 0.0001969650 Acc: 89.1694030762\n",
      "training Loss: 0.0001930709 Acc: 89.1654129028\n",
      "validation Loss: 0.0001974865 Acc: 89.2877807617\n",
      "training Loss: 0.0001928005 Acc: 89.1430664062\n",
      "validation Loss: 0.0001972225 Acc: 89.2201385498\n",
      "training Loss: 0.0001930062 Acc: 89.1793060303\n",
      "validation Loss: 0.0001972807 Acc: 89.2104721069\n",
      "training Loss: 0.0001935052 Acc: 89.1762847900\n",
      "validation Loss: 0.0001969623 Acc: 89.1947708130\n",
      "training Loss: 0.0001929004 Acc: 89.2125244141\n",
      "validation Loss: 0.0001970506 Acc: 89.1355819702\n",
      "training Loss: 0.0001929294 Acc: 89.2040710449\n",
      "validation Loss: 0.0001969926 Acc: 89.1887283325\n",
      "training Loss: 0.0001926995 Acc: 89.1645126343\n",
      "validation Loss: 0.0001970207 Acc: 89.2080535889\n",
      "training Loss: 0.0001927873 Acc: 89.1566543579\n",
      "validation Loss: 0.0001969884 Acc: 89.2600021362\n",
      "training Loss: 0.0001928145 Acc: 89.1877593994\n",
      "validation Loss: 0.0001973850 Acc: 89.1996002197\n",
      "training Loss: 0.0001927992 Acc: 89.1288757324\n",
      "validation Loss: 0.0001968812 Acc: 89.2491302490\n",
      "training Loss: 0.0001926726 Acc: 89.1533355713\n",
      "validation Loss: 0.0001970973 Acc: 89.2213439941\n",
      "training Loss: 0.0001930004 Acc: 89.1811218262\n",
      "validation Loss: 0.0001969666 Acc: 89.2237625122\n",
      "training Loss: 0.0001929309 Acc: 89.1454849243\n",
      "validation Loss: 0.0001968464 Acc: 89.2080535889\n",
      "training Loss: 0.0001928037 Acc: 89.1947097778\n",
      "validation Loss: 0.0001967567 Acc: 89.2213439941\n",
      "training Loss: 0.0001923374 Acc: 89.2095031738\n",
      "validation Loss: 0.0001973303 Acc: 89.2068481445\n",
      "training Loss: 0.0001932367 Acc: 89.1578674316\n",
      "validation Loss: 0.0001973322 Acc: 89.2237625122\n",
      "training Loss: 0.0001929206 Acc: 89.1741714478\n",
      "validation Loss: 0.0001969698 Acc: 89.2358398438\n",
      "training Loss: 0.0001927451 Acc: 89.1563568115\n",
      "validation Loss: 0.0001970468 Acc: 89.2527542114\n",
      "training Loss: 0.0001928497 Acc: 89.1947097778\n",
      "validation Loss: 0.0001967948 Acc: 89.2297973633\n",
      "training Loss: 0.0001929780 Acc: 89.1817245483\n",
      "validation Loss: 0.0001967462 Acc: 89.2189254761\n",
      "training Loss: 0.0001927905 Acc: 89.1651153564\n",
      "validation Loss: 0.0001971375 Acc: 89.2769088745\n",
      "training Loss: 0.0001924700 Acc: 89.1983337402\n",
      "validation Loss: 0.0001966130 Acc: 89.2430877686\n",
      "training Loss: 0.0001927005 Acc: 89.1512222290\n",
      "validation Loss: 0.0001969525 Acc: 89.2467117310\n",
      "training Loss: 0.0001931363 Acc: 89.1696472168\n",
      "validation Loss: 0.0001971612 Acc: 89.2455062866\n",
      "training Loss: 0.0001927216 Acc: 89.2007522583\n",
      "validation Loss: 0.0001973226 Acc: 89.2165145874\n",
      "training Loss: 0.0001927563 Acc: 89.1901779175\n",
      "validation Loss: 0.0001970462 Acc: 89.1814804077\n",
      "training Loss: 0.0001924758 Acc: 89.1892700195\n",
      "validation Loss: 0.0001969312 Acc: 89.2201385498\n",
      "training Loss: 0.0001926164 Acc: 89.1777954102\n",
      "validation Loss: 0.0001970412 Acc: 89.2189254761\n",
      "training Loss: 0.0001926146 Acc: 89.1983337402\n",
      "validation Loss: 0.0001969291 Acc: 89.2153015137\n",
      "Early stopped.\n",
      "Best val acc: 89.723854\n",
      "----------\n",
      "training Loss: 0.0002827686 Acc: 82.7928161621\n",
      "validation Loss: 0.0002347709 Acc: 87.6531982422\n",
      "training Loss: 0.0002301158 Acc: 86.8949127197\n",
      "validation Loss: 0.0002298053 Acc: 85.7936172485\n",
      "training Loss: 0.0002239448 Acc: 87.2463760376\n",
      "validation Loss: 0.0002214236 Acc: 86.4941406250\n",
      "training Loss: 0.0002209460 Acc: 87.4590148926\n",
      "validation Loss: 0.0002183117 Acc: 88.1589279175\n",
      "training Loss: 0.0002193562 Acc: 87.6368637085\n",
      "validation Loss: 0.0002159558 Acc: 87.3265304565\n",
      "training Loss: 0.0002174025 Acc: 87.7448501587\n",
      "validation Loss: 0.0002139071 Acc: 88.2859649658\n",
      "training Loss: 0.0002164203 Acc: 87.7738876343\n",
      "validation Loss: 0.0002179140 Acc: 87.8806533813\n",
      "training Loss: 0.0002157286 Acc: 87.7182312012\n",
      "validation Loss: 0.0002125359 Acc: 88.2472534180\n",
      "training Loss: 0.0002150743 Acc: 87.8482894897\n",
      "validation Loss: 0.0002129125 Acc: 88.5630264282\n",
      "training Loss: 0.0002144771 Acc: 87.9154434204\n",
      "validation Loss: 0.0002112669 Acc: 87.3035430908\n",
      "training Loss: 0.0002134032 Acc: 87.9889373779\n",
      "validation Loss: 0.0002087478 Acc: 87.9205856323\n",
      "training Loss: 0.0002132484 Acc: 87.9602050781\n",
      "validation Loss: 0.0002166445 Acc: 87.1740875244\n",
      "training Loss: 0.0002126426 Acc: 88.1235427856\n",
      "validation Loss: 0.0002176309 Acc: 85.3483810425\n",
      "training Loss: 0.0002111535 Acc: 88.1301956177\n",
      "validation Loss: 0.0002091494 Acc: 89.0723876953\n",
      "training Loss: 0.0002101710 Acc: 88.1894760132\n",
      "validation Loss: 0.0002052800 Acc: 88.4432525635\n",
      "training Loss: 0.0002110345 Acc: 88.1637649536\n",
      "validation Loss: 0.0002064598 Acc: 88.8836441040\n",
      "training Loss: 0.0002090298 Acc: 88.2886886597\n",
      "validation Loss: 0.0002081246 Acc: 89.1631240845\n",
      "training Loss: 0.0002090957 Acc: 88.2569274902\n",
      "validation Loss: 0.0002058063 Acc: 87.9496231079\n",
      "training Loss: 0.0002081107 Acc: 88.3201446533\n",
      "validation Loss: 0.0002046025 Acc: 88.8292007446\n",
      "training Loss: 0.0002088182 Acc: 88.3007888794\n",
      "validation Loss: 0.0002058512 Acc: 89.0832748413\n",
      "training Loss: 0.0002086019 Acc: 88.3328475952\n",
      "validation Loss: 0.0002061360 Acc: 88.7711257935\n",
      "training Loss: 0.0002084993 Acc: 88.3836669922\n",
      "validation Loss: 0.0002063543 Acc: 87.7620925903\n",
      "training Loss: 0.0002070429 Acc: 88.4423446655\n",
      "validation Loss: 0.0002053257 Acc: 88.1807098389\n",
      "training Loss: 0.0002033928 Acc: 88.5418548584\n",
      "validation Loss: 0.0002009248 Acc: 88.7009506226\n",
      "training Loss: 0.0002024099 Acc: 88.5727081299\n",
      "validation Loss: 0.0002007387 Acc: 88.9852752686\n",
      "training Loss: 0.0002022297 Acc: 88.6162643433\n",
      "validation Loss: 0.0002025835 Acc: 89.1449813843\n",
      "training Loss: 0.0002020266 Acc: 88.6949081421\n",
      "validation Loss: 0.0002000053 Acc: 88.9090499878\n",
      "training Loss: 0.0002018780 Acc: 88.6377410889\n",
      "validation Loss: 0.0002014722 Acc: 88.7469253540\n",
      "training Loss: 0.0002024285 Acc: 88.6380386353\n",
      "validation Loss: 0.0002029439 Acc: 88.9259872437\n",
      "training Loss: 0.0002023174 Acc: 88.6162643433\n",
      "validation Loss: 0.0001983574 Acc: 88.7783889771\n",
      "training Loss: 0.0002017184 Acc: 88.7063980103\n",
      "validation Loss: 0.0002019383 Acc: 87.4741363525\n",
      "training Loss: 0.0002018016 Acc: 88.6691970825\n",
      "validation Loss: 0.0001985981 Acc: 89.2478179932\n",
      "training Loss: 0.0002013151 Acc: 88.7030715942\n",
      "validation Loss: 0.0002010152 Acc: 89.5333480835\n",
      "training Loss: 0.0002019339 Acc: 88.7266616821\n",
      "validation Loss: 0.0001984044 Acc: 88.4226837158\n",
      "training Loss: 0.0001994634 Acc: 88.7593307495\n",
      "validation Loss: 0.0001980635 Acc: 89.3567047119\n",
      "training Loss: 0.0001980174 Acc: 88.8927230835\n",
      "validation Loss: 0.0001972707 Acc: 89.4341430664\n",
      "training Loss: 0.0001978247 Acc: 88.8461380005\n",
      "validation Loss: 0.0001962922 Acc: 88.6936950684\n",
      "training Loss: 0.0001974847 Acc: 88.9287109375\n",
      "validation Loss: 0.0001975552 Acc: 89.2163619995\n",
      "training Loss: 0.0001972700 Acc: 88.8724517822\n",
      "validation Loss: 0.0001958620 Acc: 88.8074264526\n",
      "training Loss: 0.0001972803 Acc: 88.9417190552\n",
      "validation Loss: 0.0001962604 Acc: 88.3621902466\n",
      "training Loss: 0.0001968717 Acc: 88.8939285278\n",
      "validation Loss: 0.0001965607 Acc: 89.0457687378\n",
      "training Loss: 0.0001970830 Acc: 88.9214553833\n",
      "validation Loss: 0.0001960027 Acc: 88.8993759155\n",
      "training Loss: 0.0001970133 Acc: 88.8594512939\n",
      "validation Loss: 0.0001960866 Acc: 89.0215682983\n",
      "training Loss: 0.0001949706 Acc: 89.0267105103\n",
      "validation Loss: 0.0001948166 Acc: 89.2417678833\n",
      "training Loss: 0.0001952809 Acc: 89.0415344238\n",
      "validation Loss: 0.0001944948 Acc: 89.2332992554\n",
      "training Loss: 0.0001949341 Acc: 89.0158233643\n",
      "validation Loss: 0.0001945520 Acc: 88.8497695923\n",
      "training Loss: 0.0001945655 Acc: 89.0385131836\n",
      "validation Loss: 0.0001943376 Acc: 88.7747573853\n",
      "training Loss: 0.0001943178 Acc: 89.0227813721\n",
      "validation Loss: 0.0001941877 Acc: 89.2175750732\n",
      "training Loss: 0.0001942583 Acc: 89.0714797974\n",
      "validation Loss: 0.0001950127 Acc: 89.2042617798\n",
      "training Loss: 0.0001943460 Acc: 89.0757141113\n",
      "validation Loss: 0.0001955769 Acc: 89.4522857666\n",
      "training Loss: 0.0001944159 Acc: 89.0460739136\n",
      "validation Loss: 0.0001937221 Acc: 89.1679687500\n",
      "training Loss: 0.0001942854 Acc: 89.0503082275\n",
      "validation Loss: 0.0001942544 Acc: 89.3446121216\n",
      "training Loss: 0.0001936249 Acc: 89.1201782227\n",
      "validation Loss: 0.0001941769 Acc: 89.2720184326\n",
      "training Loss: 0.0001941795 Acc: 89.0373001099\n",
      "validation Loss: 0.0001967988 Acc: 89.8019409180\n",
      "training Loss: 0.0001940927 Acc: 89.0748062134\n",
      "validation Loss: 0.0001958668 Acc: 89.6507110596\n",
      "training Loss: 0.0001928575 Acc: 89.1098937988\n",
      "validation Loss: 0.0001940306 Acc: 89.1328811646\n",
      "training Loss: 0.0001925465 Acc: 89.1428604126\n",
      "validation Loss: 0.0001942089 Acc: 89.3191986084\n",
      "training Loss: 0.0001925663 Acc: 89.1313705444\n",
      "validation Loss: 0.0001939068 Acc: 89.3615493774\n",
      "Early stopped.\n",
      "Best val acc: 89.801941\n",
      "----------\n",
      "training Loss: 0.0002825725 Acc: 82.7164764404\n",
      "validation Loss: 0.0002369801 Acc: 85.9254302979\n",
      "training Loss: 0.0002315220 Acc: 86.8572082520\n",
      "validation Loss: 0.0002254357 Acc: 87.0735397339\n",
      "training Loss: 0.0002270747 Acc: 87.2143325806\n",
      "validation Loss: 0.0002227495 Acc: 86.7484436035\n",
      "training Loss: 0.0002224675 Acc: 87.4554290771\n",
      "validation Loss: 0.0002169307 Acc: 88.1805496216\n",
      "training Loss: 0.0002203339 Acc: 87.6185836792\n",
      "validation Loss: 0.0002209008 Acc: 86.5115661621\n",
      "training Loss: 0.0002205343 Acc: 87.6128463745\n",
      "validation Loss: 0.0002202423 Acc: 87.9763107300\n",
      "training Loss: 0.0002175176 Acc: 87.7844543457\n",
      "validation Loss: 0.0002124973 Acc: 87.5919952393\n",
      "training Loss: 0.0002166738 Acc: 87.7645111084\n",
      "validation Loss: 0.0002127677 Acc: 88.5769500732\n",
      "training Loss: 0.0002147899 Acc: 87.9152755737\n",
      "validation Loss: 0.0002118782 Acc: 88.0971603394\n",
      "training Loss: 0.0002152603 Acc: 87.9373321533\n",
      "validation Loss: 0.0002162413 Acc: 88.9769744873\n",
      "training Loss: 0.0002145158 Acc: 87.9672470093\n",
      "validation Loss: 0.0002102712 Acc: 89.0784912109\n",
      "training Loss: 0.0002139313 Acc: 88.0735931396\n",
      "validation Loss: 0.0002139471 Acc: 88.9540100098\n",
      "training Loss: 0.0002119285 Acc: 88.0950469971\n",
      "validation Loss: 0.0002083813 Acc: 88.2832794189\n",
      "training Loss: 0.0002122522 Acc: 88.1467132568\n",
      "validation Loss: 0.0002144532 Acc: 89.5280685425\n",
      "training Loss: 0.0002112238 Acc: 88.1430892944\n",
      "validation Loss: 0.0002139547 Acc: 89.2936096191\n",
      "training Loss: 0.0002116049 Acc: 88.1584930420\n",
      "validation Loss: 0.0002154133 Acc: 89.6199111938\n",
      "training Loss: 0.0002097906 Acc: 88.2666625977\n",
      "validation Loss: 0.0002058130 Acc: 88.5781555176\n",
      "training Loss: 0.0002100485 Acc: 88.2545776367\n",
      "validation Loss: 0.0002119062 Acc: 89.2525177002\n",
      "training Loss: 0.0002092533 Acc: 88.3509521484\n",
      "validation Loss: 0.0002058973 Acc: 89.0337753296\n",
      "training Loss: 0.0002099280 Acc: 88.2754211426\n",
      "validation Loss: 0.0002054814 Acc: 88.1998901367\n",
      "training Loss: 0.0002089258 Acc: 88.2666625977\n",
      "validation Loss: 0.0002067301 Acc: 89.0712432861\n",
      "training Loss: 0.0002084182 Acc: 88.4041290283\n",
      "validation Loss: 0.0002026330 Acc: 88.6953887939\n",
      "training Loss: 0.0002082024 Acc: 88.3319168091\n",
      "validation Loss: 0.0002121236 Acc: 89.3431625366\n",
      "training Loss: 0.0002089706 Acc: 88.3415908813\n",
      "validation Loss: 0.0002076049 Acc: 88.9322586060\n",
      "training Loss: 0.0002083059 Acc: 88.3727111816\n",
      "validation Loss: 0.0002056333 Acc: 89.4084167480\n",
      "training Loss: 0.0002075610 Acc: 88.4107742310\n",
      "validation Loss: 0.0002059827 Acc: 89.1087036133\n",
      "training Loss: 0.0002039215 Acc: 88.5249862671\n",
      "validation Loss: 0.0002026483 Acc: 89.5171890259\n",
      "training Loss: 0.0002022090 Acc: 88.7325515747\n",
      "validation Loss: 0.0001992074 Acc: 88.3872070312\n",
      "training Loss: 0.0002023071 Acc: 88.6458358765\n",
      "validation Loss: 0.0002024301 Acc: 89.2464752197\n",
      "training Loss: 0.0002022317 Acc: 88.6615447998\n",
      "validation Loss: 0.0002012589 Acc: 89.6537551880\n",
      "training Loss: 0.0002017282 Acc: 88.6156234741\n",
      "validation Loss: 0.0001997342 Acc: 88.6627578735\n",
      "training Loss: 0.0002020125 Acc: 88.6382827759\n",
      "validation Loss: 0.0002018984 Acc: 89.3189926147\n",
      "training Loss: 0.0001991673 Acc: 88.8917694092\n",
      "validation Loss: 0.0001979136 Acc: 89.4652175903\n",
      "training Loss: 0.0001987431 Acc: 88.8216781616\n",
      "validation Loss: 0.0001970645 Acc: 89.2174758911\n",
      "training Loss: 0.0001984246 Acc: 88.8648834229\n",
      "validation Loss: 0.0001967421 Acc: 89.2041778564\n",
      "training Loss: 0.0001982494 Acc: 88.9002304077\n",
      "validation Loss: 0.0001967154 Acc: 89.1220016479\n",
      "training Loss: 0.0001982833 Acc: 88.8778762817\n",
      "validation Loss: 0.0001970737 Acc: 88.9890594482\n",
      "training Loss: 0.0001979214 Acc: 88.8443374634\n",
      "validation Loss: 0.0001983944 Acc: 89.5244369507\n",
      "training Loss: 0.0001981139 Acc: 88.8929824829\n",
      "validation Loss: 0.0001966202 Acc: 88.5588226318\n",
      "training Loss: 0.0001978173 Acc: 88.8911666870\n",
      "validation Loss: 0.0001967613 Acc: 88.7292251587\n",
      "training Loss: 0.0001977787 Acc: 88.8950958252\n",
      "validation Loss: 0.0001976562 Acc: 89.2827301025\n",
      "training Loss: 0.0001976743 Acc: 88.9192657471\n",
      "validation Loss: 0.0001985852 Acc: 89.2102203369\n",
      "training Loss: 0.0001973609 Acc: 88.8222808838\n",
      "validation Loss: 0.0001977412 Acc: 89.5546493530\n",
      "training Loss: 0.0001960050 Acc: 88.9561233521\n",
      "validation Loss: 0.0001964158 Acc: 89.3987503052\n",
      "training Loss: 0.0001957961 Acc: 89.0250167847\n",
      "validation Loss: 0.0001964370 Acc: 89.4144592285\n",
      "training Loss: 0.0001949545 Acc: 88.9833221436\n",
      "validation Loss: 0.0001964204 Acc: 89.4205093384\n",
      "training Loss: 0.0001950425 Acc: 89.0283355713\n",
      "validation Loss: 0.0001951009 Acc: 89.4072113037\n",
      "training Loss: 0.0001951825 Acc: 88.9884567261\n",
      "validation Loss: 0.0001957069 Acc: 89.2658157349\n",
      "training Loss: 0.0001948809 Acc: 89.0407257080\n",
      "validation Loss: 0.0001948609 Acc: 89.3504104614\n",
      "training Loss: 0.0001947337 Acc: 89.0364913940\n",
      "validation Loss: 0.0001954932 Acc: 89.3479919434\n",
      "training Loss: 0.0001943668 Acc: 89.0304489136\n",
      "validation Loss: 0.0001953606 Acc: 89.3117370605\n",
      "training Loss: 0.0001944347 Acc: 89.0419311523\n",
      "validation Loss: 0.0001956521 Acc: 89.4712677002\n",
      "training Loss: 0.0001948109 Acc: 88.9981231689\n",
      "validation Loss: 0.0001962010 Acc: 89.4374237061\n",
      "training Loss: 0.0001936447 Acc: 89.0809097290\n",
      "validation Loss: 0.0001954576 Acc: 89.4507217407\n",
      "training Loss: 0.0001938444 Acc: 89.1274337769\n",
      "validation Loss: 0.0001945243 Acc: 89.1425399780\n",
      "training Loss: 0.0001934736 Acc: 89.0748672485\n",
      "validation Loss: 0.0001940102 Acc: 89.1860504150\n",
      "training Loss: 0.0001933824 Acc: 89.1026611328\n",
      "validation Loss: 0.0001945897 Acc: 89.4156723022\n",
      "training Loss: 0.0001931229 Acc: 89.0721435547\n",
      "validation Loss: 0.0001938426 Acc: 89.2670211792\n",
      "training Loss: 0.0001933481 Acc: 89.1059875488\n",
      "validation Loss: 0.0001938451 Acc: 89.3238220215\n",
      "training Loss: 0.0001929709 Acc: 89.1026611328\n",
      "validation Loss: 0.0001938544 Acc: 89.3117370605\n",
      "training Loss: 0.0001930925 Acc: 89.1636962891\n",
      "validation Loss: 0.0001944656 Acc: 89.3794174194\n",
      "training Loss: 0.0001932788 Acc: 89.1156539917\n",
      "validation Loss: 0.0001946095 Acc: 89.4664306641\n",
      "training Loss: 0.0001924005 Acc: 89.1446609497\n",
      "validation Loss: 0.0001939276 Acc: 89.3685379028\n",
      "training Loss: 0.0001926667 Acc: 89.0969238281\n",
      "validation Loss: 0.0001940766 Acc: 89.3661193848\n",
      "training Loss: 0.0001920214 Acc: 89.1742706299\n",
      "validation Loss: 0.0001943302 Acc: 89.2440567017\n",
      "Early stopped.\n",
      "Best val acc: 89.653755\n",
      "----------\n",
      "training Loss: 0.0002901597 Acc: 82.5166549683\n",
      "validation Loss: 0.0002375069 Acc: 86.2543258667\n",
      "training Loss: 0.0002310847 Acc: 87.1618499756\n",
      "validation Loss: 0.0002253754 Acc: 86.6537094116\n",
      "training Loss: 0.0002240090 Acc: 87.4813003540\n",
      "validation Loss: 0.0002199841 Acc: 87.8108520508\n",
      "training Loss: 0.0002213823 Acc: 87.5914077759\n",
      "validation Loss: 0.0002196086 Acc: 87.4090499878\n",
      "training Loss: 0.0002194157 Acc: 87.6809997559\n",
      "validation Loss: 0.0002154804 Acc: 87.4838562012\n",
      "training Loss: 0.0002188512 Acc: 87.6782836914\n",
      "validation Loss: 0.0002151074 Acc: 88.0606155396\n",
      "training Loss: 0.0002162694 Acc: 87.7380065918\n",
      "validation Loss: 0.0002124409 Acc: 88.2572937012\n",
      "training Loss: 0.0002153285 Acc: 87.9054260254\n",
      "validation Loss: 0.0002118173 Acc: 87.5272979736\n",
      "training Loss: 0.0002140180 Acc: 87.9123687744\n",
      "validation Loss: 0.0002093557 Acc: 87.5791778564\n",
      "training Loss: 0.0002139744 Acc: 88.0082931519\n",
      "validation Loss: 0.0002159485 Acc: 85.9997329712\n",
      "training Loss: 0.0002126115 Acc: 87.9923019409\n",
      "validation Loss: 0.0002068094 Acc: 88.1474914551\n",
      "training Loss: 0.0002114408 Acc: 88.0704345703\n",
      "validation Loss: 0.0002077796 Acc: 88.4901657104\n",
      "training Loss: 0.0002113031 Acc: 88.1802368164\n",
      "validation Loss: 0.0002095760 Acc: 87.9001388550\n",
      "training Loss: 0.0002094137 Acc: 88.1980361938\n",
      "validation Loss: 0.0002101487 Acc: 89.3806457520\n",
      "training Loss: 0.0002096082 Acc: 88.2206573486\n",
      "validation Loss: 0.0002152894 Acc: 86.4365234375\n",
      "training Loss: 0.0002057197 Acc: 88.4046707153\n",
      "validation Loss: 0.0002005550 Acc: 88.9112777710\n",
      "training Loss: 0.0002043130 Acc: 88.4544372559\n",
      "validation Loss: 0.0002013983 Acc: 88.5010299683\n",
      "training Loss: 0.0002042585 Acc: 88.5304565430\n",
      "validation Loss: 0.0001997559 Acc: 88.5263671875\n",
      "training Loss: 0.0002034174 Acc: 88.5446395874\n",
      "validation Loss: 0.0001990978 Acc: 89.1815567017\n",
      "training Loss: 0.0002029510 Acc: 88.5886764526\n",
      "validation Loss: 0.0001990083 Acc: 88.9016189575\n",
      "training Loss: 0.0002026172 Acc: 88.6001434326\n",
      "validation Loss: 0.0002015020 Acc: 88.2922821045\n",
      "training Loss: 0.0002025074 Acc: 88.6245727539\n",
      "validation Loss: 0.0002001773 Acc: 88.6373748779\n",
      "training Loss: 0.0002022927 Acc: 88.6649932861\n",
      "validation Loss: 0.0001994419 Acc: 88.3815765381\n",
      "training Loss: 0.0002026703 Acc: 88.5180892944\n",
      "validation Loss: 0.0002010193 Acc: 88.6651306152\n",
      "training Loss: 0.0001999784 Acc: 88.7754058838\n",
      "validation Loss: 0.0001961643 Acc: 89.2672271729\n",
      "training Loss: 0.0001997671 Acc: 88.7500610352\n",
      "validation Loss: 0.0001974825 Acc: 88.3345184326\n",
      "training Loss: 0.0001988724 Acc: 88.7666549683\n",
      "validation Loss: 0.0001962586 Acc: 88.8099212646\n",
      "training Loss: 0.0001984660 Acc: 88.8007431030\n",
      "validation Loss: 0.0001956928 Acc: 88.8509445190\n",
      "training Loss: 0.0001980502 Acc: 88.8245697021\n",
      "validation Loss: 0.0001959234 Acc: 89.3565139771\n",
      "training Loss: 0.0001982743 Acc: 88.8746490479\n",
      "validation Loss: 0.0001979535 Acc: 88.9860839844\n",
      "training Loss: 0.0001978579 Acc: 88.8435745239\n",
      "validation Loss: 0.0001958973 Acc: 88.7532119751\n",
      "training Loss: 0.0001971249 Acc: 88.8517227173\n",
      "validation Loss: 0.0001958808 Acc: 88.8533554077\n",
      "training Loss: 0.0001963932 Acc: 88.9404067993\n",
      "validation Loss: 0.0001939084 Acc: 89.3287582397\n",
      "training Loss: 0.0001957917 Acc: 89.0224609375\n",
      "validation Loss: 0.0001937556 Acc: 89.0379714966\n",
      "training Loss: 0.0001952577 Acc: 88.9645385742\n",
      "validation Loss: 0.0001934889 Acc: 88.8871459961\n",
      "training Loss: 0.0001958619 Acc: 88.9778137207\n",
      "validation Loss: 0.0001940954 Acc: 89.2141342163\n",
      "training Loss: 0.0001953737 Acc: 89.0634841919\n",
      "validation Loss: 0.0001934092 Acc: 89.0934753418\n",
      "training Loss: 0.0001949307 Acc: 89.0348281860\n",
      "validation Loss: 0.0001935339 Acc: 89.1067428589\n",
      "training Loss: 0.0001952473 Acc: 88.9965133667\n",
      "validation Loss: 0.0001947154 Acc: 88.8485336304\n",
      "training Loss: 0.0001948834 Acc: 89.0109939575\n",
      "validation Loss: 0.0001937657 Acc: 88.8690414429\n",
      "training Loss: 0.0001945273 Acc: 89.0212478638\n",
      "validation Loss: 0.0001929346 Acc: 89.1598358154\n",
      "training Loss: 0.0001945940 Acc: 89.0318069458\n",
      "validation Loss: 0.0001934815 Acc: 89.1658706665\n",
      "training Loss: 0.0001946020 Acc: 89.0429687500\n",
      "validation Loss: 0.0001929070 Acc: 88.8437042236\n",
      "training Loss: 0.0001943682 Acc: 89.0468902588\n",
      "validation Loss: 0.0001932610 Acc: 88.7713088989\n",
      "training Loss: 0.0001938423 Acc: 89.0695190430\n",
      "validation Loss: 0.0001935143 Acc: 89.1948242188\n",
      "training Loss: 0.0001939494 Acc: 89.0830917358\n",
      "validation Loss: 0.0001928116 Acc: 88.9872894287\n",
      "training Loss: 0.0001942047 Acc: 89.0824890137\n",
      "validation Loss: 0.0001933336 Acc: 89.2020645142\n",
      "training Loss: 0.0001936198 Acc: 89.0468902588\n",
      "validation Loss: 0.0001928800 Acc: 88.8895568848\n",
      "training Loss: 0.0001939222 Acc: 89.1054153442\n",
      "validation Loss: 0.0001932172 Acc: 89.3384170532\n",
      "training Loss: 0.0001939610 Acc: 89.0562438965\n",
      "validation Loss: 0.0001931365 Acc: 89.3963317871\n",
      "training Loss: 0.0001931280 Acc: 89.1244201660\n",
      "validation Loss: 0.0001924236 Acc: 89.1477737427\n",
      "training Loss: 0.0001925047 Acc: 89.1250228882\n",
      "validation Loss: 0.0001922672 Acc: 89.1936187744\n",
      "training Loss: 0.0001921992 Acc: 89.1934967041\n",
      "validation Loss: 0.0001921681 Acc: 89.4204635620\n",
      "training Loss: 0.0001924092 Acc: 89.1880645752\n",
      "validation Loss: 0.0001921536 Acc: 89.4349441528\n",
      "training Loss: 0.0001922169 Acc: 89.1551895142\n",
      "validation Loss: 0.0001923056 Acc: 89.2792892456\n",
      "training Loss: 0.0001921486 Acc: 89.1376876831\n",
      "validation Loss: 0.0001921478 Acc: 89.1851730347\n",
      "training Loss: 0.0001924284 Acc: 89.1772079468\n",
      "validation Loss: 0.0001919254 Acc: 89.3154907227\n",
      "training Loss: 0.0001920556 Acc: 89.1280364990\n",
      "validation Loss: 0.0001919190 Acc: 89.0958862305\n",
      "training Loss: 0.0001924461 Acc: 89.1747970581\n",
      "validation Loss: 0.0001921266 Acc: 89.0234909058\n",
      "training Loss: 0.0001921289 Acc: 89.1763000488\n",
      "validation Loss: 0.0001917872 Acc: 89.0826110840\n",
      "training Loss: 0.0001922269 Acc: 89.1479492188\n",
      "validation Loss: 0.0001914951 Acc: 89.3504791260\n",
      "training Loss: 0.0001916030 Acc: 89.1995315552\n",
      "validation Loss: 0.0001916676 Acc: 89.3649597168\n",
      "training Loss: 0.0001916266 Acc: 89.1913833618\n",
      "validation Loss: 0.0001914788 Acc: 89.3939208984\n",
      "training Loss: 0.0001917732 Acc: 89.1551895142\n",
      "validation Loss: 0.0001912943 Acc: 89.1996536255\n",
      "training Loss: 0.0001911684 Acc: 89.1983261108\n",
      "validation Loss: 0.0001915391 Acc: 89.2913589478\n",
      "training Loss: 0.0001919327 Acc: 89.1343688965\n",
      "validation Loss: 0.0001916202 Acc: 89.2068939209\n",
      "training Loss: 0.0001919238 Acc: 89.1603164673\n",
      "validation Loss: 0.0001917525 Acc: 89.3915023804\n",
      "training Loss: 0.0001917560 Acc: 89.1959075928\n",
      "validation Loss: 0.0001917236 Acc: 89.1863784790\n",
      "training Loss: 0.0001910464 Acc: 89.2140121460\n",
      "validation Loss: 0.0001909684 Acc: 89.1151885986\n",
      "training Loss: 0.0001905915 Acc: 89.1944046021\n",
      "validation Loss: 0.0001910786 Acc: 89.0500335693\n",
      "training Loss: 0.0001906498 Acc: 89.2173309326\n",
      "validation Loss: 0.0001911108 Acc: 89.1996536255\n",
      "training Loss: 0.0001903269 Acc: 89.2631759644\n",
      "validation Loss: 0.0001912933 Acc: 89.2117233276\n",
      "training Loss: 0.0001904464 Acc: 89.2118988037\n",
      "validation Loss: 0.0001906758 Acc: 89.1043319702\n",
      "training Loss: 0.0001904571 Acc: 89.2640838623\n",
      "validation Loss: 0.0001908916 Acc: 89.3130722046\n",
      "training Loss: 0.0001905754 Acc: 89.2194366455\n",
      "validation Loss: 0.0001908038 Acc: 89.2249908447\n",
      "training Loss: 0.0001905081 Acc: 89.2426681519\n",
      "validation Loss: 0.0001909105 Acc: 89.3070449829\n",
      "training Loss: 0.0001907518 Acc: 89.2206497192\n",
      "validation Loss: 0.0001910901 Acc: 89.2129287720\n",
      "training Loss: 0.0001901940 Acc: 89.2348251343\n",
      "validation Loss: 0.0001910594 Acc: 89.2165451050\n",
      "training Loss: 0.0001898849 Acc: 89.2616729736\n",
      "validation Loss: 0.0001905214 Acc: 89.2889404297\n",
      "training Loss: 0.0001899393 Acc: 89.2613677979\n",
      "validation Loss: 0.0001908126 Acc: 89.2527465820\n",
      "training Loss: 0.0001898125 Acc: 89.2121963501\n",
      "validation Loss: 0.0001905417 Acc: 89.2467117310\n",
      "training Loss: 0.0001901187 Acc: 89.2664947510\n",
      "validation Loss: 0.0001910398 Acc: 89.2346420288\n",
      "training Loss: 0.0001901084 Acc: 89.2372360229\n",
      "validation Loss: 0.0001907618 Acc: 89.3287582397\n",
      "training Loss: 0.0001898156 Acc: 89.3017883301\n",
      "validation Loss: 0.0001909183 Acc: 89.2829132080\n",
      "training Loss: 0.0001896543 Acc: 89.3075256348\n",
      "validation Loss: 0.0001905895 Acc: 89.4108123779\n",
      "training Loss: 0.0001892425 Acc: 89.3108444214\n",
      "validation Loss: 0.0001907134 Acc: 89.2804946899\n",
      "Early stopped.\n",
      "Best val acc: 89.434944\n",
      "----------\n",
      "Average best_acc across k-fold: 89.6447525024414\n"
     ]
    }
   ],
   "source": [
    "if 'CURRENT_TIME' in globals():\n",
    "    OUTPUT_DIRPATH, OLD_TIME = os.path.split(OUTPUT_DIRPATH)\n",
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "OUTPUT_DIRPATH = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME)\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json')\n",
    "    best_conf = optimize_hyperparams_RR(\n",
    "        data_list_dict, data_hlf_dict, label_dict, {f'fold_{fold}': training_weights(data_aux_dict[f'fold_{fold}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold}']) for fold in range(len(data_list_dict))},\n",
    "        config_file, NUM_EPOCHS=30, SEED=SEED\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "    # with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    # config_file = os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json')\n",
    "    # with open(config_file, 'r') as f:\n",
    "        best_conf = json.load(f)\n",
    "        print(best_conf)\n",
    "\n",
    "fom = []\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx in range(len(data_hlf_dict)):\n",
    "    weight = training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}'])\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], weight,\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    model_file = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME + '_ttH_Killer_IN_model_'+ f'{fold_idx}.torch')\n",
    "    # state_file = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME + '_ttH_Killer_IN_performance_'+ f'{fold_idx}.torch')\n",
    "    \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], \n",
    "        dnn_input=np.shape(data_hlf_dict[f'fold_{fold_idx}'])[-1], rnn_input=np.shape(data_list_dict[f'fold_{fold_idx}'])[-1],\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(train_data_list, train_data_hlf, train_label, train_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(val_data_list, val_data_hlf, val_label, val_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        NUM_EPOCHS, model, optimizer, scheduler, \n",
    "        model_filename=model_file, data_loader=data_loader, \n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "# with open(os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json'), 'r') as f:\n",
    "    best_conf = json.load(f)\n",
    "    print(best_conf)\n",
    "# weight_test_dict = {\n",
    "#     f'fold_{fold_idx}': copy.deepcopy(training_weights(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_test_dict[f'fold_{fold_idx}'])) for fold_idx in range(len(data_test_aux_dict))\n",
    "# }  # DO NOT USE SCALED FOR TRAINING!!\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, \n",
    "        save=True, dict_lists=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1756  |       0.9706      |    0.2832 +/- 0.0022     |\n",
      "|   0.2715  |       0.9500      |    0.2030 +/- 0.0034     |\n",
      "|   0.3945  |       0.9198      |    0.1383 +/- 0.0027     |\n",
      "|   0.7945  |       0.7538      |    0.0315 +/- 0.0009     |\n",
      "|   0.9287  |       0.5777      |    0.0088 +/- 0.0005     |\n",
      "|   0.9772  |       0.3839      |    0.0023 +/- 0.0002     |\n",
      "+-----------+-------------------+--------------------------+\n",
      "============================================================\n",
      "\n",
      "==============================0_lepton==============================\n",
      "0_lepton\n",
      "==============================1_lepton==============================\n",
      "1_lepton\n",
      "==============================2+_lepton==============================\n",
      "2+_lepton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    }
   ],
   "source": [
    "# with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf.json'), 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "    NNtable.add_row(\n",
    "        [\n",
    "            IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "            \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "        ]\n",
    "    )\n",
    "print(NNtable)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = os.path.join(OUTPUT_DIRPATH, f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}/test_data_performance')\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [\n",
    "            (data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) \n",
    "            for i in range(len(data_test_aux_dict))\n",
    "        ]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "    print(plot_type)\n",
    "\n",
    "    weights_plot = {\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1) & mask_arr[fold_idx]],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0) & mask_arr[fold_idx]],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    density_weights_plot = {\n",
    "        fold_idx: {'sig': None, 'bkg': None} for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "\n",
    "    plot_train_val_losses(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME,\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    "    )\n",
    "    plot_roc(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], run2=False,\n",
    "        mask=None\n",
    "    )\n",
    "    plot_roc(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_sum', method='round_robin_sum',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], run2=False,\n",
    "        mask=None\n",
    "    )\n",
    "    # print(f\"num bkg: {np.sum(label_test==0)}\")\n",
    "    # print(f\"num sig: {np.sum(label_test==1)}\")\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, n_bins=25, \n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], n_bins=25, weights=density_weights_plot,\n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted_sum', method='round_robin_sum',\n",
    "        weights=weights_plot, n_bins=25, \n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density_sum', method='round_robin_sum', \n",
    "        n_bins=25, weights=density_weights_plot,\n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, n_bins=25,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin_sum', \n",
    "        weights=weights_plot, n_bins=25,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    # for fold_idx in range(len(data_test_aux_dict)):\n",
    "    #     for score_cut in [0.2, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    #         plot_input_vars_after_score_cut(\n",
    "    #             IN_perf, score_cut, plot_destdir, fold_idx, method='round_robin', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #             mask=mask_arr[fold_idx]\n",
    "    #         )\n",
    "    #     plot_input_vars_after_score_cut(\n",
    "    #         IN_perf, [0.2, 0.6, 0.9], plot_destdir, fold_idx, method='round_robin', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #         mask=mask_arr[fold_idx]\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimized cut-boundaries for ttH score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(IN_perf, weights, method='arr', bins=50, mask=None, n_folds=5):\n",
    "    if method == 'round_robin':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "    elif method == 'round_robin_sum':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_perf['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "\n",
    "        flat_preds = np.concatenate([\n",
    "            np.exp(IN_perf['all_preds'][i])[mask[i]][:, 1] for i in range(len(IN_perf['all_preds']))\n",
    "        ], axis=None)\n",
    "        flat_labels = np.concatenate([\n",
    "            np.array(IN_perf['all_labels'][i])[mask[i]] for i in range(len(IN_perf['all_preds']))\n",
    "        ], axis=None)\n",
    "        sig_preds = flat_preds[flat_labels == 1]\n",
    "        bkg_preds = flat_preds[flat_labels == 0]\n",
    "        \n",
    "        sig_weight = np.concatenate([\n",
    "            weight['sig'] for weight in weights.values()\n",
    "        ], axis=None)\n",
    "        bkg_weight = np.concatenate([\n",
    "            weight['bkg'] for weight in weights.values()\n",
    "        ], axis=None)\n",
    "        flat_weights = {'sig': sig_weight, 'bkg': bkg_weight}\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_preds, weight=flat_weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_preds, weight=flat_weights['bkg'])\n",
    "            \n",
    "        \n",
    "        cut_boundaries = []\n",
    "        cut_s_over_root_bs = []\n",
    "        prev_s_over_root_b = 0\n",
    "        sig_weights = []\n",
    "        bkg_weights = []\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                cut_boundaries.append(bins - i)\n",
    "                cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        cut_boundaries.append(0)\n",
    "        cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "        cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "        return cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights\n",
    "    elif method == 'arr':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "                    \n",
    "        sig_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "\n",
    "        cut_boundaries = []\n",
    "        cut_s_over_root_bs = []\n",
    "        prev_s_over_root_b = 0\n",
    "        sig_weights = []\n",
    "        bkg_weights = []\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                cut_boundaries.append(bins - i)\n",
    "                cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        cut_boundaries.append(0)\n",
    "        cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "        cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold, cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "==============================0_lepton==============================\n",
      "==============================1_lepton==============================\n",
      "==============================2+_lepton==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2545966/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    }
   ],
   "source": [
    "# with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf.json'), 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = os.path.join(OUTPUT_DIRPATH, f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}')\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [(data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "\n",
    "    weights_plot = {\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1) & mask_arr[fold_idx]],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0) & mask_arr[fold_idx]],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    \n",
    "    (\n",
    "        cut_boundaries_fold, cut_s_over_root_bs_fold, \n",
    "        sig_weights_fold, bkg_weights_fold\n",
    "    ) = optimize_cut_boundaries(\n",
    "        IN_perf, weights_plot, method='round_robin',\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    fold_labels = [\n",
    "        [\n",
    "            f\"s/âb={cut_s_over_root_bs_fold[fold_idx][cut_idx]:.04f}, s={sig_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{sig_weights_fold[fold_idx][cut_idx]['w2']:.04f}, b={bkg_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{bkg_weights_fold[fold_idx][cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[fold_idx]))\n",
    "        ] for fold_idx in range(len(weight_test_dict))\n",
    "    ]\n",
    "    fold_colors = [copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries_fold[i]) // len(cmap_petroff10)) + 1)) for i in range(len(weight_test_dict))]\n",
    "    for fold_idx in range(len(weight_test_dict)):\n",
    "        s_over_root_b(\n",
    "            IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_{plot_type}_fold{fold_idx}', \n",
    "            labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, method='round_robin',\n",
    "            lines_fold=cut_boundaries_fold, lines_labels=fold_labels, only_fold=fold_idx, lines_colors=fold_colors,\n",
    "            mask=mask_arr\n",
    "        )\n",
    "\n",
    "    (\n",
    "        cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights\n",
    "    ) = optimize_cut_boundaries(\n",
    "        IN_perf, weights_plot, method='round_robin_sum',\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    labels = [\n",
    "        f\"s/âb={cut_s_over_root_bs[cut_idx]:.04f}, s={sig_weights[cut_idx]['value']:.04f}Â±{sig_weights[cut_idx]['w2']:.04f}, b={bkg_weights[cut_idx]['value']:.04f}Â±{bkg_weights[cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs))\n",
    "    ]\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_{plot_type}_sum', \n",
    "        weights=weights_plot, method='round_robin_sum',\n",
    "        lines=cut_boundaries, lines_labels=labels, lines_colors=cmap_petroff10,\n",
    "        mask=mask_arr\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "# with open(os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json'), 'r') as f:\n",
    "#     best_conf = json.load(f)\n",
    "#     print(best_conf)\n",
    "\n",
    "for fold_idx in range(len(data_list_dict)):\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "        training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            train_data_list, train_data_hlf, train_label, train_weight,\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, only_fold_idx=fold_idx,\n",
    "            train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr,\n",
    "            # dict_lists=True\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            val_data_list, val_data_hlf, val_label, val_weight,\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, only_fold_idx=fold_idx,\n",
    "            train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr,\n",
    "            # dict_lists=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_train_val.json'), 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_train_val.json'), 'r') as f:\n",
    "    IN_perf_dict = json.load(f)\n",
    "    \n",
    "plot_destdir = os.path.join(OUTPUT_DIRPATH, 'plots/train_val_comparison')\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold ']\n",
    "val_weights_arr = []\n",
    "for fold_idx, (train_IN_dict, val_IN_dict) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'])):\n",
    "    all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight,\n",
    "        train_indices, val_indices\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "        training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "        all_indices,\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    weights_plot = [\n",
    "        {\n",
    "            'sig': _weight[_label == 1],\n",
    "            'bkg': _weight[_label == 0],\n",
    "        } for _weight, _label in [(train_weight, train_label), (val_weight, val_label)]\n",
    "    ]\n",
    "    density_weights_plot = [\n",
    "        {'sig': None, 'bkg': None} for _ in ['train', 'val']\n",
    "    ]\n",
    "\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_weighted_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights_plot\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_density_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=density_weights_plot\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights_plot\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Vars Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/pre_std/\"\n",
    "output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison_v3/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict[\"fold_0\"]:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        train_mask = np.ones_like(all_indices, dtype=bool)\n",
    "        train_mask[val_indices] = np.zeros_like(val_indices, dtype=bool)\n",
    "        val_mask = np.logical_not(train_mask)\n",
    "\n",
    "        sig_mask = (label_dict[f'fold_{fold_idx}'] == 1)\n",
    "        sig_test_mask = (label_test_dict[f'fold_{fold_idx}'] == 1)\n",
    "        bkg_mask = (label_dict[f'fold_{fold_idx}'] == 0)\n",
    "        bkg_test_mask = (label_test_dict[f'fold_{fold_idx}'] == 0)\n",
    "        \n",
    "        sig_train_mask = train_mask & sig_mask\n",
    "        sig_val_mask = val_mask & sig_mask\n",
    "        bkg_train_mask = train_mask & bkg_mask\n",
    "        bkg_val_mask = val_mask & bkg_mask\n",
    "\n",
    "        sig_train_np = data_df_dict[f'fold_{fold_idx}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "        sig_val_np = data_df_dict[f'fold_{fold_idx}'].loc[sig_val_mask, var_name].to_numpy()\n",
    "        sig_test_np = data_test_df_dict[f'fold_{fold_idx}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "        bkg_train_np = data_df_dict[f'fold_{fold_idx}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "        bkg_val_np = data_df_dict[f'fold_{fold_idx}'].loc[bkg_val_mask, var_name].to_numpy()\n",
    "        bkg_test_np = data_test_df_dict[f'fold_{fold_idx}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### post-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/post_std/\"\n",
    "output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison_v3/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        train_mask = np.ones_like(all_indices, dtype=bool)\n",
    "        train_mask[val_indices] = np.zeros_like(val_indices, dtype=bool)\n",
    "        val_mask = np.logical_not(train_mask)\n",
    "\n",
    "        data, data_test = None, None\n",
    "        if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "            data, data_test = data_list_dict[f'fold_{fold_idx}'], data_list_test_dict[f'fold_{fold_idx}']\n",
    "        else:\n",
    "            data, data_test = data_hlf_dict[f'fold_{fold_idx}'], data_hlf_test_dict[f'fold_{fold_idx}']\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, fold_idx, var_name,\n",
    "            train_index=train_mask, val_index=val_mask\n",
    "        )\n",
    "        if re.search('lepton1', var_name) is not None or re.search('lepton2', var_name) is not None:\n",
    "            sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "            sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "            sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "            bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "            bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "            bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "        else:\n",
    "            sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np)\n",
    "            sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np)\n",
    "            sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np)\n",
    "            bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np)\n",
    "            bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np)\n",
    "            bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np)\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian smearing on test set (for feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    mask_arr = data_list_index_map(var_name, particle_list_to_smear, np.ones(len(particle_list_to_smear), dtype=bool), n_pFields=N_PARTICLE_FIELDS)\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear = np.where(mask_arr, particle_list_to_smear*rng.normal(), particle_list_to_smear)\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear = np.where(mask_arr, particle_list_to_smear+rng.normal(), particle_list_to_smear)\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns_dict['fold_0'][var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate smeared variable test-data on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "# with open(os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json'), 'r') as f:\n",
    "    best_conf = json.load(f)\n",
    "    print(best_conf)\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "\n",
    "    weight_test_dict = {\n",
    "        f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "\n",
    "    gauss_data_list_dict, gauss_data_hlf_dict = None, None\n",
    "    if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "        gauss_data_list_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_test_dict))\n",
    "        }\n",
    "        gauss_data_hlf_dict = data_hlf_test_dict\n",
    "    else:\n",
    "        gauss_data_list_dict = data_list_test_dict\n",
    "        gauss_data_hlf_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_test_dict))\n",
    "        }\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list_dict, gauss_data_hlf_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True,\n",
    "    )\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_gauss_smear.json'), 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_gauss_smear.json'), 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "\n",
    "plot_destdir = os.path.join(OUTPUT_DIRPATH, 'plots/gauss_smear_performance')\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "IN_perf_smear_list = []\n",
    "label_arr = []\n",
    "for var_name, IN_perf_smear in IN_perf_smear_dict.items():\n",
    "    IN_perf_smear_list.append(IN_perf_smear)\n",
    "    label_arr.append(var_name)\n",
    "sort = np.argsort([IN_perf_smear['mean_area'] for IN_perf_smear in IN_perf_smear_list])\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_all', \n",
    "    method='round_robin_sum_arr', labels=label_arr, yscale='log', run2=False, sort=sort\n",
    ")\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5', \n",
    "    method='round_robin_sum_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False\n",
    ")\n",
    "# with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf.json'), 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "# plot_roc(\n",
    "#     [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5_and_orig', \n",
    "#     method='round_robin_sum_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False, run3=IN_perf\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m     bkg_val_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39mbkg_val_np)\n\u001b[1;32m     85\u001b[0m     bkg_test_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39mbkg_test_np)\n\u001b[0;32m---> 87\u001b[0m \u001b[43mmake_input_plot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir_gauss_smear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43msig_train_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig_val_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig_test_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbkg_train_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbkg_val_hist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbkg_test_hist\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_arr_fold\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 232\u001b[0m, in \u001b[0;36mmake_input_plot\u001b[0;34m(output_dir, var_name, hist_list, fold_idx, labels, density, plot_prefix, plot_postfix, alpha, linestyle)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_dir_):\n\u001b[1;32m    231\u001b[0m         os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir_)\n\u001b[0;32m--> 232\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_dir_\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplot_prefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m1dhist_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvar_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mplot_postfix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_fold\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox_inches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplot_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m1dhist_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mplot_postfix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, bbox_inches\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/pyplot.py:1228\u001b[0m, in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m fig \u001b[38;5;241m=\u001b[39m gcf()\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;66;03m# savefig default implementation has no return, so mypy is unhappy\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# presumably this is here because subclasses can return?\u001b[39;00m\n\u001b[0;32m-> 1228\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msavefig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[func-returns-value]\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mdraw_idle()  \u001b[38;5;66;03m# Need this if 'transparent=True', to reset colors.\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/figure.py:3395\u001b[0m, in \u001b[0;36mFigure.savefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3393\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes:\n\u001b[1;32m   3394\u001b[0m         _recursively_make_axes_transparent(stack, ax)\n\u001b[0;32m-> 3395\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/backend_bases.py:2175\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2172\u001b[0m     \u001b[38;5;66;03m# we do this instead of `self.figure.draw_without_rendering`\u001b[39;00m\n\u001b[1;32m   2173\u001b[0m     \u001b[38;5;66;03m# so that we can inject the orientation\u001b[39;00m\n\u001b[1;32m   2174\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2175\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/figure.py:3162\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3159\u001b[0m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3162\u001b[0m     \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3165\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/axes/_base.py:3137\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3135\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3137\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3140\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/legend.py:777\u001b[0m, in \u001b[0;36mLegend.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    774\u001b[0m     Shadow(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegendPatch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shadow_props)\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegendPatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m--> 777\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_legend_box\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegend\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/artist.py:39\u001b[0m, in \u001b[0;36m_prevent_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n\u001b[1;32m     37\u001b[0m     renderer\u001b[38;5;241m.\u001b[39m_rasterizing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/offsetbox.py:383\u001b[0m, in \u001b[0;36mOffsetBox.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c, (ox, oy) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_visible_children(), offsets):\n\u001b[1;32m    382\u001b[0m     c\u001b[38;5;241m.\u001b[39mset_offset((px \u001b[38;5;241m+\u001b[39m ox, py \u001b[38;5;241m+\u001b[39m oy))\n\u001b[0;32m--> 383\u001b[0m     \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m _bbox_artist(\u001b[38;5;28mself\u001b[39m, renderer, fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, props\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m))\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/artist.py:39\u001b[0m, in \u001b[0;36m_prevent_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n\u001b[1;32m     37\u001b[0m     renderer\u001b[38;5;241m.\u001b[39m_rasterizing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/offsetbox.py:383\u001b[0m, in \u001b[0;36mOffsetBox.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c, (ox, oy) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_visible_children(), offsets):\n\u001b[1;32m    382\u001b[0m     c\u001b[38;5;241m.\u001b[39mset_offset((px \u001b[38;5;241m+\u001b[39m ox, py \u001b[38;5;241m+\u001b[39m oy))\n\u001b[0;32m--> 383\u001b[0m     \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m _bbox_artist(\u001b[38;5;28mself\u001b[39m, renderer, fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, props\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m))\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/artist.py:39\u001b[0m, in \u001b[0;36m_prevent_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n\u001b[1;32m     37\u001b[0m     renderer\u001b[38;5;241m.\u001b[39m_rasterizing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/offsetbox.py:383\u001b[0m, in \u001b[0;36mOffsetBox.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c, (ox, oy) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_visible_children(), offsets):\n\u001b[1;32m    382\u001b[0m     c\u001b[38;5;241m.\u001b[39mset_offset((px \u001b[38;5;241m+\u001b[39m ox, py \u001b[38;5;241m+\u001b[39m oy))\n\u001b[0;32m--> 383\u001b[0m     \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m _bbox_artist(\u001b[38;5;28mself\u001b[39m, renderer, fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, props\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m))\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/artist.py:39\u001b[0m, in \u001b[0;36m_prevent_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n\u001b[1;32m     37\u001b[0m     renderer\u001b[38;5;241m.\u001b[39m_rasterizing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/offsetbox.py:379\u001b[0m, in \u001b[0;36mOffsetBox.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw\u001b[39m(\u001b[38;5;28mself\u001b[39m, renderer):\n\u001b[1;32m    375\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;124;03m    Update the location of children if necessary and draw them\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;124;03m    to the given *renderer*.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m     bbox, offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_bbox_and_child_offsets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     px, py \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_offset(bbox, renderer)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c, (ox, oy) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_visible_children(), offsets):\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/offsetbox.py:479\u001b[0m, in \u001b[0;36mHPacker._get_bbox_and_child_offsets\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    476\u001b[0m pad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad \u001b[38;5;241m*\u001b[39m dpicor\n\u001b[1;32m    477\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m*\u001b[39m dpicor\n\u001b[0;32m--> 479\u001b[0m bboxes \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mget_bbox(renderer) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_visible_children()]\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bboxes:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Bbox\u001b[38;5;241m.\u001b[39mfrom_bounds(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpadded(pad), []\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/offsetbox.py:479\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    476\u001b[0m pad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad \u001b[38;5;241m*\u001b[39m dpicor\n\u001b[1;32m    477\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m*\u001b[39m dpicor\n\u001b[0;32m--> 479\u001b[0m bboxes \u001b[38;5;241m=\u001b[39m [\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_bbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_visible_children()]\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m bboxes:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Bbox\u001b[38;5;241m.\u001b[39mfrom_bounds(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpadded(pad), []\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/offsetbox.py:763\u001b[0m, in \u001b[0;36mTextArea.get_bbox\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bbox\u001b[39m(\u001b[38;5;28mself\u001b[39m, renderer):\n\u001b[0;32m--> 763\u001b[0m     _, h_, d_ \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_width_height_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fontproperties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mismath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTeX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_usetex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m     bbox, info, yd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text\u001b[38;5;241m.\u001b[39m_get_layout(renderer)\n\u001b[1;32m    768\u001b[0m     w, h \u001b[38;5;241m=\u001b[39m bbox\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/backends/_backend_pdf_ps.py:125\u001b[0m, in \u001b[0;36mRendererPDFPSBase.get_text_width_height_descent\u001b[0;34m(self, s, prop, ismath)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m w, h, d\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     font \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_font_ttf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     font\u001b[38;5;241m.\u001b[39mset_text(s, \u001b[38;5;241m0.0\u001b[39m, flags\u001b[38;5;241m=\u001b[39mft2font\u001b[38;5;241m.\u001b[39mLOAD_NO_HINTING)\n\u001b[1;32m    127\u001b[0m     w, h \u001b[38;5;241m=\u001b[39m font\u001b[38;5;241m.\u001b[39mget_width_height()\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/backends/_backend_pdf_ps.py:141\u001b[0m, in \u001b[0;36mRendererPDFPSBase._get_font_ttf\u001b[0;34m(self, prop)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_font_ttf\u001b[39m(\u001b[38;5;28mself\u001b[39m, prop):\n\u001b[0;32m--> 141\u001b[0m     fnames \u001b[38;5;241m=\u001b[39m \u001b[43mfont_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfontManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_fonts_by_props\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     font \u001b[38;5;241m=\u001b[39m font_manager\u001b[38;5;241m.\u001b[39mget_font(fnames)\n\u001b[1;32m    143\u001b[0m     font\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/font_manager.py:1362\u001b[0m, in \u001b[0;36mFontManager._find_fonts_by_props\u001b[0;34m(self, prop, fontext, directory, fallback_to_default, rebuild_if_missing)\u001b[0m\n\u001b[1;32m   1358\u001b[0m cprop\u001b[38;5;241m.\u001b[39mset_family(family)  \u001b[38;5;66;03m# set current prop's family\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1361\u001b[0m     fpaths\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m-> 1362\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindfont\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcprop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfallback_to_default\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# don't fallback to default\u001b[39;49;00m\n\u001b[1;32m   1365\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrebuild_if_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrebuild_if_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1367\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m family \u001b[38;5;129;01min\u001b[39;00m font_family_aliases:\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/font_manager.py:1294\u001b[0m, in \u001b[0;36mFontManager.findfont\u001b[0;34m(self, prop, fontext, directory, fallback_to_default, rebuild_if_missing)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;124;03mFind the path to the font file most closely matching the given font properties.\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;124;03m   https://www.freedesktop.org/software/fontconfig/fontconfig-user.html\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;66;03m# Pass the relevant rcParams (and the font manager, as `self`) to\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;66;03m# _findfont_cached so to prevent using a stale cache entry after an\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# rcParam was changed.\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m rc_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfont.serif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfont.sans-serif\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfont.cursive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfont.fantasy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfont.monospace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_findfont_cached(\n\u001b[1;32m   1298\u001b[0m     prop, fontext, directory, fallback_to_default, rebuild_if_missing,\n\u001b[1;32m   1299\u001b[0m     rc_params)\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, _ExceptionProxy):\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/font_manager.py:1294\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;124;03mFind the path to the font file most closely matching the given font properties.\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;124;03m   https://www.freedesktop.org/software/fontconfig/fontconfig-user.html\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;66;03m# Pass the relevant rcParams (and the font manager, as `self`) to\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;66;03m# _findfont_cached so to prevent using a stale cache entry after an\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;66;03m# rcParam was changed.\u001b[39;00m\n\u001b[0;32m-> 1294\u001b[0m rc_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mtuple\u001b[39m(\u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfont.serif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfont.sans-serif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfont.cursive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfont.fantasy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfont.monospace\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1297\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_findfont_cached(\n\u001b[1;32m   1298\u001b[0m     prop, fontext, directory, fallback_to_default, rebuild_if_missing,\n\u001b[1;32m   1299\u001b[0m     rc_params)\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, _ExceptionProxy):\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/matplotlib/__init__.py:746\u001b[0m, in \u001b[0;36mRcParams.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 746\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m _deprecated_map:\n\u001b[1;32m    747\u001b[0m         version, alt_key, alt_val, inverse_alt \u001b[38;5;241m=\u001b[39m _deprecated_map[key]\n\u001b[1;32m    748\u001b[0m         _api\u001b[38;5;241m.\u001b[39mwarn_deprecated(\n\u001b[1;32m    749\u001b[0m             version, name\u001b[38;5;241m=\u001b[39mkey, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrcparam\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39malt_key)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/gauss_smear/\"\n",
    "output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison_v3/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "\n",
    "    gauss_data_list_dict, gauss_data_hlf_dict = None, None\n",
    "    if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "        gauss_data_list_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_dict))\n",
    "        }\n",
    "        gauss_data_hlf_dict = data_hlf_dict\n",
    "\n",
    "        gauss_data_list_test_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_test_dict))\n",
    "        }\n",
    "        gauss_data_hlf_test_dict = data_hlf_test_dict\n",
    "    else:\n",
    "        gauss_data_list_dict = data_list_dict\n",
    "        gauss_data_hlf_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_dict))\n",
    "        }\n",
    "\n",
    "        gauss_data_list_test_dict = data_list_test_dict\n",
    "        gauss_data_hlf_test_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_test_dict))\n",
    "        }\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        train_mask = np.ones_like(all_indices, dtype=bool)\n",
    "        train_mask[val_indices] = np.zeros_like(val_indices, dtype=bool)\n",
    "        val_mask = np.logical_not(train_mask)\n",
    "\n",
    "        data, data_test = None, None\n",
    "        if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "            data, data_test = gauss_data_list_dict[f'fold_{fold_idx}'], gauss_data_list_test_dict[f'fold_{fold_idx}']\n",
    "        else:\n",
    "            data, data_test = gauss_data_hlf_dict[f'fold_{fold_idx}'], gauss_data_hlf_test_dict[f'fold_{fold_idx}']\n",
    "        \n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, fold_idx, var_name,\n",
    "            train_index=train_mask, val_index=val_mask\n",
    "        )\n",
    "        if re.search('lepton1', var_name) is not None or re.search('lepton2', var_name) is not None:\n",
    "            sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "            sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "            sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "            bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "            bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "            bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "        else:\n",
    "            sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np)\n",
    "            sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np)\n",
    "            sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np)\n",
    "            bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np)\n",
    "            bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np)\n",
    "            bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np)\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_gauss_smear, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "# with open(os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json'), 'r') as f:\n",
    "    best_conf = json.load(f)\n",
    "    print(best_conf)\n",
    "\n",
    "weight_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_aux_dict))\n",
    "}\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y, w2 in [('train', data_list_dict, data_hlf_dict, label_dict, weight_dict), ('test', data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, w2,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True,\n",
    "    )\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_full_eval.json'), 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_full_eval.json'), 'r') as f:\n",
    "    IN_full_eval_dict = json.load(f)\n",
    "\n",
    "plot_destdir = os.path.join(OUTPUT_DIRPATH, 'plots/mass_sculpting_check')\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {fold_idx: {'mass': [], 'dijet_mass': []} for fold_idx in range(len(data_aux_dict))}\n",
    "for var_name in hist_dict[0].keys():\n",
    "    for fold_idx in range(len(data_aux_dict)):\n",
    "        for i, score_cut in enumerate(score_cuts):\n",
    "            sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold_idx)\n",
    "            sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "            sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "            bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "            bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "            hist_dict[fold_idx][var_name].extend(\n",
    "                [\n",
    "                    copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                    copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "                ]\n",
    "            )\n",
    "        for mod_factor, label_mod in enumerate(['sig_train', 'sig_test', 'bkg_train', 'bkg_test']):\n",
    "            plot_list = []\n",
    "            label_list = []\n",
    "            for i in range(len(hist_dict[fold_idx][var_name])):\n",
    "                if (i - mod_factor) % 4 == 0:\n",
    "                    plot_list.append(hist_dict[fold_idx][var_name][i])\n",
    "                    label_list.append(label_arr[i])\n",
    "            make_input_plot(\n",
    "                plot_destdir, var_name, plot_list, labels=label_list, density=True, \n",
    "                plot_prefix=CURRENT_TIME+'_', plot_postfix='_'+label_mod, alpha=0.5,\n",
    "                linestyle=False, fold_idx=fold_idx\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset size plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_dirs = glob.glob(OUTPUT_DIRPATH[:-1]+'_mod*') + [OUTPUT_DIRPATH[:-1]]\n",
    "final_train_losses_arr, final_val_losses_arr = [], []\n",
    "mod_values_arr = []\n",
    "\n",
    "for train_size_dir in train_size_dirs:\n",
    "    if len(glob.glob(train_size_dir + '/*IN_perf.json')) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        mod_values_arr.append([\n",
    "            float(\n",
    "                train_size_dir[\n",
    "                    train_size_dir.find('_mod')+4 : train_size_dir.find('-')\n",
    "                ]\n",
    "            ),\n",
    "            float(\n",
    "                train_size_dir[train_size_dir.find('-')+1:]\n",
    "            )\n",
    "        ])\n",
    "    except:\n",
    "         mod_values_arr.append([2, 2])\n",
    "    IN_perf_path = glob.glob(f'{train_size_dir}/*IN_perf.json')[0]\n",
    "    with open(IN_perf_path, 'r') as f:\n",
    "        IN_perf = json.load(f)\n",
    "    final_train_losses_arr.append([train_losses[-7 if len(train_losses) < NUM_EPOCHS else -1] for train_losses in IN_perf['train_losses_arr']])\n",
    "    final_val_losses_arr.append([val_losses[-7 if len(val_losses) < NUM_EPOCHS else -1] for val_losses in IN_perf['val_losses_arr']])\n",
    "\n",
    "final_train_losses_arr = np.array(final_train_losses_arr)\n",
    "final_val_losses_arr = np.array(final_val_losses_arr)\n",
    "mod_values_arr = np.array(mod_values_arr)\n",
    "dataset_sizes = (len(label) + len(label_test)) / mod_values_arr\n",
    "sorted_indices = np.argsort(dataset_sizes[:, 0])\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_train_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_train_losses_arr, axis=1)-np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_train_losses_arr, axis=1)+np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[0], alpha=0.5, label='Train data'\n",
    ")\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_val_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_val_losses_arr, axis=1)-np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_val_losses_arr, axis=1)+np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[1], alpha=0.5, label='Val data'\n",
    ")\n",
    "plt.xlabel('Size of train dataset')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.pdf')\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
