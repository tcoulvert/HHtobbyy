{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu3.fnal.gov      Thu Nov 21 00:40:28 2024  555.42.06\n",
      "[0] Tesla P100-PCIE-12GB | 41Â°C,   0 % |     0 / 12288 MB |\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams_RR\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "# V2_MERGED = True\n",
    "\n",
    "SIGNAL_FILEPATHS = [\n",
    "    # Test sig files #\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE/GluGluToHH/nominal/*',\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE_merged_v2/GluGluToHH/nominal/*',\n",
    "    # ggF HH # \n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\",\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GluGluToHH/nominal/*\",\n",
    "    # VBF HH #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [\n",
    "    # ttH (i.e. the main bkg to reduce) #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/ttHToGG/nominal/*\",\n",
    "    # # Other single H samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\",\n",
    "    # # Prompt-Prompt samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\",\n",
    "    # # Prompt-Fake samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\",\n",
    "    # Fake-Fake samples #\n",
    "    # ADD HERE #\n",
    "]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v10'\n",
    "CRITERION = \"NLLLoss\"\n",
    "N_PARTICLES, N_PARTICLE_FIELDS = 6, 7\n",
    "MOD_VALS = (5, 5)\n",
    "# VARS = 'base_vars'\n",
    "# VARS = 'extra_vars'\n",
    "# CURRENT_TIME = '2024-08-30_14-35-01'\n",
    "# VARS = 'extra_vars+'\n",
    "# CURRENT_TIME = '2024-10-09_20-47-24'\n",
    "VARS = 'extra_vars+max'\n",
    "# CURRENT_TIME = '2024-10-29_00-47-20'\n",
    "# CURRENT_TIME = '2024-10-21_12-09-38'\n",
    "CURRENT_TIME = '2024-11-20_18-34-50'\n",
    "# VARS = 'extra_vars+max+opt_space'\n",
    "# CURRENT_TIME = '2024-10-24_20-29-37'\n",
    "# VARS = 'extra_vars+max+moved_vars_to_RNN'\n",
    "# CURRENT_TIME = '2024-10-21_00-35-33'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 6, 9\n",
    "# VARS = 'extra_vars_no_dijet_mass'\n",
    "# VARS = 'no_bad_vars'\n",
    "# VARS = 'simplified_bad_vars'\n",
    "# VARS = 'extra_vars_and_bools'\n",
    "# VARS = 'extra_vars_in_RNN'\n",
    "# VARS = f'extra_vars_mod{MOD_VALS[0]}-{MOD_VALS[1]}'\n",
    "# VARS = 'extra_vars_lead_lep_only'\n",
    "# CURRENT_TIME = '2024-10-02_18-03-26'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 3, 6\n",
    "# VARS = 'extra_vars_no_lep'\n",
    "# CURRENT_TIME = '2024-10-04_12-49-31'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 2, 6\n",
    "if 'CURRENT_TIME' in globals():\n",
    "    OUTPUT_DIRPATH = os.path.join(CURRENT_DIRPATH, f\"model_outputs/{VERSION}/{VARS}\", CURRENT_TIME)\n",
    "else:\n",
    "    OUTPUT_DIRPATH = os.path.join(CURRENT_DIRPATH, f\"model_outputs/{VERSION}/{VARS}\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awkward unique bkg eventWeights: \n",
      "[-0.000142, -0.000132, 0.000132, 0.000142]\n",
      "numpy unique bkg eventWeights: \n",
      "[-0.00014245 -0.0001316   0.0001316   0.00014245]\n",
      "pandas unique bkg eventWeights: \n",
      "[-0.00014245 -0.0001316   0.0001316   0.00014245]\n",
      "pandas after reindex unique bkg eventWeights: \n",
      "[-0.00014245 -0.0001316   0.0001316   0.00014245]\n",
      "pandas train (after train/test split) unique bkg eventWeights: \n",
      "[-0.00014245 -0.0001316   0.0001316   0.00014245]\n",
      "pandas test (after train/test split) unique bkg eventWeights: \n",
      "[-0.00014245 -0.0001316   0.0001316   0.00014245]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m (\n\u001b[1;32m      2\u001b[0m     data_df_dict, data_test_df_dict, \n\u001b[1;32m      3\u001b[0m     data_list_dict, data_hlf_dict, label_dict, \n\u001b[1;32m      4\u001b[0m     data_list_test_dict, data_hlf_test_dict, label_test_dict, \n\u001b[1;32m      5\u001b[0m     high_level_fields_dict, input_hlf_vars_dict, hlf_vars_columns_dict,\n\u001b[1;32m      6\u001b[0m     data_aux_dict, data_test_aux_dict\n\u001b[0;32m----> 7\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mN_PARTICLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_PARTICLE_FIELDS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSIGNAL_FILEPATHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBKG_FILEPATHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIRPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod_vals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMOD_VALS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_fold_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCURRENT_TIME\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/data_processing.py:429\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(n_particles, n_particle_fields, signal_filepaths, bkg_filepaths, output_dirpath, seed, mod_vals, k_fold_test, save_std)\u001b[0m\n\u001b[1;32m    425\u001b[0m         sorted_particle_list[i, np\u001b[38;5;241m.\u001b[39msum(nonzero_indices[i]):, :] \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(copy_arr)\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sorted_particle_list\n\u001b[0;32m--> 429\u001b[0m normed_sig_list \u001b[38;5;241m=\u001b[39m \u001b[43mto_p_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormed_sig_train_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m normed_sig_test_list \u001b[38;5;241m=\u001b[39m to_p_list(normed_sig_test_frame)\n\u001b[1;32m    431\u001b[0m normed_bkg_list \u001b[38;5;241m=\u001b[39m to_p_list(normed_bkg_train_frame)\n",
      "File \u001b[0;32m/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/data_processing.py:423\u001b[0m, in \u001b[0;36mprocess_data.<locals>.to_p_list\u001b[0;34m(data_frame)\u001b[0m\n\u001b[1;32m    420\u001b[0m zero_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_not(nonzero_indices)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_frame)):\n\u001b[0;32m--> 423\u001b[0m     copy_arr \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_particle_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     sorted_particle_list[i, :np\u001b[38;5;241m.\u001b[39msum(nonzero_indices[i]), :] \u001b[38;5;241m=\u001b[39m sorted_particle_list[i, nonzero_indices[i], :]\n\u001b[1;32m    425\u001b[0m     sorted_particle_list[i, np\u001b[38;5;241m.\u001b[39msum(nonzero_indices[i]):, :] \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(copy_arr)\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/copy.py:128\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    124\u001b[0m     d[PyStringMap] \u001b[38;5;241m=\u001b[39m PyStringMap\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m d, t\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeepcopy\u001b[39m(x, memo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, _nil\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deep copy operation on arbitrary Python objects.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    See the module's __doc__ string for more info.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(\n",
    "    data_df_dict, data_test_df_dict, \n",
    "    data_list_dict, data_hlf_dict, label_dict, \n",
    "    data_list_test_dict, data_hlf_test_dict, label_test_dict, \n",
    "    high_level_fields_dict, input_hlf_vars_dict, hlf_vars_columns_dict,\n",
    "    data_aux_dict, data_test_aux_dict\n",
    ") = process_data(\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, \n",
    "    seed=SEED, mod_vals=MOD_VALS, k_fold_test=True,\n",
    "    save_std=False if 'CURRENT_TIME' in globals() else True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_weights(event_weights, labels):\n",
    "    sum_of_bkg = np.sum(event_weights[labels==0])\n",
    "    sum_of_sig = np.sum(event_weights[labels==1])\n",
    "\n",
    "    sig_scale_factor = sum_of_bkg / sum_of_sig\n",
    "\n",
    "    weights = np.where(labels==0, event_weights, event_weights*sig_scale_factor)\n",
    "    mean_weight = np.mean(weights)\n",
    "    abs_weights = np.abs(weights)\n",
    "    scaled_weights = abs_weights / mean_weight\n",
    "\n",
    "    return scaled_weights\n",
    "\n",
    "def plot_train_val_losses(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None, n_folds=5\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dotted']\n",
    "        linestyles = linestyles * ((2*len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:2*len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx if fold_idx%2 == 0 else fold_idx+1]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx+1 if fold_idx%2 == 0 else fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='std', \n",
    "    labels=None, yscale='linear', run2=True, sort=None, run3=None,\n",
    "    mask=None, n_folds=5\n",
    "):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d, AUC = %.4f\" % (fold_idx, float(auc(IN_info['fprs'][fold_idx],IN_info['base_tpr']))), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "    elif method == 'round_robin_sum':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "\n",
    "        base_tpr = np.linspace(0, 1, 5000)\n",
    "        flat_preds = np.concatenate([\n",
    "            np.exp(IN_info['all_preds'][i])[mask[i]][:, 1] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "        flat_labels = np.concatenate([\n",
    "            np.array(IN_info['all_labels'][i])[mask[i]] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "\n",
    "        fpr, tpr, threshold = roc_curve(flat_labels, flat_preds)\n",
    "        fpr = np.interp(base_tpr, tpr, fpr)\n",
    "        threshold = np.interp(base_tpr, tpr, threshold)\n",
    "        fpr[0] = 0.0\n",
    "\n",
    "        all_auc = float(auc(fpr, base_tpr))\n",
    "\n",
    "        plt.plot(\n",
    "            fpr, base_tpr,\n",
    "            label=\"Run3 NN - sum over folds, AUC = %.4f\" % all_auc, linestyle='solid',\n",
    "            alpha=1\n",
    "        )\n",
    "    elif method == 'round_robin_sum_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for IN_idx in index_arr:\n",
    "            if mask is None:\n",
    "                mask = [np.ones(np.shape(IN_info[IN_idx]['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "\n",
    "            base_tpr = np.linspace(0, 1, 5000)\n",
    "            flat_preds = np.concatenate([\n",
    "                np.exp(IN_info[IN_idx]['all_preds'][i])[mask[i]][:, 1] for i in range(len(IN_info[IN_idx]['all_preds']))\n",
    "            ], axis=None)\n",
    "            flat_labels = np.concatenate([\n",
    "                np.array(IN_info[IN_idx]['all_labels'][i])[mask[i]] for i in range(len(IN_info[IN_idx]['all_preds']))\n",
    "            ], axis=None)\n",
    "\n",
    "            fpr, tpr, threshold = roc_curve(flat_labels, flat_preds)\n",
    "            fpr = np.interp(base_tpr, tpr, fpr)\n",
    "            threshold = np.interp(base_tpr, tpr, threshold)\n",
    "            fpr[0] = 0.0\n",
    "\n",
    "            all_auc = float(auc(fpr, base_tpr))\n",
    "\n",
    "            plt.plot(\n",
    "                fpr, base_tpr,\n",
    "                label=(labels[IN_idx]+', ' if labels is not None else '') + \"AUC = %.4f\" % all_auc, \n",
    "                linestyle=linestyles[IN_idx], alpha=0.8\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for i in index_arr:\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_fprs'])[0], dtype=bool)\n",
    "            plt.plot(\n",
    "                np.array(IN_info[i]['mean_fprs'])[mask], np.array(IN_info[i]['base_tpr'])[mask], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i], alpha=0.5\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if run3 is not None:\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(run3['mean_fprs'])[0], dtype=bool)\n",
    "        plt.plot(\n",
    "            np.array(run3['mean_fprs'])[mask], np.array(run3['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (run3['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, n_bins=50, all_sig=False, all_bkg=False,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        # for cut in np.linspace(0, 1, 10, endpoint=False):\n",
    "        #     print(f\"output score > {cut:.2f}\")\n",
    "        #     print('='*60)\n",
    "        #     print(f\"num sig > {cut:.2f} = {len(sig_np[sig_np > cut])}\")\n",
    "        #     print(f\"num bkg > {cut:.2f} = {len(bkg_np[bkg_np > cut])}\")\n",
    "        #     print('-'*60)\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'] if weights[fold_idx]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'] if weights[fold_idx]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[fold_idx]['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights[fold_idx]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "    elif method == 'round_robin_sum':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "\n",
    "        flat_preds = np.concatenate([\n",
    "            np.exp(IN_info['all_preds'][i])[mask[i]][:, 1] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "        flat_labels = np.concatenate([\n",
    "            np.array(IN_info['all_labels'][i])[mask[i]] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "        sig_preds = flat_preds[flat_labels == 1]\n",
    "        bkg_preds = flat_preds[flat_labels == 0]\n",
    "\n",
    "        if weights[0]['sig'] is not None:\n",
    "            sig_weight = np.concatenate([\n",
    "                weight['sig'] for weight in weights.values()\n",
    "            ], axis=None)\n",
    "            bkg_weight = np.concatenate([\n",
    "                weight['bkg'] for weight in weights.values()\n",
    "            ], axis=None)\n",
    "            full_weights = {'sig': sig_weight, 'bkg': bkg_weight}\n",
    "        else:\n",
    "            full_weights = {'sig': None, 'bkg': None}\n",
    "\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_preds, weight=full_weights['sig'] if full_weights['sig'] is not None else np.ones_like(sig_preds))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_preds, weight=full_weights['bkg'] if full_weights['bkg'] is not None else np.ones_like(bkg_preds))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if full_weights['sig'] is not None else False),\n",
    "            alpha=0.5, density=(False if full_weights['sig'] is not None else True), histtype='step',\n",
    "            label=[\n",
    "                f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal - sum over folds', \n",
    "                f'{\"ttH\" if not all_bkg else \"all\"} background - sum over folds'\n",
    "            ], linestyle=['solid', 'solid']\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=1, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=[\n",
    "                f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal', \n",
    "                f'{\"ttH\" if not all_bkg else \"all\"} background'\n",
    "            ]\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'] if weights[i]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'] if weights[i]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[i]['sig'] is not None else False),\n",
    "                alpha=0.7, density=(False if weights[i]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[i] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[i], linestyles[i]]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.ylabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def s_over_root_b(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, lines_fold=None, lines=None, lines_labels=None, \n",
    "        lines_colors=None, only_fold=None, no_fold=False, n_bins=50,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/âb', alpha=0.8 \n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "    elif method == 'round_robin_sum':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "\n",
    "        flat_preds = np.concatenate([\n",
    "            np.exp(IN_info['all_preds'][i])[mask[i]][:, 1] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "        flat_labels = np.concatenate([\n",
    "            np.array(IN_info['all_labels'][i])[mask[i]] for i in range(len(IN_info['all_preds']))\n",
    "        ], axis=None)\n",
    "        sig_preds = flat_preds[flat_labels == 1]\n",
    "        bkg_preds = flat_preds[flat_labels == 0]\n",
    "        \n",
    "        sig_weight = np.concatenate([\n",
    "            weight['sig'] for weight in weights.values()\n",
    "        ], axis=None)\n",
    "        bkg_weight = np.concatenate([\n",
    "            weight['bkg'] for weight in weights.values()\n",
    "        ], axis=None)\n",
    "        full_weights = {'sig': sig_weight, 'bkg': bkg_weight}\n",
    "\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_preds, weight=full_weights['sig'] if full_weights['sig'] is not None else np.ones_like(sig_preds))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_preds, weight=full_weights['bkg'] if full_weights['bkg'] is not None else np.ones_like(bkg_preds))\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/âb - sum over folds', \n",
    "            alpha=0.5, linestyle='solid', \n",
    "        )\n",
    "        if lines is not None:\n",
    "            for i in range(len(lines)):\n",
    "                plt.vlines(\n",
    "                    lines[i], 0, np.max(s_over_root_b_points), \n",
    "                    label='s/âb'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                    alpha=0.5, colors=lines_colors[i]\n",
    "                )\n",
    "    elif method == 'arr':\n",
    "        if mask is None or np.all(mask):\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "        if only_fold is None:\n",
    "            sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb - avg. over folds', \n",
    "                alpha=0.5, \n",
    "            )\n",
    "            if lines is not None:\n",
    "                for i in range(len(lines)):\n",
    "                    plt.vlines(\n",
    "                        lines[i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb - avg.'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[i]\n",
    "                    )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'])\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "                label='s/âb'+(' - '+labels[i] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[i], \n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.ylabel(\"s/âb\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 150., 2000, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    # 'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # 'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Integer(0, 10, name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($\\Sigma E_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t0}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t1}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma$ ln($p_{T}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'ln($M_{jj}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, fold, var_name, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label_dict[f'fold_{fold}'] == 1\n",
    "    sig_test_mask = label_test_dict[f'fold_{fold}'] == 1\n",
    "    bkg_mask = label_dict[f'fold_{fold}'] == 0\n",
    "    bkg_test_mask = label_test_dict[f'fold_{fold}'] == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_train_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            # index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold):\n",
    "    sig_train_mask = (label_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['all_preds'][fold])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux_dict[f'fold_{fold}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux_dict[f'fold_{fold}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux_dict[f'fold_{fold}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux_dict[f'fold_{fold}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(\n",
    "    output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, \n",
    "    plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True\n",
    "):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir_ = os.path.join(output_dir, \"fold\")\n",
    "        if not os.path.exists(output_dir_):\n",
    "            os.makedirs(output_dir_)\n",
    "        plt.savefig(f'{output_dir_}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir_}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_input_vars_after_score_cut(\n",
    "    IN_info, score_cut, destdir, fold, plot_prefix, plot_postfix='', method='std', \n",
    "    weights={'sig': None, 'bkg': None}, all_sig=False, all_bkg=False,\n",
    "    mask=None\n",
    "):\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "        bkg_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            sig_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=sig_var.loc[sig_mask], \n",
    "                weight=weights['sig'][sig_mask] if weights['sig'] is not None else np.ones(np.sum(sig_mask))\n",
    "            )\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            bkg_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=bkg_var.loc[bkg_mask], \n",
    "                weight=weights['bkg'][bkg_mask] if weights['bkg'] is not None else np.ones(np.sum(bkg_mask))\n",
    "            )\n",
    "            make_input_plot(\n",
    "                destdir, var_name, [sig_hist, bkg_hist], fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore{score_cut}', labels=['HH signal', 'ttH background'], density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['all_preds'][fold])[0], dtype=bool)\n",
    "        \n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut if score_cut is list else [score_cut]:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['all_preds'][fold]\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['all_labels'][fold]) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['all_preds'][fold]\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['all_labels'][fold]) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used method 'std'. You used {method}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0002925883 Acc: 81.9851455688\n",
      "validation Loss: 0.0002361357 Acc: 87.1801986694\n",
      "training Loss: 0.0002316002 Acc: 86.8297348022\n",
      "validation Loss: 0.0002228017 Acc: 87.3747634888\n",
      "training Loss: 0.0002256538 Acc: 87.2995376587\n",
      "validation Loss: 0.0002180064 Acc: 87.4231033325\n",
      "training Loss: 0.0002227159 Acc: 87.4484863281\n",
      "validation Loss: 0.0002204007 Acc: 87.6358032227\n",
      "training Loss: 0.0002207515 Acc: 87.5261306763\n",
      "validation Loss: 0.0002137119 Acc: 88.1179962158\n",
      "training Loss: 0.0002182257 Acc: 87.6684341431\n",
      "validation Loss: 0.0002122377 Acc: 88.1542510986\n",
      "training Loss: 0.0002177279 Acc: 87.7110290527\n",
      "validation Loss: 0.0002149447 Acc: 88.5022964478\n",
      "training Loss: 0.0002159306 Acc: 87.9010696411\n",
      "validation Loss: 0.0002133098 Acc: 87.6672210693\n",
      "training Loss: 0.0002152097 Acc: 87.9923095703\n",
      "validation Loss: 0.0002180310 Acc: 89.5500717163\n",
      "training Loss: 0.0002138725 Acc: 87.9907989502\n",
      "validation Loss: 0.0002099658 Acc: 88.7186203003\n",
      "training Loss: 0.0002137878 Acc: 88.0164794922\n",
      "validation Loss: 0.0002090462 Acc: 87.4315643311\n",
      "training Loss: 0.0002131642 Acc: 87.9651184082\n",
      "validation Loss: 0.0002106508 Acc: 88.9482345581\n",
      "training Loss: 0.0002117285 Acc: 88.1007766724\n",
      "validation Loss: 0.0002119142 Acc: 87.0327606201\n",
      "training Loss: 0.0002119407 Acc: 88.0793228149\n",
      "validation Loss: 0.0002121183 Acc: 87.8255386353\n",
      "training Loss: 0.0002114214 Acc: 88.0699615479\n",
      "validation Loss: 0.0002091557 Acc: 88.9421920776\n",
      "training Loss: 0.0002072243 Acc: 88.3460998535\n",
      "validation Loss: 0.0002024526 Acc: 88.2847671509\n",
      "training Loss: 0.0002062437 Acc: 88.4140777588\n",
      "validation Loss: 0.0002026814 Acc: 88.3077316284\n",
      "training Loss: 0.0002050713 Acc: 88.5158920288\n",
      "validation Loss: 0.0002033040 Acc: 88.0334014893\n",
      "training Loss: 0.0002052995 Acc: 88.4687652588\n",
      "validation Loss: 0.0002034535 Acc: 89.1476440430\n",
      "training Loss: 0.0002051829 Acc: 88.5455017090\n",
      "validation Loss: 0.0002031338 Acc: 88.1566696167\n",
      "training Loss: 0.0002022431 Acc: 88.7503433228\n",
      "validation Loss: 0.0001985083 Acc: 88.7234573364\n",
      "training Loss: 0.0002021248 Acc: 88.6615219116\n",
      "validation Loss: 0.0001992040 Acc: 88.9119796753\n",
      "training Loss: 0.0002016844 Acc: 88.7020034790\n",
      "validation Loss: 0.0001997530 Acc: 88.8032150269\n",
      "training Loss: 0.0002012457 Acc: 88.7201309204\n",
      "validation Loss: 0.0001981721 Acc: 88.7270812988\n",
      "training Loss: 0.0002008948 Acc: 88.7282867432\n",
      "validation Loss: 0.0001993845 Acc: 88.4128723145\n",
      "training Loss: 0.0002006572 Acc: 88.7086486816\n",
      "validation Loss: 0.0001985072 Acc: 88.5458068848\n",
      "training Loss: 0.0002001170 Acc: 88.7246627808\n",
      "validation Loss: 0.0001986877 Acc: 89.0352478027\n",
      "training Loss: 0.0002006471 Acc: 88.7959671021\n",
      "validation Loss: 0.0001985826 Acc: 89.0159149170\n",
      "training Loss: 0.0001991830 Acc: 88.8853988647\n",
      "validation Loss: 0.0001967819 Acc: 89.0050354004\n",
      "training Loss: 0.0001986362 Acc: 88.8793563843\n",
      "validation Loss: 0.0001972803 Acc: 88.5300979614\n",
      "training Loss: 0.0001982087 Acc: 88.8763351440\n",
      "validation Loss: 0.0001960995 Acc: 88.6412811279\n",
      "training Loss: 0.0001980021 Acc: 88.9050369263\n",
      "validation Loss: 0.0001969079 Acc: 88.7137908936\n",
      "training Loss: 0.0001978742 Acc: 88.8597183228\n",
      "validation Loss: 0.0001969154 Acc: 88.4708786011\n",
      "training Loss: 0.0001977903 Acc: 88.8627395630\n",
      "validation Loss: 0.0001970008 Acc: 89.3035354614\n",
      "training Loss: 0.0001979857 Acc: 88.8760299683\n",
      "validation Loss: 0.0001965068 Acc: 88.9917449951\n",
      "training Loss: 0.0001966102 Acc: 88.9727096558\n",
      "validation Loss: 0.0001953430 Acc: 89.0279998779\n",
      "training Loss: 0.0001963021 Acc: 88.9554901123\n",
      "validation Loss: 0.0001956560 Acc: 88.7681732178\n",
      "training Loss: 0.0001959228 Acc: 88.9872131348\n",
      "validation Loss: 0.0001957304 Acc: 88.8237609863\n",
      "training Loss: 0.0001962851 Acc: 88.9337387085\n",
      "validation Loss: 0.0001951952 Acc: 89.0425033569\n",
      "training Loss: 0.0001959137 Acc: 88.9884185791\n",
      "validation Loss: 0.0001954276 Acc: 88.5748138428\n",
      "training Loss: 0.0001956163 Acc: 89.0059432983\n",
      "validation Loss: 0.0001951144 Acc: 89.1452255249\n",
      "training Loss: 0.0001957995 Acc: 89.0011062622\n",
      "validation Loss: 0.0001952151 Acc: 88.9566955566\n",
      "training Loss: 0.0001956713 Acc: 89.0343399048\n",
      "validation Loss: 0.0001955000 Acc: 88.8237609863\n",
      "training Loss: 0.0001957876 Acc: 88.9772415161\n",
      "validation Loss: 0.0001947417 Acc: 88.9941635132\n",
      "training Loss: 0.0001951302 Acc: 89.0119857788\n",
      "validation Loss: 0.0001950523 Acc: 89.0787582397\n",
      "training Loss: 0.0001950816 Acc: 89.0219573975\n",
      "validation Loss: 0.0001944292 Acc: 88.9204406738\n",
      "training Loss: 0.0001951872 Acc: 89.0273971558\n",
      "validation Loss: 0.0001944210 Acc: 89.0594177246\n",
      "training Loss: 0.0001950931 Acc: 89.0261840820\n",
      "validation Loss: 0.0001947177 Acc: 89.1113891602\n",
      "training Loss: 0.0001952076 Acc: 89.0412902832\n",
      "validation Loss: 0.0001944328 Acc: 88.8491439819\n",
      "training Loss: 0.0001952652 Acc: 89.0165176392\n",
      "validation Loss: 0.0001943971 Acc: 89.0860061646\n",
      "training Loss: 0.0001949289 Acc: 89.0310211182\n",
      "validation Loss: 0.0001945394 Acc: 88.9893264771\n",
      "training Loss: 0.0001944593 Acc: 89.0929565430\n",
      "validation Loss: 0.0001942980 Acc: 88.9881210327\n",
      "training Loss: 0.0001952001 Acc: 89.0273971558\n",
      "validation Loss: 0.0001941332 Acc: 88.9869079590\n",
      "training Loss: 0.0001944425 Acc: 89.0382690430\n",
      "validation Loss: 0.0001942016 Acc: 88.9820785522\n",
      "training Loss: 0.0001943325 Acc: 89.0850982666\n",
      "validation Loss: 0.0001946688 Acc: 88.8310165405\n",
      "training Loss: 0.0001943408 Acc: 89.0796661377\n",
      "validation Loss: 0.0001942542 Acc: 89.0304183960\n",
      "training Loss: 0.0001945632 Acc: 89.0563964844\n",
      "validation Loss: 0.0001940073 Acc: 89.2322387695\n",
      "training Loss: 0.0001939769 Acc: 89.0687866211\n",
      "validation Loss: 0.0001944504 Acc: 89.0594177246\n",
      "training Loss: 0.0001941915 Acc: 89.0415954590\n",
      "validation Loss: 0.0001937151 Acc: 89.0702972412\n",
      "training Loss: 0.0001939127 Acc: 89.0854034424\n",
      "validation Loss: 0.0001940568 Acc: 88.7319183350\n",
      "training Loss: 0.0001942685 Acc: 89.0708999634\n",
      "validation Loss: 0.0001942744 Acc: 88.8189315796\n",
      "training Loss: 0.0001942817 Acc: 89.0618362427\n",
      "validation Loss: 0.0001943710 Acc: 88.6497344971\n",
      "training Loss: 0.0001940273 Acc: 89.0358505249\n",
      "validation Loss: 0.0001943662 Acc: 89.4195556641\n",
      "training Loss: 0.0001934234 Acc: 89.1131973267\n",
      "validation Loss: 0.0001939630 Acc: 89.0328292847\n",
      "training Loss: 0.0001931349 Acc: 89.1376724243\n",
      "validation Loss: 0.0001935136 Acc: 88.9240646362\n",
      "training Loss: 0.0001930187 Acc: 89.1023254395\n",
      "validation Loss: 0.0001936028 Acc: 88.9941635132\n",
      "training Loss: 0.0001929776 Acc: 89.1219635010\n",
      "validation Loss: 0.0001936824 Acc: 88.7971725464\n",
      "training Loss: 0.0001929529 Acc: 89.0733184814\n",
      "validation Loss: 0.0001933519 Acc: 88.9796600342\n",
      "training Loss: 0.0001928282 Acc: 89.1225662231\n",
      "validation Loss: 0.0001929903 Acc: 89.1802673340\n",
      "training Loss: 0.0001931310 Acc: 89.1234741211\n",
      "validation Loss: 0.0001927766 Acc: 89.1041336060\n",
      "training Loss: 0.0001929185 Acc: 89.1277008057\n",
      "validation Loss: 0.0001932682 Acc: 88.9276962280\n",
      "training Loss: 0.0001930523 Acc: 89.1270980835\n",
      "validation Loss: 0.0001927719 Acc: 89.0388717651\n",
      "training Loss: 0.0001922677 Acc: 89.1920547485\n",
      "validation Loss: 0.0001935120 Acc: 88.9699935913\n",
      "training Loss: 0.0001930342 Acc: 89.1464309692\n",
      "validation Loss: 0.0001931062 Acc: 88.9820785522\n",
      "training Loss: 0.0001925405 Acc: 89.1292114258\n",
      "validation Loss: 0.0001931544 Acc: 89.0086593628\n",
      "training Loss: 0.0001922844 Acc: 89.1334381104\n",
      "validation Loss: 0.0001929990 Acc: 89.0376663208\n",
      "training Loss: 0.0001924003 Acc: 89.1751327515\n",
      "validation Loss: 0.0001926818 Acc: 89.1150131226\n",
      "training Loss: 0.0001926180 Acc: 89.1428070068\n",
      "validation Loss: 0.0001933770 Acc: 89.0050354004\n",
      "training Loss: 0.0001926867 Acc: 89.1917495728\n",
      "validation Loss: 0.0001934003 Acc: 88.9252777100\n",
      "training Loss: 0.0001922217 Acc: 89.1446228027\n",
      "validation Loss: 0.0001931571 Acc: 89.1125946045\n",
      "training Loss: 0.0001923540 Acc: 89.2017211914\n",
      "validation Loss: 0.0001926331 Acc: 88.9929504395\n",
      "training Loss: 0.0001923997 Acc: 89.1730194092\n",
      "validation Loss: 0.0001931104 Acc: 89.1270980835\n",
      "training Loss: 0.0001925114 Acc: 89.1521759033\n",
      "validation Loss: 0.0001929537 Acc: 89.1959838867\n",
      "training Loss: 0.0001914519 Acc: 89.1860122681\n",
      "validation Loss: 0.0001928627 Acc: 89.0134963989\n",
      "training Loss: 0.0001919345 Acc: 89.2153167725\n",
      "validation Loss: 0.0001929465 Acc: 88.9470291138\n",
      "training Loss: 0.0001917068 Acc: 89.1591186523\n",
      "validation Loss: 0.0001927240 Acc: 89.0328292847\n",
      "training Loss: 0.0001919069 Acc: 89.2092742920\n",
      "validation Loss: 0.0001929965 Acc: 88.9917449951\n",
      "training Loss: 0.0001919474 Acc: 89.2409973145\n",
      "validation Loss: 0.0001927101 Acc: 89.0666732788\n",
      "Early stopped.\n",
      "Best val acc: 89.550072\n",
      "----------\n",
      "training Loss: 0.0002859177 Acc: 82.2866287231\n",
      "validation Loss: 0.0002400507 Acc: 86.0951385498\n",
      "training Loss: 0.0002340205 Acc: 86.6981353760\n",
      "validation Loss: 0.0002470346 Acc: 88.4905242920\n",
      "training Loss: 0.0002276346 Acc: 87.1529388428\n",
      "validation Loss: 0.0002254885 Acc: 87.8189010620\n",
      "training Loss: 0.0002230033 Acc: 87.4960021973\n",
      "validation Loss: 0.0002241810 Acc: 87.9300308228\n",
      "training Loss: 0.0002209743 Acc: 87.6056213379\n",
      "validation Loss: 0.0002201138 Acc: 87.8273544312\n",
      "training Loss: 0.0002197347 Acc: 87.7019577026\n",
      "validation Loss: 0.0002205436 Acc: 87.3296737671\n",
      "training Loss: 0.0002189053 Acc: 87.6778030396\n",
      "validation Loss: 0.0002162118 Acc: 87.1883392334\n",
      "training Loss: 0.0002175884 Acc: 87.8330230713\n",
      "validation Loss: 0.0002152744 Acc: 87.5809326172\n",
      "training Loss: 0.0002158201 Acc: 87.8393630981\n",
      "validation Loss: 0.0002224461 Acc: 89.2805328369\n",
      "training Loss: 0.0002159377 Acc: 87.9408340454\n",
      "validation Loss: 0.0002159594 Acc: 88.2658462524\n",
      "training Loss: 0.0002142556 Acc: 88.0006332397\n",
      "validation Loss: 0.0002145854 Acc: 87.5628128052\n",
      "training Loss: 0.0002137568 Acc: 88.0534820557\n",
      "validation Loss: 0.0002134475 Acc: 89.0027008057\n",
      "training Loss: 0.0002123635 Acc: 88.1313934326\n",
      "validation Loss: 0.0002128748 Acc: 89.0002899170\n",
      "training Loss: 0.0002124801 Acc: 88.0649566650\n",
      "validation Loss: 0.0002143322 Acc: 87.5531463623\n",
      "training Loss: 0.0002113537 Acc: 88.1775970459\n",
      "validation Loss: 0.0002134826 Acc: 88.9785461426\n",
      "training Loss: 0.0002109195 Acc: 88.2183685303\n",
      "validation Loss: 0.0002110814 Acc: 88.9386825562\n",
      "training Loss: 0.0002105739 Acc: 88.2256164551\n",
      "validation Loss: 0.0002096943 Acc: 88.1426315308\n",
      "training Loss: 0.0002099976 Acc: 88.1824264526\n",
      "validation Loss: 0.0002089843 Acc: 88.7200393677\n",
      "training Loss: 0.0002098554 Acc: 88.2799758911\n",
      "validation Loss: 0.0002080129 Acc: 87.8817138672\n",
      "training Loss: 0.0002092202 Acc: 88.2896347046\n",
      "validation Loss: 0.0002076219 Acc: 88.5219345093\n",
      "training Loss: 0.0002094625 Acc: 88.2588348389\n",
      "validation Loss: 0.0002074062 Acc: 88.7768173218\n",
      "training Loss: 0.0002081331 Acc: 88.4028854370\n",
      "validation Loss: 0.0002070449 Acc: 88.6076965332\n",
      "training Loss: 0.0002096148 Acc: 88.2857131958\n",
      "validation Loss: 0.0002052235 Acc: 88.9700851440\n",
      "training Loss: 0.0002094067 Acc: 88.3128890991\n",
      "validation Loss: 0.0002063898 Acc: 88.5654220581\n",
      "training Loss: 0.0002088754 Acc: 88.4056015015\n",
      "validation Loss: 0.0002067633 Acc: 88.0774002075\n",
      "training Loss: 0.0002086585 Acc: 88.3062438965\n",
      "validation Loss: 0.0002073507 Acc: 88.3781890869\n",
      "training Loss: 0.0002086925 Acc: 88.3125915527\n",
      "validation Loss: 0.0002079858 Acc: 88.5762939453\n",
      "training Loss: 0.0002041960 Acc: 88.5807571411\n",
      "validation Loss: 0.0002030832 Acc: 89.1959762573\n",
      "training Loss: 0.0002026069 Acc: 88.6586685181\n",
      "validation Loss: 0.0002034037 Acc: 88.1221008301\n",
      "training Loss: 0.0002032379 Acc: 88.6287765503\n",
      "validation Loss: 0.0002022862 Acc: 88.7200393677\n",
      "training Loss: 0.0002026109 Acc: 88.7109146118\n",
      "validation Loss: 0.0002023842 Acc: 88.6209869385\n",
      "training Loss: 0.0002029175 Acc: 88.6764907837\n",
      "validation Loss: 0.0002036964 Acc: 89.5607833862\n",
      "training Loss: 0.0002023425 Acc: 88.6668243408\n",
      "validation Loss: 0.0002041219 Acc: 89.1947708130\n",
      "training Loss: 0.0002024176 Acc: 88.6955184937\n",
      "validation Loss: 0.0002028313 Acc: 88.8408355713\n",
      "training Loss: 0.0001994606 Acc: 88.8809356689\n",
      "validation Loss: 0.0001991897 Acc: 88.9894180298\n",
      "training Loss: 0.0001991439 Acc: 88.8749008179\n",
      "validation Loss: 0.0002014450 Acc: 88.6282348633\n",
      "training Loss: 0.0001987728 Acc: 88.8779220581\n",
      "validation Loss: 0.0001994005 Acc: 89.0051193237\n",
      "training Loss: 0.0001984035 Acc: 88.8999633789\n",
      "validation Loss: 0.0001999947 Acc: 89.0836334229\n",
      "training Loss: 0.0001980685 Acc: 88.9630813599\n",
      "validation Loss: 0.0001995975 Acc: 89.0329055786\n",
      "training Loss: 0.0001971242 Acc: 88.9990158081\n",
      "validation Loss: 0.0001990278 Acc: 89.2225494385\n",
      "training Loss: 0.0001969628 Acc: 88.9966049194\n",
      "validation Loss: 0.0001990871 Acc: 88.6825942993\n",
      "training Loss: 0.0001960221 Acc: 89.0261993408\n",
      "validation Loss: 0.0001979310 Acc: 89.1706085205\n",
      "training Loss: 0.0001958811 Acc: 89.0286102295\n",
      "validation Loss: 0.0001993095 Acc: 88.4699935913\n",
      "training Loss: 0.0001962592 Acc: 89.0047531128\n",
      "validation Loss: 0.0001988163 Acc: 88.9797515869\n",
      "training Loss: 0.0001961160 Acc: 89.0053558350\n",
      "validation Loss: 0.0001986485 Acc: 88.7599029541\n",
      "training Loss: 0.0001960156 Acc: 89.0101928711\n",
      "validation Loss: 0.0001984446 Acc: 88.7901000977\n",
      "training Loss: 0.0001949733 Acc: 89.0485458374\n",
      "validation Loss: 0.0001978747 Acc: 89.2128906250\n",
      "training Loss: 0.0001946505 Acc: 89.1445770264\n",
      "validation Loss: 0.0001973968 Acc: 88.9725036621\n",
      "training Loss: 0.0001945768 Acc: 89.1210250854\n",
      "validation Loss: 0.0001974638 Acc: 89.0691375732\n",
      "training Loss: 0.0001946280 Acc: 89.1098480225\n",
      "validation Loss: 0.0001979558 Acc: 89.0800094604\n",
      "training Loss: 0.0001948310 Acc: 89.0998840332\n",
      "validation Loss: 0.0001980565 Acc: 89.3578414917\n",
      "training Loss: 0.0001942782 Acc: 89.1318969727\n",
      "validation Loss: 0.0001979327 Acc: 89.1694030762\n",
      "training Loss: 0.0001937654 Acc: 89.1237411499\n",
      "validation Loss: 0.0001975400 Acc: 89.0099487305\n",
      "training Loss: 0.0001940464 Acc: 89.1246490479\n",
      "validation Loss: 0.0001975611 Acc: 89.0473937988\n",
      "training Loss: 0.0001940945 Acc: 89.1334075928\n",
      "validation Loss: 0.0001972807 Acc: 89.1077957153\n",
      "training Loss: 0.0001937835 Acc: 89.0986785889\n",
      "validation Loss: 0.0001974584 Acc: 89.1307449341\n",
      "training Loss: 0.0001937662 Acc: 89.1826324463\n",
      "validation Loss: 0.0001977002 Acc: 88.9785461426\n",
      "training Loss: 0.0001939313 Acc: 89.1095504761\n",
      "validation Loss: 0.0001973463 Acc: 88.9350585938\n",
      "training Loss: 0.0001936398 Acc: 89.1578674316\n",
      "validation Loss: 0.0001977232 Acc: 89.0087432861\n",
      "training Loss: 0.0001936872 Acc: 89.1053161621\n",
      "validation Loss: 0.0001973021 Acc: 89.0123672485\n",
      "training Loss: 0.0001933621 Acc: 89.1500167847\n",
      "validation Loss: 0.0001970388 Acc: 89.1597366333\n",
      "training Loss: 0.0001932796 Acc: 89.1560516357\n",
      "validation Loss: 0.0001970996 Acc: 89.1367874146\n",
      "training Loss: 0.0001933482 Acc: 89.1793060303\n",
      "validation Loss: 0.0001971318 Acc: 89.2225494385\n",
      "training Loss: 0.0001934008 Acc: 89.1213226318\n",
      "validation Loss: 0.0001976487 Acc: 89.2394638062\n",
      "training Loss: 0.0001931726 Acc: 89.1796112061\n",
      "validation Loss: 0.0001970315 Acc: 89.1307449341\n",
      "training Loss: 0.0001933174 Acc: 89.1826324463\n",
      "validation Loss: 0.0001968752 Acc: 89.1379928589\n",
      "training Loss: 0.0001929258 Acc: 89.1814193726\n",
      "validation Loss: 0.0001966230 Acc: 89.1041717529\n",
      "training Loss: 0.0001925447 Acc: 89.2291336060\n",
      "validation Loss: 0.0001968510 Acc: 89.0691375732\n",
      "training Loss: 0.0001930145 Acc: 89.1814193726\n",
      "validation Loss: 0.0001969348 Acc: 88.9882049561\n",
      "training Loss: 0.0001934537 Acc: 89.1358184814\n",
      "validation Loss: 0.0001967330 Acc: 89.0111618042\n",
      "training Loss: 0.0001930300 Acc: 89.2233963013\n",
      "validation Loss: 0.0001969452 Acc: 89.0920944214\n",
      "training Loss: 0.0001929483 Acc: 89.2067871094\n",
      "validation Loss: 0.0001970122 Acc: 89.1222915649\n",
      "training Loss: 0.0001926783 Acc: 89.1654129028\n",
      "validation Loss: 0.0001973395 Acc: 89.1694030762\n",
      "training Loss: 0.0001928013 Acc: 89.2176589966\n",
      "validation Loss: 0.0001970896 Acc: 89.1355819702\n",
      "Early stopped.\n",
      "Best val acc: 89.560783\n",
      "----------\n",
      "training Loss: 0.0002843889 Acc: 82.3608932495\n",
      "validation Loss: 0.0002366804 Acc: 86.7966079712\n",
      "training Loss: 0.0002307903 Acc: 86.7969055176\n",
      "validation Loss: 0.0002241575 Acc: 88.0560913086\n",
      "training Loss: 0.0002233775 Acc: 87.3622207642\n",
      "validation Loss: 0.0002209391 Acc: 88.0306854248\n",
      "training Loss: 0.0002206903 Acc: 87.5143661499\n",
      "validation Loss: 0.0002173034 Acc: 87.5564117432\n",
      "training Loss: 0.0002196013 Acc: 87.5827255249\n",
      "validation Loss: 0.0002211252 Acc: 88.8509750366\n",
      "training Loss: 0.0002180068 Acc: 87.7009887695\n",
      "validation Loss: 0.0002168308 Acc: 88.7384567261\n",
      "training Loss: 0.0002163234 Acc: 87.7744903564\n",
      "validation Loss: 0.0002137189 Acc: 88.1407775879\n",
      "training Loss: 0.0002162371 Acc: 87.8083648682\n",
      "validation Loss: 0.0002112018 Acc: 88.2327346802\n",
      "training Loss: 0.0002135241 Acc: 88.0070877075\n",
      "validation Loss: 0.0002107299 Acc: 88.6065826416\n",
      "training Loss: 0.0002127268 Acc: 87.9465942383\n",
      "validation Loss: 0.0002112076 Acc: 86.7663574219\n",
      "training Loss: 0.0002128871 Acc: 87.9511337280\n",
      "validation Loss: 0.0002085239 Acc: 88.1625595093\n",
      "training Loss: 0.0002124282 Acc: 88.0170669556\n",
      "validation Loss: 0.0002086970 Acc: 87.7282104492\n",
      "training Loss: 0.0002125628 Acc: 87.9856109619\n",
      "validation Loss: 0.0002081501 Acc: 88.8872756958\n",
      "training Loss: 0.0002098191 Acc: 88.1556015015\n",
      "validation Loss: 0.0002066559 Acc: 88.5545578003\n",
      "training Loss: 0.0002101735 Acc: 88.2000656128\n",
      "validation Loss: 0.0002079804 Acc: 87.5733489990\n",
      "training Loss: 0.0002106665 Acc: 88.1075057983\n",
      "validation Loss: 0.0002102037 Acc: 89.3167800903\n",
      "training Loss: 0.0002107000 Acc: 88.1350326538\n",
      "validation Loss: 0.0002058094 Acc: 89.0385131836\n",
      "training Loss: 0.0002079720 Acc: 88.3485794067\n",
      "validation Loss: 0.0002083214 Acc: 87.2115936279\n",
      "training Loss: 0.0002078762 Acc: 88.2539062500\n",
      "validation Loss: 0.0002034724 Acc: 88.4069519043\n",
      "training Loss: 0.0002088099 Acc: 88.2345428467\n",
      "validation Loss: 0.0002038431 Acc: 88.8449325562\n",
      "training Loss: 0.0002075994 Acc: 88.2632827759\n",
      "validation Loss: 0.0002100948 Acc: 89.5962600708\n",
      "training Loss: 0.0002074430 Acc: 88.4254074097\n",
      "validation Loss: 0.0002029446 Acc: 88.0355224609\n",
      "training Loss: 0.0002073219 Acc: 88.3519058228\n",
      "validation Loss: 0.0002064548 Acc: 87.6652984619\n",
      "training Loss: 0.0002065081 Acc: 88.3966674805\n",
      "validation Loss: 0.0002030292 Acc: 88.9296188354\n",
      "training Loss: 0.0002058178 Acc: 88.4474868774\n",
      "validation Loss: 0.0002040927 Acc: 88.2266845703\n",
      "training Loss: 0.0002066465 Acc: 88.3939437866\n",
      "validation Loss: 0.0002045269 Acc: 89.2260437012\n",
      "training Loss: 0.0002027635 Acc: 88.5751266479\n",
      "validation Loss: 0.0002001423 Acc: 88.9005813599\n",
      "training Loss: 0.0002021071 Acc: 88.7148666382\n",
      "validation Loss: 0.0002013198 Acc: 89.3191986084\n",
      "training Loss: 0.0002018492 Acc: 88.5884323120\n",
      "validation Loss: 0.0001972318 Acc: 88.7989501953\n",
      "training Loss: 0.0002010035 Acc: 88.7215194702\n",
      "validation Loss: 0.0001988020 Acc: 88.6779632568\n",
      "training Loss: 0.0002014322 Acc: 88.6940002441\n",
      "validation Loss: 0.0001979499 Acc: 88.8703384399\n",
      "training Loss: 0.0002007536 Acc: 88.6821975708\n",
      "validation Loss: 0.0001987145 Acc: 88.7312011719\n",
      "training Loss: 0.0002004210 Acc: 88.6815948486\n",
      "validation Loss: 0.0001973106 Acc: 88.6682891846\n",
      "training Loss: 0.0001978532 Acc: 88.8627777100\n",
      "validation Loss: 0.0001957268 Acc: 88.5727081299\n",
      "training Loss: 0.0001971622 Acc: 88.8818283081\n",
      "validation Loss: 0.0001956914 Acc: 89.0905380249\n",
      "training Loss: 0.0001973380 Acc: 88.9144973755\n",
      "validation Loss: 0.0001954698 Acc: 89.0421371460\n",
      "training Loss: 0.0001971198 Acc: 88.8951416016\n",
      "validation Loss: 0.0001957895 Acc: 89.1473999023\n",
      "training Loss: 0.0001978829 Acc: 88.8615646362\n",
      "validation Loss: 0.0001971315 Acc: 89.2816925049\n",
      "training Loss: 0.0001972264 Acc: 88.9114761353\n",
      "validation Loss: 0.0001951516 Acc: 88.7154693604\n",
      "training Loss: 0.0001974741 Acc: 88.8769912720\n",
      "validation Loss: 0.0001965906 Acc: 88.5690765381\n",
      "training Loss: 0.0001972462 Acc: 88.8881835938\n",
      "validation Loss: 0.0001968787 Acc: 88.5763397217\n",
      "training Loss: 0.0001969954 Acc: 88.8918151855\n",
      "validation Loss: 0.0001946443 Acc: 88.9284133911\n",
      "training Loss: 0.0001966055 Acc: 88.9613800049\n",
      "validation Loss: 0.0001945515 Acc: 88.9405059814\n",
      "training Loss: 0.0001967194 Acc: 88.8954391479\n",
      "validation Loss: 0.0001950017 Acc: 88.9598693848\n",
      "training Loss: 0.0001968564 Acc: 88.8775939941\n",
      "validation Loss: 0.0001956255 Acc: 89.0494003296\n",
      "training Loss: 0.0001963157 Acc: 88.9889068604\n",
      "validation Loss: 0.0001960287 Acc: 88.3077468872\n",
      "training Loss: 0.0001964011 Acc: 88.9220581055\n",
      "validation Loss: 0.0001952426 Acc: 89.2526550293\n",
      "training Loss: 0.0001945699 Acc: 89.0484924316\n",
      "validation Loss: 0.0001937732 Acc: 88.9997940063\n",
      "training Loss: 0.0001943612 Acc: 89.0590744019\n",
      "validation Loss: 0.0001936835 Acc: 89.0276184082\n",
      "training Loss: 0.0001937823 Acc: 89.0291366577\n",
      "validation Loss: 0.0001930846 Acc: 89.0651245117\n",
      "training Loss: 0.0001937819 Acc: 89.0563583374\n",
      "validation Loss: 0.0001941691 Acc: 88.9417190552\n",
      "training Loss: 0.0001938774 Acc: 89.0255050659\n",
      "validation Loss: 0.0001941323 Acc: 89.2006301880\n",
      "training Loss: 0.0001943202 Acc: 89.0442581177\n",
      "validation Loss: 0.0001935867 Acc: 88.9622879028\n",
      "training Loss: 0.0001938889 Acc: 89.0348815918\n",
      "validation Loss: 0.0001935549 Acc: 89.0990066528\n",
      "training Loss: 0.0001927682 Acc: 89.0983963013\n",
      "validation Loss: 0.0001928371 Acc: 88.9393005371\n",
      "training Loss: 0.0001925365 Acc: 89.1204757690\n",
      "validation Loss: 0.0001924917 Acc: 89.0312500000\n",
      "training Loss: 0.0001925851 Acc: 89.1316680908\n",
      "validation Loss: 0.0001928403 Acc: 89.1425552368\n",
      "training Loss: 0.0001927098 Acc: 89.0993041992\n",
      "validation Loss: 0.0001922910 Acc: 89.1026306152\n",
      "training Loss: 0.0001923238 Acc: 89.1428604126\n",
      "validation Loss: 0.0001918109 Acc: 89.1449813843\n",
      "training Loss: 0.0001922427 Acc: 89.1555633545\n",
      "validation Loss: 0.0001924309 Acc: 89.1909561157\n",
      "training Loss: 0.0001920605 Acc: 89.1416549683\n",
      "validation Loss: 0.0001922860 Acc: 89.0856933594\n",
      "training Loss: 0.0001920064 Acc: 89.1682662964\n",
      "validation Loss: 0.0001924579 Acc: 89.0735931396\n",
      "training Loss: 0.0001918250 Acc: 89.1437683105\n",
      "validation Loss: 0.0001923448 Acc: 89.1764373779\n",
      "training Loss: 0.0001914265 Acc: 89.1864166260\n",
      "validation Loss: 0.0001920880 Acc: 89.2768554688\n",
      "training Loss: 0.0001918351 Acc: 89.1470947266\n",
      "validation Loss: 0.0001926782 Acc: 89.0288314819\n",
      "training Loss: 0.0001914563 Acc: 89.1855087280\n",
      "validation Loss: 0.0001921891 Acc: 89.1510314941\n",
      "Early stopped.\n",
      "Best val acc: 89.596260\n",
      "----------\n",
      "training Loss: 0.0002822810 Acc: 82.3666076660\n",
      "validation Loss: 0.0002292944 Acc: 87.1049575806\n",
      "training Loss: 0.0002307478 Acc: 86.8109817505\n",
      "validation Loss: 0.0002249513 Acc: 87.5690307617\n",
      "training Loss: 0.0002252479 Acc: 87.1949920654\n",
      "validation Loss: 0.0002221406 Acc: 87.3176574707\n",
      "training Loss: 0.0002236804 Acc: 87.3729476929\n",
      "validation Loss: 0.0002166607 Acc: 88.0778274536\n",
      "training Loss: 0.0002197723 Acc: 87.5799102783\n",
      "validation Loss: 0.0002272586 Acc: 86.2082290649\n",
      "training Loss: 0.0002199594 Acc: 87.6167678833\n",
      "validation Loss: 0.0002147466 Acc: 87.6113357544\n",
      "training Loss: 0.0002180725 Acc: 87.6959304810\n",
      "validation Loss: 0.0002152329 Acc: 88.7848205566\n",
      "training Loss: 0.0002165001 Acc: 87.7663269043\n",
      "validation Loss: 0.0002108577 Acc: 88.6337509155\n",
      "training Loss: 0.0002165338 Acc: 87.8158798218\n",
      "validation Loss: 0.0002129378 Acc: 87.6645050049\n",
      "training Loss: 0.0002144259 Acc: 87.9252471924\n",
      "validation Loss: 0.0002122887 Acc: 88.5310287476\n",
      "training Loss: 0.0002139814 Acc: 87.8636169434\n",
      "validation Loss: 0.0002069481 Acc: 88.3304061890\n",
      "training Loss: 0.0002120705 Acc: 88.0482177734\n",
      "validation Loss: 0.0002104170 Acc: 89.2319717407\n",
      "training Loss: 0.0002121717 Acc: 88.0603027344\n",
      "validation Loss: 0.0002064852 Acc: 88.8222808838\n",
      "training Loss: 0.0002120933 Acc: 88.1044158936\n",
      "validation Loss: 0.0002073627 Acc: 87.8518295288\n",
      "training Loss: 0.0002108651 Acc: 88.0799407959\n",
      "validation Loss: 0.0002077931 Acc: 88.9515914917\n",
      "training Loss: 0.0002108313 Acc: 88.1865921021\n",
      "validation Loss: 0.0002066613 Acc: 88.7183456421\n",
      "training Loss: 0.0002101110 Acc: 88.1769256592\n",
      "validation Loss: 0.0002063422 Acc: 88.4162139893\n",
      "training Loss: 0.0002094670 Acc: 88.1805496216\n",
      "validation Loss: 0.0002065248 Acc: 88.7509765625\n",
      "training Loss: 0.0002091060 Acc: 88.2826690674\n",
      "validation Loss: 0.0002056332 Acc: 88.0343170166\n",
      "training Loss: 0.0002100968 Acc: 88.2307052612\n",
      "validation Loss: 0.0002043620 Acc: 88.3461227417\n",
      "training Loss: 0.0002099367 Acc: 88.2711944580\n",
      "validation Loss: 0.0002041257 Acc: 88.9576339722\n",
      "training Loss: 0.0002085733 Acc: 88.3246688843\n",
      "validation Loss: 0.0002053420 Acc: 89.1485824585\n",
      "training Loss: 0.0002086770 Acc: 88.3349380493\n",
      "validation Loss: 0.0002066954 Acc: 89.6404571533\n",
      "training Loss: 0.0002078557 Acc: 88.3687820435\n",
      "validation Loss: 0.0002020951 Acc: 89.1437530518\n",
      "training Loss: 0.0002081741 Acc: 88.3252716064\n",
      "validation Loss: 0.0002041966 Acc: 88.8222808838\n",
      "training Loss: 0.0002077850 Acc: 88.3431015015\n",
      "validation Loss: 0.0002018540 Acc: 89.0241088867\n",
      "training Loss: 0.0002074315 Acc: 88.3811645508\n",
      "validation Loss: 0.0002047137 Acc: 88.2639389038\n",
      "training Loss: 0.0002069158 Acc: 88.4035263062\n",
      "validation Loss: 0.0002018369 Acc: 88.9515914917\n",
      "training Loss: 0.0002069772 Acc: 88.3962707520\n",
      "validation Loss: 0.0002013908 Acc: 88.5697021484\n",
      "training Loss: 0.0002075911 Acc: 88.3591156006\n",
      "validation Loss: 0.0002031897 Acc: 89.2730636597\n",
      "training Loss: 0.0002071637 Acc: 88.4144058228\n",
      "validation Loss: 0.0002026832 Acc: 88.6724243164\n",
      "training Loss: 0.0002066558 Acc: 88.4582138062\n",
      "validation Loss: 0.0002022524 Acc: 88.6313323975\n",
      "training Loss: 0.0002064645 Acc: 88.4458236694\n",
      "validation Loss: 0.0002088059 Acc: 89.2392272949\n",
      "training Loss: 0.0002031660 Acc: 88.6340560913\n",
      "validation Loss: 0.0001986735 Acc: 89.0216903687\n",
      "training Loss: 0.0002010317 Acc: 88.8032455444\n",
      "validation Loss: 0.0001994749 Acc: 89.4374237061\n",
      "training Loss: 0.0002007865 Acc: 88.7210693359\n",
      "validation Loss: 0.0001973294 Acc: 89.2198867798\n",
      "training Loss: 0.0002013067 Acc: 88.6999206543\n",
      "validation Loss: 0.0001974536 Acc: 89.1014556885\n",
      "training Loss: 0.0002007498 Acc: 88.8192596436\n",
      "validation Loss: 0.0001972634 Acc: 88.8754577637\n",
      "training Loss: 0.0002004006 Acc: 88.7322463989\n",
      "validation Loss: 0.0001976847 Acc: 89.0132293701\n",
      "training Loss: 0.0002006979 Acc: 88.7416152954\n",
      "validation Loss: 0.0001986241 Acc: 88.7026367188\n",
      "training Loss: 0.0002011224 Acc: 88.7201614380\n",
      "validation Loss: 0.0001976732 Acc: 89.1896743774\n",
      "training Loss: 0.0002005780 Acc: 88.7769622803\n",
      "validation Loss: 0.0001983980 Acc: 88.8331604004\n",
      "training Loss: 0.0001974798 Acc: 88.8899612427\n",
      "validation Loss: 0.0001968981 Acc: 88.8633728027\n",
      "training Loss: 0.0001972303 Acc: 88.8588409424\n",
      "validation Loss: 0.0001948387 Acc: 89.1655044556\n",
      "training Loss: 0.0001972976 Acc: 88.9355850220\n",
      "validation Loss: 0.0001975706 Acc: 88.4464263916\n",
      "training Loss: 0.0001968120 Acc: 88.9092941284\n",
      "validation Loss: 0.0001961779 Acc: 89.0978240967\n",
      "training Loss: 0.0001970731 Acc: 88.9416275024\n",
      "validation Loss: 0.0001953633 Acc: 89.4108352661\n",
      "training Loss: 0.0001966551 Acc: 88.9857330322\n",
      "validation Loss: 0.0001952533 Acc: 89.2899856567\n",
      "training Loss: 0.0001956752 Acc: 88.9660949707\n",
      "validation Loss: 0.0001938346 Acc: 89.0712432861\n",
      "training Loss: 0.0001948582 Acc: 89.0434417725\n",
      "validation Loss: 0.0001935453 Acc: 89.0301513672\n",
      "training Loss: 0.0001949919 Acc: 88.9673080444\n",
      "validation Loss: 0.0001938568 Acc: 89.3757934570\n",
      "training Loss: 0.0001945504 Acc: 89.0355911255\n",
      "validation Loss: 0.0001938066 Acc: 89.3395309448\n",
      "training Loss: 0.0001946945 Acc: 89.0688247681\n",
      "validation Loss: 0.0001938116 Acc: 89.4362182617\n",
      "training Loss: 0.0001942944 Acc: 89.0736541748\n",
      "validation Loss: 0.0001934800 Acc: 89.1449584961\n",
      "training Loss: 0.0001944421 Acc: 89.0470657349\n",
      "validation Loss: 0.0001935531 Acc: 89.5208129883\n",
      "training Loss: 0.0001937527 Acc: 89.0742645264\n",
      "validation Loss: 0.0001940985 Acc: 88.9395065308\n",
      "training Loss: 0.0001943045 Acc: 89.0494842529\n",
      "validation Loss: 0.0001929978 Acc: 89.1932983398\n",
      "training Loss: 0.0001938392 Acc: 89.1331787109\n",
      "validation Loss: 0.0001942834 Acc: 89.2041778564\n",
      "training Loss: 0.0001941328 Acc: 89.0636901855\n",
      "validation Loss: 0.0001933697 Acc: 89.1123275757\n",
      "training Loss: 0.0001938640 Acc: 89.1241149902\n",
      "validation Loss: 0.0001933584 Acc: 89.0700302124\n",
      "training Loss: 0.0001939154 Acc: 89.0416336060\n",
      "validation Loss: 0.0001934453 Acc: 89.3334884644\n",
      "training Loss: 0.0001930599 Acc: 89.1029663086\n",
      "validation Loss: 0.0001927791 Acc: 88.8947906494\n",
      "training Loss: 0.0001928464 Acc: 89.1017532349\n",
      "validation Loss: 0.0001927491 Acc: 89.3818359375\n",
      "training Loss: 0.0001926351 Acc: 89.1482849121\n",
      "validation Loss: 0.0001925820 Acc: 89.3769989014\n",
      "training Loss: 0.0001924782 Acc: 89.1313629150\n",
      "validation Loss: 0.0001934052 Acc: 89.0664062500\n",
      "training Loss: 0.0001923420 Acc: 89.1268310547\n",
      "validation Loss: 0.0001935859 Acc: 89.2948150635\n",
      "training Loss: 0.0001929238 Acc: 89.1724548340\n",
      "validation Loss: 0.0001928280 Acc: 89.3032760620\n",
      "training Loss: 0.0001921077 Acc: 89.2050857544\n",
      "validation Loss: 0.0001929832 Acc: 89.1087036133\n",
      "training Loss: 0.0001921071 Acc: 89.1449584961\n",
      "validation Loss: 0.0001923586 Acc: 89.2573547363\n",
      "training Loss: 0.0001919945 Acc: 89.1775894165\n",
      "validation Loss: 0.0001921370 Acc: 89.3467864990\n",
      "training Loss: 0.0001917716 Acc: 89.1700363159\n",
      "validation Loss: 0.0001920521 Acc: 89.3286590576\n",
      "training Loss: 0.0001917784 Acc: 89.1797027588\n",
      "validation Loss: 0.0001925550 Acc: 89.1099090576\n",
      "training Loss: 0.0001917248 Acc: 89.2026672363\n",
      "validation Loss: 0.0001921959 Acc: 89.1920928955\n",
      "training Loss: 0.0001920072 Acc: 89.1413345337\n",
      "validation Loss: 0.0001923651 Acc: 89.2464752197\n",
      "training Loss: 0.0001918332 Acc: 89.1830291748\n",
      "validation Loss: 0.0001922235 Acc: 89.2597732544\n",
      "training Loss: 0.0001913745 Acc: 89.2075042725\n",
      "validation Loss: 0.0001922695 Acc: 89.3492050171\n",
      "training Loss: 0.0001913698 Acc: 89.2047805786\n",
      "validation Loss: 0.0001918855 Acc: 89.3274459839\n",
      "training Loss: 0.0001913245 Acc: 89.1939086914\n",
      "validation Loss: 0.0001921006 Acc: 89.2779006958\n",
      "training Loss: 0.0001909280 Acc: 89.1932983398\n",
      "validation Loss: 0.0001920370 Acc: 89.2561416626\n",
      "training Loss: 0.0001911969 Acc: 89.2138442993\n",
      "validation Loss: 0.0001926544 Acc: 89.3649139404\n",
      "training Loss: 0.0001914573 Acc: 89.2020645142\n",
      "validation Loss: 0.0001917915 Acc: 89.3721618652\n",
      "training Loss: 0.0001915202 Acc: 89.2035751343\n",
      "validation Loss: 0.0001926478 Acc: 89.3165740967\n",
      "training Loss: 0.0001911306 Acc: 89.2195892334\n",
      "validation Loss: 0.0001922127 Acc: 89.2440567017\n",
      "training Loss: 0.0001915328 Acc: 89.1733627319\n",
      "validation Loss: 0.0001917374 Acc: 89.3721618652\n",
      "training Loss: 0.0001911999 Acc: 89.2498016357\n",
      "validation Loss: 0.0001920570 Acc: 89.3214035034\n",
      "training Loss: 0.0001909554 Acc: 89.2455673218\n",
      "validation Loss: 0.0001915908 Acc: 89.1751708984\n",
      "training Loss: 0.0001910823 Acc: 89.1842346191\n",
      "validation Loss: 0.0001916388 Acc: 89.3419494629\n",
      "training Loss: 0.0001905413 Acc: 89.2346954346\n",
      "validation Loss: 0.0001922741 Acc: 89.0845336914\n",
      "training Loss: 0.0001908461 Acc: 89.2141494751\n",
      "validation Loss: 0.0001922836 Acc: 89.3395309448\n",
      "training Loss: 0.0001911057 Acc: 89.2639999390\n",
      "validation Loss: 0.0001922532 Acc: 89.2863616943\n",
      "training Loss: 0.0001905222 Acc: 89.2525177002\n",
      "validation Loss: 0.0001917359 Acc: 89.1739654541\n",
      "training Loss: 0.0001908992 Acc: 89.1960220337\n",
      "validation Loss: 0.0001918119 Acc: 89.2827301025\n",
      "training Loss: 0.0001907783 Acc: 89.2458724976\n",
      "validation Loss: 0.0001916620 Acc: 89.3310775757\n",
      "Early stopped.\n",
      "Best val acc: 89.640457\n",
      "----------\n",
      "training Loss: 0.0002907188 Acc: 81.4620666504\n",
      "validation Loss: 0.0002394029 Acc: 86.0902252197\n",
      "training Loss: 0.0002318262 Acc: 86.6445083618\n",
      "validation Loss: 0.0002240220 Acc: 87.2908020020\n",
      "training Loss: 0.0002247979 Acc: 87.2900543213\n",
      "validation Loss: 0.0002217324 Acc: 86.5137481689\n",
      "training Loss: 0.0002213997 Acc: 87.4547576904\n",
      "validation Loss: 0.0002191723 Acc: 87.7637939453\n",
      "training Loss: 0.0002187542 Acc: 87.6770782471\n",
      "validation Loss: 0.0002204711 Acc: 85.6534347534\n",
      "training Loss: 0.0002171429 Acc: 87.7603302002\n",
      "validation Loss: 0.0002151525 Acc: 87.6021041870\n",
      "training Loss: 0.0002150966 Acc: 87.8079910278\n",
      "validation Loss: 0.0002143566 Acc: 87.5248794556\n",
      "training Loss: 0.0002135586 Acc: 87.9440383911\n",
      "validation Loss: 0.0002153079 Acc: 89.1164016724\n",
      "training Loss: 0.0002139150 Acc: 87.8764724731\n",
      "validation Loss: 0.0002075826 Acc: 87.9097900391\n",
      "training Loss: 0.0002113924 Acc: 88.0167388916\n",
      "validation Loss: 0.0002074362 Acc: 87.9797744751\n",
      "training Loss: 0.0002111319 Acc: 88.1027069092\n",
      "validation Loss: 0.0002096347 Acc: 87.9254760742\n",
      "training Loss: 0.0002106433 Acc: 88.1838531494\n",
      "validation Loss: 0.0002053642 Acc: 88.0352783203\n",
      "training Loss: 0.0002100780 Acc: 88.1464538574\n",
      "validation Loss: 0.0002078408 Acc: 89.3842620850\n",
      "training Loss: 0.0002104715 Acc: 88.1156845093\n",
      "validation Loss: 0.0002064751 Acc: 88.2174758911\n",
      "training Loss: 0.0002083486 Acc: 88.2764663696\n",
      "validation Loss: 0.0002047178 Acc: 87.8784179688\n",
      "training Loss: 0.0002083564 Acc: 88.3895874023\n",
      "validation Loss: 0.0002037505 Acc: 88.4238052368\n",
      "training Loss: 0.0002080965 Acc: 88.2532348633\n",
      "validation Loss: 0.0002051008 Acc: 89.0162506104\n",
      "training Loss: 0.0002080868 Acc: 88.3295593262\n",
      "validation Loss: 0.0002067613 Acc: 88.8461151123\n",
      "training Loss: 0.0002075842 Acc: 88.3482589722\n",
      "validation Loss: 0.0002036236 Acc: 88.9281692505\n",
      "training Loss: 0.0002074322 Acc: 88.3835525513\n",
      "validation Loss: 0.0002028729 Acc: 88.4817199707\n",
      "training Loss: 0.0002064714 Acc: 88.4179382324\n",
      "validation Loss: 0.0002032190 Acc: 88.6747817993\n",
      "training Loss: 0.0002063826 Acc: 88.4272918701\n",
      "validation Loss: 0.0002055969 Acc: 88.7857894897\n",
      "training Loss: 0.0002065620 Acc: 88.4384536743\n",
      "validation Loss: 0.0002037212 Acc: 89.0331420898\n",
      "training Loss: 0.0002072271 Acc: 88.4025573730\n",
      "validation Loss: 0.0002039397 Acc: 87.5309143066\n",
      "training Loss: 0.0002021279 Acc: 88.7036056519\n",
      "validation Loss: 0.0001996522 Acc: 89.0053939819\n",
      "training Loss: 0.0002008128 Acc: 88.7331695557\n",
      "validation Loss: 0.0001973778 Acc: 89.1791381836\n",
      "training Loss: 0.0002009226 Acc: 88.7011947632\n",
      "validation Loss: 0.0001986658 Acc: 89.6605758667\n",
      "training Loss: 0.0002008037 Acc: 88.7033081055\n",
      "validation Loss: 0.0001977372 Acc: 88.9450607300\n",
      "training Loss: 0.0002003311 Acc: 88.7760086060\n",
      "validation Loss: 0.0001968240 Acc: 88.3646850586\n",
      "training Loss: 0.0002000501 Acc: 88.7645416260\n",
      "validation Loss: 0.0001978509 Acc: 89.4023666382\n",
      "training Loss: 0.0002003770 Acc: 88.7856597900\n",
      "validation Loss: 0.0001973279 Acc: 89.2479171753\n",
      "training Loss: 0.0001996173 Acc: 88.7865600586\n",
      "validation Loss: 0.0001968582 Acc: 89.2093048096\n",
      "training Loss: 0.0001997170 Acc: 88.7754058838\n",
      "validation Loss: 0.0001972836 Acc: 88.9643630981\n",
      "training Loss: 0.0001972882 Acc: 88.8809814453\n",
      "validation Loss: 0.0001945403 Acc: 89.1429443359\n",
      "training Loss: 0.0001965385 Acc: 88.9367904663\n",
      "validation Loss: 0.0001944166 Acc: 89.0488281250\n",
      "training Loss: 0.0001962897 Acc: 88.9310531616\n",
      "validation Loss: 0.0001943583 Acc: 88.7797546387\n",
      "training Loss: 0.0001962551 Acc: 88.9539794922\n",
      "validation Loss: 0.0001944179 Acc: 89.2937698364\n",
      "training Loss: 0.0001957380 Acc: 88.9714813232\n",
      "validation Loss: 0.0001948417 Acc: 89.2479171753\n",
      "training Loss: 0.0001959208 Acc: 88.9799270630\n",
      "validation Loss: 0.0001945318 Acc: 88.9209289551\n",
      "training Loss: 0.0001960479 Acc: 88.9639358521\n",
      "validation Loss: 0.0001948711 Acc: 88.6687469482\n",
      "training Loss: 0.0001944237 Acc: 89.0345230103\n",
      "validation Loss: 0.0001928770 Acc: 89.2575683594\n",
      "training Loss: 0.0001941764 Acc: 89.0776596069\n",
      "validation Loss: 0.0001927037 Acc: 89.3070449829\n",
      "training Loss: 0.0001940069 Acc: 89.0725326538\n",
      "validation Loss: 0.0001928757 Acc: 89.0789947510\n",
      "training Loss: 0.0001940962 Acc: 89.1033020020\n",
      "validation Loss: 0.0001927404 Acc: 88.9281692505\n",
      "training Loss: 0.0001936317 Acc: 89.0683135986\n",
      "validation Loss: 0.0001924945 Acc: 89.0657196045\n",
      "training Loss: 0.0001935710 Acc: 89.0290908813\n",
      "validation Loss: 0.0001924326 Acc: 89.3951263428\n",
      "training Loss: 0.0001935178 Acc: 89.0357284546\n",
      "validation Loss: 0.0001930010 Acc: 89.2165451050\n",
      "training Loss: 0.0001931068 Acc: 89.0782623291\n",
      "validation Loss: 0.0001929470 Acc: 89.3359985352\n",
      "training Loss: 0.0001930544 Acc: 89.1117477417\n",
      "validation Loss: 0.0001925168 Acc: 89.2780838013\n",
      "training Loss: 0.0001930965 Acc: 89.1026992798\n",
      "validation Loss: 0.0001926171 Acc: 89.0464172363\n",
      "training Loss: 0.0001924490 Acc: 89.0773620605\n",
      "validation Loss: 0.0001917276 Acc: 89.2744674683\n",
      "training Loss: 0.0001918314 Acc: 89.1488494873\n",
      "validation Loss: 0.0001914506 Acc: 89.3637542725\n",
      "training Loss: 0.0001919131 Acc: 89.1805267334\n",
      "validation Loss: 0.0001919706 Acc: 89.1815567017\n",
      "training Loss: 0.0001917900 Acc: 89.1702728271\n",
      "validation Loss: 0.0001913868 Acc: 89.3239364624\n",
      "training Loss: 0.0001917847 Acc: 89.2103881836\n",
      "validation Loss: 0.0001921789 Acc: 89.0391769409\n",
      "training Loss: 0.0001918911 Acc: 89.1753997803\n",
      "validation Loss: 0.0001921495 Acc: 89.1176071167\n",
      "training Loss: 0.0001917251 Acc: 89.1645355225\n",
      "validation Loss: 0.0001914822 Acc: 89.4301147461\n",
      "training Loss: 0.0001920847 Acc: 89.2025451660\n",
      "validation Loss: 0.0001910801 Acc: 89.3215179443\n",
      "training Loss: 0.0001918418 Acc: 89.1645355225\n",
      "validation Loss: 0.0001912757 Acc: 89.4023666382\n",
      "training Loss: 0.0001911966 Acc: 89.1925888062\n",
      "validation Loss: 0.0001911747 Acc: 89.1417388916\n",
      "training Loss: 0.0001914342 Acc: 89.2299957275\n",
      "validation Loss: 0.0001915638 Acc: 89.1767272949\n",
      "training Loss: 0.0001917139 Acc: 89.1669540405\n",
      "validation Loss: 0.0001908467 Acc: 89.2334365845\n",
      "training Loss: 0.0001912948 Acc: 89.2303009033\n",
      "validation Loss: 0.0001908036 Acc: 89.3191070557\n",
      "training Loss: 0.0001914232 Acc: 89.1992263794\n",
      "validation Loss: 0.0001909455 Acc: 89.3842620850\n",
      "training Loss: 0.0001916278 Acc: 89.1455307007\n",
      "validation Loss: 0.0001914081 Acc: 89.5363006592\n",
      "training Loss: 0.0001910393 Acc: 89.2185363770\n",
      "validation Loss: 0.0001908803 Acc: 89.3106613159\n",
      "training Loss: 0.0001906479 Acc: 89.2115936279\n",
      "validation Loss: 0.0001911712 Acc: 89.1103668213\n",
      "training Loss: 0.0001908047 Acc: 89.2833938599\n",
      "validation Loss: 0.0001909872 Acc: 89.2599868774\n",
      "training Loss: 0.0001907675 Acc: 89.2441787720\n",
      "validation Loss: 0.0001909298 Acc: 89.2370605469\n",
      "training Loss: 0.0001905382 Acc: 89.2535247803\n",
      "validation Loss: 0.0001909455 Acc: 89.3589248657\n",
      "Early stopped.\n",
      "Best val acc: 89.660576\n",
      "----------\n",
      "Average best_acc across k-fold: 89.60163879394531\n"
     ]
    }
   ],
   "source": [
    "if 'CURRENT_TIME' in globals():\n",
    "    OUTPUT_DIRPATH, OLD_TIME = os.path.split(OUTPUT_DIRPATH)\n",
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "OUTPUT_DIRPATH = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME)\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json')\n",
    "    best_conf = optimize_hyperparams_RR(\n",
    "        data_list_dict, data_hlf_dict, label_dict, {f'fold_{fold}': training_weights(data_aux_dict[f'fold_{fold}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold}']) for fold in range(len(data_list_dict))},\n",
    "        config_file, NUM_EPOCHS=30, SEED=SEED\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "    # with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    # config_file = os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json')\n",
    "    # with open(config_file, 'r') as f:\n",
    "        best_conf = json.load(f)\n",
    "        print(best_conf)\n",
    "\n",
    "fom = []\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx in range(len(data_hlf_dict)):\n",
    "    weight = training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}'])\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], weight,\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    model_file = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME + '_ttH_Killer_IN_model_'+ f'{fold_idx}.torch')\n",
    "    # state_file = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME + '_ttH_Killer_IN_performance_'+ f'{fold_idx}.torch')\n",
    "    \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], \n",
    "        dnn_input=np.shape(data_hlf_dict[f'fold_{fold_idx}'])[-1], rnn_input=np.shape(data_list_dict[f'fold_{fold_idx}'])[-1],\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(train_data_list, train_data_hlf, train_label, train_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(val_data_list, val_data_hlf, val_label, val_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        NUM_EPOCHS, model, optimizer, scheduler, \n",
    "        model_filename=model_file, data_loader=data_loader, \n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "# with open(os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json'), 'r') as f:\n",
    "    best_conf = json.load(f)\n",
    "    print(best_conf)\n",
    "# weight_test_dict = {\n",
    "#     f'fold_{fold_idx}': copy.deepcopy(training_weights(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_test_dict[f'fold_{fold_idx}'])) for fold_idx in range(len(data_test_aux_dict))\n",
    "# }  # DO NOT USE SCALED FOR TRAINING!!\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, \n",
    "        save=True, dict_lists=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1626  |       0.9706      |    0.2982 +/- 0.0104     |\n",
      "|   0.2480  |       0.9500      |    0.2144 +/- 0.0103     |\n",
      "|   0.3617  |       0.9198      |    0.1465 +/- 0.0091     |\n",
      "|   0.7642  |       0.7538      |    0.0343 +/- 0.0030     |\n",
      "|   0.9193  |       0.5777      |    0.0099 +/- 0.0011     |\n",
      "|   0.9751  |       0.3839      |    0.0024 +/- 0.0003     |\n",
      "+-----------+-------------------+--------------------------+\n",
      "============================================================\n",
      "\n",
      "==============================0_lepton==============================\n",
      "0_lepton\n",
      "==============================1_lepton==============================\n",
      "1_lepton\n",
      "==============================2+_lepton==============================\n",
      "2+_lepton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    }
   ],
   "source": [
    "# with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf.json'), 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "    NNtable.add_row(\n",
    "        [\n",
    "            IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "            \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "        ]\n",
    "    )\n",
    "print(NNtable)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = os.path.join(OUTPUT_DIRPATH, f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}/test_data_performance')\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [\n",
    "            (data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) \n",
    "            for i in range(len(data_test_aux_dict))\n",
    "        ]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "    print(plot_type)\n",
    "\n",
    "    weights_plot = {\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1) & mask_arr[fold_idx]],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0) & mask_arr[fold_idx]],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    density_weights_plot = {\n",
    "        fold_idx: {'sig': None, 'bkg': None} for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "\n",
    "    plot_train_val_losses(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME,\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    "    )\n",
    "    plot_roc(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], run2=False,\n",
    "        mask=None\n",
    "    )\n",
    "    plot_roc(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_sum', method='round_robin_sum',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], run2=False,\n",
    "        mask=None\n",
    "    )\n",
    "    # print(f\"num bkg: {np.sum(label_test==0)}\")\n",
    "    # print(f\"num sig: {np.sum(label_test==1)}\")\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, n_bins=25, \n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], n_bins=25, weights=density_weights_plot,\n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted_sum', method='round_robin_sum',\n",
    "        weights=weights_plot, n_bins=25, \n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density_sum', method='round_robin_sum', \n",
    "        n_bins=25, weights=density_weights_plot,\n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, n_bins=25,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin_sum', \n",
    "        weights=weights_plot, n_bins=25,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    # for fold_idx in range(len(data_test_aux_dict)):\n",
    "    #     for score_cut in [0.2, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    #         plot_input_vars_after_score_cut(\n",
    "    #             IN_perf, score_cut, plot_destdir, fold_idx, method='round_robin', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #             mask=mask_arr[fold_idx]\n",
    "    #         )\n",
    "    #     plot_input_vars_after_score_cut(\n",
    "    #         IN_perf, [0.2, 0.6, 0.9], plot_destdir, fold_idx, method='round_robin', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #         mask=mask_arr[fold_idx]\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimized cut-boundaries for ttH score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(IN_perf, weights, method='arr', bins=50, mask=None, n_folds=5):\n",
    "    if method == 'round_robin':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "    elif method == 'round_robin_sum':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_perf['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "\n",
    "        flat_preds = np.concatenate([\n",
    "            np.exp(IN_perf['all_preds'][i])[mask[i]][:, 1] for i in range(len(IN_perf['all_preds']))\n",
    "        ], axis=None)\n",
    "        flat_labels = np.concatenate([\n",
    "            np.array(IN_perf['all_labels'][i])[mask[i]] for i in range(len(IN_perf['all_preds']))\n",
    "        ], axis=None)\n",
    "        sig_preds = flat_preds[flat_labels == 1]\n",
    "        bkg_preds = flat_preds[flat_labels == 0]\n",
    "        \n",
    "        sig_weight = np.concatenate([\n",
    "            weight['sig'] for weight in weights.values()\n",
    "        ], axis=None)\n",
    "        bkg_weight = np.concatenate([\n",
    "            weight['bkg'] for weight in weights.values()\n",
    "        ], axis=None)\n",
    "        flat_weights = {'sig': sig_weight, 'bkg': bkg_weight}\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_preds, weight=flat_weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_preds, weight=flat_weights['bkg'])\n",
    "            \n",
    "        \n",
    "        cut_boundaries = []\n",
    "        cut_s_over_root_bs = []\n",
    "        prev_s_over_root_b = 0\n",
    "        sig_weights = []\n",
    "        bkg_weights = []\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                cut_boundaries.append(bins - i)\n",
    "                cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        cut_boundaries.append(0)\n",
    "        cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "        cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "        return cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights\n",
    "    elif method == 'arr':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "                    \n",
    "        sig_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "\n",
    "        cut_boundaries = []\n",
    "        cut_s_over_root_bs = []\n",
    "        prev_s_over_root_b = 0\n",
    "        sig_weights = []\n",
    "        bkg_weights = []\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                cut_boundaries.append(bins - i)\n",
    "                cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        cut_boundaries.append(0)\n",
    "        cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "        cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold, cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "==============================0_lepton==============================\n",
      "==============================1_lepton==============================\n",
      "==============================2+_lepton==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2426169/1388588532.py:37: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:458: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2426169/2416538179.py:495: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    }
   ],
   "source": [
    "# with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf.json'), 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = os.path.join(OUTPUT_DIRPATH, f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}')\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [(data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "\n",
    "    weights_plot = {\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1) & mask_arr[fold_idx]],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0) & mask_arr[fold_idx]],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    \n",
    "    (\n",
    "        cut_boundaries_fold, cut_s_over_root_bs_fold, \n",
    "        sig_weights_fold, bkg_weights_fold\n",
    "    ) = optimize_cut_boundaries(\n",
    "        IN_perf, weights_plot, method='round_robin',\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    fold_labels = [\n",
    "        [\n",
    "            f\"s/âb={cut_s_over_root_bs_fold[fold_idx][cut_idx]:.04f}, s={sig_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{sig_weights_fold[fold_idx][cut_idx]['w2']:.04f}, b={bkg_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{bkg_weights_fold[fold_idx][cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[fold_idx]))\n",
    "        ] for fold_idx in range(len(weight_test_dict))\n",
    "    ]\n",
    "    fold_colors = [copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries_fold[i]) // len(cmap_petroff10)) + 1)) for i in range(len(weight_test_dict))]\n",
    "    for fold_idx in range(len(weight_test_dict)):\n",
    "        s_over_root_b(\n",
    "            IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_{plot_type}_fold{fold_idx}', \n",
    "            labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, method='round_robin',\n",
    "            lines_fold=cut_boundaries_fold, lines_labels=fold_labels, only_fold=fold_idx, lines_colors=fold_colors,\n",
    "            mask=mask_arr\n",
    "        )\n",
    "\n",
    "    (\n",
    "        cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights\n",
    "    ) = optimize_cut_boundaries(\n",
    "        IN_perf, weights_plot, method='round_robin_sum',\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    labels = [\n",
    "        f\"s/âb={cut_s_over_root_bs[cut_idx]:.04f}, s={sig_weights[cut_idx]['value']:.04f}Â±{sig_weights[cut_idx]['w2']:.04f}, b={bkg_weights[cut_idx]['value']:.04f}Â±{bkg_weights[cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs))\n",
    "    ]\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_{plot_type}_sum', \n",
    "        weights=weights_plot, method='round_robin_sum',\n",
    "        lines=cut_boundaries, lines_labels=labels, lines_colors=cmap_petroff10,\n",
    "        mask=mask_arr\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "# with open(os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json'), 'r') as f:\n",
    "#     best_conf = json.load(f)\n",
    "#     print(best_conf)\n",
    "\n",
    "for fold_idx in range(len(data_list_dict)):\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "        training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            train_data_list, train_data_hlf, train_label, train_weight,\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, only_fold_idx=fold_idx,\n",
    "            train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr,\n",
    "            # dict_lists=True\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            val_data_list, val_data_hlf, val_label, val_weight,\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, only_fold_idx=fold_idx,\n",
    "            train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr,\n",
    "            # dict_lists=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_train_val.json'), 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_train_val.json'), 'r') as f:\n",
    "    IN_perf_dict = json.load(f)\n",
    "    \n",
    "plot_destdir = os.path.join(OUTPUT_DIRPATH, 'plots/train_val_comparison')\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold ']\n",
    "val_weights_arr = []\n",
    "for fold_idx, (train_IN_dict, val_IN_dict) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'])):\n",
    "    all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight,\n",
    "        train_indices, val_indices\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "        training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "        all_indices,\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    weights_plot = [\n",
    "        {\n",
    "            'sig': _weight[_label == 1],\n",
    "            'bkg': _weight[_label == 0],\n",
    "        } for _weight, _label in [(train_weight, train_label), (val_weight, val_label)]\n",
    "    ]\n",
    "    density_weights_plot = [\n",
    "        {'sig': None, 'bkg': None} for _ in ['train', 'val']\n",
    "    ]\n",
    "\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_weighted_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights_plot\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_density_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=density_weights_plot\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights_plot\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Vars Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/pre_std/\"\n",
    "output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison_v3/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict[\"fold_0\"]:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        train_mask = np.ones_like(all_indices, dtype=bool)\n",
    "        train_mask[val_indices] = np.zeros_like(val_indices, dtype=bool)\n",
    "        val_mask = np.logical_not(train_mask)\n",
    "\n",
    "        sig_mask = (label_dict[f'fold_{fold_idx}'] == 1)\n",
    "        sig_test_mask = (label_test_dict[f'fold_{fold_idx}'] == 1)\n",
    "        bkg_mask = (label_dict[f'fold_{fold_idx}'] == 0)\n",
    "        bkg_test_mask = (label_test_dict[f'fold_{fold_idx}'] == 0)\n",
    "        \n",
    "        sig_train_mask = train_mask & sig_mask\n",
    "        sig_val_mask = val_mask & sig_mask\n",
    "        bkg_train_mask = train_mask & bkg_mask\n",
    "        bkg_val_mask = val_mask & bkg_mask\n",
    "\n",
    "        sig_train_np = data_df_dict[f'fold_{fold_idx}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "        sig_val_np = data_df_dict[f'fold_{fold_idx}'].loc[sig_val_mask, var_name].to_numpy()\n",
    "        sig_test_np = data_test_df_dict[f'fold_{fold_idx}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "        bkg_train_np = data_df_dict[f'fold_{fold_idx}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "        bkg_val_np = data_df_dict[f'fold_{fold_idx}'].loc[bkg_val_mask, var_name].to_numpy()\n",
    "        bkg_test_np = data_test_df_dict[f'fold_{fold_idx}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### post-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/post_std/\"\n",
    "output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison_v3/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        train_mask = np.ones_like(all_indices, dtype=bool)\n",
    "        train_mask[val_indices] = np.zeros_like(val_indices, dtype=bool)\n",
    "        val_mask = np.logical_not(train_mask)\n",
    "\n",
    "        data, data_test = None, None\n",
    "        if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "            data, data_test = data_list_dict[f'fold_{fold_idx}'], data_list_test_dict[f'fold_{fold_idx}']\n",
    "        else:\n",
    "            data, data_test = data_hlf_dict[f'fold_{fold_idx}'], data_hlf_test_dict[f'fold_{fold_idx}']\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, fold_idx, var_name,\n",
    "            train_index=train_mask, val_index=val_mask\n",
    "        )\n",
    "        if re.search('lepton1', var_name) is not None or re.search('lepton2', var_name) is not None:\n",
    "            sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "            sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "            sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "            bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "            bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "            bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "        else:\n",
    "            sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np)\n",
    "            sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np)\n",
    "            sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np)\n",
    "            bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np)\n",
    "            bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np)\n",
    "            bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np)\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian smearing on test set (for feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    mask_arr = data_list_index_map(var_name, particle_list_to_smear, np.ones(len(particle_list_to_smear), dtype=bool), n_pFields=N_PARTICLE_FIELDS)\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear = np.where(mask_arr, particle_list_to_smear*rng.normal(), particle_list_to_smear)\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear = np.where(mask_arr, particle_list_to_smear+rng.normal(), particle_list_to_smear)\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns_dict['fold_0'][var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate smeared variable test-data on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "# with open(os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json'), 'r') as f:\n",
    "    best_conf = json.load(f)\n",
    "    print(best_conf)\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "\n",
    "    weight_test_dict = {\n",
    "        f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "\n",
    "    gauss_data_list_dict, gauss_data_hlf_dict = None, None\n",
    "    if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "        gauss_data_list_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_test_dict))\n",
    "        }\n",
    "        gauss_data_hlf_dict = data_hlf_test_dict\n",
    "    else:\n",
    "        gauss_data_list_dict = data_list_test_dict\n",
    "        gauss_data_hlf_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_test_dict))\n",
    "        }\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list_dict, gauss_data_hlf_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True,\n",
    "    )\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_gauss_smear.json'), 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_gauss_smear.json'), 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "\n",
    "plot_destdir = os.path.join(OUTPUT_DIRPATH, 'plots/gauss_smear_performance')\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "IN_perf_smear_list = []\n",
    "label_arr = []\n",
    "for var_name, IN_perf_smear in IN_perf_smear_dict.items():\n",
    "    IN_perf_smear_list.append(IN_perf_smear)\n",
    "    label_arr.append(var_name)\n",
    "sort = np.argsort([IN_perf_smear['mean_area'] for IN_perf_smear in IN_perf_smear_list])\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_all', \n",
    "    method='round_robin_sum_arr', labels=label_arr, yscale='log', run2=False, sort=sort\n",
    ")\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5', \n",
    "    method='round_robin_sum_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False\n",
    ")\n",
    "# with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf.json'), 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "# plot_roc(\n",
    "#     [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5_and_orig', \n",
    "#     method='round_robin_sum_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False, run3=IN_perf\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/gauss_smear/\"\n",
    "output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison_v3/{VERSION}{'_RNN' if re.search('vars_to_RNN', VARS) is not None else ''}/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "\n",
    "    gauss_data_list_dict, gauss_data_hlf_dict = None, None\n",
    "    if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "        gauss_data_list_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_dict))\n",
    "        }\n",
    "        gauss_data_hlf_dict = data_hlf_dict\n",
    "\n",
    "        gauss_data_list_test_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_test_dict))\n",
    "        }\n",
    "        gauss_data_hlf_test_dict = data_hlf_test_dict\n",
    "    else:\n",
    "        gauss_data_list_dict = data_list_dict\n",
    "        gauss_data_hlf_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_dict))\n",
    "        }\n",
    "\n",
    "        gauss_data_list_test_dict = data_list_test_dict\n",
    "        gauss_data_hlf_test_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_test_dict))\n",
    "        }\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        train_mask = np.ones_like(all_indices, dtype=bool)\n",
    "        train_mask[val_indices] = np.zeros_like(val_indices, dtype=bool)\n",
    "        val_mask = np.logical_not(train_mask)\n",
    "\n",
    "        data, data_test = None, None\n",
    "        if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "            data, data_test = gauss_data_list_dict[f'fold_{fold_idx}'], gauss_data_list_test_dict[f'fold_{fold_idx}']\n",
    "        else:\n",
    "            data, data_test = gauss_data_hlf_dict[f'fold_{fold_idx}'], gauss_data_hlf_test_dict[f'fold_{fold_idx}']\n",
    "        \n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, fold_idx, var_name,\n",
    "            train_index=train_mask, val_index=val_mask\n",
    "        )\n",
    "        if re.search('lepton1', var_name) is not None or re.search('lepton2', var_name) is not None:\n",
    "            sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "            sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "            sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "            bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "            bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "            bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "        else:\n",
    "            sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np)\n",
    "            sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np)\n",
    "            sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np)\n",
    "            bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np)\n",
    "            bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np)\n",
    "            bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np)\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_gauss_smear, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "# with open(os.path.join(OUTPUT_DIRPATH, 'ttH_Killer_IN_config.json'), 'r') as f:\n",
    "    best_conf = json.load(f)\n",
    "    print(best_conf)\n",
    "\n",
    "weight_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_aux_dict))\n",
    "}\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y, w2 in [('train', data_list_dict, data_hlf_dict, label_dict, weight_dict), ('test', data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, w2,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True,\n",
    "    )\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_full_eval.json'), 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_IN_perf_full_eval.json'), 'r') as f:\n",
    "    IN_full_eval_dict = json.load(f)\n",
    "\n",
    "plot_destdir = os.path.join(OUTPUT_DIRPATH, 'plots/mass_sculpting_check')\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {fold_idx: {'mass': [], 'dijet_mass': []} for fold_idx in range(len(data_aux_dict))}\n",
    "for var_name in hist_dict[0].keys():\n",
    "    for fold_idx in range(len(data_aux_dict)):\n",
    "        for i, score_cut in enumerate(score_cuts):\n",
    "            sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold_idx)\n",
    "            sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "            sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "            bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "            bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "            hist_dict[fold_idx][var_name].extend(\n",
    "                [\n",
    "                    copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                    copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "                ]\n",
    "            )\n",
    "        for mod_factor, label_mod in enumerate(['sig_train', 'sig_test', 'bkg_train', 'bkg_test']):\n",
    "            plot_list = []\n",
    "            label_list = []\n",
    "            for i in range(len(hist_dict[fold_idx][var_name])):\n",
    "                if (i - mod_factor) % 4 == 0:\n",
    "                    plot_list.append(hist_dict[fold_idx][var_name][i])\n",
    "                    label_list.append(label_arr[i])\n",
    "            make_input_plot(\n",
    "                plot_destdir, var_name, plot_list, labels=label_list, density=True, \n",
    "                plot_prefix=CURRENT_TIME+'_', plot_postfix='_'+label_mod, alpha=0.5,\n",
    "                linestyle=False, fold_idx=fold_idx\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset size plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_dirs = glob.glob(OUTPUT_DIRPATH[:-1]+'_mod*') + [OUTPUT_DIRPATH[:-1]]\n",
    "final_train_losses_arr, final_val_losses_arr = [], []\n",
    "mod_values_arr = []\n",
    "\n",
    "for train_size_dir in train_size_dirs:\n",
    "    if len(glob.glob(train_size_dir + '/*IN_perf.json')) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        mod_values_arr.append([\n",
    "            float(\n",
    "                train_size_dir[\n",
    "                    train_size_dir.find('_mod')+4 : train_size_dir.find('-')\n",
    "                ]\n",
    "            ),\n",
    "            float(\n",
    "                train_size_dir[train_size_dir.find('-')+1:]\n",
    "            )\n",
    "        ])\n",
    "    except:\n",
    "         mod_values_arr.append([2, 2])\n",
    "    IN_perf_path = glob.glob(f'{train_size_dir}/*IN_perf.json')[0]\n",
    "    with open(IN_perf_path, 'r') as f:\n",
    "        IN_perf = json.load(f)\n",
    "    final_train_losses_arr.append([train_losses[-7 if len(train_losses) < NUM_EPOCHS else -1] for train_losses in IN_perf['train_losses_arr']])\n",
    "    final_val_losses_arr.append([val_losses[-7 if len(val_losses) < NUM_EPOCHS else -1] for val_losses in IN_perf['val_losses_arr']])\n",
    "\n",
    "final_train_losses_arr = np.array(final_train_losses_arr)\n",
    "final_val_losses_arr = np.array(final_val_losses_arr)\n",
    "mod_values_arr = np.array(mod_values_arr)\n",
    "dataset_sizes = (len(label) + len(label_test)) / mod_values_arr\n",
    "sorted_indices = np.argsort(dataset_sizes[:, 0])\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_train_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_train_losses_arr, axis=1)-np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_train_losses_arr, axis=1)+np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[0], alpha=0.5, label='Train data'\n",
    ")\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_val_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_val_losses_arr, axis=1)-np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_val_losses_arr, axis=1)+np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[1], alpha=0.5, label='Val data'\n",
    ")\n",
    "plt.xlabel('Size of train dataset')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.pdf')\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
