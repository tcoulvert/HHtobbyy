{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu3.fnal.gov      Mon Oct 21 12:07:34 2024  555.42.06\n",
      "[0] Tesla P100-PCIE-12GB | 52°C,  94 % |  1524 / 12288 MB | aherrera(306M) ckapsiak(454M) ckapsiak(762M)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import auc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "V2_MERGED = True\n",
    "\n",
    "SIGNAL_FILEPATHS = [\n",
    "    # Test sig files #\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE/GluGluToHH/nominal/*',\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE_merged_v2/GluGluToHH/nominal/*',\n",
    "    # ggF HH # \n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\",\n",
    "    # VBF HH #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [\n",
    "    # ttH (i.e. the main bkg to reduce) #\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    # # Other single H samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\",\n",
    "    # # Prompt-Prompt samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\",\n",
    "    # # Prompt-Fake samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\",\n",
    "    # Fake-Fake samples #\n",
    "    # ADD HERE #\n",
    "]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v9'\n",
    "CRITERION = \"NLLLoss\"\n",
    "N_PARTICLES, N_PARTICLE_FIELDS = 6, 7\n",
    "MOD_VALS = (5, 5)\n",
    "# VARS = 'base_vars'\n",
    "# VARS = 'extra_vars'\n",
    "# CURRENT_TIME = '2024-08-30_14-35-01'\n",
    "# VARS = 'extra_vars+'\n",
    "# CURRENT_TIME = '2024-10-09_20-47-24'\n",
    "VARS = 'extra_vars+max'\n",
    "# VARS = 'extra_vars+max+moved_vars_to_RNN'\n",
    "# CURRENT_TIME = '2024-10-21_00-35-33'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 6, 9\n",
    "# VARS = 'extra_vars_no_dijet_mass'\n",
    "# VARS = 'no_bad_vars'\n",
    "# VARS = 'simplified_bad_vars'\n",
    "# VARS = 'extra_vars_and_bools'\n",
    "# VARS = 'extra_vars_in_RNN'\n",
    "# VARS = f'extra_vars_mod{MOD_VALS[0]}-{MOD_VALS[1]}'\n",
    "# VARS = 'extra_vars_lead_lep_only'\n",
    "# CURRENT_TIME = '2024-10-02_18-03-26'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 3, 6\n",
    "# VARS = 'extra_vars_no_lep'\n",
    "# CURRENT_TIME = '2024-10-04_12-49-31'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 2, 6\n",
    "OUTPUT_DIRPATH = CURRENT_DIRPATH + f\"/model_outputs/{VERSION}/{VARS}/\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413735, 6, 7)\n",
      "Data HLF: (413735, 15)\n",
      "n signal = 136530, n bkg = 277205\n",
      "Data list test: (103521, 6, 7)\n",
      "Data HLF test: (103521, 15)\n",
      "n signal = 34224, n bkg = 69297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413918, 6, 7)\n",
      "Data HLF: (413918, 15)\n",
      "n signal = 136466, n bkg = 277452\n",
      "Data list test: (103338, 6, 7)\n",
      "Data HLF test: (103338, 15)\n",
      "n signal = 34288, n bkg = 69050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413265, 6, 7)\n",
      "Data HLF: (413265, 15)\n",
      "n signal = 136638, n bkg = 276627\n",
      "Data list test: (103991, 6, 7)\n",
      "Data HLF test: (103991, 15)\n",
      "n signal = 34116, n bkg = 69875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413725, 6, 7)\n",
      "Data HLF: (413725, 15)\n",
      "n signal = 136671, n bkg = 277054\n",
      "Data list test: (103531, 6, 7)\n",
      "Data HLF test: (103531, 15)\n",
      "n signal = 34083, n bkg = 69448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (414381, 6, 7)\n",
      "Data HLF: (414381, 15)\n",
      "n signal = 136711, n bkg = 277670\n",
      "Data list test: (102875, 6, 7)\n",
      "Data HLF test: (102875, 15)\n",
      "n signal = 34043, n bkg = 68832\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    data_df_dict, data_test_df_dict, \n",
    "    data_list_dict, data_hlf_dict, label_dict, \n",
    "    data_list_test_dict, data_hlf_test_dict, label_test_dict, \n",
    "    high_level_fields_dict, input_hlf_vars_dict, hlf_vars_columns_dict,\n",
    "    data_aux_dict, data_test_aux_dict\n",
    ") = process_data(\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED, mod_vals=MOD_VALS, k_fold_test=True\n",
    ")\n",
    "# skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_weights(event_weights, labels):\n",
    "    sum_of_bkg = np.sum(event_weights[labels==0])\n",
    "    sum_of_sig = np.sum(event_weights[labels==1])\n",
    "\n",
    "    sig_scale_factor = sum_of_bkg / sum_of_sig\n",
    "\n",
    "    weights = np.where(labels==0, event_weights, event_weights*sig_scale_factor)\n",
    "    mean_weight = np.mean(weights)\n",
    "    abs_weights = np.abs(weights)\n",
    "    scaled_weights = abs_weights / mean_weight\n",
    "\n",
    "    return scaled_weights\n",
    "\n",
    "def plot_train_val_losses(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None, n_folds=5\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dotted']\n",
    "        linestyles = linestyles * ((2*len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:2*len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx if fold_idx%2 == 0 else fold_idx+1]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx+1 if fold_idx%2 == 0 else fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='std', \n",
    "    labels=None, yscale='linear', run2=True, sort=None, run3=None,\n",
    "    mask=None, n_folds=5\n",
    "):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d, AUC = %.4f\" % (fold_idx, float(auc(IN_info['fprs'][fold_idx],IN_info['base_tpr']))), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for i in index_arr:\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_fprs'])[0], dtype=bool)\n",
    "            plt.plot(\n",
    "                np.array(IN_info[i]['mean_fprs'])[mask], np.array(IN_info[i]['base_tpr'])[mask], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i], alpha=0.5\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if run3 is not None:\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(run3['mean_fprs'])[0], dtype=bool)\n",
    "        plt.plot(\n",
    "            np.array(run3['mean_fprs'])[mask], np.array(run3['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (run3['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, n_bins=50, all_sig=False, all_bkg=False,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        # for cut in np.linspace(0, 1, 10, endpoint=False):\n",
    "        #     print(f\"output score > {cut:.2f}\")\n",
    "        #     print('='*60)\n",
    "        #     print(f\"num sig > {cut:.2f} = {len(sig_np[sig_np > cut])}\")\n",
    "        #     print(f\"num bkg > {cut:.2f} = {len(bkg_np[bkg_np > cut])}\")\n",
    "        #     print('-'*60)\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'] if weights[fold_idx]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'] if weights[fold_idx]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[fold_idx]['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights[fold_idx]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=1, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=[\n",
    "                f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal', \n",
    "                f'{\"ttH\" if not all_bkg else \"all\"} background'\n",
    "            ]\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'] if weights[i]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'] if weights[i]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[i]['sig'] is not None else False),\n",
    "                alpha=0.7, density=(False if weights[i]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[i] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[i], linestyles[i]]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def s_over_root_b(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, lines_fold=None, lines=None, lines_labels=None, \n",
    "        lines_colors=None, only_fold=None, no_fold=False, n_bins=50,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/√b', alpha=0.8 \n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/√b'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/√b'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "    elif method == 'arr':\n",
    "        if mask is None or np.all(mask):\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/√b'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/√b'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "        if only_fold is None:\n",
    "            sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/√b - avg. over folds', \n",
    "                alpha=0.5, \n",
    "            )\n",
    "            if lines is not None:\n",
    "                for i in range(len(lines)):\n",
    "                    plt.vlines(\n",
    "                        lines[i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/√b - avg.'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[i]\n",
    "                    )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'])\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "                label='s/√b'+(' - '+labels[i] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[i], \n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    # 'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # 'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Integer(0, 10, name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($\\Sigma E_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    # 'jet1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # 'jet2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -10., 4., name='var', label=r'ln($\\chi_{t0}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -10., 4., name='var', label=r'ln($\\chi_{t1}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma$ ln($p_{T}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'ln($M_{jj}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, fold, var_name, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label_dict[f'fold_{fold}'] == 1\n",
    "    sig_test_mask = label_test_dict[f'fold_{fold}'] == 1\n",
    "    bkg_mask = label_dict[f'fold_{fold}'] == 0\n",
    "    bkg_test_mask = label_test_dict[f'fold_{fold}'] == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            # index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold):\n",
    "    sig_train_mask = (label_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux_dict[f'fold_{fold}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux_dict[f'fold_{fold}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux_dict[f'fold_{fold}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux_dict[f'fold_{fold}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(\n",
    "    output_dir, var_name, hist_list, fold, fold_idx=None, labels=None, density=True, \n",
    "    plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True\n",
    "):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir = output_dir + \"fold/\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_input_vars_after_score_cut(\n",
    "    IN_info, score_cut, destdir, fold, plot_prefix, plot_postfix='', method='std', \n",
    "    weights={'sig': None, 'bkg': None}, all_sig=False, all_bkg=False,\n",
    "    mask=None\n",
    "):\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "        bkg_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            sig_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=sig_var.loc[sig_mask], \n",
    "                weight=weights['sig'][sig_mask] if weights['sig'] is not None else np.ones(np.sum(sig_mask))\n",
    "            )\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            bkg_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=bkg_var.loc[bkg_mask], \n",
    "                weight=weights['bkg'][bkg_mask] if weights['bkg'] is not None else np.ones(np.sum(bkg_mask))\n",
    "            )\n",
    "            make_input_plot(\n",
    "                destdir, var_name, [sig_hist, bkg_hist], fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore{score_cut}', labels=['HH signal', 'ttH background'], density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['all_preds'][fold])[0], dtype=bool)\n",
    "        \n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut if score_cut is list else [score_cut]:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['all_preds'][fold]\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['all_labels'][fold]) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['all_preds'][fold]\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['all_labels'][fold]) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used method 'std'. You used {method}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0003072887 Acc: 80.5237655640\n",
      "validation Loss: 0.0002379752 Acc: 87.4726562500\n",
      "training Loss: 0.0002327992 Acc: 86.8010330200\n",
      "validation Loss: 0.0002264605 Acc: 87.6877670288\n",
      "training Loss: 0.0002272762 Acc: 87.2388076782\n",
      "validation Loss: 0.0002211517 Acc: 87.4194793701\n",
      "training Loss: 0.0002234455 Acc: 87.4218978882\n",
      "validation Loss: 0.0002219297 Acc: 88.7766265869\n",
      "training Loss: 0.0002197507 Acc: 87.6548385620\n",
      "validation Loss: 0.0002150196 Acc: 88.2436828613\n",
      "training Loss: 0.0002179484 Acc: 87.7155609131\n",
      "validation Loss: 0.0002143317 Acc: 88.1711730957\n",
      "training Loss: 0.0002174097 Acc: 87.7862625122\n",
      "validation Loss: 0.0002166356 Acc: 88.8926467896\n",
      "training Loss: 0.0002147840 Acc: 87.8581695557\n",
      "validation Loss: 0.0002113643 Acc: 87.9065093994\n",
      "training Loss: 0.0002142372 Acc: 87.9506149292\n",
      "validation Loss: 0.0002113174 Acc: 89.0122909546\n",
      "training Loss: 0.0002139285 Acc: 87.9720687866\n",
      "validation Loss: 0.0002102466 Acc: 88.3016891479\n",
      "training Loss: 0.0002127505 Acc: 88.0379333496\n",
      "validation Loss: 0.0002094715 Acc: 87.1366882324\n",
      "training Loss: 0.0002127089 Acc: 88.0300750732\n",
      "validation Loss: 0.0002081370 Acc: 88.3258590698\n",
      "training Loss: 0.0002113497 Acc: 88.1763076782\n",
      "validation Loss: 0.0002106732 Acc: 87.8049926758\n",
      "training Loss: 0.0002108191 Acc: 88.2034988403\n",
      "validation Loss: 0.0002096000 Acc: 87.2200775146\n",
      "training Loss: 0.0002113587 Acc: 88.0944290161\n",
      "validation Loss: 0.0002075945 Acc: 88.6110687256\n",
      "training Loss: 0.0002095722 Acc: 88.1705627441\n",
      "validation Loss: 0.0002079728 Acc: 88.0164794922\n",
      "training Loss: 0.0002095547 Acc: 88.2551651001\n",
      "validation Loss: 0.0002054506 Acc: 87.6031723022\n",
      "training Loss: 0.0002084196 Acc: 88.2853775024\n",
      "validation Loss: 0.0002050446 Acc: 88.7053298950\n",
      "training Loss: 0.0002085744 Acc: 88.1977539062\n",
      "validation Loss: 0.0002031474 Acc: 87.8932113647\n",
      "training Loss: 0.0002079461 Acc: 88.3158874512\n",
      "validation Loss: 0.0002052343 Acc: 87.6756820679\n",
      "training Loss: 0.0002081249 Acc: 88.3303909302\n",
      "validation Loss: 0.0002038338 Acc: 89.0038299561\n",
      "training Loss: 0.0002079017 Acc: 88.3068237305\n",
      "validation Loss: 0.0002044823 Acc: 89.0243759155\n",
      "training Loss: 0.0002075605 Acc: 88.3769149780\n",
      "validation Loss: 0.0002052709 Acc: 87.3626785278\n",
      "training Loss: 0.0002035892 Acc: 88.5856857300\n",
      "validation Loss: 0.0001998779 Acc: 88.3995742798\n",
      "training Loss: 0.0002027454 Acc: 88.6464157104\n",
      "validation Loss: 0.0001992323 Acc: 88.8708953857\n",
      "training Loss: 0.0002019007 Acc: 88.6862945557\n",
      "validation Loss: 0.0001992549 Acc: 88.0019836426\n",
      "training Loss: 0.0002023233 Acc: 88.6189193726\n",
      "validation Loss: 0.0001992583 Acc: 89.0134963989\n",
      "training Loss: 0.0002022341 Acc: 88.6122741699\n",
      "validation Loss: 0.0001983698 Acc: 89.1476440430\n",
      "training Loss: 0.0002009677 Acc: 88.7198333740\n",
      "validation Loss: 0.0001985919 Acc: 89.2141113281\n",
      "training Loss: 0.0002014762 Acc: 88.7083511353\n",
      "validation Loss: 0.0001990361 Acc: 88.8140945435\n",
      "training Loss: 0.0002019732 Acc: 88.6651458740\n",
      "validation Loss: 0.0001982822 Acc: 88.4575881958\n",
      "training Loss: 0.0002006857 Acc: 88.7370529175\n",
      "validation Loss: 0.0001987202 Acc: 89.2938690186\n",
      "training Loss: 0.0002010335 Acc: 88.6796493530\n",
      "validation Loss: 0.0001973243 Acc: 88.8648529053\n",
      "training Loss: 0.0002009808 Acc: 88.6923370361\n",
      "validation Loss: 0.0001991137 Acc: 88.2158813477\n",
      "training Loss: 0.0002009728 Acc: 88.6711883545\n",
      "validation Loss: 0.0001976340 Acc: 89.2092742920\n",
      "training Loss: 0.0002004892 Acc: 88.7406768799\n",
      "validation Loss: 0.0001980710 Acc: 88.5155944824\n",
      "training Loss: 0.0002004932 Acc: 88.7554779053\n",
      "validation Loss: 0.0001988329 Acc: 88.1905059814\n",
      "training Loss: 0.0001980538 Acc: 88.8597183228\n",
      "validation Loss: 0.0001958091 Acc: 88.9554901123\n",
      "training Loss: 0.0001972058 Acc: 88.9171218872\n",
      "validation Loss: 0.0001966444 Acc: 88.6352386475\n",
      "training Loss: 0.0001970149 Acc: 88.8938522339\n",
      "validation Loss: 0.0001950828 Acc: 88.8285980225\n",
      "training Loss: 0.0001969176 Acc: 88.9718017578\n",
      "validation Loss: 0.0001950461 Acc: 88.4720840454\n",
      "training Loss: 0.0001967649 Acc: 88.8983840942\n",
      "validation Loss: 0.0001948706 Acc: 88.9458236694\n",
      "training Loss: 0.0001967290 Acc: 88.9660644531\n",
      "validation Loss: 0.0001957862 Acc: 88.8213424683\n",
      "training Loss: 0.0001967707 Acc: 88.8799591064\n",
      "validation Loss: 0.0001961564 Acc: 89.1681823730\n",
      "training Loss: 0.0001964971 Acc: 88.9388732910\n",
      "validation Loss: 0.0001944270 Acc: 89.0920486450\n",
      "training Loss: 0.0001960095 Acc: 88.9180221558\n",
      "validation Loss: 0.0001945134 Acc: 89.0956726074\n",
      "training Loss: 0.0001961133 Acc: 88.9814682007\n",
      "validation Loss: 0.0001954130 Acc: 89.3555068970\n",
      "training Loss: 0.0001963387 Acc: 88.9476318359\n",
      "validation Loss: 0.0001953175 Acc: 88.9143981934\n",
      "training Loss: 0.0001960832 Acc: 88.9941635132\n",
      "validation Loss: 0.0001960287 Acc: 89.0654602051\n",
      "training Loss: 0.0001947117 Acc: 89.0554885864\n",
      "validation Loss: 0.0001939319 Acc: 89.3361663818\n",
      "training Loss: 0.0001943049 Acc: 89.0373611450\n",
      "validation Loss: 0.0001936133 Acc: 89.0267868042\n",
      "training Loss: 0.0001938235 Acc: 89.1038360596\n",
      "validation Loss: 0.0001934217 Acc: 88.9808654785\n",
      "training Loss: 0.0001934231 Acc: 89.0603256226\n",
      "validation Loss: 0.0001934884 Acc: 89.2576141357\n",
      "training Loss: 0.0001937344 Acc: 89.0630493164\n",
      "validation Loss: 0.0001935352 Acc: 88.6944503784\n",
      "training Loss: 0.0001928861 Acc: 89.1092681885\n",
      "validation Loss: 0.0001932235 Acc: 88.8890228271\n",
      "training Loss: 0.0001938969 Acc: 89.0325317383\n",
      "validation Loss: 0.0001939911 Acc: 89.1089706421\n",
      "training Loss: 0.0001938006 Acc: 89.0850982666\n",
      "validation Loss: 0.0001938217 Acc: 88.7609176636\n",
      "training Loss: 0.0001930601 Acc: 89.1041336060\n",
      "validation Loss: 0.0001935525 Acc: 89.4993133545\n",
      "training Loss: 0.0001937515 Acc: 89.0376663208\n",
      "validation Loss: 0.0001932012 Acc: 89.0630493164\n",
      "training Loss: 0.0001927948 Acc: 89.1313247681\n",
      "validation Loss: 0.0001927877 Acc: 88.8491439819\n",
      "training Loss: 0.0001929859 Acc: 89.1189422607\n",
      "validation Loss: 0.0001928815 Acc: 88.8008041382\n",
      "training Loss: 0.0001933587 Acc: 89.0929565430\n",
      "validation Loss: 0.0001930289 Acc: 89.2394866943\n",
      "training Loss: 0.0001928649 Acc: 89.1222610474\n",
      "validation Loss: 0.0001934780 Acc: 88.8841857910\n",
      "training Loss: 0.0001929964 Acc: 89.0996017456\n",
      "validation Loss: 0.0001938591 Acc: 89.2926635742\n",
      "training Loss: 0.0001925474 Acc: 89.1488494873\n",
      "validation Loss: 0.0001922201 Acc: 88.9240646362\n",
      "training Loss: 0.0001919298 Acc: 89.1814804077\n",
      "validation Loss: 0.0001924884 Acc: 88.9409866333\n",
      "training Loss: 0.0001917565 Acc: 89.1956787109\n",
      "validation Loss: 0.0001927091 Acc: 89.0328292847\n",
      "training Loss: 0.0001917884 Acc: 89.1811752319\n",
      "validation Loss: 0.0001921909 Acc: 88.9965744019\n",
      "training Loss: 0.0001915422 Acc: 89.2183380127\n",
      "validation Loss: 0.0001923788 Acc: 88.8938522339\n",
      "training Loss: 0.0001915640 Acc: 89.1579132080\n",
      "validation Loss: 0.0001923487 Acc: 89.0497512817\n",
      "training Loss: 0.0001911737 Acc: 89.1733245850\n",
      "validation Loss: 0.0001924683 Acc: 89.0618362427\n",
      "training Loss: 0.0001911506 Acc: 89.1923522949\n",
      "validation Loss: 0.0001917386 Acc: 88.9168167114\n",
      "training Loss: 0.0001916806 Acc: 89.2252883911\n",
      "validation Loss: 0.0001923838 Acc: 89.1814804077\n",
      "training Loss: 0.0001911590 Acc: 89.2388839722\n",
      "validation Loss: 0.0001923872 Acc: 88.9566955566\n",
      "training Loss: 0.0001911026 Acc: 89.2135009766\n",
      "validation Loss: 0.0001922637 Acc: 89.2890319824\n",
      "training Loss: 0.0001915500 Acc: 89.1802673340\n",
      "validation Loss: 0.0001921787 Acc: 89.3820877075\n",
      "training Loss: 0.0001909215 Acc: 89.2077636719\n",
      "validation Loss: 0.0001916031 Acc: 89.1585159302\n",
      "training Loss: 0.0001909347 Acc: 89.2576141357\n",
      "validation Loss: 0.0001919014 Acc: 89.1222610474\n",
      "training Loss: 0.0001906236 Acc: 89.2041397095\n",
      "validation Loss: 0.0001918972 Acc: 89.2044372559\n",
      "training Loss: 0.0001905279 Acc: 89.2515716553\n",
      "validation Loss: 0.0001914909 Acc: 89.0425033569\n",
      "training Loss: 0.0001908157 Acc: 89.2385787964\n",
      "validation Loss: 0.0001918580 Acc: 89.1005096436\n",
      "training Loss: 0.0001903558 Acc: 89.2310256958\n",
      "validation Loss: 0.0001919250 Acc: 89.1645584106\n",
      "training Loss: 0.0001903626 Acc: 89.2724151611\n",
      "validation Loss: 0.0001918134 Acc: 89.0908432007\n",
      "training Loss: 0.0001904397 Acc: 89.2434158325\n",
      "validation Loss: 0.0001916356 Acc: 89.2503662109\n",
      "training Loss: 0.0001901449 Acc: 89.2996063232\n",
      "validation Loss: 0.0001916527 Acc: 89.1621398926\n",
      "training Loss: 0.0001898787 Acc: 89.2781600952\n",
      "validation Loss: 0.0001915994 Acc: 89.1524734497\n",
      "training Loss: 0.0001896744 Acc: 89.2440185547\n",
      "validation Loss: 0.0001914050 Acc: 89.2213592529\n",
      "training Loss: 0.0001898196 Acc: 89.3292160034\n",
      "validation Loss: 0.0001915237 Acc: 89.1706008911\n",
      "training Loss: 0.0001899015 Acc: 89.2494583130\n",
      "validation Loss: 0.0001915486 Acc: 89.2600326538\n",
      "training Loss: 0.0001896311 Acc: 89.2700042725\n",
      "validation Loss: 0.0001916758 Acc: 89.0787582397\n",
      "training Loss: 0.0001895660 Acc: 89.2630538940\n",
      "validation Loss: 0.0001914989 Acc: 89.2382812500\n",
      "training Loss: 0.0001899673 Acc: 89.2727203369\n",
      "validation Loss: 0.0001914538 Acc: 89.2406921387\n",
      "training Loss: 0.0001896318 Acc: 89.2739257812\n",
      "validation Loss: 0.0001913731 Acc: 89.0666732788\n",
      "training Loss: 0.0001898709 Acc: 89.2675857544\n",
      "validation Loss: 0.0001914103 Acc: 89.0968856812\n",
      "training Loss: 0.0001900505 Acc: 89.2603302002\n",
      "validation Loss: 0.0001915933 Acc: 89.0666732788\n",
      "training Loss: 0.0001897279 Acc: 89.2573089600\n",
      "validation Loss: 0.0001918071 Acc: 89.1863098145\n",
      "training Loss: 0.0001894353 Acc: 89.2965850830\n",
      "validation Loss: 0.0001912198 Acc: 89.1706008911\n",
      "training Loss: 0.0001895562 Acc: 89.3035354614\n",
      "validation Loss: 0.0001913493 Acc: 89.1536865234\n",
      "training Loss: 0.0001897969 Acc: 89.2905426025\n",
      "validation Loss: 0.0001913736 Acc: 89.1609344482\n",
      "training Loss: 0.0001894684 Acc: 89.3325424194\n",
      "validation Loss: 0.0001913981 Acc: 89.1693954468\n",
      "training Loss: 0.0001892705 Acc: 89.2681884766\n",
      "validation Loss: 0.0001912902 Acc: 89.1681823730\n",
      "training Loss: 0.0001894074 Acc: 89.2863159180\n",
      "validation Loss: 0.0001912032 Acc: 89.1693954468\n",
      "training Loss: 0.0001897475 Acc: 89.2672805786\n",
      "validation Loss: 0.0001913480 Acc: 89.1778564453\n",
      "training Loss: 0.0001890801 Acc: 89.2742309570\n",
      "validation Loss: 0.0001910670 Acc: 89.1645584106\n",
      "training Loss: 0.0001892363 Acc: 89.3129043579\n",
      "validation Loss: 0.0001914098 Acc: 89.1283035278\n",
      "training Loss: 0.0001892340 Acc: 89.3065567017\n",
      "validation Loss: 0.0001911547 Acc: 89.1657714844\n",
      "training Loss: 0.0001891922 Acc: 89.3068618774\n",
      "validation Loss: 0.0001915850 Acc: 89.1621398926\n",
      "training Loss: 0.0001893889 Acc: 89.3527832031\n",
      "validation Loss: 0.0001911923 Acc: 89.1379699707\n",
      "training Loss: 0.0001897376 Acc: 89.2766494751\n",
      "validation Loss: 0.0001914992 Acc: 89.1548919678\n",
      "training Loss: 0.0001893913 Acc: 89.3168334961\n",
      "validation Loss: 0.0001913564 Acc: 89.1476440430\n",
      "training Loss: 0.0001895415 Acc: 89.2887344360\n",
      "validation Loss: 0.0001915487 Acc: 89.1573104858\n",
      "Early stopped.\n",
      "Best val acc: 89.499313\n",
      "----------\n",
      "training Loss: 0.0002914461 Acc: 81.8273010254\n",
      "validation Loss: 0.0002404113 Acc: 85.8450851440\n",
      "training Loss: 0.0002339397 Acc: 86.7044754028\n",
      "validation Loss: 0.0002288025 Acc: 87.8780899048\n",
      "training Loss: 0.0002266711 Acc: 87.2673950195\n",
      "validation Loss: 0.0002273112 Acc: 88.1571273804\n",
      "training Loss: 0.0002232910 Acc: 87.4271469116\n",
      "validation Loss: 0.0002208400 Acc: 87.0675430298\n",
      "training Loss: 0.0002205548 Acc: 87.6669311523\n",
      "validation Loss: 0.0002197261 Acc: 87.1508941650\n",
      "training Loss: 0.0002204290 Acc: 87.6086425781\n",
      "validation Loss: 0.0002197621 Acc: 87.6944808960\n",
      "training Loss: 0.0002177241 Acc: 87.8538589478\n",
      "validation Loss: 0.0002163332 Acc: 87.8950042725\n",
      "training Loss: 0.0002180497 Acc: 87.7511825562\n",
      "validation Loss: 0.0002227039 Acc: 88.5424652100\n",
      "training Loss: 0.0002166696 Acc: 87.9151687622\n",
      "validation Loss: 0.0002159885 Acc: 89.1681900024\n",
      "training Loss: 0.0002158798 Acc: 87.8668441772\n",
      "validation Loss: 0.0002121352 Acc: 88.1076049805\n",
      "training Loss: 0.0002145043 Acc: 88.0066680908\n",
      "validation Loss: 0.0002179525 Acc: 86.5879821777\n",
      "training Loss: 0.0002147937 Acc: 88.0205612183\n",
      "validation Loss: 0.0002112335 Acc: 88.0339202881\n",
      "training Loss: 0.0002135357 Acc: 88.0356597900\n",
      "validation Loss: 0.0002130858 Acc: 89.0908813477\n",
      "training Loss: 0.0002134701 Acc: 88.0821685791\n",
      "validation Loss: 0.0002158135 Acc: 89.0558547974\n",
      "training Loss: 0.0002111961 Acc: 88.2117233276\n",
      "validation Loss: 0.0002091950 Acc: 87.8732604980\n",
      "training Loss: 0.0002122304 Acc: 88.1694412231\n",
      "validation Loss: 0.0002086722 Acc: 88.0061340332\n",
      "training Loss: 0.0002105873 Acc: 88.1461944580\n",
      "validation Loss: 0.0002114496 Acc: 88.3890609741\n",
      "training Loss: 0.0002107142 Acc: 88.2011566162\n",
      "validation Loss: 0.0002090313 Acc: 88.4289169312\n",
      "training Loss: 0.0002104569 Acc: 88.2395095825\n",
      "validation Loss: 0.0002115059 Acc: 86.8670272827\n",
      "training Loss: 0.0002096709 Acc: 88.2703094482\n",
      "validation Loss: 0.0002098101 Acc: 87.4867095947\n",
      "training Loss: 0.0002058310 Acc: 88.5979690552\n",
      "validation Loss: 0.0002041666 Acc: 88.2259826660\n",
      "training Loss: 0.0002044240 Acc: 88.6414566040\n",
      "validation Loss: 0.0002056512 Acc: 88.0520324707\n",
      "training Loss: 0.0002042995 Acc: 88.5517654419\n",
      "validation Loss: 0.0002040586 Acc: 88.5316009521\n",
      "training Loss: 0.0002040902 Acc: 88.5526733398\n",
      "validation Loss: 0.0002031395 Acc: 88.9362640381\n",
      "training Loss: 0.0002037777 Acc: 88.6571655273\n",
      "validation Loss: 0.0002040053 Acc: 88.3842239380\n",
      "training Loss: 0.0002034632 Acc: 88.6339111328\n",
      "validation Loss: 0.0002033551 Acc: 88.6765518188\n",
      "training Loss: 0.0002036241 Acc: 88.6565551758\n",
      "validation Loss: 0.0002036712 Acc: 89.0063247681\n",
      "training Loss: 0.0002033730 Acc: 88.6221313477\n",
      "validation Loss: 0.0002036270 Acc: 88.7780227661\n",
      "training Loss: 0.0002011398 Acc: 88.7849044800\n",
      "validation Loss: 0.0002011274 Acc: 88.4265060425\n",
      "training Loss: 0.0002002763 Acc: 88.8422851562\n",
      "validation Loss: 0.0002016061 Acc: 88.9930419922\n",
      "training Loss: 0.0002000954 Acc: 88.8558731079\n",
      "validation Loss: 0.0002005019 Acc: 89.0546417236\n",
      "training Loss: 0.0001995535 Acc: 88.9328842163\n",
      "validation Loss: 0.0002005701 Acc: 88.8541259766\n",
      "training Loss: 0.0001994980 Acc: 88.8474197388\n",
      "validation Loss: 0.0002010212 Acc: 89.2056427002\n",
      "training Loss: 0.0001993624 Acc: 88.8453063965\n",
      "validation Loss: 0.0001998314 Acc: 88.9725036621\n",
      "training Loss: 0.0001996195 Acc: 88.8779220581\n",
      "validation Loss: 0.0002000666 Acc: 88.8275451660\n",
      "training Loss: 0.0001990267 Acc: 88.8996658325\n",
      "validation Loss: 0.0002008341 Acc: 88.7526550293\n",
      "training Loss: 0.0001988964 Acc: 88.9244232178\n",
      "validation Loss: 0.0002010693 Acc: 88.4832763672\n",
      "training Loss: 0.0001993454 Acc: 88.8631210327\n",
      "validation Loss: 0.0001994244 Acc: 88.8565368652\n",
      "training Loss: 0.0001986361 Acc: 88.9111404419\n",
      "validation Loss: 0.0002005002 Acc: 88.7888946533\n",
      "training Loss: 0.0001988996 Acc: 88.8845596313\n",
      "validation Loss: 0.0001995749 Acc: 88.9688796997\n",
      "training Loss: 0.0001983972 Acc: 88.8830490112\n",
      "validation Loss: 0.0002003710 Acc: 89.0582656860\n",
      "training Loss: 0.0001987401 Acc: 88.8893966675\n",
      "validation Loss: 0.0001996860 Acc: 89.2249679565\n",
      "training Loss: 0.0001971457 Acc: 88.9969024658\n",
      "validation Loss: 0.0001990059 Acc: 89.3566360474\n",
      "training Loss: 0.0001968754 Acc: 88.9679107666\n",
      "validation Loss: 0.0001991869 Acc: 88.7852706909\n",
      "training Loss: 0.0001968240 Acc: 88.9866333008\n",
      "validation Loss: 0.0001986967 Acc: 89.0014953613\n",
      "training Loss: 0.0001959402 Acc: 88.9987182617\n",
      "validation Loss: 0.0001985401 Acc: 89.0268630981\n",
      "training Loss: 0.0001960628 Acc: 89.0379714966\n",
      "validation Loss: 0.0001980082 Acc: 89.1331634521\n",
      "training Loss: 0.0001960219 Acc: 88.9887466431\n",
      "validation Loss: 0.0001977725 Acc: 89.0945053101\n",
      "training Loss: 0.0001959367 Acc: 89.0370712280\n",
      "validation Loss: 0.0001988962 Acc: 88.9290161133\n",
      "training Loss: 0.0001958731 Acc: 89.0554885864\n",
      "validation Loss: 0.0001984464 Acc: 89.2793273926\n",
      "training Loss: 0.0001960309 Acc: 89.0702896118\n",
      "validation Loss: 0.0001988838 Acc: 88.7526550293\n",
      "training Loss: 0.0001960288 Acc: 89.0437164307\n",
      "validation Loss: 0.0001986260 Acc: 89.2092666626\n",
      "training Loss: 0.0001950212 Acc: 89.1225357056\n",
      "validation Loss: 0.0001977133 Acc: 88.9519653320\n",
      "training Loss: 0.0001947237 Acc: 89.0923309326\n",
      "validation Loss: 0.0001979241 Acc: 89.1198730469\n",
      "training Loss: 0.0001947642 Acc: 89.1176986694\n",
      "validation Loss: 0.0001972983 Acc: 89.1259155273\n",
      "training Loss: 0.0001947856 Acc: 89.0911254883\n",
      "validation Loss: 0.0001972110 Acc: 89.0691375732\n",
      "training Loss: 0.0001945198 Acc: 89.1364212036\n",
      "validation Loss: 0.0001972262 Acc: 88.9990768433\n",
      "training Loss: 0.0001945954 Acc: 89.1240463257\n",
      "validation Loss: 0.0001976025 Acc: 89.0135726929\n",
      "training Loss: 0.0001939952 Acc: 89.1406555176\n",
      "validation Loss: 0.0001972114 Acc: 89.0957183838\n",
      "training Loss: 0.0001940057 Acc: 89.1587753296\n",
      "validation Loss: 0.0001970296 Acc: 89.1959762573\n",
      "training Loss: 0.0001936907 Acc: 89.1463928223\n",
      "validation Loss: 0.0001972916 Acc: 89.2455062866\n",
      "training Loss: 0.0001941968 Acc: 89.0998840332\n",
      "validation Loss: 0.0001970556 Acc: 88.9302215576\n",
      "training Loss: 0.0001939217 Acc: 89.1225357056\n",
      "validation Loss: 0.0001971990 Acc: 89.1355819702\n",
      "training Loss: 0.0001941135 Acc: 89.1083374023\n",
      "validation Loss: 0.0001967837 Acc: 88.8517074585\n",
      "training Loss: 0.0001939541 Acc: 89.1349105835\n",
      "validation Loss: 0.0001969422 Acc: 89.1005477905\n",
      "training Loss: 0.0001935032 Acc: 89.1503143311\n",
      "validation Loss: 0.0001970485 Acc: 89.1174621582\n",
      "training Loss: 0.0001942639 Acc: 89.0859909058\n",
      "validation Loss: 0.0001967360 Acc: 88.8529129028\n",
      "training Loss: 0.0001935657 Acc: 89.1219253540\n",
      "validation Loss: 0.0001971683 Acc: 88.7937240601\n",
      "training Loss: 0.0001930962 Acc: 89.1554489136\n",
      "validation Loss: 0.0001970887 Acc: 89.1597366333\n",
      "training Loss: 0.0001935028 Acc: 89.1297836304\n",
      "validation Loss: 0.0001971493 Acc: 89.1573257446\n",
      "training Loss: 0.0001936010 Acc: 89.1400451660\n",
      "validation Loss: 0.0001969749 Acc: 88.8806991577\n",
      "training Loss: 0.0001932512 Acc: 89.1605834961\n",
      "validation Loss: 0.0001972849 Acc: 89.2334213257\n",
      "training Loss: 0.0001929510 Acc: 89.1705474854\n",
      "validation Loss: 0.0001968599 Acc: 89.0691375732\n",
      "training Loss: 0.0001930001 Acc: 89.1657180786\n",
      "validation Loss: 0.0001966268 Acc: 88.8372116089\n",
      "training Loss: 0.0001924350 Acc: 89.1774978638\n",
      "validation Loss: 0.0001966229 Acc: 89.2515411377\n",
      "training Loss: 0.0001930083 Acc: 89.1959152222\n",
      "validation Loss: 0.0001962396 Acc: 89.1186676025\n",
      "training Loss: 0.0001924861 Acc: 89.1735687256\n",
      "validation Loss: 0.0001963975 Acc: 89.1005477905\n",
      "training Loss: 0.0001924641 Acc: 89.1735687256\n",
      "validation Loss: 0.0001966091 Acc: 89.2587890625\n",
      "training Loss: 0.0001924151 Acc: 89.1765899658\n",
      "validation Loss: 0.0001968303 Acc: 89.0196151733\n",
      "training Loss: 0.0001923865 Acc: 89.2221908569\n",
      "validation Loss: 0.0001964814 Acc: 89.0908813477\n",
      "training Loss: 0.0001921775 Acc: 89.2043762207\n",
      "validation Loss: 0.0001969603 Acc: 89.0631027222\n",
      "training Loss: 0.0001920399 Acc: 89.2466506958\n",
      "validation Loss: 0.0001964991 Acc: 89.1404113770\n",
      "training Loss: 0.0001922191 Acc: 89.2321548462\n",
      "validation Loss: 0.0001963637 Acc: 89.1549072266\n",
      "Early stopped.\n",
      "Best val acc: 89.356636\n",
      "----------\n",
      "training Loss: 0.0002840356 Acc: 82.4830932617\n",
      "validation Loss: 0.0002331457 Acc: 86.6066589355\n",
      "training Loss: 0.0002304994 Acc: 86.6653366089\n",
      "validation Loss: 0.0002335890 Acc: 85.5588989258\n",
      "training Loss: 0.0002248025 Acc: 87.2984008789\n",
      "validation Loss: 0.0002211021 Acc: 86.6151275635\n",
      "training Loss: 0.0002218119 Acc: 87.4115295410\n",
      "validation Loss: 0.0002195135 Acc: 87.6991729736\n",
      "training Loss: 0.0002197603 Acc: 87.5621566772\n",
      "validation Loss: 0.0002197756 Acc: 87.5902862549\n",
      "training Loss: 0.0002180521 Acc: 87.6244659424\n",
      "validation Loss: 0.0002182899 Acc: 88.7529754639\n",
      "training Loss: 0.0002167411 Acc: 87.7306289673\n",
      "validation Loss: 0.0002159199 Acc: 87.5273742676\n",
      "training Loss: 0.0002161109 Acc: 87.8374023438\n",
      "validation Loss: 0.0002163303 Acc: 88.0766601562\n",
      "training Loss: 0.0002164112 Acc: 87.7611846924\n",
      "validation Loss: 0.0002143508 Acc: 88.3186340332\n",
      "training Loss: 0.0002146472 Acc: 87.8189544678\n",
      "validation Loss: 0.0002132675 Acc: 88.9284133911\n",
      "training Loss: 0.0002139266 Acc: 87.9296569824\n",
      "validation Loss: 0.0002148446 Acc: 87.2781372070\n",
      "training Loss: 0.0002132464 Acc: 87.9753265381\n",
      "validation Loss: 0.0002117189 Acc: 87.6144866943\n",
      "training Loss: 0.0002131543 Acc: 88.0007400513\n",
      "validation Loss: 0.0002060608 Acc: 88.2702407837\n",
      "training Loss: 0.0002114759 Acc: 88.0110244751\n",
      "validation Loss: 0.0002099354 Acc: 89.3421859741\n",
      "training Loss: 0.0002106481 Acc: 88.1773757935\n",
      "validation Loss: 0.0002113594 Acc: 88.7844390869\n",
      "training Loss: 0.0002101476 Acc: 88.1347274780\n",
      "validation Loss: 0.0002069865 Acc: 88.2678146362\n",
      "training Loss: 0.0002098190 Acc: 88.2263793945\n",
      "validation Loss: 0.0002077743 Acc: 87.2708816528\n",
      "training Loss: 0.0002056193 Acc: 88.3715667725\n",
      "validation Loss: 0.0002026096 Acc: 88.4359893799\n",
      "training Loss: 0.0002053263 Acc: 88.4266128540\n",
      "validation Loss: 0.0002036700 Acc: 89.1304626465\n",
      "training Loss: 0.0002043021 Acc: 88.5246124268\n",
      "validation Loss: 0.0002015527 Acc: 88.7505569458\n",
      "training Loss: 0.0002042305 Acc: 88.5711975098\n",
      "validation Loss: 0.0002007948 Acc: 89.0965805054\n",
      "training Loss: 0.0002035488 Acc: 88.5149383545\n",
      "validation Loss: 0.0002007485 Acc: 88.2581405640\n",
      "training Loss: 0.0002036350 Acc: 88.6135406494\n",
      "validation Loss: 0.0002003029 Acc: 88.6791763306\n",
      "training Loss: 0.0002037992 Acc: 88.5530471802\n",
      "validation Loss: 0.0002024329 Acc: 87.7935485840\n",
      "training Loss: 0.0002027168 Acc: 88.6111221313\n",
      "validation Loss: 0.0001995490 Acc: 89.1231994629\n",
      "training Loss: 0.0002025174 Acc: 88.5724029541\n",
      "validation Loss: 0.0002005294 Acc: 88.7602386475\n",
      "training Loss: 0.0002027058 Acc: 88.6147537231\n",
      "validation Loss: 0.0001986377 Acc: 88.9876937866\n",
      "training Loss: 0.0002019533 Acc: 88.6313858032\n",
      "validation Loss: 0.0002011193 Acc: 88.9042129517\n",
      "training Loss: 0.0002021364 Acc: 88.5506286621\n",
      "validation Loss: 0.0001999631 Acc: 88.9804382324\n",
      "training Loss: 0.0002021449 Acc: 88.6725234985\n",
      "validation Loss: 0.0001990788 Acc: 88.4033203125\n",
      "training Loss: 0.0002017308 Acc: 88.6244277954\n",
      "validation Loss: 0.0001988213 Acc: 88.7057952881\n",
      "training Loss: 0.0001987720 Acc: 88.7499542236\n",
      "validation Loss: 0.0001970153 Acc: 89.2768554688\n",
      "training Loss: 0.0001982917 Acc: 88.8331298828\n",
      "validation Loss: 0.0001963323 Acc: 88.8207321167\n",
      "training Loss: 0.0001976932 Acc: 88.8361587524\n",
      "validation Loss: 0.0001969987 Acc: 89.1207809448\n",
      "training Loss: 0.0001980331 Acc: 88.8304138184\n",
      "validation Loss: 0.0001957081 Acc: 89.0264129639\n",
      "training Loss: 0.0001976027 Acc: 88.8694305420\n",
      "validation Loss: 0.0001966358 Acc: 88.8654937744\n",
      "training Loss: 0.0001976880 Acc: 88.8588409424\n",
      "validation Loss: 0.0001956197 Acc: 88.6295700073\n",
      "training Loss: 0.0001972880 Acc: 88.9196395874\n",
      "validation Loss: 0.0001952031 Acc: 89.1873245239\n",
      "training Loss: 0.0001968680 Acc: 88.9108657837\n",
      "validation Loss: 0.0001962305 Acc: 88.5702896118\n",
      "training Loss: 0.0001969365 Acc: 88.8800125122\n",
      "validation Loss: 0.0001960088 Acc: 89.3700180054\n",
      "training Loss: 0.0001968833 Acc: 88.8540039062\n",
      "validation Loss: 0.0001957839 Acc: 89.1268310547\n",
      "training Loss: 0.0001972515 Acc: 88.8945312500\n",
      "validation Loss: 0.0001954797 Acc: 89.4450302124\n",
      "training Loss: 0.0001958227 Acc: 88.9810409546\n",
      "validation Loss: 0.0001939044 Acc: 89.4244613647\n",
      "training Loss: 0.0001943370 Acc: 89.0642166138\n",
      "validation Loss: 0.0001941799 Acc: 88.7457199097\n",
      "training Loss: 0.0001950337 Acc: 88.9892044067\n",
      "validation Loss: 0.0001938335 Acc: 88.8122634888\n",
      "training Loss: 0.0001943012 Acc: 89.0415344238\n",
      "validation Loss: 0.0001941670 Acc: 89.2308807373\n",
      "training Loss: 0.0001943207 Acc: 89.0663375854\n",
      "validation Loss: 0.0001943113 Acc: 88.6803894043\n",
      "training Loss: 0.0001944928 Acc: 89.0624008179\n",
      "validation Loss: 0.0001941028 Acc: 88.4384078979\n",
      "training Loss: 0.0001942889 Acc: 88.9967651367\n",
      "validation Loss: 0.0001935652 Acc: 89.3397674561\n",
      "training Loss: 0.0001943497 Acc: 89.0678482056\n",
      "validation Loss: 0.0001937607 Acc: 89.4038925171\n",
      "training Loss: 0.0001943036 Acc: 89.0572662354\n",
      "validation Loss: 0.0001929829 Acc: 88.8921127319\n",
      "training Loss: 0.0001941654 Acc: 89.0161285400\n",
      "validation Loss: 0.0001934544 Acc: 89.4861679077\n",
      "training Loss: 0.0001945695 Acc: 89.0400238037\n",
      "validation Loss: 0.0001938188 Acc: 88.8050003052\n",
      "training Loss: 0.0001937404 Acc: 89.1123123169\n",
      "validation Loss: 0.0001928280 Acc: 89.2357177734\n",
      "training Loss: 0.0001936330 Acc: 89.0881118774\n",
      "validation Loss: 0.0001938929 Acc: 88.6077957153\n",
      "training Loss: 0.0001936355 Acc: 89.0642166138\n",
      "validation Loss: 0.0001933250 Acc: 89.3470306396\n",
      "training Loss: 0.0001936464 Acc: 89.0723876953\n",
      "validation Loss: 0.0001936960 Acc: 89.3579177856\n",
      "training Loss: 0.0001939194 Acc: 89.0509109497\n",
      "validation Loss: 0.0001938979 Acc: 89.0070495605\n",
      "training Loss: 0.0001923435 Acc: 89.1189651489\n",
      "validation Loss: 0.0001926632 Acc: 89.1135253906\n",
      "training Loss: 0.0001926850 Acc: 89.1319732666\n",
      "validation Loss: 0.0001923761 Acc: 89.0990066528\n",
      "training Loss: 0.0001921701 Acc: 89.1395339966\n",
      "validation Loss: 0.0001926951 Acc: 88.9332504272\n",
      "training Loss: 0.0001921884 Acc: 89.1410446167\n",
      "validation Loss: 0.0001923001 Acc: 89.3337173462\n",
      "training Loss: 0.0001919892 Acc: 89.2166671753\n",
      "validation Loss: 0.0001926042 Acc: 89.2478179932\n",
      "training Loss: 0.0001918808 Acc: 89.1265258789\n",
      "validation Loss: 0.0001919320 Acc: 88.9284133911\n",
      "training Loss: 0.0001920666 Acc: 89.1655502319\n",
      "validation Loss: 0.0001923396 Acc: 89.4934234619\n",
      "training Loss: 0.0001915998 Acc: 89.1543579102\n",
      "validation Loss: 0.0001922194 Acc: 89.2393493652\n",
      "training Loss: 0.0001913831 Acc: 89.1830902100\n",
      "validation Loss: 0.0001921252 Acc: 89.0373001099\n",
      "training Loss: 0.0001921401 Acc: 89.1673583984\n",
      "validation Loss: 0.0001925866 Acc: 89.0566558838\n",
      "training Loss: 0.0001914371 Acc: 89.1591949463\n",
      "validation Loss: 0.0001913159 Acc: 89.2804870605\n",
      "training Loss: 0.0001912104 Acc: 89.2136383057\n",
      "validation Loss: 0.0001916752 Acc: 89.4716491699\n",
      "training Loss: 0.0001911145 Acc: 89.2018432617\n",
      "validation Loss: 0.0001916602 Acc: 89.2538681030\n",
      "training Loss: 0.0001909216 Acc: 89.1864166260\n",
      "validation Loss: 0.0001915757 Acc: 89.1328811646\n",
      "training Loss: 0.0001910101 Acc: 89.2423706055\n",
      "validation Loss: 0.0001911994 Acc: 89.4776916504\n",
      "training Loss: 0.0001910848 Acc: 89.2166671753\n",
      "validation Loss: 0.0001913342 Acc: 89.4026794434\n",
      "training Loss: 0.0001906224 Acc: 89.2792739868\n",
      "validation Loss: 0.0001914565 Acc: 89.1631240845\n",
      "training Loss: 0.0001907934 Acc: 89.2060775757\n",
      "validation Loss: 0.0001914578 Acc: 89.3821182251\n",
      "training Loss: 0.0001906728 Acc: 89.2529602051\n",
      "validation Loss: 0.0001919780 Acc: 89.4244613647\n",
      "training Loss: 0.0001907975 Acc: 89.2317886353\n",
      "validation Loss: 0.0001916120 Acc: 89.1728057861\n",
      "training Loss: 0.0001902622 Acc: 89.2523574829\n",
      "validation Loss: 0.0001913492 Acc: 89.3772735596\n",
      "training Loss: 0.0001902272 Acc: 89.2408599854\n",
      "validation Loss: 0.0001913222 Acc: 89.2115249634\n",
      "Early stopped.\n",
      "Best val acc: 89.493423\n",
      "----------\n",
      "training Loss: 0.0002850313 Acc: 82.5862579346\n",
      "validation Loss: 0.0002332412 Acc: 87.0094833374\n",
      "training Loss: 0.0002313061 Acc: 86.7910385132\n",
      "validation Loss: 0.0002249763 Acc: 87.5364074707\n",
      "training Loss: 0.0002258514 Acc: 87.1895523071\n",
      "validation Loss: 0.0002190503 Acc: 86.8427047729\n",
      "training Loss: 0.0002225142 Acc: 87.3536148071\n",
      "validation Loss: 0.0002172825 Acc: 88.4621429443\n",
      "training Loss: 0.0002196120 Acc: 87.6533279419\n",
      "validation Loss: 0.0002181390 Acc: 87.4070892334\n",
      "training Loss: 0.0002191078 Acc: 87.5814208984\n",
      "validation Loss: 0.0002152958 Acc: 88.6845092773\n",
      "training Loss: 0.0002170614 Acc: 87.7747879028\n",
      "validation Loss: 0.0002121294 Acc: 87.3962173462\n",
      "training Loss: 0.0002160510 Acc: 87.8590850830\n",
      "validation Loss: 0.0002141067 Acc: 87.2971115112\n",
      "training Loss: 0.0002168870 Acc: 87.8593826294\n",
      "validation Loss: 0.0002125669 Acc: 88.8851242065\n",
      "training Loss: 0.0002139617 Acc: 87.9905090332\n",
      "validation Loss: 0.0002105633 Acc: 89.2972335815\n",
      "training Loss: 0.0002131055 Acc: 88.0116577148\n",
      "validation Loss: 0.0002120941 Acc: 86.7145996094\n",
      "training Loss: 0.0002125717 Acc: 87.9805374146\n",
      "validation Loss: 0.0002080155 Acc: 88.6663818359\n",
      "training Loss: 0.0002112085 Acc: 88.0977706909\n",
      "validation Loss: 0.0002113951 Acc: 89.4047927856\n",
      "training Loss: 0.0002115605 Acc: 88.1065292358\n",
      "validation Loss: 0.0002077218 Acc: 88.2373504639\n",
      "training Loss: 0.0002104975 Acc: 88.0880966187\n",
      "validation Loss: 0.0002044355 Acc: 88.7328491211\n",
      "training Loss: 0.0002098872 Acc: 88.1745071411\n",
      "validation Loss: 0.0002072642 Acc: 89.4047927856\n",
      "training Loss: 0.0002098008 Acc: 88.2609176636\n",
      "validation Loss: 0.0002060139 Acc: 88.2083511353\n",
      "training Loss: 0.0002107448 Acc: 88.2467193604\n",
      "validation Loss: 0.0002036328 Acc: 88.7715225220\n",
      "training Loss: 0.0002086464 Acc: 88.3065414429\n",
      "validation Loss: 0.0002057723 Acc: 87.6802215576\n",
      "training Loss: 0.0002082798 Acc: 88.3367538452\n",
      "validation Loss: 0.0002054853 Acc: 88.6555023193\n",
      "training Loss: 0.0002086283 Acc: 88.3032150269\n",
      "validation Loss: 0.0002051413 Acc: 88.1998901367\n",
      "training Loss: 0.0002099236 Acc: 88.2385635376\n",
      "validation Loss: 0.0002031896 Acc: 88.7908630371\n",
      "training Loss: 0.0002081669 Acc: 88.3165130615\n",
      "validation Loss: 0.0002025128 Acc: 88.0584869385\n",
      "training Loss: 0.0002078208 Acc: 88.3823776245\n",
      "validation Loss: 0.0002009697 Acc: 88.2723999023\n",
      "training Loss: 0.0002072666 Acc: 88.2938537598\n",
      "validation Loss: 0.0002022237 Acc: 88.7014312744\n",
      "training Loss: 0.0002075176 Acc: 88.2956619263\n",
      "validation Loss: 0.0002019905 Acc: 89.4180908203\n",
      "training Loss: 0.0002067355 Acc: 88.4104766846\n",
      "validation Loss: 0.0002047180 Acc: 89.5570678711\n",
      "training Loss: 0.0002073396 Acc: 88.3929519653\n",
      "validation Loss: 0.0002038140 Acc: 88.9624710083\n",
      "training Loss: 0.0002019226 Acc: 88.7138137817\n",
      "validation Loss: 0.0001976083 Acc: 88.6603393555\n",
      "training Loss: 0.0002020044 Acc: 88.6388854980\n",
      "validation Loss: 0.0001978593 Acc: 89.4748916626\n",
      "training Loss: 0.0002013342 Acc: 88.7265090942\n",
      "validation Loss: 0.0001989755 Acc: 89.3443679810\n",
      "training Loss: 0.0002018345 Acc: 88.7168350220\n",
      "validation Loss: 0.0001977167 Acc: 89.1002426147\n",
      "training Loss: 0.0002015347 Acc: 88.7216720581\n",
      "validation Loss: 0.0001983889 Acc: 88.8960037231\n",
      "training Loss: 0.0001986838 Acc: 88.8630676270\n",
      "validation Loss: 0.0001975560 Acc: 89.3890838623\n",
      "training Loss: 0.0001984332 Acc: 88.8920745850\n",
      "validation Loss: 0.0001951678 Acc: 89.3516159058\n",
      "training Loss: 0.0001976287 Acc: 88.8981170654\n",
      "validation Loss: 0.0001952997 Acc: 88.8657913208\n",
      "training Loss: 0.0001980789 Acc: 88.8673019409\n",
      "validation Loss: 0.0001955111 Acc: 88.7811889648\n",
      "training Loss: 0.0001976202 Acc: 88.8047561646\n",
      "validation Loss: 0.0001953373 Acc: 89.1932983398\n",
      "training Loss: 0.0001977617 Acc: 88.9129257202\n",
      "validation Loss: 0.0001969091 Acc: 89.7601013184\n",
      "training Loss: 0.0001965889 Acc: 88.9047622681\n",
      "validation Loss: 0.0001944711 Acc: 89.4483032227\n",
      "training Loss: 0.0001953147 Acc: 89.0461654663\n",
      "validation Loss: 0.0001943626 Acc: 89.0978240967\n",
      "training Loss: 0.0001952901 Acc: 88.9896621704\n",
      "validation Loss: 0.0001945198 Acc: 89.2343902588\n",
      "training Loss: 0.0001950378 Acc: 89.0652008057\n",
      "validation Loss: 0.0001939450 Acc: 89.4289627075\n",
      "training Loss: 0.0001953004 Acc: 89.0488815308\n",
      "validation Loss: 0.0001943614 Acc: 88.9540100098\n",
      "training Loss: 0.0001952931 Acc: 89.0053710938\n",
      "validation Loss: 0.0001943472 Acc: 88.9818115234\n",
      "training Loss: 0.0001954256 Acc: 88.9999389648\n",
      "validation Loss: 0.0001950846 Acc: 89.1401290894\n",
      "training Loss: 0.0001952538 Acc: 89.0425338745\n",
      "validation Loss: 0.0001936614 Acc: 89.3262405396\n",
      "training Loss: 0.0001951716 Acc: 89.0192718506\n",
      "validation Loss: 0.0001943376 Acc: 89.2851486206\n",
      "training Loss: 0.0001945042 Acc: 89.0609664917\n",
      "validation Loss: 0.0001939686 Acc: 89.1703414917\n",
      "training Loss: 0.0001948239 Acc: 89.0295486450\n",
      "validation Loss: 0.0001940509 Acc: 89.0772857666\n",
      "training Loss: 0.0001945074 Acc: 89.0422363281\n",
      "validation Loss: 0.0001935242 Acc: 89.1280441284\n",
      "training Loss: 0.0001945724 Acc: 89.0561294556\n",
      "validation Loss: 0.0001938750 Acc: 89.3310775757\n",
      "training Loss: 0.0001946347 Acc: 89.0546188354\n",
      "validation Loss: 0.0001938810 Acc: 89.3443679810\n",
      "training Loss: 0.0001939539 Acc: 89.0195770264\n",
      "validation Loss: 0.0001941858 Acc: 89.4289627075\n",
      "training Loss: 0.0001944741 Acc: 89.0724487305\n",
      "validation Loss: 0.0001951321 Acc: 88.7340621948\n",
      "training Loss: 0.0001936472 Acc: 89.0661010742\n",
      "validation Loss: 0.0001934589 Acc: 89.2198867798\n",
      "training Loss: 0.0001934221 Acc: 89.1235122681\n",
      "validation Loss: 0.0001933902 Acc: 89.3117370605\n",
      "training Loss: 0.0001931074 Acc: 89.0963134766\n",
      "validation Loss: 0.0001941815 Acc: 89.3540344238\n",
      "training Loss: 0.0001931213 Acc: 89.1652069092\n",
      "validation Loss: 0.0001936936 Acc: 89.3951263428\n",
      "training Loss: 0.0001930783 Acc: 89.1788024902\n",
      "validation Loss: 0.0001929370 Acc: 89.2573547363\n",
      "training Loss: 0.0001931385 Acc: 89.1497955322\n",
      "validation Loss: 0.0001929708 Acc: 89.0120239258\n",
      "training Loss: 0.0001929018 Acc: 89.1352920532\n",
      "validation Loss: 0.0001931628 Acc: 89.1836318970\n",
      "training Loss: 0.0001925820 Acc: 89.1676177979\n",
      "validation Loss: 0.0001926444 Acc: 89.1268310547\n",
      "training Loss: 0.0001927537 Acc: 89.0945053101\n",
      "validation Loss: 0.0001926328 Acc: 89.4700546265\n",
      "training Loss: 0.0001926743 Acc: 89.1621780396\n",
      "validation Loss: 0.0001929730 Acc: 89.1642990112\n",
      "training Loss: 0.0001927024 Acc: 89.1156539917\n",
      "validation Loss: 0.0001930408 Acc: 89.2718582153\n",
      "training Loss: 0.0001923210 Acc: 89.1579513550\n",
      "validation Loss: 0.0001931879 Acc: 89.2368087769\n",
      "training Loss: 0.0001916077 Acc: 89.1993408203\n",
      "validation Loss: 0.0001921168 Acc: 89.3866653442\n",
      "training Loss: 0.0001916682 Acc: 89.1842346191\n",
      "validation Loss: 0.0001925553 Acc: 89.3153610229\n",
      "training Loss: 0.0001918841 Acc: 89.1830291748\n",
      "validation Loss: 0.0001921046 Acc: 89.2936096191\n",
      "training Loss: 0.0001915564 Acc: 89.1736602783\n",
      "validation Loss: 0.0001926858 Acc: 89.1606674194\n",
      "training Loss: 0.0001920514 Acc: 89.1788024902\n",
      "validation Loss: 0.0001925904 Acc: 89.3032760620\n",
      "training Loss: 0.0001916278 Acc: 89.2259292603\n",
      "validation Loss: 0.0001926806 Acc: 89.2670211792\n",
      "training Loss: 0.0001913622 Acc: 89.2301635742\n",
      "validation Loss: 0.0001928249 Acc: 89.2658157349\n",
      "training Loss: 0.0001914421 Acc: 89.1612777710\n",
      "validation Loss: 0.0001921913 Acc: 89.3081130981\n",
      "training Loss: 0.0001914043 Acc: 89.2138442993\n",
      "validation Loss: 0.0001926684 Acc: 89.3902893066\n",
      "training Loss: 0.0001914430 Acc: 89.2389221191\n",
      "validation Loss: 0.0001915887 Acc: 89.2984466553\n",
      "training Loss: 0.0001907123 Acc: 89.2126388550\n",
      "validation Loss: 0.0001922866 Acc: 89.2815246582\n",
      "training Loss: 0.0001912512 Acc: 89.2470779419\n",
      "validation Loss: 0.0001922425 Acc: 89.2827301025\n",
      "training Loss: 0.0001911471 Acc: 89.2241210938\n",
      "validation Loss: 0.0001922715 Acc: 89.2428512573\n",
      "training Loss: 0.0001912433 Acc: 89.1972274780\n",
      "validation Loss: 0.0001920554 Acc: 89.3274459839\n",
      "training Loss: 0.0001909364 Acc: 89.2090148926\n",
      "validation Loss: 0.0001920976 Acc: 89.2682342529\n",
      "training Loss: 0.0001912618 Acc: 89.2494964600\n",
      "validation Loss: 0.0001920287 Acc: 89.2259292603\n",
      "training Loss: 0.0001910967 Acc: 89.2065963745\n",
      "validation Loss: 0.0001918979 Acc: 89.3117370605\n",
      "Early stopped.\n",
      "Best val acc: 89.760101\n",
      "----------\n",
      "training Loss: 0.0002880421 Acc: 82.7248001099\n",
      "validation Loss: 0.0002356675 Acc: 86.9348526001\n",
      "training Loss: 0.0002303849 Acc: 86.9485778809\n",
      "validation Loss: 0.0002273413 Acc: 85.3735046387\n",
      "training Loss: 0.0002234762 Acc: 87.3895950317\n",
      "validation Loss: 0.0002202537 Acc: 87.2654647827\n",
      "training Loss: 0.0002190565 Acc: 87.6209640503\n",
      "validation Loss: 0.0002214650 Acc: 87.9496078491\n",
      "training Loss: 0.0002184944 Acc: 87.6520385742\n",
      "validation Loss: 0.0002187684 Acc: 86.4871978760\n",
      "training Loss: 0.0002167816 Acc: 87.7835617065\n",
      "validation Loss: 0.0002119582 Acc: 87.6576080322\n",
      "training Loss: 0.0002158041 Acc: 87.8457031250\n",
      "validation Loss: 0.0002116510 Acc: 87.9109954834\n",
      "training Loss: 0.0002157047 Acc: 87.8285064697\n",
      "validation Loss: 0.0002132671 Acc: 88.3188323975\n",
      "training Loss: 0.0002137026 Acc: 87.9407196045\n",
      "validation Loss: 0.0002086896 Acc: 88.4491424561\n",
      "training Loss: 0.0002122520 Acc: 88.0719451904\n",
      "validation Loss: 0.0002108994 Acc: 88.1499023438\n",
      "training Loss: 0.0002120106 Acc: 88.0122146606\n",
      "validation Loss: 0.0002086093 Acc: 88.2054061890\n",
      "training Loss: 0.0002106071 Acc: 88.1262435913\n",
      "validation Loss: 0.0002078190 Acc: 88.0485534668\n",
      "training Loss: 0.0002123229 Acc: 88.0677185059\n",
      "validation Loss: 0.0002047189 Acc: 87.8603210449\n",
      "training Loss: 0.0002093706 Acc: 88.2375488281\n",
      "validation Loss: 0.0002037568 Acc: 88.8678359985\n",
      "training Loss: 0.0002094063 Acc: 88.2746505737\n",
      "validation Loss: 0.0002035312 Acc: 88.3019409180\n",
      "training Loss: 0.0002082268 Acc: 88.3334808350\n",
      "validation Loss: 0.0002077052 Acc: 87.3233795166\n",
      "training Loss: 0.0002091536 Acc: 88.3461456299\n",
      "validation Loss: 0.0002085645 Acc: 89.3999481201\n",
      "training Loss: 0.0002072959 Acc: 88.3518753052\n",
      "validation Loss: 0.0002098153 Acc: 88.9788436890\n",
      "training Loss: 0.0002070208 Acc: 88.3732986450\n",
      "validation Loss: 0.0002024679 Acc: 88.7906112671\n",
      "training Loss: 0.0002071457 Acc: 88.4423751831\n",
      "validation Loss: 0.0002042879 Acc: 88.5854873657\n",
      "training Loss: 0.0002073836 Acc: 88.4484100342\n",
      "validation Loss: 0.0002134839 Acc: 89.5652542114\n",
      "training Loss: 0.0002073817 Acc: 88.3823471069\n",
      "validation Loss: 0.0002024885 Acc: 89.2985992432\n",
      "training Loss: 0.0002056643 Acc: 88.4806823730\n",
      "validation Loss: 0.0002049747 Acc: 88.3007354736\n",
      "training Loss: 0.0002028318 Acc: 88.6128082275\n",
      "validation Loss: 0.0001999854 Acc: 89.2068939209\n",
      "training Loss: 0.0002016262 Acc: 88.6610717773\n",
      "validation Loss: 0.0002005549 Acc: 89.0536575317\n",
      "training Loss: 0.0002007479 Acc: 88.7588119507\n",
      "validation Loss: 0.0002004362 Acc: 87.6793289185\n",
      "training Loss: 0.0002004805 Acc: 88.7482528687\n",
      "validation Loss: 0.0001979276 Acc: 89.0777893066\n",
      "training Loss: 0.0002000568 Acc: 88.7594146729\n",
      "validation Loss: 0.0001994963 Acc: 88.9752273560\n",
      "training Loss: 0.0002009566 Acc: 88.7078323364\n",
      "validation Loss: 0.0001978518 Acc: 88.3863983154\n",
      "training Loss: 0.0001994668 Acc: 88.8040618896\n",
      "validation Loss: 0.0001981775 Acc: 88.2898712158\n",
      "training Loss: 0.0001998988 Acc: 88.7461395264\n",
      "validation Loss: 0.0001993582 Acc: 89.4904479980\n",
      "training Loss: 0.0001997479 Acc: 88.8037567139\n",
      "validation Loss: 0.0001966406 Acc: 88.3815765381\n",
      "training Loss: 0.0002003390 Acc: 88.7717819214\n",
      "validation Loss: 0.0002000025 Acc: 89.3999481201\n",
      "training Loss: 0.0001997457 Acc: 88.7808303833\n",
      "validation Loss: 0.0001968816 Acc: 88.7459716797\n",
      "training Loss: 0.0001999439 Acc: 88.7220077515\n",
      "validation Loss: 0.0001974657 Acc: 88.9607467651\n",
      "training Loss: 0.0001998503 Acc: 88.7461395264\n",
      "validation Loss: 0.0001986481 Acc: 89.0512390137\n",
      "training Loss: 0.0001970751 Acc: 88.9048156738\n",
      "validation Loss: 0.0001942960 Acc: 89.2829132080\n",
      "training Loss: 0.0001966475 Acc: 88.9470443726\n",
      "validation Loss: 0.0001950644 Acc: 89.4409790039\n",
      "training Loss: 0.0001960139 Acc: 89.0188369751\n",
      "validation Loss: 0.0001954373 Acc: 89.4047775269\n",
      "training Loss: 0.0001962651 Acc: 88.9078292847\n",
      "validation Loss: 0.0001951154 Acc: 89.1272583008\n",
      "training Loss: 0.0001960722 Acc: 88.9865646362\n",
      "validation Loss: 0.0001956735 Acc: 89.5350875854\n",
      "training Loss: 0.0001945167 Acc: 89.0245666504\n",
      "validation Loss: 0.0001938318 Acc: 89.0970916748\n",
      "training Loss: 0.0001939019 Acc: 89.0529251099\n",
      "validation Loss: 0.0001935021 Acc: 89.0874404907\n",
      "training Loss: 0.0001939376 Acc: 89.0710220337\n",
      "validation Loss: 0.0001927781 Acc: 89.0162506104\n",
      "training Loss: 0.0001933305 Acc: 89.0912322998\n",
      "validation Loss: 0.0001929218 Acc: 88.9088592529\n",
      "training Loss: 0.0001936822 Acc: 89.0583572388\n",
      "validation Loss: 0.0001924546 Acc: 88.8147430420\n",
      "training Loss: 0.0001933170 Acc: 89.0480957031\n",
      "validation Loss: 0.0001929983 Acc: 89.4747619629\n",
      "training Loss: 0.0001935409 Acc: 89.0655975342\n",
      "validation Loss: 0.0001935966 Acc: 89.0367584229\n",
      "training Loss: 0.0001926686 Acc: 89.1066207886\n",
      "validation Loss: 0.0001930303 Acc: 88.6518554688\n",
      "training Loss: 0.0001930320 Acc: 89.1213989258\n",
      "validation Loss: 0.0001930799 Acc: 88.9498901367\n",
      "training Loss: 0.0001922112 Acc: 89.1376876831\n",
      "validation Loss: 0.0001927411 Acc: 89.1417388916\n",
      "training Loss: 0.0001924156 Acc: 89.1060180664\n",
      "validation Loss: 0.0001920668 Acc: 89.2744674683\n",
      "training Loss: 0.0001916584 Acc: 89.1983261108\n",
      "validation Loss: 0.0001924790 Acc: 89.4952697754\n",
      "training Loss: 0.0001919342 Acc: 89.2312011719\n",
      "validation Loss: 0.0001918818 Acc: 89.3191070557\n",
      "training Loss: 0.0001915631 Acc: 89.1729812622\n",
      "validation Loss: 0.0001915251 Acc: 88.9884948730\n",
      "training Loss: 0.0001919871 Acc: 89.1530761719\n",
      "validation Loss: 0.0001918756 Acc: 89.1936187744\n",
      "training Loss: 0.0001917376 Acc: 89.1799240112\n",
      "validation Loss: 0.0001912121 Acc: 89.1525955200\n",
      "training Loss: 0.0001913528 Acc: 89.1687622070\n",
      "validation Loss: 0.0001925015 Acc: 89.2310256958\n",
      "training Loss: 0.0001909712 Acc: 89.1856536865\n",
      "validation Loss: 0.0001916689 Acc: 89.2201690674\n",
      "training Loss: 0.0001916702 Acc: 89.2043533325\n",
      "validation Loss: 0.0001921054 Acc: 89.1127777100\n",
      "training Loss: 0.0001913113 Acc: 89.1814270020\n",
      "validation Loss: 0.0001919695 Acc: 89.4204635620\n",
      "training Loss: 0.0001909215 Acc: 89.2025451660\n",
      "validation Loss: 0.0001918817 Acc: 89.3541030884\n",
      "training Loss: 0.0001907719 Acc: 89.2447814941\n",
      "validation Loss: 0.0001907662 Acc: 89.1912078857\n",
      "training Loss: 0.0001906254 Acc: 89.2019424438\n",
      "validation Loss: 0.0001913882 Acc: 89.2841186523\n",
      "training Loss: 0.0001904819 Acc: 89.2360305786\n",
      "validation Loss: 0.0001918708 Acc: 89.3516845703\n",
      "training Loss: 0.0001901262 Acc: 89.2321090698\n",
      "validation Loss: 0.0001916041 Acc: 89.1682815552\n",
      "training Loss: 0.0001909625 Acc: 89.1772079468\n",
      "validation Loss: 0.0001918231 Acc: 89.3565139771\n",
      "training Loss: 0.0001906713 Acc: 89.2351226807\n",
      "validation Loss: 0.0001913661 Acc: 89.2382659912\n",
      "training Loss: 0.0001902744 Acc: 89.2538299561\n",
      "validation Loss: 0.0001913660 Acc: 89.3359985352\n",
      "training Loss: 0.0001903949 Acc: 89.2396469116\n",
      "validation Loss: 0.0001911867 Acc: 89.3070449829\n",
      "Early stopped.\n",
      "Best val acc: 89.565254\n",
      "----------\n",
      "Average best_acc across k-fold: 89.53494262695312\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json'\n",
    "    best_conf = optimize_hyperparams( # NEED TO FIX THIS FUNC STILL !!!\n",
    "        data_list_dict, data_hlf_dict, label_dict, {fold: training_weights(data_aux_dict[f'fold_{fold}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold}']) for fold in len(data_list_dict)},\n",
    "        config_file, epochs=10,\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "    # with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "        best_conf = json.load(f)\n",
    "        print(best_conf)\n",
    "\n",
    "fom = []\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx in range(len(data_hlf_dict)):\n",
    "    weight = training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}'])\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], weight,\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    model_file = OUTPUT_DIRPATH + CURRENT_TIME +'_ReallyTopclassStyle_'+ f'{fold_idx}.torch'\n",
    "    state_file = OUTPUT_DIRPATH + CURRENT_TIME +'_BestPerfReallyTopclass_'+ f'{fold_idx}.torch'\n",
    "    \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], \n",
    "        dnn_input=np.shape(data_hlf_dict[f'fold_{fold_idx}'])[-1], rnn_input=np.shape(data_list_dict[f'fold_{fold_idx}'])[-1],\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(train_data_list, train_data_hlf, train_label, train_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(val_data_list, val_data_hlf, val_label, val_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        NUM_EPOCHS, model, optimizer, scheduler, \n",
    "        state_file, model_file, data_loader=data_loader, \n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(OUTPUT_DIRPATH+'/train+val_losses.json', 'w') as f:\n",
    "#     json.dump({'train_losses': train_losses_arr, 'val_losses': val_losses_arr}, f)\n",
    "\n",
    "# with open(OUTPUT_DIRPATH+'/train+val_losses.json', 'r') as f:\n",
    "#     train_val_losses_dict = json.load(f)\n",
    "# train_losses_arr = train_val_losses_dict['train_losses']\n",
    "# val_losses_arr = train_val_losses_dict['val_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "# weight_test_dict = {\n",
    "#     f'fold_{fold_idx}': copy.deepcopy(training_weights(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_test_dict[f'fold_{fold_idx}'])) for fold_idx in range(len(data_test_aux_dict))\n",
    "# }  # DO NOT USE SCALED FOR TRAINING!!\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, \n",
    "        save=True, dict_lists=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1736  |       0.9706      |    0.2847 +/- 0.0156     |\n",
      "|   0.2726  |       0.9500      |    0.2030 +/- 0.0117     |\n",
      "|   0.3999  |       0.9198      |    0.1379 +/- 0.0093     |\n",
      "|   0.8012  |       0.7538      |    0.0316 +/- 0.0022     |\n",
      "|   0.9332  |       0.5777      |    0.0089 +/- 0.0012     |\n",
      "|   0.9789  |       0.3839      |    0.0022 +/- 0.0003     |\n",
      "+-----------+-------------------+--------------------------+\n",
      "============================================================\n",
      "\n",
      "==============================0_lepton==============================\n",
      "0_lepton\n",
      "==============================1_lepton==============================\n",
      "1_lepton\n",
      "==============================2+_lepton==============================\n",
      "2+_lepton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "    NNtable.add_row(\n",
    "        [\n",
    "            IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "            \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "        ]\n",
    "    )\n",
    "print(NNtable)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = OUTPUT_DIRPATH + f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}/' + CURRENT_TIME\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [\n",
    "            (data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) \n",
    "            for i in range(len(data_test_aux_dict))\n",
    "        ]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "    print(plot_type)\n",
    "\n",
    "    weights_plot = {\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1) & mask_arr[fold_idx]],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0) & mask_arr[fold_idx]],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    density_weights_plot = {\n",
    "        fold_idx: {'sig': None, 'bkg': None} for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "\n",
    "    plot_train_val_losses(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME,\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    "    )\n",
    "    plot_roc(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], run2=False,\n",
    "        mask=None\n",
    "    )\n",
    "    # print(f\"num bkg: {np.sum(label_test==0)}\")\n",
    "    # print(f\"num sig: {np.sum(label_test==1)}\")\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, n_bins=25, \n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], n_bins=25, weights=density_weights_plot,\n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, n_bins=25,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    # for fold_idx in range(len(data_test_aux_dict)):\n",
    "    #     for score_cut in [0.2, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    #         plot_input_vars_after_score_cut(\n",
    "    #             IN_perf, score_cut, plot_destdir, fold_idx, method='round_robin', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #             mask=mask_arr[fold_idx]\n",
    "    #         )\n",
    "    #     plot_input_vars_after_score_cut(\n",
    "    #         IN_perf, [0.2, 0.6, 0.9], plot_destdir, fold_idx, method='round_robin', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #         mask=mask_arr[fold_idx]\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimized cut-boundaries for ttH score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(IN_perf, weights, method='arr', bins=50, mask=None, n_folds=5):\n",
    "    if method == 'round_robin':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "    elif method == 'arr':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "                    \n",
    "        sig_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "\n",
    "        cut_boundaries = []\n",
    "        cut_s_over_root_bs = []\n",
    "        prev_s_over_root_b = 0\n",
    "        sig_weights = []\n",
    "        bkg_weights = []\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                cut_boundaries.append(bins - i)\n",
    "                cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        cut_boundaries.append(0)\n",
    "        cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "        cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold, cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "==============================0_lepton==============================\n",
      "==============================1_lepton==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2275618/172047263.py:37: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================2+_lepton==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2275618/172047263.py:37: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
      "/tmp/ipykernel_2275618/172047263.py:40: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_2275618/172047263.py:41: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_2275618/1041492062.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    }
   ],
   "source": [
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = OUTPUT_DIRPATH + f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}/' + CURRENT_TIME\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [(data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "\n",
    "    weights_plot = {\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1) & mask_arr[fold_idx]],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0) & mask_arr[fold_idx]],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    \n",
    "    (\n",
    "        cut_boundaries_fold, cut_s_over_root_bs_fold, \n",
    "        sig_weights_fold, bkg_weights_fold\n",
    "    ) = optimize_cut_boundaries(\n",
    "        IN_perf, weights_plot, method='round_robin',\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    fold_labels = [\n",
    "        [\n",
    "            f\"s/√b={cut_s_over_root_bs_fold[fold_idx][cut_idx]:.04f}, s={sig_weights_fold[fold_idx][cut_idx]['value']:.04f}±{sig_weights_fold[fold_idx][cut_idx]['w2']:.04f}, b={bkg_weights_fold[fold_idx][cut_idx]['value']:.04f}±{bkg_weights_fold[fold_idx][cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[fold_idx]))\n",
    "        ] for fold_idx in range(len(weight_test_dict))\n",
    "    ]\n",
    "    fold_colors = [copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries_fold[i]) // len(cmap_petroff10)) + 1)) for i in range(len(weight_test_dict))]\n",
    "    for fold_idx in range(len(weight_test_dict)):\n",
    "        s_over_root_b(\n",
    "            IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_{plot_type}_fold{fold_idx}', \n",
    "            labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, method='round_robin',\n",
    "            lines_fold=cut_boundaries_fold, lines_labels=fold_labels, only_fold=fold_idx, lines_colors=fold_colors,\n",
    "            mask=mask_arr\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n"
     ]
    }
   ],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for fold_idx in range(len(data_list_dict)):\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "        training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            train_data_list, train_data_hlf, train_label, train_weight,\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, only_fold_idx=fold_idx,\n",
    "            # dict_lists=True\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            val_data_list, val_data_hlf, val_label, val_weight,\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, only_fold_idx=fold_idx,\n",
    "            # dict_lists=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "spans must have compatible lengths",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     16\u001b[0m density_weights_plot \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     fold_idx: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msig\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbkg\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m} \u001b[38;5;28;01mfor\u001b[39;00m fold_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_test_aux_dict))\n\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     20\u001b[0m plot_roc(\n\u001b[1;32m     21\u001b[0m     [train_IN_dict, val_IN_dict], plot_destdir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mCURRENT_TIME, plot_postfix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_train_val_comparison_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     22\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIN_arr\u001b[39m\u001b[38;5;124m'\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[labels_arr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_idx), labels_arr[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_idx)]\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 24\u001b[0m \u001b[43mplot_output_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_IN_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_IN_dict\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_destdir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mCURRENT_TIME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_postfix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_train_val_weighted_comparison\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIN_arr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlabels_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_plot\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m plot_output_score(\n\u001b[1;32m     29\u001b[0m     [train_IN_dict, val_IN_dict], plot_destdir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mCURRENT_TIME, plot_postfix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_train_val_density_comparison\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     30\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIN_arr\u001b[39m\u001b[38;5;124m'\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[labels_arr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_idx), labels_arr[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_idx)], weights\u001b[38;5;241m=\u001b[39mdensity_weights_plot\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     32\u001b[0m s_over_root_b(\n\u001b[1;32m     33\u001b[0m     [train_IN_dict, val_IN_dict], plot_destdir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mCURRENT_TIME, plot_postfix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_train_val_comparison\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     34\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIN_arr\u001b[39m\u001b[38;5;124m'\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[labels_arr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_idx), labels_arr[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_idx)], weights\u001b[38;5;241m=\u001b[39mweights_plot\n\u001b[1;32m     35\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 288\u001b[0m, in \u001b[0;36mplot_output_score\u001b[0;34m(IN_info, plot_prefix, plot_postfix, method, labels, weights, n_bins, all_sig, all_bkg, mask, n_folds)\u001b[0m\n\u001b[1;32m    280\u001b[0m bkg_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\n\u001b[1;32m    281\u001b[0m     IN_info[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_pred\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    282\u001b[0m )[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m     ),\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    286\u001b[0m ]\n\u001b[1;32m    287\u001b[0m hist_axis \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39maxis\u001b[38;5;241m.\u001b[39mRegular(n_bins, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1.\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar\u001b[39m\u001b[38;5;124m'\u001b[39m, growth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, underflow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overflow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 288\u001b[0m sig_hist \u001b[38;5;241m=\u001b[39m \u001b[43mhist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhist_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msig_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig_np\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m bkg_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(hist_axis, storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39mbkg_np, weight\u001b[38;5;241m=\u001b[39mweights[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbkg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m weights[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msig\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mones_like(bkg_np))\n\u001b[1;32m    290\u001b[0m hep\u001b[38;5;241m.\u001b[39mhistplot(\n\u001b[1;32m    291\u001b[0m     [sig_hist, bkg_hist],\n\u001b[1;32m    292\u001b[0m     yerr\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m weights[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msig\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     ], linestyle\u001b[38;5;241m=\u001b[39m[linestyles[i], linestyles[i]]\n\u001b[1;32m    298\u001b[0m )\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/hist/basehist.py:263\u001b[0m, in \u001b[0;36mBaseHist.fill\u001b[0;34m(self, weight, sample, threads, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll axes must be accounted for in fill, you may have used a disallowed name in the axes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m data \u001b[38;5;241m=\u001b[39m (data_dict[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim))\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/boost_histogram/_internal/hist.py:511\u001b[0m, in \u001b[0;36mHistogram.fill\u001b[0;34m(self, weight, sample, threads, *args)\u001b[0m\n\u001b[1;32m    508\u001b[0m     threads \u001b[38;5;241m=\u001b[39m cpu_count()\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threads \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m threads \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_ars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_ars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_ars\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hist\u001b[38;5;241m.\u001b[39m_storage_type \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    515\u001b[0m     _core\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mmean,\n\u001b[1;32m    516\u001b[0m     _core\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mweighted_mean,\n\u001b[1;32m    517\u001b[0m }:\n",
      "\u001b[0;31mValueError\u001b[0m: spans must have compatible lengths"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'r') as f:\n",
    "    IN_perf_dict = json.load(f)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold ']\n",
    "val_weights_arr = []\n",
    "for fold_idx, (train_IN_dict, val_IN_dict) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'])):\n",
    "    weights_plot = { # NEED TO FUIX THIS TO USE TRAIN AND VAL WEIGHTS, NOT WEIGHTS ACROSS ALL FOLDS\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1)],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0)],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    density_weights_plot = {\n",
    "        fold_idx: {'sig': None, 'bkg': None} for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_weighted_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights_plot\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_density_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=density_weights_plot\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights_plot\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Vars Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict[\"fold_0\"]:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "        sig_mask = (label_dict[f'fold_{fold_idx}'] == 1)\n",
    "        sig_test_mask = (label_test_dict[f'fold_{fold_idx}'] == 1)\n",
    "        bkg_mask = (label_dict[f'fold_{fold_idx}'] == 0)\n",
    "        bkg_test_mask = (label_test_dict[f'fold_{fold_idx}'] == 0)\n",
    "        \n",
    "        sig_train_mask = (all_indices == train_indices) & sig_mask\n",
    "        sig_val_mask = (all_indices == val_indices) & sig_mask\n",
    "        bkg_train_mask = (all_indices == train_indices) & bkg_mask\n",
    "        bkg_val_mask = (all_indices == val_indices) & bkg_mask\n",
    "\n",
    "        sig_train_np = data_df_dict[f'fold_{fold_idx}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "        sig_val_np = data_df_dict[f'fold_{fold_idx}'].loc[sig_val_mask, var_name].to_numpy()\n",
    "        sig_test_np = data_test_df_dict[f'fold_{fold_idx}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "        bkg_train_np = data_df_dict[f'fold_{fold_idx}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "        bkg_val_np = data_df_dict[f'fold_{fold_idx}'].loc[bkg_val_mask, var_name].to_numpy()\n",
    "        bkg_test_np = data_test_df_dict[f'fold_{fold_idx}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    # sig_train_np = data_df.loc[sig_mask, var_name].to_numpy()\n",
    "    # sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "    # bkg_train_np = data_df.loc[bkg_mask, var_name].to_numpy()\n",
    "    # bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "    # sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    # sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    # bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    # bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    # pre_std_hists[var_name] = [\n",
    "    #     copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "    #     copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    # ]\n",
    "    # make_input_plot(output_dir_pre_std, var_name, pre_std_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### post-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 44\u001b[0m\n\u001b[1;32m     37\u001b[0m     bkg_test_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39mbkg_test_np[bkg_test_np \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     39\u001b[0m     make_input_plot(\n\u001b[1;32m     40\u001b[0m         output_dir_post_std, var_name, \n\u001b[1;32m     41\u001b[0m         [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n\u001b[1;32m     42\u001b[0m         fold_idx\u001b[38;5;241m=\u001b[39mfold_idx, labels\u001b[38;5;241m=\u001b[39mlabel_arr_fold\n\u001b[1;32m     43\u001b[0m     )\n\u001b[0;32m---> 44\u001b[0m sig_train_np, sig_test_np, bkg_train_np, bkg_test_np \u001b[38;5;241m=\u001b[39m \u001b[43mpost_std_np_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m sig_train_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39msig_train_np[sig_train_np \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     47\u001b[0m sig_test_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39msig_test_np[sig_test_np \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[18], line 159\u001b[0m, in \u001b[0;36mpost_std_np_arrays\u001b[0;34m(data, data_test, var_name, train_index, val_index)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m train_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m val_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m (high_level_fields \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(input_hlf_vars)):\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;66;03m# index2, index3 = index_map[var_name]\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m         sig_train_np \u001b[38;5;241m=\u001b[39m data[\u001b[43mdata_list_index_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig_mask\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    160\u001b[0m         sig_test_np \u001b[38;5;241m=\u001b[39m data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n\u001b[1;32m    161\u001b[0m         bkg_train_np \u001b[38;5;241m=\u001b[39m data[data_list_index_map(var_name, data, bkg_mask)]\n",
      "File \u001b[0;32m/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/data_processing.py:30\u001b[0m, in \u001b[0;36mdata_list_index_map\u001b[0;34m(variable_name, data_list, event_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_list)):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m event_mask[i]:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     lepton1_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(np\u001b[38;5;241m.\u001b[39mwhere(data_list[i, :, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m     mask_arr[i, lepton1_idx, index3] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "\n",
    "        data, data_test = None, None\n",
    "        if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "            data, data_test = data_list_dict[f'fold_{fold_idx}'], data_list_test_dict[f'fold_{fold_idx}']\n",
    "        else:\n",
    "            data, data_test = data_hlf_dict[f'fold_{fold_idx}'], data_hlf_test_dict[f'fold_{fold_idx}']\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name,\n",
    "            train_index=(all_indices == train_indices), val_index=(all_indices == val_indices)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    # sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    # sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    # sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    # bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    # bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    # post_std_hists[var_name] = [\n",
    "    #     copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "    #     copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    # ]\n",
    "    # make_input_plot(output_dir_post_std, var_name, post_std_hists[var_name], labels=label_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian smearing on test set (for feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    mask_arr = data_list_index_map(var_name, particle_list_to_smear, np.ones(len(particle_list_to_smear), dtype=bool), n_pFields=N_PARTICLE_FIELDS)\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear[mask_arr] *= rng.normal(size=len(particle_list_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear[mask_arr] += rng.normal(size=len(particle_list_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns_dict['fold_0'][var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate smeared variable test-data on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "\n",
    "    weight_test_dict = {\n",
    "        f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "\n",
    "    gauss_data_list_dict, gauss_data_hlf_dict = None, None\n",
    "    if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "        gauss_data_list_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_test_dict))\n",
    "        }\n",
    "        gauss_data_hlf_dict = data_hlf_test_dict\n",
    "    else:\n",
    "        gauss_data_list_dict = data_list_test_dict\n",
    "        gauss_data_hlf_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_test_dict))\n",
    "        }\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list_dict, gauss_data_hlf_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "IN_perf_smear_list = []\n",
    "label_arr = []\n",
    "for var_name, IN_perf_smear in IN_perf_smear_dict.items():\n",
    "    IN_perf_smear_list.append(IN_perf_smear)\n",
    "    label_arr.append(var_name)\n",
    "sort = np.argsort([IN_perf_smear['mean_area'] for IN_perf_smear in IN_perf_smear_list])\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_all', \n",
    "    method='IN_arr', labels=label_arr, yscale='log', run2=False, sort=sort\n",
    ")\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5', \n",
    "    method='IN_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False\n",
    ")\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5_and_orig', \n",
    "    method='IN_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False, run3=IN_perf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields_dict['fold_0']:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "\n",
    "    gauss_data_list_dict, gauss_data_hlf_dict = None, None\n",
    "    if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "        gauss_data_list_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_dict))\n",
    "        }\n",
    "        gauss_data_hlf_dict = data_hlf_dict\n",
    "\n",
    "        gauss_data_list_test_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_list(var_name, copy.deepcopy(data_list_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_list_test_dict))\n",
    "        }\n",
    "        gauss_data_hlf_test_dict = data_hlf_test_dict\n",
    "    else:\n",
    "        gauss_data_list_dict = data_list_dict\n",
    "        gauss_data_hlf_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_dict))\n",
    "        }\n",
    "\n",
    "        gauss_data_list_test_dict = data_list_test_dict\n",
    "        gauss_data_hlf_test_dict = {\n",
    "            f\"fold_{fold_idx}\": smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_hlf_test_dict))\n",
    "        }\n",
    "    \n",
    "    for fold_idx in range(len(data_list_dict)):\n",
    "        all_indices = np.arange(np.shape(data_list_dict[f'fold_{fold_idx}'])[0])\n",
    "        (\n",
    "            train_data_list, val_data_list,\n",
    "            train_data_hlf, val_data_hlf,\n",
    "            train_label, val_label,\n",
    "            train_weight, val_weight,\n",
    "            train_indices, val_indices\n",
    "        ) = train_test_split(\n",
    "            data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "            training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "            all_indices,\n",
    "            test_size=0.2, random_state=SEED\n",
    "        )\n",
    "\n",
    "        data, data_test = None, None\n",
    "        if var_name in (high_level_fields_dict['fold_0'] - set(input_hlf_vars_dict['fold_0'])):\n",
    "            data, data_test = gauss_data_list_dict[f'fold_{fold_idx}'], gauss_data_list_test_dict[f'fold_{fold_idx}']\n",
    "        else:\n",
    "            data, data_test = gauss_data_hlf_dict[f'fold_{fold_idx}'], gauss_data_hlf_test_dict[f'fold_{fold_idx}']\n",
    "        \n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name,\n",
    "            train_index=(all_indices == train_indices), val_index=(all_indices == val_indices)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    # sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    # sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    # sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    # bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    # bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    # gauss_hists[var_name] = [\n",
    "    #     copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "    #     copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    # ]\n",
    "    # make_input_plot(output_dir_gauss_smear, var_name, gauss_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "\n",
    "weight_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_aux_dict))\n",
    "}\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y, w2 in [('train', data_list_dict, data_hlf_dict, label_dict, weight_dict), ('test', data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, w2,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'r') as f:\n",
    "    IN_full_eval_dict = json.load(f)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {'mass': [], 'dijet_mass': []}\n",
    "for var_name in hist_dict.keys():\n",
    "    for i, score_cut in enumerate(score_cuts):\n",
    "        sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict)\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "        hist_dict[var_name].extend(\n",
    "            [\n",
    "                copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "            ]\n",
    "        )\n",
    "    for mod_factor, label_mod in enumerate(['sig_train', 'sig_test', 'bkg_train', 'bkg_test']):\n",
    "        plot_list = []\n",
    "        label_list = []\n",
    "        for i in range(len(hist_dict[var_name])):\n",
    "            if (i - mod_factor) % 4 == 0:\n",
    "                plot_list.append(hist_dict[var_name][i])\n",
    "                label_list.append(label_arr[i])\n",
    "        make_input_plot(\n",
    "            plot_destdir, var_name, plot_list, labels=label_list, density=True, \n",
    "            plot_prefix=CURRENT_TIME+'_', plot_postfix='_'+label_mod, alpha=0.5,\n",
    "            linestyle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset size plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_dirs = glob.glob(OUTPUT_DIRPATH[:-1]+'_mod*') + [OUTPUT_DIRPATH[:-1]]\n",
    "final_train_losses_arr, final_val_losses_arr = [], []\n",
    "mod_values_arr = []\n",
    "\n",
    "for train_size_dir in train_size_dirs:\n",
    "    if len(glob.glob(train_size_dir + '/*IN_perf.json')) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        mod_values_arr.append([\n",
    "            float(\n",
    "                train_size_dir[\n",
    "                    train_size_dir.find('_mod')+4 : train_size_dir.find('-')\n",
    "                ]\n",
    "            ),\n",
    "            float(\n",
    "                train_size_dir[train_size_dir.find('-')+1:]\n",
    "            )\n",
    "        ])\n",
    "    except:\n",
    "         mod_values_arr.append([2, 2])\n",
    "    IN_perf_path = glob.glob(f'{train_size_dir}/*IN_perf.json')[0]\n",
    "    with open(IN_perf_path, 'r') as f:\n",
    "        IN_perf = json.load(f)\n",
    "    final_train_losses_arr.append([train_losses[-7 if len(train_losses) < NUM_EPOCHS else -1] for train_losses in IN_perf['train_losses_arr']])\n",
    "    final_val_losses_arr.append([val_losses[-7 if len(val_losses) < NUM_EPOCHS else -1] for val_losses in IN_perf['val_losses_arr']])\n",
    "\n",
    "final_train_losses_arr = np.array(final_train_losses_arr)\n",
    "final_val_losses_arr = np.array(final_val_losses_arr)\n",
    "mod_values_arr = np.array(mod_values_arr)\n",
    "dataset_sizes = (len(label) + len(label_test)) / mod_values_arr\n",
    "sorted_indices = np.argsort(dataset_sizes[:, 0])\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_train_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_train_losses_arr, axis=1)-np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_train_losses_arr, axis=1)+np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[0], alpha=0.5, label='Train data'\n",
    ")\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_val_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_val_losses_arr, axis=1)-np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_val_losses_arr, axis=1)+np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[1], alpha=0.5, label='Val data'\n",
    ")\n",
    "plt.xlabel('Size of train dataset')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.pdf')\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
