{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu3.fnal.gov      Fri Oct 18 17:12:39 2024  555.42.06\n",
      "[0] Tesla P100-PCIE-12GB | 54Â°C,  95 % |   308 / 12288 MB | aherrera(306M)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import auc\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "V2_MERGED = True\n",
    "\n",
    "SIGNAL_FILEPATHS = [\n",
    "    # Test sig files #\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE/GluGluToHH/nominal/*',\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE_merged_v2/GluGluToHH/nominal/*',\n",
    "    # ggF HH # \n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\",\n",
    "    # VBF HH #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [\n",
    "    # ttH (i.e. the main bkg to reduce) #\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    # # Other single H samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\",\n",
    "    # # Prompt-Prompt samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\",\n",
    "    # # Prompt-Fake samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\",\n",
    "    # Fake-Fake samples #\n",
    "    # ADD HERE #\n",
    "]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v9'\n",
    "CRITERION = \"NLLLoss\"\n",
    "N_PARTICLES, N_PARTICLE_FIELDS = 6, 9\n",
    "MOD_VALS = (5, 5)\n",
    "# VARS = 'base_vars'\n",
    "# VARS = 'extra_vars'\n",
    "# CURRENT_TIME = '2024-08-30_14-35-01'\n",
    "# VARS = 'extra_vars+'\n",
    "# CURRENT_TIME = '2024-10-09_20-47-24'\n",
    "VARS = 'extra_vars+max+moved_vars_to_RNN'\n",
    "# VARS = 'extra_vars_no_dijet_mass'\n",
    "# VARS = 'no_bad_vars'\n",
    "# VARS = 'simplified_bad_vars'\n",
    "# VARS = 'extra_vars_and_bools'\n",
    "# VARS = 'extra_vars_in_RNN'\n",
    "# VARS = f'extra_vars_mod{MOD_VALS[0]}-{MOD_VALS[1]}'\n",
    "# VARS = 'extra_vars_lead_lep_only'\n",
    "# CURRENT_TIME = '2024-10-02_18-03-26'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 3, 6\n",
    "# VARS = 'extra_vars_no_lep'\n",
    "# CURRENT_TIME = '2024-10-04_12-49-31'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 2, 6\n",
    "OUTPUT_DIRPATH = CURRENT_DIRPATH + f\"/model_outputs/{VERSION}/{VARS}/\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413735, 6, 9)\n",
      "Data HLF: (413735, 8)\n",
      "n signal = 136530, n bkg = 277205\n",
      "Data list test: (103521, 6, 9)\n",
      "Data HLF test: (103521, 8)\n",
      "n signal = 34224, n bkg = 69297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413918, 6, 9)\n",
      "Data HLF: (413918, 8)\n",
      "n signal = 136466, n bkg = 277452\n",
      "Data list test: (103338, 6, 9)\n",
      "Data HLF test: (103338, 8)\n",
      "n signal = 34288, n bkg = 69050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413265, 6, 9)\n",
      "Data HLF: (413265, 8)\n",
      "n signal = 136638, n bkg = 276627\n",
      "Data list test: (103991, 6, 9)\n",
      "Data HLF test: (103991, 8)\n",
      "n signal = 34116, n bkg = 69875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413725, 6, 9)\n",
      "Data HLF: (413725, 8)\n",
      "n signal = 136671, n bkg = 277054\n",
      "Data list test: (103531, 6, 9)\n",
      "Data HLF test: (103531, 8)\n",
      "n signal = 34083, n bkg = 69448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniforge3/envs/higgs-dna/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (414381, 6, 9)\n",
      "Data HLF: (414381, 8)\n",
      "n signal = 136711, n bkg = 277670\n",
      "Data list test: (102875, 6, 9)\n",
      "Data HLF test: (102875, 8)\n",
      "n signal = 34043, n bkg = 68832\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    data_df_dict, data_test_df_dict, \n",
    "    data_list_dict, data_hlf_dict, label_dict, \n",
    "    data_list_test_dict, data_hlf_test_dict, label_test_dict, \n",
    "    high_level_fields_dict, input_hlf_vars_dict, hlf_vars_columns_dict,\n",
    "    data_aux_dict, data_test_aux_dict\n",
    ") = process_data(\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED, mod_vals=MOD_VALS, k_fold_test=True\n",
    ")\n",
    "# skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_weights(event_weights, labels):\n",
    "    sum_of_bkg = np.sum(event_weights[labels==0])\n",
    "    sum_of_sig = np.sum(event_weights[labels==1])\n",
    "\n",
    "    sig_scale_factor = sum_of_bkg / sum_of_sig\n",
    "\n",
    "    weights = np.where(labels==0, event_weights, event_weights*sig_scale_factor)\n",
    "    mean_weight = np.mean(weights)\n",
    "    abs_weights = np.abs(weights)\n",
    "    scaled_weights = abs_weights / mean_weight\n",
    "\n",
    "    return scaled_weights\n",
    "\n",
    "def plot_train_val_losses(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None, n_folds=5\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dotted']\n",
    "        linestyles = linestyles * ((2*len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:2*len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx if fold_idx%2 == 0 else fold_idx+1]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx+1 if fold_idx%2 == 0 else fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='std', \n",
    "    labels=None, yscale='linear', run2=True, sort=None, run3=None,\n",
    "    mask=None, n_folds=5\n",
    "):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d, AUC = %.4f\" % (fold_idx, float(auc(IN_info['fprs'][fold_idx],IN_info['base_tpr']))), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for i in index_arr:\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_fprs'])[0], dtype=bool)\n",
    "            plt.plot(\n",
    "                np.array(IN_info[i]['mean_fprs'])[mask], np.array(IN_info[i]['base_tpr'])[mask], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i], alpha=0.5\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if run3 is not None:\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(run3['mean_fprs'])[0], dtype=bool)\n",
    "        plt.plot(\n",
    "            np.array(run3['mean_fprs'])[mask], np.array(run3['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (run3['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, n_bins=50, all_sig=False, all_bkg=False,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        # for cut in np.linspace(0, 1, 10, endpoint=False):\n",
    "        #     print(f\"output score > {cut:.2f}\")\n",
    "        #     print('='*60)\n",
    "        #     print(f\"num sig > {cut:.2f} = {len(sig_np[sig_np > cut])}\")\n",
    "        #     print(f\"num bkg > {cut:.2f} = {len(bkg_np[bkg_np > cut])}\")\n",
    "        #     print('-'*60)\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'] if weights[fold_idx]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'] if weights[fold_idx]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[fold_idx]['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights[fold_idx]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=1, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=[\n",
    "                f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal', \n",
    "                f'{\"ttH\" if not all_bkg else \"all\"} background'\n",
    "            ]\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'] if weights[i]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'] if weights[i]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[i]['sig'] is not None else False),\n",
    "                alpha=0.7, density=(False if weights[i]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[i] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[i], linestyles[i]]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def s_over_root_b(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, lines_fold=None, lines=None, lines_labels=None, \n",
    "        lines_colors=None, only_fold=None, no_fold=False, n_bins=50,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/âb', alpha=0.8 \n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "    elif method == 'arr':\n",
    "        if mask is None or np.all(mask):\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "        if only_fold is None:\n",
    "            sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb - avg. over folds', \n",
    "                alpha=0.5, \n",
    "            )\n",
    "            if lines is not None:\n",
    "                for i in range(len(lines)):\n",
    "                    plt.vlines(\n",
    "                        lines[i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb - avg.'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[i]\n",
    "                    )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'])\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "                label='s/âb'+(' - '+labels[i] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[i], \n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    # 'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # 'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Integer(0, 10, name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($\\Sigma E_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    # 'jet1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # 'jet2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -10., 4., name='var', label=r'ln($\\chi_{t0}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -10., 4., name='var', label=r'ln($\\chi_{t1}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma$ ln($p_{T}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'ln($M_{jj}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, fold, var_name, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label_dict[f'fold_{fold}'] == 1\n",
    "    sig_test_mask = label_test_dict[f'fold_{fold}'] == 1\n",
    "    bkg_mask = label_dict[f'fold_{fold}'] == 0\n",
    "    bkg_test_mask = label_test_dict[f'fold_{fold}'] == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            # index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold):\n",
    "    sig_train_mask = (label_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux_dict[f'fold_{fold}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux_dict[f'fold_{fold}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux_dict[f'fold_{fold}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux_dict[f'fold_{fold}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(\n",
    "    output_dir, var_name, hist_list, fold, fold_idx=None, labels=None, density=True, \n",
    "    plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True\n",
    "):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir = output_dir + \"fold/\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_input_vars_after_score_cut(\n",
    "    IN_info, score_cut, destdir, fold, plot_prefix, plot_postfix='', method='std', \n",
    "    weights={'sig': None, 'bkg': None}, all_sig=False, all_bkg=False,\n",
    "    mask=None\n",
    "):\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "        bkg_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            sig_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=sig_var.loc[sig_mask], \n",
    "                weight=weights['sig'][sig_mask] if weights['sig'] is not None else np.ones(np.sum(sig_mask))\n",
    "            )\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            bkg_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=bkg_var.loc[bkg_mask], \n",
    "                weight=weights['bkg'][bkg_mask] if weights['bkg'] is not None else np.ones(np.sum(bkg_mask))\n",
    "            )\n",
    "            make_input_plot(\n",
    "                destdir, var_name, [sig_hist, bkg_hist], fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore{score_cut}', labels=['HH signal', 'ttH background'], density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['all_preds'][fold])[0], dtype=bool)\n",
    "        \n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut if score_cut is list else [score_cut]:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['all_preds'][fold]\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['all_labels'][fold]) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['all_preds'][fold]\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['all_labels'][fold]) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used method 'std'. You used {method}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0002676012 Acc: 83.1259689331\n",
      "validation Loss: 0.0002285994 Acc: 86.8599472046\n",
      "training Loss: 0.0002277657 Acc: 87.1877517700\n",
      "validation Loss: 0.0002197391 Acc: 87.4859466553\n",
      "training Loss: 0.0002201446 Acc: 87.5575561523\n",
      "validation Loss: 0.0002174277 Acc: 88.9095687866\n",
      "training Loss: 0.0002164790 Acc: 87.8210067749\n",
      "validation Loss: 0.0002153156 Acc: 88.1578750610\n",
      "training Loss: 0.0002130651 Acc: 88.0098342896\n",
      "validation Loss: 0.0002094968 Acc: 88.2497253418\n",
      "training Loss: 0.0002108733 Acc: 88.1618041992\n",
      "validation Loss: 0.0002104236 Acc: 87.9355087280\n",
      "training Loss: 0.0002099319 Acc: 88.1814422607\n",
      "validation Loss: 0.0002057146 Acc: 87.5536270142\n",
      "training Loss: 0.0002089586 Acc: 88.2261581421\n",
      "validation Loss: 0.0002052077 Acc: 88.0914077759\n",
      "training Loss: 0.0002086890 Acc: 88.2473068237\n",
      "validation Loss: 0.0002036120 Acc: 88.0793228149\n",
      "training Loss: 0.0002070929 Acc: 88.4267654419\n",
      "validation Loss: 0.0002045019 Acc: 88.3294830322\n",
      "training Loss: 0.0002066830 Acc: 88.3714828491\n",
      "validation Loss: 0.0002021202 Acc: 88.6642379761\n",
      "training Loss: 0.0002055618 Acc: 88.4421768188\n",
      "validation Loss: 0.0002020053 Acc: 88.8032150269\n",
      "training Loss: 0.0002049378 Acc: 88.5158920288\n",
      "validation Loss: 0.0002012131 Acc: 88.6968688965\n",
      "training Loss: 0.0002049301 Acc: 88.4409713745\n",
      "validation Loss: 0.0002038075 Acc: 88.5881042480\n",
      "training Loss: 0.0002033864 Acc: 88.5868988037\n",
      "validation Loss: 0.0002006899 Acc: 88.7125778198\n",
      "training Loss: 0.0002045467 Acc: 88.5294952393\n",
      "validation Loss: 0.0002006040 Acc: 88.9941635132\n",
      "training Loss: 0.0002039069 Acc: 88.5376510620\n",
      "validation Loss: 0.0001996742 Acc: 89.0896301270\n",
      "training Loss: 0.0002033186 Acc: 88.5666503906\n",
      "validation Loss: 0.0002018342 Acc: 88.6412811279\n",
      "training Loss: 0.0002034901 Acc: 88.6171112061\n",
      "validation Loss: 0.0001991853 Acc: 88.8370590210\n",
      "training Loss: 0.0002028208 Acc: 88.5539627075\n",
      "validation Loss: 0.0001997537 Acc: 88.8612289429\n",
      "training Loss: 0.0002029715 Acc: 88.6101608276\n",
      "validation Loss: 0.0002019882 Acc: 89.1125946045\n",
      "training Loss: 0.0002027985 Acc: 88.6711883545\n",
      "validation Loss: 0.0002021226 Acc: 89.1996078491\n",
      "training Loss: 0.0002031937 Acc: 88.6542663574\n",
      "validation Loss: 0.0002040364 Acc: 89.3349609375\n",
      "training Loss: 0.0001987394 Acc: 88.8397750854\n",
      "validation Loss: 0.0001954512 Acc: 88.9337387085\n",
      "training Loss: 0.0001971461 Acc: 88.9038238525\n",
      "validation Loss: 0.0001945260 Acc: 89.3869247437\n",
      "training Loss: 0.0001970561 Acc: 88.8820724487\n",
      "validation Loss: 0.0001960800 Acc: 88.8829803467\n",
      "training Loss: 0.0001976748 Acc: 88.8959732056\n",
      "validation Loss: 0.0001966020 Acc: 88.4152908325\n",
      "training Loss: 0.0001967375 Acc: 88.9162139893\n",
      "validation Loss: 0.0001952497 Acc: 88.7379608154\n",
      "training Loss: 0.0001970069 Acc: 88.9219512939\n",
      "validation Loss: 0.0001944442 Acc: 88.8273849487\n",
      "training Loss: 0.0001964574 Acc: 88.9406814575\n",
      "validation Loss: 0.0001967857 Acc: 89.3736343384\n",
      "training Loss: 0.0001967597 Acc: 88.9491424561\n",
      "validation Loss: 0.0001957512 Acc: 89.1766433716\n",
      "training Loss: 0.0001967416 Acc: 88.9358520508\n",
      "validation Loss: 0.0001961238 Acc: 87.9729766846\n",
      "training Loss: 0.0001962133 Acc: 88.8624343872\n",
      "validation Loss: 0.0001969649 Acc: 88.2944412231\n",
      "training Loss: 0.0001943499 Acc: 89.0140991211\n",
      "validation Loss: 0.0001955545 Acc: 88.8817672729\n",
      "training Loss: 0.0001935409 Acc: 89.0636520386\n",
      "validation Loss: 0.0001931956 Acc: 89.1512680054\n",
      "training Loss: 0.0001930009 Acc: 89.0379714966\n",
      "validation Loss: 0.0001933404 Acc: 89.3591308594\n",
      "training Loss: 0.0001929266 Acc: 89.0835876465\n",
      "validation Loss: 0.0001938380 Acc: 89.1863098145\n",
      "training Loss: 0.0001932059 Acc: 89.0938644409\n",
      "validation Loss: 0.0001944740 Acc: 88.8056335449\n",
      "training Loss: 0.0001929535 Acc: 89.1258850098\n",
      "validation Loss: 0.0001935083 Acc: 88.7101593018\n",
      "training Loss: 0.0001914011 Acc: 89.1974945068\n",
      "validation Loss: 0.0001921227 Acc: 89.1488494873\n",
      "training Loss: 0.0001913322 Acc: 89.2159194946\n",
      "validation Loss: 0.0001921413 Acc: 89.1295166016\n",
      "training Loss: 0.0001910590 Acc: 89.2119903564\n",
      "validation Loss: 0.0001923962 Acc: 89.0425033569\n",
      "training Loss: 0.0001912345 Acc: 89.2301177979\n",
      "validation Loss: 0.0001922989 Acc: 89.0388717651\n",
      "training Loss: 0.0001909330 Acc: 89.2068557739\n",
      "validation Loss: 0.0001928297 Acc: 89.0654602051\n",
      "training Loss: 0.0001899426 Acc: 89.2406921387\n",
      "validation Loss: 0.0001921188 Acc: 89.3047485352\n",
      "training Loss: 0.0001898822 Acc: 89.2872238159\n",
      "validation Loss: 0.0001914453 Acc: 89.1754379272\n",
      "training Loss: 0.0001895866 Acc: 89.2509689331\n",
      "validation Loss: 0.0001913620 Acc: 89.2213592529\n",
      "training Loss: 0.0001895023 Acc: 89.1993026733\n",
      "validation Loss: 0.0001916294 Acc: 89.0763397217\n",
      "training Loss: 0.0001896179 Acc: 89.2621459961\n",
      "validation Loss: 0.0001916152 Acc: 88.9663619995\n",
      "training Loss: 0.0001896485 Acc: 89.2675857544\n",
      "validation Loss: 0.0001923629 Acc: 89.0159149170\n",
      "training Loss: 0.0001892443 Acc: 89.2739257812\n",
      "validation Loss: 0.0001913645 Acc: 89.3446273804\n",
      "training Loss: 0.0001889451 Acc: 89.3116912842\n",
      "validation Loss: 0.0001915905 Acc: 89.2745361328\n",
      "training Loss: 0.0001889703 Acc: 89.3017272949\n",
      "validation Loss: 0.0001911213 Acc: 89.2443237305\n",
      "training Loss: 0.0001893022 Acc: 89.2693939209\n",
      "validation Loss: 0.0001914109 Acc: 88.9373626709\n",
      "training Loss: 0.0001886245 Acc: 89.2651672363\n",
      "validation Loss: 0.0001911633 Acc: 89.1814804077\n",
      "training Loss: 0.0001884445 Acc: 89.3047485352\n",
      "validation Loss: 0.0001912731 Acc: 89.2733230591\n",
      "training Loss: 0.0001887681 Acc: 89.2751388550\n",
      "validation Loss: 0.0001917165 Acc: 89.1053466797\n",
      "training Loss: 0.0001883215 Acc: 89.3394927979\n",
      "validation Loss: 0.0001912735 Acc: 89.2962875366\n",
      "training Loss: 0.0001886423 Acc: 89.3352584839\n",
      "validation Loss: 0.0001917439 Acc: 89.1379699707\n",
      "training Loss: 0.0001887259 Acc: 89.3410034180\n",
      "validation Loss: 0.0001916810 Acc: 89.2926635742\n",
      "Early stopped.\n",
      "Best val acc: 89.386925\n",
      "----------\n",
      "training Loss: 0.0002747914 Acc: 83.0213775635\n",
      "validation Loss: 0.0002333382 Acc: 86.1035919189\n",
      "training Loss: 0.0002283399 Acc: 87.1450881958\n",
      "validation Loss: 0.0002231280 Acc: 87.6836090088\n",
      "training Loss: 0.0002212174 Acc: 87.5754241943\n",
      "validation Loss: 0.0002174406 Acc: 87.3695373535\n",
      "training Loss: 0.0002182506 Acc: 87.8152084351\n",
      "validation Loss: 0.0002151784 Acc: 88.1015625000\n",
      "training Loss: 0.0002143706 Acc: 88.0235824585\n",
      "validation Loss: 0.0002142390 Acc: 87.9421081543\n",
      "training Loss: 0.0002128095 Acc: 88.0580062866\n",
      "validation Loss: 0.0002128388 Acc: 87.8587646484\n",
      "training Loss: 0.0002120825 Acc: 88.1377334595\n",
      "validation Loss: 0.0002101156 Acc: 87.4299316406\n",
      "training Loss: 0.0002100144 Acc: 88.2760467529\n",
      "validation Loss: 0.0002101797 Acc: 88.1909484863\n",
      "training Loss: 0.0002091271 Acc: 88.3023223877\n",
      "validation Loss: 0.0002075607 Acc: 88.8468780518\n",
      "training Loss: 0.0002089860 Acc: 88.3273849487\n",
      "validation Loss: 0.0002082290 Acc: 88.6632690430\n",
      "training Loss: 0.0002083944 Acc: 88.4243240356\n",
      "validation Loss: 0.0002075006 Acc: 88.7369537354\n",
      "training Loss: 0.0002072521 Acc: 88.3905029297\n",
      "validation Loss: 0.0002070480 Acc: 88.9749221802\n",
      "training Loss: 0.0002067356 Acc: 88.4207000732\n",
      "validation Loss: 0.0002048344 Acc: 88.8251342773\n",
      "training Loss: 0.0002054168 Acc: 88.5342483521\n",
      "validation Loss: 0.0002025391 Acc: 88.8190917969\n",
      "training Loss: 0.0002052682 Acc: 88.4807968140\n",
      "validation Loss: 0.0002054288 Acc: 89.2575836182\n",
      "training Loss: 0.0002047040 Acc: 88.5635452271\n",
      "validation Loss: 0.0002046298 Acc: 88.9930419922\n",
      "training Loss: 0.0002059956 Acc: 88.4952926636\n",
      "validation Loss: 0.0002044027 Acc: 89.2672500610\n",
      "training Loss: 0.0002044458 Acc: 88.6272659302\n",
      "validation Loss: 0.0002028442 Acc: 89.2358398438\n",
      "training Loss: 0.0002004930 Acc: 88.7713165283\n",
      "validation Loss: 0.0002004581 Acc: 88.9290161133\n",
      "training Loss: 0.0001993086 Acc: 88.7912445068\n",
      "validation Loss: 0.0002011051 Acc: 88.9290161133\n",
      "training Loss: 0.0001993604 Acc: 88.7749404907\n",
      "validation Loss: 0.0002016650 Acc: 88.5702514648\n",
      "training Loss: 0.0001996058 Acc: 88.8135910034\n",
      "validation Loss: 0.0001998780 Acc: 88.6801757812\n",
      "training Loss: 0.0001990155 Acc: 88.7972869873\n",
      "validation Loss: 0.0001999216 Acc: 89.1923522949\n",
      "training Loss: 0.0001989061 Acc: 88.8700637817\n",
      "validation Loss: 0.0002004336 Acc: 89.1524887085\n",
      "training Loss: 0.0001979809 Acc: 88.9458694458\n",
      "validation Loss: 0.0001991208 Acc: 88.8070144653\n",
      "training Loss: 0.0001984707 Acc: 88.8906021118\n",
      "validation Loss: 0.0001997119 Acc: 89.2358398438\n",
      "training Loss: 0.0001984694 Acc: 88.8377532959\n",
      "validation Loss: 0.0001996072 Acc: 89.1983947754\n",
      "training Loss: 0.0001988018 Acc: 88.9081192017\n",
      "validation Loss: 0.0002002962 Acc: 89.0498123169\n",
      "training Loss: 0.0001978355 Acc: 88.9105300903\n",
      "validation Loss: 0.0001998669 Acc: 89.2092666626\n",
      "training Loss: 0.0001963649 Acc: 88.9836196899\n",
      "validation Loss: 0.0001996244 Acc: 88.8372116089\n",
      "training Loss: 0.0001956030 Acc: 89.0183486938\n",
      "validation Loss: 0.0001984925 Acc: 88.9459304810\n",
      "training Loss: 0.0001948846 Acc: 89.0467300415\n",
      "validation Loss: 0.0001978087 Acc: 89.2660369873\n",
      "training Loss: 0.0001945712 Acc: 89.0666656494\n",
      "validation Loss: 0.0001979838 Acc: 89.0703506470\n",
      "training Loss: 0.0001945716 Acc: 89.1170959473\n",
      "validation Loss: 0.0001976756 Acc: 89.1210861206\n",
      "training Loss: 0.0001939908 Acc: 89.1180038452\n",
      "validation Loss: 0.0001982931 Acc: 88.6934661865\n",
      "training Loss: 0.0001943130 Acc: 89.1044158936\n",
      "validation Loss: 0.0001969719 Acc: 88.8806991577\n",
      "training Loss: 0.0001945986 Acc: 89.0476379395\n",
      "validation Loss: 0.0001973116 Acc: 89.1379928589\n",
      "training Loss: 0.0001942434 Acc: 89.0334472656\n",
      "validation Loss: 0.0001975268 Acc: 89.2950286865\n",
      "training Loss: 0.0001942723 Acc: 89.0760269165\n",
      "validation Loss: 0.0001973173 Acc: 88.8505020142\n",
      "training Loss: 0.0001938565 Acc: 89.0422058105\n",
      "validation Loss: 0.0001978010 Acc: 88.9108963013\n",
      "training Loss: 0.0001925798 Acc: 89.1608886719\n",
      "validation Loss: 0.0001965882 Acc: 89.1573257446\n",
      "training Loss: 0.0001922784 Acc: 89.1349105835\n",
      "validation Loss: 0.0001963494 Acc: 89.1452407837\n",
      "training Loss: 0.0001924875 Acc: 89.1623992920\n",
      "validation Loss: 0.0001971608 Acc: 89.2889862061\n",
      "training Loss: 0.0001922481 Acc: 89.1267623901\n",
      "validation Loss: 0.0001974612 Acc: 89.0570602417\n",
      "training Loss: 0.0001917983 Acc: 89.1373291016\n",
      "validation Loss: 0.0001973054 Acc: 89.4061660767\n",
      "training Loss: 0.0001919102 Acc: 89.2342681885\n",
      "validation Loss: 0.0001966752 Acc: 88.8251342773\n",
      "training Loss: 0.0001903922 Acc: 89.2312469482\n",
      "validation Loss: 0.0001961518 Acc: 89.0510177612\n",
      "training Loss: 0.0001904002 Acc: 89.2548065186\n",
      "validation Loss: 0.0001960921 Acc: 89.2032241821\n",
      "training Loss: 0.0001903142 Acc: 89.2176589966\n",
      "validation Loss: 0.0001968320 Acc: 88.9785461426\n",
      "training Loss: 0.0001903377 Acc: 89.2384948730\n",
      "validation Loss: 0.0001956256 Acc: 89.0171966553\n",
      "training Loss: 0.0001904341 Acc: 89.2418212891\n",
      "validation Loss: 0.0001962737 Acc: 89.1355819702\n",
      "training Loss: 0.0001901588 Acc: 89.2321548462\n",
      "validation Loss: 0.0001961235 Acc: 89.0147857666\n",
      "training Loss: 0.0001899310 Acc: 89.2406082153\n",
      "validation Loss: 0.0001961134 Acc: 89.1307449341\n",
      "training Loss: 0.0001900880 Acc: 89.3206405640\n",
      "validation Loss: 0.0001957931 Acc: 89.1669845581\n",
      "training Loss: 0.0001898013 Acc: 89.2898330688\n",
      "validation Loss: 0.0001956392 Acc: 89.2322158813\n",
      "training Loss: 0.0001896092 Acc: 89.2777557373\n",
      "validation Loss: 0.0001959223 Acc: 89.2600021362\n",
      "training Loss: 0.0001896214 Acc: 89.2940673828\n",
      "validation Loss: 0.0001959505 Acc: 89.2370452881\n",
      "Early stopped.\n",
      "Best val acc: 89.406166\n",
      "----------\n",
      "training Loss: 0.0002690433 Acc: 83.5356826782\n",
      "validation Loss: 0.0002272536 Acc: 87.7149047852\n",
      "training Loss: 0.0002244605 Acc: 87.1910247803\n",
      "validation Loss: 0.0002190048 Acc: 86.8619384766\n",
      "training Loss: 0.0002167966 Acc: 87.7136917114\n",
      "validation Loss: 0.0002170165 Acc: 88.0814971924\n",
      "training Loss: 0.0002152193 Acc: 87.7433395386\n",
      "validation Loss: 0.0002128473 Acc: 88.6803894043\n",
      "training Loss: 0.0002114023 Acc: 88.0110244751\n",
      "validation Loss: 0.0002091664 Acc: 87.0930252075\n",
      "training Loss: 0.0002114685 Acc: 87.9747238159\n",
      "validation Loss: 0.0002146058 Acc: 89.6422348022\n",
      "training Loss: 0.0002089936 Acc: 88.1456222534\n",
      "validation Loss: 0.0002052675 Acc: 88.8050003052\n",
      "training Loss: 0.0002070819 Acc: 88.3307342529\n",
      "validation Loss: 0.0002058999 Acc: 88.0681915283\n",
      "training Loss: 0.0002057849 Acc: 88.2874755859\n",
      "validation Loss: 0.0002052551 Acc: 88.0222091675\n",
      "training Loss: 0.0002058564 Acc: 88.3745880127\n",
      "validation Loss: 0.0002022766 Acc: 89.1473999023\n",
      "training Loss: 0.0002040607 Acc: 88.4919509888\n",
      "validation Loss: 0.0002025263 Acc: 89.0034255981\n",
      "training Loss: 0.0002033325 Acc: 88.5231018066\n",
      "validation Loss: 0.0002031966 Acc: 87.5370483398\n",
      "training Loss: 0.0002037542 Acc: 88.4414367676\n",
      "validation Loss: 0.0002003123 Acc: 88.5061645508\n",
      "training Loss: 0.0002030902 Acc: 88.4592819214\n",
      "validation Loss: 0.0002022082 Acc: 88.9634933472\n",
      "training Loss: 0.0002026547 Acc: 88.5300598145\n",
      "validation Loss: 0.0002023019 Acc: 88.5110015869\n",
      "training Loss: 0.0002022182 Acc: 88.5730056763\n",
      "validation Loss: 0.0002045528 Acc: 87.0349502563\n",
      "training Loss: 0.0002034232 Acc: 88.4734954834\n",
      "validation Loss: 0.0002058193 Acc: 89.7462844849\n",
      "training Loss: 0.0001983442 Acc: 88.8137741089\n",
      "validation Loss: 0.0001994510 Acc: 88.3053207397\n",
      "training Loss: 0.0001974838 Acc: 88.8270797729\n",
      "validation Loss: 0.0001962558 Acc: 88.7747573853\n",
      "training Loss: 0.0001975694 Acc: 88.8485565186\n",
      "validation Loss: 0.0001975740 Acc: 88.5533523560\n",
      "training Loss: 0.0001972831 Acc: 88.8264770508\n",
      "validation Loss: 0.0001964241 Acc: 88.6973266602\n",
      "training Loss: 0.0001969008 Acc: 88.8231506348\n",
      "validation Loss: 0.0001970915 Acc: 88.8788070679\n",
      "training Loss: 0.0001970652 Acc: 88.8352508545\n",
      "validation Loss: 0.0001980492 Acc: 89.3518676758\n",
      "training Loss: 0.0001949499 Acc: 89.0124969482\n",
      "validation Loss: 0.0001952267 Acc: 89.1498184204\n",
      "training Loss: 0.0001941008 Acc: 88.9882965088\n",
      "validation Loss: 0.0001949442 Acc: 89.3966369629\n",
      "training Loss: 0.0001933810 Acc: 89.0409317017\n",
      "validation Loss: 0.0001938789 Acc: 89.1147308350\n",
      "training Loss: 0.0001931443 Acc: 89.0763168335\n",
      "validation Loss: 0.0001948363 Acc: 89.0385131836\n",
      "training Loss: 0.0001935892 Acc: 89.0327606201\n",
      "validation Loss: 0.0001939037 Acc: 89.1340866089\n",
      "training Loss: 0.0001933314 Acc: 88.9728775024\n",
      "validation Loss: 0.0001967910 Acc: 88.3428268433\n",
      "training Loss: 0.0001936263 Acc: 88.9861831665\n",
      "validation Loss: 0.0001954617 Acc: 88.7396697998\n",
      "training Loss: 0.0001921696 Acc: 89.0862960815\n",
      "validation Loss: 0.0001938912 Acc: 89.1885375977\n",
      "training Loss: 0.0001913414 Acc: 89.1195678711\n",
      "validation Loss: 0.0001930132 Acc: 89.0481872559\n",
      "training Loss: 0.0001914693 Acc: 89.1147308350\n",
      "validation Loss: 0.0001935363 Acc: 88.9441375732\n",
      "training Loss: 0.0001913613 Acc: 89.0793457031\n",
      "validation Loss: 0.0001927238 Acc: 89.2986297607\n",
      "training Loss: 0.0001907191 Acc: 89.1561737061\n",
      "validation Loss: 0.0001934837 Acc: 89.0965805054\n",
      "training Loss: 0.0001912230 Acc: 89.1709899902\n",
      "validation Loss: 0.0001930902 Acc: 89.2695999146\n",
      "training Loss: 0.0001910601 Acc: 89.0944671631\n",
      "validation Loss: 0.0001933067 Acc: 88.9320373535\n",
      "training Loss: 0.0001906204 Acc: 89.1846008301\n",
      "validation Loss: 0.0001935612 Acc: 89.1643371582\n",
      "training Loss: 0.0001901174 Acc: 89.1697845459\n",
      "validation Loss: 0.0001929324 Acc: 89.0627059937\n",
      "training Loss: 0.0001895103 Acc: 89.1985168457\n",
      "validation Loss: 0.0001928966 Acc: 89.1510314941\n",
      "training Loss: 0.0001897266 Acc: 89.1864166260\n",
      "validation Loss: 0.0001924695 Acc: 89.1026306152\n",
      "training Loss: 0.0001897643 Acc: 89.2450942993\n",
      "validation Loss: 0.0001921523 Acc: 88.8485565186\n",
      "training Loss: 0.0001899273 Acc: 89.1591949463\n",
      "validation Loss: 0.0001929260 Acc: 89.0288314819\n",
      "training Loss: 0.0001897854 Acc: 89.2224121094\n",
      "validation Loss: 0.0001925031 Acc: 89.1377182007\n",
      "training Loss: 0.0001893208 Acc: 89.1909561157\n",
      "validation Loss: 0.0001927503 Acc: 89.0433502197\n",
      "training Loss: 0.0001893227 Acc: 89.1812744141\n",
      "validation Loss: 0.0001925688 Acc: 89.1522369385\n",
      "training Loss: 0.0001889436 Acc: 89.2221069336\n",
      "validation Loss: 0.0001920062 Acc: 89.1038436890\n",
      "training Loss: 0.0001888642 Acc: 89.2302780151\n",
      "validation Loss: 0.0001922293 Acc: 89.1691741943\n",
      "training Loss: 0.0001886472 Acc: 89.3116378784\n",
      "validation Loss: 0.0001918953 Acc: 89.1014251709\n",
      "training Loss: 0.0001887547 Acc: 89.2596130371\n",
      "validation Loss: 0.0001918819 Acc: 89.2478179932\n",
      "training Loss: 0.0001886451 Acc: 89.2593154907\n",
      "validation Loss: 0.0001927899 Acc: 89.2466049194\n",
      "training Loss: 0.0001886044 Acc: 89.2596130371\n",
      "validation Loss: 0.0001924429 Acc: 89.2224121094\n",
      "training Loss: 0.0001883798 Acc: 89.2124328613\n",
      "validation Loss: 0.0001922034 Acc: 89.2671737671\n",
      "training Loss: 0.0001883394 Acc: 89.2414627075\n",
      "validation Loss: 0.0001917198 Acc: 89.1510314941\n",
      "training Loss: 0.0001884471 Acc: 89.2444915771\n",
      "validation Loss: 0.0001923450 Acc: 89.1897430420\n",
      "training Loss: 0.0001882391 Acc: 89.2738342285\n",
      "validation Loss: 0.0001924583 Acc: 89.1135253906\n",
      "training Loss: 0.0001879486 Acc: 89.2759475708\n",
      "validation Loss: 0.0001919508 Acc: 89.1449813843\n",
      "training Loss: 0.0001879051 Acc: 89.3068008423\n",
      "validation Loss: 0.0001924373 Acc: 89.2248306274\n",
      "training Loss: 0.0001879056 Acc: 89.3264617920\n",
      "validation Loss: 0.0001916634 Acc: 89.2042617798\n",
      "training Loss: 0.0001880270 Acc: 89.2950057983\n",
      "validation Loss: 0.0001921218 Acc: 89.0796432495\n",
      "training Loss: 0.0001878197 Acc: 89.2581024170\n",
      "validation Loss: 0.0001915287 Acc: 89.2345123291\n",
      "training Loss: 0.0001880183 Acc: 89.2714080811\n",
      "validation Loss: 0.0001916821 Acc: 89.1873245239\n",
      "training Loss: 0.0001876231 Acc: 89.2956085205\n",
      "validation Loss: 0.0001922480 Acc: 89.1316680908\n",
      "training Loss: 0.0001878991 Acc: 89.2686920166\n",
      "validation Loss: 0.0001915732 Acc: 89.1679687500\n",
      "training Loss: 0.0001879554 Acc: 89.2928848267\n",
      "validation Loss: 0.0001919389 Acc: 89.2538681030\n",
      "training Loss: 0.0001879176 Acc: 89.3382568359\n",
      "validation Loss: 0.0001920708 Acc: 89.1365127563\n",
      "training Loss: 0.0001879861 Acc: 89.2224121094\n",
      "validation Loss: 0.0001925149 Acc: 89.2103118896\n",
      "training Loss: 0.0001877935 Acc: 89.3358383179\n",
      "validation Loss: 0.0001921638 Acc: 89.2466049194\n",
      "Early stopped.\n",
      "Best val acc: 89.746284\n",
      "----------\n",
      "training Loss: 0.0002749388 Acc: 82.7593765259\n",
      "validation Loss: 0.0002321318 Acc: 88.2361450195\n",
      "training Loss: 0.0002269241 Acc: 87.1394042969\n",
      "validation Loss: 0.0002194466 Acc: 87.8059082031\n",
      "training Loss: 0.0002190712 Acc: 87.5678253174\n",
      "validation Loss: 0.0002140629 Acc: 88.3473281860\n",
      "training Loss: 0.0002142366 Acc: 87.8844604492\n",
      "validation Loss: 0.0002136910 Acc: 88.2506484985\n",
      "training Loss: 0.0002120515 Acc: 87.9382400513\n",
      "validation Loss: 0.0002095249 Acc: 88.1007919312\n",
      "training Loss: 0.0002098837 Acc: 88.1246566772\n",
      "validation Loss: 0.0002081362 Acc: 87.8965454102\n",
      "training Loss: 0.0002089181 Acc: 88.2189254761\n",
      "validation Loss: 0.0002080519 Acc: 87.9750976562\n",
      "training Loss: 0.0002085258 Acc: 88.1790390015\n",
      "validation Loss: 0.0002065571 Acc: 88.0572814941\n",
      "training Loss: 0.0002071988 Acc: 88.3663635254\n",
      "validation Loss: 0.0002065588 Acc: 87.5774917603\n",
      "training Loss: 0.0002065100 Acc: 88.3479309082\n",
      "validation Loss: 0.0002024672 Acc: 88.6397933960\n",
      "training Loss: 0.0002066365 Acc: 88.3896255493\n",
      "validation Loss: 0.0002030175 Acc: 89.1147460938\n",
      "training Loss: 0.0002059377 Acc: 88.4298095703\n",
      "validation Loss: 0.0002042517 Acc: 88.2554779053\n",
      "training Loss: 0.0002048947 Acc: 88.4823837280\n",
      "validation Loss: 0.0002011561 Acc: 88.5950775146\n",
      "training Loss: 0.0002042636 Acc: 88.4769439697\n",
      "validation Loss: 0.0002025608 Acc: 88.1346282959\n",
      "training Loss: 0.0002047625 Acc: 88.5243759155\n",
      "validation Loss: 0.0002016117 Acc: 88.5745315552\n",
      "training Loss: 0.0002034415 Acc: 88.5606384277\n",
      "validation Loss: 0.0002004682 Acc: 89.0035629272\n",
      "training Loss: 0.0002032951 Acc: 88.5258865356\n",
      "validation Loss: 0.0002014292 Acc: 87.3623733521\n",
      "training Loss: 0.0002026419 Acc: 88.5570068359\n",
      "validation Loss: 0.0001995914 Acc: 88.7497711182\n",
      "training Loss: 0.0002034511 Acc: 88.6101837158\n",
      "validation Loss: 0.0001993748 Acc: 88.9963073730\n",
      "training Loss: 0.0002026220 Acc: 88.6301269531\n",
      "validation Loss: 0.0002022723 Acc: 88.9153366089\n",
      "training Loss: 0.0002022619 Acc: 88.6110916138\n",
      "validation Loss: 0.0002010621 Acc: 89.1630859375\n",
      "training Loss: 0.0002026075 Acc: 88.6086730957\n",
      "validation Loss: 0.0002033783 Acc: 89.0180664062\n",
      "training Loss: 0.0002028975 Acc: 88.7198562622\n",
      "validation Loss: 0.0002003017 Acc: 89.5461959839\n",
      "training Loss: 0.0001980605 Acc: 88.8395004272\n",
      "validation Loss: 0.0001978462 Acc: 88.6857147217\n",
      "training Loss: 0.0001974784 Acc: 88.8056640625\n",
      "validation Loss: 0.0001963576 Acc: 89.2307662964\n",
      "training Loss: 0.0001971954 Acc: 88.8410110474\n",
      "validation Loss: 0.0001975463 Acc: 88.9274215698\n",
      "training Loss: 0.0001963723 Acc: 88.8654861450\n",
      "validation Loss: 0.0001971200 Acc: 88.8089904785\n",
      "training Loss: 0.0001973134 Acc: 88.8458480835\n",
      "validation Loss: 0.0001960442 Acc: 89.0712432861\n",
      "training Loss: 0.0001965735 Acc: 88.9038543701\n",
      "validation Loss: 0.0001972034 Acc: 89.2295608521\n",
      "training Loss: 0.0001965409 Acc: 88.8654861450\n",
      "validation Loss: 0.0001968087 Acc: 88.3690795898\n",
      "training Loss: 0.0001969788 Acc: 88.8534011841\n",
      "validation Loss: 0.0001977369 Acc: 89.5812377930\n",
      "training Loss: 0.0001963200 Acc: 88.8609542847\n",
      "validation Loss: 0.0001974841 Acc: 89.1449584961\n",
      "training Loss: 0.0001941781 Acc: 89.0446548462\n",
      "validation Loss: 0.0001958972 Acc: 89.4809341431\n",
      "training Loss: 0.0001930048 Acc: 89.0241088867\n",
      "validation Loss: 0.0001944910 Acc: 89.2597732544\n",
      "training Loss: 0.0001933613 Acc: 89.0259170532\n",
      "validation Loss: 0.0001941638 Acc: 89.0192718506\n",
      "training Loss: 0.0001931815 Acc: 89.0585479736\n",
      "validation Loss: 0.0001953702 Acc: 89.4688491821\n",
      "training Loss: 0.0001928184 Acc: 89.1123275757\n",
      "validation Loss: 0.0001941801 Acc: 89.0809097290\n",
      "training Loss: 0.0001930742 Acc: 89.0784912109\n",
      "validation Loss: 0.0001947055 Acc: 89.0531082153\n",
      "training Loss: 0.0001925594 Acc: 89.0609664917\n",
      "validation Loss: 0.0001949215 Acc: 89.1050796509\n",
      "training Loss: 0.0001918584 Acc: 89.1065902710\n",
      "validation Loss: 0.0001940196 Acc: 89.3165740967\n",
      "training Loss: 0.0001912309 Acc: 89.1210937500\n",
      "validation Loss: 0.0001940146 Acc: 89.4035873413\n",
      "training Loss: 0.0001907454 Acc: 89.1534194946\n",
      "validation Loss: 0.0001936831 Acc: 88.9225921631\n",
      "training Loss: 0.0001902237 Acc: 89.1331787109\n",
      "validation Loss: 0.0001937150 Acc: 89.2658157349\n",
      "training Loss: 0.0001902538 Acc: 89.1954193115\n",
      "validation Loss: 0.0001924898 Acc: 89.3008575439\n",
      "training Loss: 0.0001906034 Acc: 89.1458663940\n",
      "validation Loss: 0.0001937041 Acc: 88.9576339722\n",
      "training Loss: 0.0001906854 Acc: 89.1836318970\n",
      "validation Loss: 0.0001940853 Acc: 89.5183944702\n",
      "training Loss: 0.0001903002 Acc: 89.2096176147\n",
      "validation Loss: 0.0001932531 Acc: 88.8633728027\n",
      "training Loss: 0.0001898325 Acc: 89.2165679932\n",
      "validation Loss: 0.0001936878 Acc: 89.1099090576\n",
      "training Loss: 0.0001890931 Acc: 89.2035751343\n",
      "validation Loss: 0.0001930279 Acc: 89.3177795410\n",
      "training Loss: 0.0001894838 Acc: 89.2050857544\n",
      "validation Loss: 0.0001930834 Acc: 89.0168533325\n",
      "training Loss: 0.0001889347 Acc: 89.2159652710\n",
      "validation Loss: 0.0001924236 Acc: 89.3697509766\n",
      "training Loss: 0.0001891638 Acc: 89.2262344360\n",
      "validation Loss: 0.0001927579 Acc: 89.3999633789\n",
      "training Loss: 0.0001888659 Acc: 89.2697448730\n",
      "validation Loss: 0.0001933452 Acc: 88.9878540039\n",
      "training Loss: 0.0001888512 Acc: 89.2655105591\n",
      "validation Loss: 0.0001928860 Acc: 89.0470657349\n",
      "training Loss: 0.0001885984 Acc: 89.2416458130\n",
      "validation Loss: 0.0001925912 Acc: 89.1461715698\n",
      "training Loss: 0.0001883981 Acc: 89.3147583008\n",
      "validation Loss: 0.0001922752 Acc: 89.1775894165\n",
      "training Loss: 0.0001881081 Acc: 89.2724609375\n",
      "validation Loss: 0.0001924688 Acc: 89.2863616943\n",
      "training Loss: 0.0001884477 Acc: 89.3093185425\n",
      "validation Loss: 0.0001928116 Acc: 89.0857391357\n",
      "training Loss: 0.0001879374 Acc: 89.3168716431\n",
      "validation Loss: 0.0001930075 Acc: 89.3757934570\n",
      "training Loss: 0.0001881942 Acc: 89.3268432617\n",
      "validation Loss: 0.0001925665 Acc: 89.1473770142\n",
      "training Loss: 0.0001876689 Acc: 89.3392333984\n",
      "validation Loss: 0.0001925339 Acc: 89.2658157349\n",
      "training Loss: 0.0001876147 Acc: 89.2914962769\n",
      "validation Loss: 0.0001926469 Acc: 89.1679229736\n",
      "training Loss: 0.0001877288 Acc: 89.3041839600\n",
      "validation Loss: 0.0001929347 Acc: 89.2452697754\n",
      "Early stopped.\n",
      "Best val acc: 89.581238\n",
      "----------\n",
      "training Loss: 0.0002669913 Acc: 83.4367065430\n",
      "validation Loss: 0.0002298456 Acc: 86.9252014160\n",
      "training Loss: 0.0002251372 Acc: 87.2067947388\n",
      "validation Loss: 0.0002203888 Acc: 87.2920074463\n",
      "training Loss: 0.0002174492 Acc: 87.7229232788\n",
      "validation Loss: 0.0002136413 Acc: 88.4563827515\n",
      "training Loss: 0.0002148078 Acc: 87.9413223267\n",
      "validation Loss: 0.0002093492 Acc: 87.8422241211\n",
      "training Loss: 0.0002112662 Acc: 88.0125122070\n",
      "validation Loss: 0.0002101406 Acc: 87.8458404541\n",
      "training Loss: 0.0002104162 Acc: 88.0972824097\n",
      "validation Loss: 0.0002098026 Acc: 87.2027206421\n",
      "training Loss: 0.0002085825 Acc: 88.2773666382\n",
      "validation Loss: 0.0002044405 Acc: 88.7423477173\n",
      "training Loss: 0.0002069034 Acc: 88.3180923462\n",
      "validation Loss: 0.0002045151 Acc: 88.7749252319\n",
      "training Loss: 0.0002060689 Acc: 88.3548965454\n",
      "validation Loss: 0.0002047480 Acc: 88.7544174194\n",
      "training Loss: 0.0002059210 Acc: 88.4297027588\n",
      "validation Loss: 0.0002016708 Acc: 88.7339019775\n",
      "training Loss: 0.0002045515 Acc: 88.4300079346\n",
      "validation Loss: 0.0002005711 Acc: 88.8207778931\n",
      "training Loss: 0.0002054470 Acc: 88.4475021362\n",
      "validation Loss: 0.0002021239 Acc: 89.3034210205\n",
      "training Loss: 0.0002032637 Acc: 88.4978790283\n",
      "validation Loss: 0.0002014097 Acc: 88.7049484253\n",
      "training Loss: 0.0002034941 Acc: 88.4849090576\n",
      "validation Loss: 0.0001995265 Acc: 89.1055374146\n",
      "training Loss: 0.0002034944 Acc: 88.5401077271\n",
      "validation Loss: 0.0001995033 Acc: 88.4877548218\n",
      "training Loss: 0.0002025837 Acc: 88.5923004150\n",
      "validation Loss: 0.0002016926 Acc: 88.0364837646\n",
      "training Loss: 0.0002023363 Acc: 88.5618286133\n",
      "validation Loss: 0.0001993465 Acc: 88.8605957031\n",
      "training Loss: 0.0002027837 Acc: 88.5633392334\n",
      "validation Loss: 0.0002007663 Acc: 87.8615264893\n",
      "training Loss: 0.0002022899 Acc: 88.5389022827\n",
      "validation Loss: 0.0002014146 Acc: 89.1924133301\n",
      "training Loss: 0.0002021837 Acc: 88.5729904175\n",
      "validation Loss: 0.0002034600 Acc: 88.2355728149\n",
      "training Loss: 0.0002020361 Acc: 88.6381454468\n",
      "validation Loss: 0.0002010107 Acc: 89.3010101318\n",
      "training Loss: 0.0001975304 Acc: 88.8571548462\n",
      "validation Loss: 0.0001957839 Acc: 89.0017700195\n",
      "training Loss: 0.0001965931 Acc: 88.9138641357\n",
      "validation Loss: 0.0001957240 Acc: 89.0053939819\n",
      "training Loss: 0.0001961954 Acc: 88.9395065308\n",
      "validation Loss: 0.0001975907 Acc: 88.4515609741\n",
      "training Loss: 0.0001964780 Acc: 88.8936538696\n",
      "validation Loss: 0.0001947097 Acc: 88.7339019775\n",
      "training Loss: 0.0001957704 Acc: 88.9675598145\n",
      "validation Loss: 0.0001944082 Acc: 88.9221343994\n",
      "training Loss: 0.0001956632 Acc: 88.8426742554\n",
      "validation Loss: 0.0001953141 Acc: 89.3323822021\n",
      "training Loss: 0.0001964085 Acc: 88.9482498169\n",
      "validation Loss: 0.0001943239 Acc: 88.8593902588\n",
      "training Loss: 0.0001958895 Acc: 88.9259262085\n",
      "validation Loss: 0.0001942227 Acc: 88.9305801392\n",
      "training Loss: 0.0001959420 Acc: 88.9702682495\n",
      "validation Loss: 0.0001950765 Acc: 88.9293746948\n",
      "training Loss: 0.0001955081 Acc: 88.9388961792\n",
      "validation Loss: 0.0001946957 Acc: 89.1212234497\n",
      "training Loss: 0.0001955718 Acc: 88.9515686035\n",
      "validation Loss: 0.0001942164 Acc: 89.0729598999\n",
      "training Loss: 0.0001949330 Acc: 88.9376907349\n",
      "validation Loss: 0.0001957800 Acc: 89.0041809082\n",
      "training Loss: 0.0001929005 Acc: 89.0770568848\n",
      "validation Loss: 0.0001919547 Acc: 89.2237854004\n",
      "training Loss: 0.0001922621 Acc: 89.0897293091\n",
      "validation Loss: 0.0001920105 Acc: 88.8183670044\n",
      "training Loss: 0.0001917359 Acc: 89.1060180664\n",
      "validation Loss: 0.0001925706 Acc: 89.0898513794\n",
      "training Loss: 0.0001918434 Acc: 89.1316604614\n",
      "validation Loss: 0.0001923867 Acc: 89.5206146240\n",
      "training Loss: 0.0001914995 Acc: 89.1449279785\n",
      "validation Loss: 0.0001933282 Acc: 89.4976882935\n",
      "training Loss: 0.0001904850 Acc: 89.2643890381\n",
      "validation Loss: 0.0001907807 Acc: 89.1815567017\n",
      "training Loss: 0.0001898438 Acc: 89.2366333008\n",
      "validation Loss: 0.0001912820 Acc: 89.3179016113\n",
      "training Loss: 0.0001894250 Acc: 89.2806777954\n",
      "validation Loss: 0.0001908992 Acc: 89.1694869995\n",
      "training Loss: 0.0001896268 Acc: 89.2378387451\n",
      "validation Loss: 0.0001908709 Acc: 89.3215179443\n",
      "training Loss: 0.0001894752 Acc: 89.2704162598\n",
      "validation Loss: 0.0001912833 Acc: 89.4602813721\n",
      "training Loss: 0.0001886169 Acc: 89.3229064941\n",
      "validation Loss: 0.0001903434 Acc: 89.0753707886\n",
      "training Loss: 0.0001883114 Acc: 89.2981719971\n",
      "validation Loss: 0.0001900296 Acc: 89.1381149292\n",
      "training Loss: 0.0001884208 Acc: 89.2996826172\n",
      "validation Loss: 0.0001907814 Acc: 89.4397659302\n",
      "training Loss: 0.0001886144 Acc: 89.3069229126\n",
      "validation Loss: 0.0001904440 Acc: 89.3516845703\n",
      "training Loss: 0.0001883286 Acc: 89.2779617310\n",
      "validation Loss: 0.0001902044 Acc: 89.3468627930\n",
      "training Loss: 0.0001879418 Acc: 89.3087310791\n",
      "validation Loss: 0.0001902492 Acc: 89.3782348633\n",
      "training Loss: 0.0001874941 Acc: 89.3455276489\n",
      "validation Loss: 0.0001903530 Acc: 89.2696380615\n",
      "training Loss: 0.0001874562 Acc: 89.3648376465\n",
      "validation Loss: 0.0001903845 Acc: 89.1972427368\n",
      "training Loss: 0.0001873916 Acc: 89.4173278809\n",
      "validation Loss: 0.0001897163 Acc: 89.2901535034\n",
      "training Loss: 0.0001872133 Acc: 89.4176254272\n",
      "validation Loss: 0.0001901527 Acc: 89.2370605469\n",
      "training Loss: 0.0001873703 Acc: 89.3720779419\n",
      "validation Loss: 0.0001900651 Acc: 89.2937698364\n",
      "training Loss: 0.0001872804 Acc: 89.3766021729\n",
      "validation Loss: 0.0001897878 Acc: 89.3215179443\n",
      "training Loss: 0.0001869334 Acc: 89.3980178833\n",
      "validation Loss: 0.0001900943 Acc: 89.2358551025\n",
      "training Loss: 0.0001867956 Acc: 89.3889694214\n",
      "validation Loss: 0.0001895787 Acc: 89.3154907227\n",
      "training Loss: 0.0001872208 Acc: 89.3639297485\n",
      "validation Loss: 0.0001897986 Acc: 89.3661651611\n",
      "training Loss: 0.0001867950 Acc: 89.3820343018\n",
      "validation Loss: 0.0001896864 Acc: 89.3890914917\n",
      "training Loss: 0.0001867795 Acc: 89.3986206055\n",
      "validation Loss: 0.0001899132 Acc: 89.3697891235\n",
      "training Loss: 0.0001874119 Acc: 89.3953018188\n",
      "validation Loss: 0.0001897316 Acc: 89.2660140991\n",
      "training Loss: 0.0001868464 Acc: 89.3953018188\n",
      "validation Loss: 0.0001895127 Acc: 89.3384170532\n",
      "training Loss: 0.0001868542 Acc: 89.4158172607\n",
      "validation Loss: 0.0001899838 Acc: 89.2406768799\n",
      "training Loss: 0.0001871071 Acc: 89.3672485352\n",
      "validation Loss: 0.0001897370 Acc: 89.3372039795\n",
      "training Loss: 0.0001866378 Acc: 89.4128036499\n",
      "validation Loss: 0.0001897221 Acc: 89.3203125000\n",
      "training Loss: 0.0001865437 Acc: 89.3262252808\n",
      "validation Loss: 0.0001894306 Acc: 89.3287582397\n",
      "training Loss: 0.0001865114 Acc: 89.4399490356\n",
      "validation Loss: 0.0001895321 Acc: 89.2756729126\n",
      "training Loss: 0.0001867780 Acc: 89.4052581787\n",
      "validation Loss: 0.0001893426 Acc: 89.3504791260\n",
      "training Loss: 0.0001866563 Acc: 89.3934936523\n",
      "validation Loss: 0.0001898223 Acc: 89.3299713135\n",
      "training Loss: 0.0001868075 Acc: 89.3953018188\n",
      "validation Loss: 0.0001899526 Acc: 89.3444442749\n",
      "training Loss: 0.0001865042 Acc: 89.4351196289\n",
      "validation Loss: 0.0001896005 Acc: 89.3492736816\n",
      "training Loss: 0.0001869812 Acc: 89.3790130615\n",
      "validation Loss: 0.0001895474 Acc: 89.2551574707\n",
      "training Loss: 0.0001867058 Acc: 89.3898773193\n",
      "validation Loss: 0.0001898898 Acc: 89.3118667603\n",
      "training Loss: 0.0001865023 Acc: 89.3944015503\n",
      "validation Loss: 0.0001896785 Acc: 89.3492736816\n",
      "training Loss: 0.0001868656 Acc: 89.3980178833\n",
      "validation Loss: 0.0001896704 Acc: 89.2443008423\n",
      "Early stopped.\n",
      "Best val acc: 89.520615\n",
      "----------\n",
      "Average best_acc across k-fold: 89.52824401855469\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json'\n",
    "    best_conf = optimize_hyperparams( # NEED TO FIX THIS FUNC STILL !!!\n",
    "        data_list_dict, data_hlf_dict, label_dict, {fold: training_weights(data_aux_dict[f'fold_{fold}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold}']) for fold in len(data_list_dict)},\n",
    "        config_file, epochs=10,\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "    # with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "        best_conf = json.load(f)\n",
    "        print(best_conf)\n",
    "\n",
    "fom = []\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx in range(len(data_hlf_dict)):\n",
    "    weight = training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}'])\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], weight,\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    model_file = OUTPUT_DIRPATH + CURRENT_TIME +'_ReallyTopclassStyle_'+ f'{fold_idx}.torch'\n",
    "    state_file = OUTPUT_DIRPATH + CURRENT_TIME +'_BestPerfReallyTopclass_'+ f'{fold_idx}.torch'\n",
    "    \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], \n",
    "        dnn_input=np.shape(data_hlf_dict[f'fold_{fold_idx}'])[-1], rnn_input=np.shape(data_list_dict[f'fold_{fold_idx}'])[-1],\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(train_data_list, train_data_hlf, train_label, train_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(val_data_list, val_data_hlf, val_label, val_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        NUM_EPOCHS, model, optimizer, scheduler, \n",
    "        state_file, model_file, data_loader=data_loader, \n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(OUTPUT_DIRPATH+'/train+val_losses.json', 'w') as f:\n",
    "#     json.dump({'train_losses': train_losses_arr, 'val_losses': val_losses_arr}, f)\n",
    "\n",
    "with open(OUTPUT_DIRPATH+'/train+val_losses.json', 'r') as f:\n",
    "    train_val_losses_dict = json.load(f)\n",
    "train_losses_arr = train_val_losses_dict['train_losses']\n",
    "val_losses_arr = train_val_losses_dict['val_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "# weight_test_dict = {\n",
    "#     f'fold_{fold_idx}': copy.deepcopy(training_weights(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_test_dict[f'fold_{fold_idx}'])) for fold_idx in range(len(data_test_aux_dict))\n",
    "# }  # DO NOT USE SCALED FOR TRAINING!!\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, \n",
    "        save=True, dict_lists=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1665  |       0.9706      |    0.2867 +/- 0.0103     |\n",
      "|   0.2614  |       0.9500      |    0.2062 +/- 0.0057     |\n",
      "|   0.3873  |       0.9198      |    0.1392 +/- 0.0048     |\n",
      "|   0.7918  |       0.7538      |    0.0324 +/- 0.0012     |\n",
      "|   0.9285  |       0.5777      |    0.0093 +/- 0.0007     |\n",
      "|   0.9780  |       0.3839      |    0.0023 +/- 0.0003     |\n",
      "+-----------+-------------------+--------------------------+\n",
      "============================================================\n",
      "\n",
      "==============================0_lepton==============================\n",
      "0_lepton\n",
      "==============================1_lepton==============================\n",
      "1_lepton\n",
      "==============================2+_lepton==============================\n",
      "2+_lepton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "    NNtable.add_row(\n",
    "        [\n",
    "            IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "            \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "        ]\n",
    "    )\n",
    "print(NNtable)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = OUTPUT_DIRPATH + f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}/' + CURRENT_TIME\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [\n",
    "            (data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) \n",
    "            for i in range(len(data_test_aux_dict))\n",
    "        ]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "    print(plot_type)\n",
    "\n",
    "    weights_plot = {\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1) & mask_arr[fold_idx]],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0) & mask_arr[fold_idx]],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    density_weights_plot = {\n",
    "        fold_idx: {'sig': None, 'bkg': None} for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "\n",
    "    plot_train_val_losses(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME,\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    "    )\n",
    "    plot_roc(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], run2=False,\n",
    "        mask=None\n",
    "    )\n",
    "    # print(f\"num bkg: {np.sum(label_test==0)}\")\n",
    "    # print(f\"num sig: {np.sum(label_test==1)}\")\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, n_bins=25, \n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], n_bins=25, weights=density_weights_plot,\n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, n_bins=25,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    # for fold_idx in range(len(data_test_aux_dict)):\n",
    "    #     for score_cut in [0.2, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    #         plot_input_vars_after_score_cut(\n",
    "    #             IN_perf, score_cut, plot_destdir, fold_idx, method='round_robin', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #             mask=mask_arr[fold_idx]\n",
    "    #         )\n",
    "    #     plot_input_vars_after_score_cut(\n",
    "    #         IN_perf, [0.2, 0.6, 0.9], plot_destdir, fold_idx, method='round_robin', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #         mask=mask_arr[fold_idx]\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimized cut-boundaries for ttH score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(IN_perf, weights, method='arr', bins=50, mask=None, n_folds=5):\n",
    "    if method == 'round_robin':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[fold_idx]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[fold_idx]['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "    elif method == 'arr':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "                    \n",
    "        sig_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "\n",
    "        cut_boundaries = []\n",
    "        cut_s_over_root_bs = []\n",
    "        prev_s_over_root_b = 0\n",
    "        sig_weights = []\n",
    "        bkg_weights = []\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                cut_boundaries.append(bins - i)\n",
    "                cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        cut_boundaries.append(0)\n",
    "        cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "        cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold, cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "==============================0_lepton==============================\n",
      "==============================1_lepton==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================2+_lepton==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3608441/172047263.py:37: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
      "/tmp/ipykernel_3608441/172047263.py:40: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_3608441/172047263.py:41: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_3608441/1041492062.py:366: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    }
   ],
   "source": [
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = OUTPUT_DIRPATH + f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}/' + CURRENT_TIME\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [(data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "\n",
    "    weights_plot = {\n",
    "        fold_idx: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 1) & mask_arr[fold_idx]],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][(label_test_dict[f\"fold_{fold_idx}\"] == 0) & mask_arr[fold_idx]],\n",
    "        } for fold_idx in range(len(data_test_aux_dict))\n",
    "    }\n",
    "    \n",
    "    (\n",
    "        cut_boundaries_fold, cut_s_over_root_bs_fold, \n",
    "        sig_weights_fold, bkg_weights_fold\n",
    "    ) = optimize_cut_boundaries(\n",
    "        IN_perf, weights_plot, method='round_robin',\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    fold_labels = [\n",
    "        [\n",
    "            f\"s/âb={cut_s_over_root_bs_fold[fold_idx][cut_idx]:.04f}, s={sig_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{sig_weights_fold[fold_idx][cut_idx]['w2']:.04f}, b={bkg_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{bkg_weights_fold[fold_idx][cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[fold_idx]))\n",
    "        ] for fold_idx in range(len(weight_test_dict))\n",
    "    ]\n",
    "    fold_colors = [copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries_fold[i]) // len(cmap_petroff10)) + 1)) for i in range(len(weight_test_dict))]\n",
    "    for fold_idx in range(len(weight_test_dict)):\n",
    "        s_over_root_b(\n",
    "            IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_{plot_type}_fold{fold_idx}', \n",
    "            labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weights_plot, method='round_robin',\n",
    "            lines_fold=cut_boundaries_fold, lines_labels=fold_labels, only_fold=fold_idx, lines_colors=fold_colors,\n",
    "            mask=mask_arr\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_list_dict)):\n\u001b[1;32m      7\u001b[0m     (\n\u001b[1;32m      8\u001b[0m         train_data_list, val_data_list,\n\u001b[1;32m      9\u001b[0m         train_data_hlf, val_data_hlf,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m         test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mSEED\n\u001b[1;32m     16\u001b[0m     )\n\u001b[1;32m     17\u001b[0m     IN_perf_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 18\u001b[0m         \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_data_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_hlf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mOUTPUT_DIRPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCURRENT_TIME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_fold_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdict_lists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m     IN_perf_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     25\u001b[0m         evaluate(\n\u001b[1;32m     26\u001b[0m             val_data_list, val_data_hlf, val_label, val_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m         )\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUTPUT_DIRPATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCURRENT_TIME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_IN_perf_train_val.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:60\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(p_list, hlf, label, weight, OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, train_losses_arr, val_losses_arr, save, only_fold_idx, dict_lists)\u001b[0m\n\u001b[1;32m     54\u001b[0m     val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     55\u001b[0m         ParticleHLF(p_list, hlf, label, weight), \n\u001b[1;32m     56\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbest_conf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     57\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     all_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mhlf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfold_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m),\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     61\u001b[0m     all_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hlf[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m])))\n\u001b[1;32m     62\u001b[0m     val_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     63\u001b[0m         ParticleHLF(p_list[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], hlf[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], label[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], weight[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]), \n\u001b[1;32m     64\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbest_conf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     65\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for fold_idx in range(len(data_list_dict)):\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(\n",
    "        data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], \n",
    "        training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}']),\n",
    "        test_size=0.2, random_state=SEED\n",
    "    )\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            train_data_list, train_data_hlf, train_label, train_weight,\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, only_fold_idx=fold_idx,\n",
    "            dict_lists=True\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            val_data_list, val_data_hlf, val_label, val_weight,\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, only_fold_idx=fold_idx,\n",
    "            dict_lists=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'r') as f:\n",
    "    IN_perf_dict = json.load(f)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold ']\n",
    "val_weights_arr = []\n",
    "for fold_idx, (train_IN_dict, val_IN_dict, (train_index, val_index)) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'], skf.split(data_hlf, label))):\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )\n",
    "    rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "    rectified_train_index[val_index] = False\n",
    "    sig_train_mask = rectified_train_index & (label == 1)\n",
    "    sig_val_mask = np.logical_not(rectified_train_index) & (label == 1)\n",
    "    bkg_train_mask = rectified_train_index & (label == 0)\n",
    "    bkg_val_mask = np.logical_not(rectified_train_index) & (label == 0)\n",
    "    weights = [\n",
    "        {'sig': data_aux.loc[sig_train_mask, \"eventWeight\"], 'bkg': data_aux.loc[bkg_train_mask, \"eventWeight\"]},\n",
    "        {'sig': data_aux.loc[sig_val_mask, \"eventWeight\"], 'bkg': data_aux.loc[bkg_val_mask, \"eventWeight\"]}\n",
    "    ]\n",
    "    val_weights_arr.append(copy.deepcopy(weights[1]))\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_weighted_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_density_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=[{'sig': None, 'bkg': None}]*len(labels_arr)\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights\n",
    "    )\n",
    "labels_arr = ['val - fold 0', 'val - fold 1', 'val - fold 2', 'val - fold 3', 'val - fold 4']\n",
    "s_over_root_b(\n",
    "    IN_perf_dict['val'], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_val_comparison', \n",
    "    method='IN_arr', labels=labels_arr, weights=val_weights_arr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Vars Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    sig_mask = (label == 1)\n",
    "    sig_test_mask = (label_test == 1)\n",
    "    bkg_mask = (label == 0)\n",
    "    bkg_test_mask = (label_test == 0)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        \n",
    "        sig_train_mask = rectified_train_index & sig_mask\n",
    "        sig_val_mask = np.logical_not(rectified_train_index) & sig_mask\n",
    "        bkg_train_mask = rectified_train_index & bkg_mask\n",
    "        bkg_val_mask = np.logical_not(rectified_train_index) & bkg_mask\n",
    "\n",
    "        sig_train_np = data_df.loc[sig_train_mask, var_name].to_numpy()\n",
    "        sig_val_np = data_df.loc[sig_val_mask, var_name].to_numpy()\n",
    "        sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "        bkg_train_np = data_df.loc[bkg_train_mask, var_name].to_numpy()\n",
    "        bkg_val_np = data_df.loc[bkg_val_mask, var_name].to_numpy()\n",
    "        bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np = data_df.loc[sig_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_df.loc[bkg_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "    sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    pre_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_pre_std, var_name, pre_std_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### post-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 44\u001b[0m\n\u001b[1;32m     37\u001b[0m     bkg_test_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39mbkg_test_np[bkg_test_np \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     39\u001b[0m     make_input_plot(\n\u001b[1;32m     40\u001b[0m         output_dir_post_std, var_name, \n\u001b[1;32m     41\u001b[0m         [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n\u001b[1;32m     42\u001b[0m         fold_idx\u001b[38;5;241m=\u001b[39mfold_idx, labels\u001b[38;5;241m=\u001b[39mlabel_arr_fold\n\u001b[1;32m     43\u001b[0m     )\n\u001b[0;32m---> 44\u001b[0m sig_train_np, sig_test_np, bkg_train_np, bkg_test_np \u001b[38;5;241m=\u001b[39m \u001b[43mpost_std_np_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m sig_train_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39msig_train_np[sig_train_np \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     47\u001b[0m sig_test_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39msig_test_np[sig_test_np \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[18], line 159\u001b[0m, in \u001b[0;36mpost_std_np_arrays\u001b[0;34m(data, data_test, var_name, train_index, val_index)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m train_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m val_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m (high_level_fields \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(input_hlf_vars)):\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;66;03m# index2, index3 = index_map[var_name]\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m         sig_train_np \u001b[38;5;241m=\u001b[39m data[\u001b[43mdata_list_index_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig_mask\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    160\u001b[0m         sig_test_np \u001b[38;5;241m=\u001b[39m data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n\u001b[1;32m    161\u001b[0m         bkg_train_np \u001b[38;5;241m=\u001b[39m data[data_list_index_map(var_name, data, bkg_mask)]\n",
      "File \u001b[0;32m/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/data_processing.py:30\u001b[0m, in \u001b[0;36mdata_list_index_map\u001b[0;34m(variable_name, data_list, event_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_list)):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m event_mask[i]:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     lepton1_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(np\u001b[38;5;241m.\u001b[39mwhere(data_list[i, :, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m     mask_arr[i, lepton1_idx, index3] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = data_list, data_list_test\n",
    "    else:\n",
    "        data, data_test = data_hlf, data_hlf_test\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name,\n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    post_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_post_std, var_name, post_std_hists[var_name], labels=label_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian smearing on test set (for feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    mask_arr = data_list_index_map(var_name, particle_list_to_smear, np.ones(len(particle_list_to_smear), dtype=bool))\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear[mask_arr] *= rng.normal(size=len(particle_list_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear[mask_arr] += rng.normal(size=len(particle_list_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns[var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate smeared variable test-data on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "    gauss_data_list, gauss_data_hlf = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        gauss_data_list = smear_particle_list(var_name, copy.deepcopy(data_list_test))\n",
    "        gauss_data_hlf = data_hlf_test\n",
    "    else:\n",
    "        gauss_data_list = data_list_test\n",
    "        gauss_data_hlf = smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test))\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list, gauss_data_hlf, label_test, OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "IN_perf_smear_list = []\n",
    "label_arr = []\n",
    "for var_name, IN_perf_smear in IN_perf_smear_dict.items():\n",
    "    IN_perf_smear_list.append(IN_perf_smear)\n",
    "    label_arr.append(var_name)\n",
    "sort = np.argsort([IN_perf_smear['mean_area'] for IN_perf_smear in IN_perf_smear_list])\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_all', \n",
    "    method='IN_arr', labels=label_arr, yscale='log', run2=False, sort=sort\n",
    ")\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5', \n",
    "    method='IN_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False\n",
    ")\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5_and_orig', \n",
    "    method='IN_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False, run3=IN_perf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = smear_particle_list(var_name, data_list), smear_particle_list(var_name, data_list_test)\n",
    "    else:\n",
    "        data, data_test = smear_particle_hlf(var_name, data_hlf), smear_particle_hlf(var_name, data_hlf_test)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name, \n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_gauss_smear, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    gauss_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_gauss_smear, var_name, gauss_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y in [('train', data_list, data_hlf, label), ('test', data_list_test, data_hlf_test, label_test)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'r') as f:\n",
    "    IN_full_eval_dict = json.load(f)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {'mass': [], 'dijet_mass': []}\n",
    "for var_name in hist_dict.keys():\n",
    "    for i, score_cut in enumerate(score_cuts):\n",
    "        sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict)\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "        hist_dict[var_name].extend(\n",
    "            [\n",
    "                copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "            ]\n",
    "        )\n",
    "    for mod_factor, label_mod in enumerate(['sig_train', 'sig_test', 'bkg_train', 'bkg_test']):\n",
    "        plot_list = []\n",
    "        label_list = []\n",
    "        for i in range(len(hist_dict[var_name])):\n",
    "            if (i - mod_factor) % 4 == 0:\n",
    "                plot_list.append(hist_dict[var_name][i])\n",
    "                label_list.append(label_arr[i])\n",
    "        make_input_plot(\n",
    "            plot_destdir, var_name, plot_list, labels=label_list, density=True, \n",
    "            plot_prefix=CURRENT_TIME+'_', plot_postfix='_'+label_mod, alpha=0.5,\n",
    "            linestyle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset size plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_dirs = glob.glob(OUTPUT_DIRPATH[:-1]+'_mod*') + [OUTPUT_DIRPATH[:-1]]\n",
    "final_train_losses_arr, final_val_losses_arr = [], []\n",
    "mod_values_arr = []\n",
    "\n",
    "for train_size_dir in train_size_dirs:\n",
    "    if len(glob.glob(train_size_dir + '/*IN_perf.json')) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        mod_values_arr.append([\n",
    "            float(\n",
    "                train_size_dir[\n",
    "                    train_size_dir.find('_mod')+4 : train_size_dir.find('-')\n",
    "                ]\n",
    "            ),\n",
    "            float(\n",
    "                train_size_dir[train_size_dir.find('-')+1:]\n",
    "            )\n",
    "        ])\n",
    "    except:\n",
    "         mod_values_arr.append([2, 2])\n",
    "    IN_perf_path = glob.glob(f'{train_size_dir}/*IN_perf.json')[0]\n",
    "    with open(IN_perf_path, 'r') as f:\n",
    "        IN_perf = json.load(f)\n",
    "    final_train_losses_arr.append([train_losses[-7 if len(train_losses) < NUM_EPOCHS else -1] for train_losses in IN_perf['train_losses_arr']])\n",
    "    final_val_losses_arr.append([val_losses[-7 if len(val_losses) < NUM_EPOCHS else -1] for val_losses in IN_perf['val_losses_arr']])\n",
    "\n",
    "final_train_losses_arr = np.array(final_train_losses_arr)\n",
    "final_val_losses_arr = np.array(final_val_losses_arr)\n",
    "mod_values_arr = np.array(mod_values_arr)\n",
    "dataset_sizes = (len(label) + len(label_test)) / mod_values_arr\n",
    "sorted_indices = np.argsort(dataset_sizes[:, 0])\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_train_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_train_losses_arr, axis=1)-np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_train_losses_arr, axis=1)+np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[0], alpha=0.5, label='Train data'\n",
    ")\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_val_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_val_losses_arr, axis=1)-np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_val_losses_arr, axis=1)+np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[1], alpha=0.5, label='Val data'\n",
    ")\n",
    "plt.xlabel('Size of train dataset')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.pdf')\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
