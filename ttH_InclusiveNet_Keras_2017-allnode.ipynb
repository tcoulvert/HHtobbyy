{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mgpu-4-culture-plate-sm\u001b[0m  Tue Jul  9 07:06:39 2019\n",
      "\u001b[0;36m[0]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 37'C\u001b[0m, \u001b[0;32m  6 %\u001b[0m | \u001b[0;36m\u001b[1;33m 7779\u001b[0m / \u001b[0;33m 8114\u001b[0m MB | \u001b[1;30mllayer\u001b[0m(\u001b[0;33m7769M\u001b[0m)\n",
      "\u001b[0;36m[1]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[1;31m 51'C\u001b[0m, \u001b[1;32m 38 %\u001b[0m | \u001b[0;36m\u001b[1;33m 7779\u001b[0m / \u001b[0;33m 8114\u001b[0m MB | \u001b[1;30mthong\u001b[0m(\u001b[0;33m7769M\u001b[0m)\n",
      "\u001b[0;36m[2]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 30'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m  355\u001b[0m / \u001b[0;33m 8114\u001b[0m MB | \u001b[1;30mthong\u001b[0m(\u001b[0;33m345M\u001b[0m)\n",
      "\u001b[0;36m[3]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 25'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m    0\u001b[0m / \u001b[0;33m 8114\u001b[0m MB |\n",
      "\u001b[0;36m[4]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 27'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m    0\u001b[0m / \u001b[0;33m 8114\u001b[0m MB |\n",
      "\u001b[0;36m[5]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[1;31m 67'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m 6919\u001b[0m / \u001b[0;33m 8114\u001b[0m MB | \u001b[1;30mthong\u001b[0m(\u001b[0;33m6909M\u001b[0m)\n",
      "\u001b[0;36m[6]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 28'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m    0\u001b[0m / \u001b[0;33m 8114\u001b[0m MB |\n",
      "\u001b[0;36m[7]\u001b[0m \u001b[0;34mGeForce GTX 1080\u001b[0m |\u001b[0;31m 28'C\u001b[0m, \u001b[0;32m  0 %\u001b[0m | \u001b[0;36m\u001b[1;33m 7329\u001b[0m / \u001b[0;33m 8114\u001b[0m MB | \u001b[1;30mthong\u001b[0m(\u001b[0;33m7319M\u001b[0m)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.9\n"
     ]
    }
   ],
   "source": [
    "#import ROOT as rt\n",
    "#from root_numpy import root2array, tree2array\n",
    "import os, sys\n",
    "import gpustat\n",
    "gpustat.print_gpustat()\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions as nlr\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import json\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#from root_pandas import read_root\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, merge, Input, LSTM, Masking,GRU, Concatenate, Convolution1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#x_mean = np.load('mean2017.npy')\n",
    "#x_std = np.load('std2017.npy')\n",
    "with h5py.File(\"data2017_allnode.h5\",\"r\") as input_file:\n",
    "    data_list = input_file[\"list\"][:]\n",
    "    data_hlf = input_file['hlf'][:]\n",
    "    label = input_file[\"label\"][:]\n",
    "    weight = input_file['weight'][:]\n",
    "    nodeweight = input_file['nodeweight'][:]\n",
    "    allnode_weight = input_file['sumallnode_weight'][:]\n",
    "    \n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "skf.get_n_splits(data_hlf, label)\n",
    "\n",
    "print(type(data_hlf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106794, 8, 7)\n",
      "(106794, 9)\n",
      "(106794,)\n",
      "(106794,)\n",
      "(106794, 13)\n",
      "(106794,)\n"
     ]
    }
   ],
   "source": [
    "print(data_list.shape)\n",
    "print(data_hlf.shape)\n",
    "print(label.shape)\n",
    "print(weight.shape)\n",
    "print(nodeweight.shape)\n",
    "print(allnode_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance out the weight by dividing signal weight by 10, and multiply by a factor of 1e6 to have reasonable range\n",
    "weight = weight * 1e6\n",
    "weight[label==1] = weight[label==1]/10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2e4011ce48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e60022390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data_hlf[label==0][:,1],weights=weight[label==0]*allnode_weight[label==0],bins=40,histtype='step',label='background')\n",
    "plt.hist(data_hlf[label==1][:,1],weights=weight[label==1]*allnode_weight[label==1],bins=40,histtype='step',label='signal')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=100\n",
    "config_file = 'BestConfigInclusiveKeras2017.json'\n",
    "retrain=True\n",
    "\n",
    "def build_inclusive_model(name, num_hiddens=2, initial_node=500, \n",
    "                          dropout=0.5, gru_layers=2, \n",
    "                          gru_size=50, dropout_g=0.1):\n",
    "    inputs = []\n",
    "    a = Input(shape=(8, 7))\n",
    "    inputs.append(a)\n",
    "    for i in range(gru_layers):\n",
    "        a = Masking(mask_value=0.0)(a)\n",
    "        a = GRU(gru_size,\n",
    "                input_shape=(8, 7),\n",
    "                return_sequences=True if i < (gru_layers-1) else False)(a)\n",
    "    a = Dropout(np.float32(dropout_g))(a)\n",
    "    aHLF = Input(shape=(9,))\n",
    "    inputs.append(aHLF)\n",
    "    if(dropout > 0.0):\n",
    "        aHLF =  Dropout(dropout)(aHLF)\n",
    "    aHLF = Concatenate(axis=1)([a, aHLF])\n",
    "    for i in range(num_hiddens):\n",
    "        aHLF = Dense(int(round(initial_node/np.power(2,i))), activation='relu')(aHLF)\n",
    "        aHLF =  Dropout(np.float32(dropout))(aHLF)\n",
    "    dense_final = Dense(1, activation=\"sigmoid\")(aHLF)\n",
    "    model = Model(input=inputs, output=dense_final)\n",
    "    return model\n",
    "\n",
    "def train(num_epochs, model, batch_size, train_data=None, val_data=None, train_weight=None, val_weight=None):\n",
    "    print(\"Input data particle shape: {}\".format(train_data[0][0].shape))\n",
    "    print(\"Input data HLF shape: {}\".format(train_data[0][1].shape))\n",
    "    history = model.fit(x=[train_data[0][0],train_data[0][1]],y=train_data[1],\n",
    "              nb_epoch=num_epochs,\n",
    "              sample_weight=train_weight,\n",
    "              validation_data=([val_data[0][0], val_data[0][1]],val_data[1], val_weight),\n",
    "              batch_size=batch_size,\n",
    "              callbacks=[EarlyStopping(verbose=1,patience=10,monitor='val_loss'),\n",
    "                        ModelCheckpoint(filepath='tmp_best_allnode.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "                        ReduceLROnPlateau(patience=4, verbose=1, monitor='val_loss', factor=0.2)]\n",
    "             )\n",
    "    \n",
    "    best_acc = max(history.history['val_acc'])\n",
    "    return best_acc\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "space  = [Integer(1, 3, name='hidden_layers'),\n",
    "          Integer(10, 500, name='initial_nodes'),\n",
    "          Real(0.01,0.9,name='dropout'),\n",
    "          Integer(1, 3, name='gru_layers'),\n",
    "          Integer(10, 500, name='gru_size'),\n",
    "          Real(0.01,0.9,name='dropout_g'),\n",
    "          Real(10**-5, 10**-1, \"log-uniform\", name='learning_rate'),\n",
    "          Integer(32,512,name='batch_size')\n",
    "          ]\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**X):\n",
    "    print(\"New configuration: {}\".format(X))\n",
    "    fom = []\n",
    "    for train_index, test_index in skf.split(data_hlf, label):\n",
    "        print(\"data_list shape = {}\".format(data_list[train_index].shape))\n",
    "        print(\"data_hlf shape = {}\".format(data_hlf[train_index].shape))\n",
    "        \n",
    "        train_data = [data_list[train_index].astype(np.float32), data_hlf[train_index].astype(np.float32)], label[train_index].astype(np.float32)\n",
    "        test_data = [data_list[test_index].astype(np.float32), data_hlf[test_index].astype(np.float32)], label[test_index].astype(np.float32)\n",
    "        train_weight = weight[train_index].astype(np.float32)*allnode_weight[train_index].astype(np.float32)\n",
    "        test_weight = weight[test_index].astype(np.float32)*allnode_weight[test_index].astype(np.float32)\n",
    "        model = build_inclusive_model(\"tmp\", num_hiddens=X['hidden_layers'], initial_node=X['initial_nodes'], \n",
    "                          dropout=X['dropout'], gru_layers=X['gru_layers'], \n",
    "                          gru_size=X['gru_size'], dropout_g=X['dropout_g'])\n",
    "        \n",
    "        model.compile(optimizer=optimizers.Adam(lr=X['learning_rate']), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        \n",
    "        \n",
    "#         train_loader = DataLoader(ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), batch_size = X['batch_size'], shuffle=True)\n",
    "#         val_loader = DataLoader(ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), batch_size = X['batch_size'], shuffle=True)\n",
    "#         data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "#         model = InclusiveNetwork(X['hidden_layers'], X['initial_nodes'], X['dropout'], X['gru_layers'], X['gru_size'], X['dropout_g']).cuda()\n",
    "#         optimizer = AMSGrad(model.parameters(), lr=X['learning_rate'])\n",
    "#         criterion= nn.NLLLoss()\n",
    "#         scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=4)\n",
    "#        best_acc = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader)\n",
    "    \n",
    "        best_acc = train(EPOCHS, model, batch_size=X['batch_size'], \n",
    "                         train_data=train_data, val_data=test_data,\n",
    "                        train_weight=train_weight, val_weight=test_weight)\n",
    "        fom.append(best_acc)\n",
    "        \n",
    "    Y = np.mean(np.asarray(fom))\n",
    "    print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "    return -Y\n",
    "\n",
    "\n",
    "# train_index, test_index = skf.split(data_hlf, label).next()\n",
    "# model = InclusiveNetwork(best_hidden_layers, best_initial_nodes, best_dropout, best_gru_layers, best_gru_size, best_dropout_g).cuda()\n",
    "# train_loader = DataLoader(ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), batch_size = best_batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), batch_size = best_batch_size, shuffle=True)\n",
    "# data_loader = {\"training\": train_loader, \"validation\": val_loader}\n",
    "# optimizer = AMSGrad(model.parameters(), lr=best_learning_rate)\n",
    "# criterion= nn.NLLLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=3)\n",
    "# best_acc = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader)\n",
    "# torch.save(model.state_dict(), model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New configuration: {'hidden_layers': 2, 'dropout_g': 0.35209971949050295, 'initial_nodes': 424, 'dropout': 0.77357159968425371, 'gru_layers': 3, 'batch_size': 59, 'gru_size': 316, 'learning_rate': 0.00015493103643906704}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_3 (Masking)              (None, 6, 7)          0           input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 6, 316)        409536      masking_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "masking_4 (Masking)              (None, 6, 316)        0           lstm_3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 6, 316)        800112      masking_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "masking_5 (Masking)              (None, 6, 316)        0           lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 316)           800112      masking_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 316)           0           lstm_5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 9)             0           input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 325)           0           dropout_6[0][0]                  \n",
      "                                                                   dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 424)           138224      merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)              (None, 424)           0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 212)           90100       dropout_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 212)           0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 1)             213         dropout_9[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,238,297\n",
      "Trainable params: 2,238,297\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4934 - acc: 0.7575Epoch 00000: val_acc improved from -inf to 0.81099, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 643s - loss: 0.4934 - acc: 0.7575 - val_loss: 0.4397 - val_acc: 0.8110\n",
      "Epoch 2/100\n",
      "344265/565732 [=================>............] - ETA: 217s - loss: 0.4668 - acc: 0.7745Epoch 00002: val_acc improved from 0.81598 to 0.82664, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 636s - loss: 0.4608 - acc: 0.7770 - val_loss: 0.4193 - val_acc: 0.8266\n",
      "Epoch 4/100\n",
      "176941/565732 [========>.....................] - ETA: 381s - loss: 0.4584 - acc: 0.7791"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541915/565732 [===========================>..] - ETA: 23s - loss: 0.4578 - acc: 0.7789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360372/565732 [==================>...........] - ETA: 201s - loss: 0.4576 - acc: 0.7795"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4569 - acc: 0.7799Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 636s - loss: 0.4569 - acc: 0.7799 - val_loss: 0.4196 - val_acc: 0.8261\n",
      "Epoch 6/100\n",
      "142898/565732 [======>.......................] - ETA: 416s - loss: 0.4561 - acc: 0.7798"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488225/565732 [========================>.....] - ETA: 76s - loss: 0.4559 - acc: 0.7802Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 641s - loss: 0.4553 - acc: 0.7807 - val_loss: 0.4178 - val_acc: 0.8266\n",
      "Epoch 7/100\n",
      "176941/565732 [========>.....................] - ETA: 383s - loss: 0.4536 - acc: 0.7802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548818/565732 [============================>.] - ETA: 16s - loss: 0.4552 - acc: 0.7800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323851/565732 [================>.............] - ETA: 237s - loss: 0.4530 - acc: 0.7821"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4533 - acc: 0.7814Epoch 00007: val_acc did not improve\n",
      "\n",
      "Epoch 00007: reducing learning rate to 3.0986208003e-05.\n",
      "565732/565732 [==============================] - 636s - loss: 0.4533 - acc: 0.7814 - val_loss: 0.4190 - val_acc: 0.8230\n",
      "Epoch 9/100\n",
      "110684/565732 [====>.........................] - ETA: 446s - loss: 0.4540 - acc: 0.7806"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "471292/565732 [=======================>......] - ETA: 92s - loss: 0.4519 - acc: 0.7813"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248036/565732 [============>.................] - ETA: 312s - loss: 0.4508 - acc: 0.7823"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287389/565732 [==============>...............] - ETA: 275s - loss: 0.4527 - acc: 0.7820"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4531 - acc: 0.7817Epoch 00010: val_acc did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 3.0986208003e-05.\n",
      "565732/565732 [==============================] - 636s - loss: 0.4530 - acc: 0.7817 - val_loss: 0.4165 - val_acc: 0.8175\n",
      "Epoch 12/100\n",
      " 95403/565732 [====>.........................] - ETA: 458s - loss: 0.4539 - acc: 0.7799"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450760/565732 [======================>.......] - ETA: 112s - loss: 0.4506 - acc: 0.7822"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236413/565732 [===========>..................] - ETA: 322s - loss: 0.4503 - acc: 0.7828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410522/565732 [====================>.........] - ETA: 151s - loss: 0.4492 - acc: 0.7829"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192694/565732 [=========>....................] - ETA: 364s - loss: 0.4490 - acc: 0.7830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500202/565732 [=========================>....] - ETA: 64s - loss: 0.4497 - acc: 0.7825"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271872/565732 [=============>................] - ETA: 290s - loss: 0.4503 - acc: 0.7828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4496 - acc: 0.7825Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 636s - loss: 0.4496 - acc: 0.7825 - val_loss: 0.4155 - val_acc: 0.8263\n",
      "Epoch 17/100\n",
      " 52038/565732 [=>............................] - ETA: 502s - loss: 0.4492 - acc: 0.7830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402852/565732 [====================>.........] - ETA: 160s - loss: 0.4487 - acc: 0.7830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4493 - acc: 0.7828Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 636s - loss: 0.4493 - acc: 0.7828 - val_loss: 0.4153 - val_acc: 0.8269\n",
      "Epoch 00016: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_7 (InputLayer)             (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_9 (Masking)              (None, 6, 7)          0           input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                    (None, 6, 316)        409536      masking_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "masking_10 (Masking)             (None, 6, 316)        0           lstm_9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                   (None, 6, 316)        800112      masking_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_11 (Masking)             (None, 6, 316)        0           lstm_10[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                   (None, 316)           800112      masking_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 316)           0           lstm_11[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 9)             0           input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 325)           0           dropout_14[0][0]                 \n",
      "                                                                   dropout_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 424)           138224      merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 424)           0           dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 212)           90100       dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)             (None, 212)           0           dense_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 1)             213         dropout_17[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,238,297\n",
      "Trainable params: 2,238,297\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "144196/565732 [======>.......................] - ETA: 421s - loss: 0.5378 - acc: 0.7251"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486809/565732 [========================>.....] - ETA: 77s - loss: 0.4963 - acc: 0.7549"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272167/565732 [=============>................] - ETA: 287s - loss: 0.4669 - acc: 0.7741"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4648 - acc: 0.7754Epoch 00001: val_acc improved from 0.81438 to 0.82252, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 634s - loss: 0.4648 - acc: 0.7754 - val_loss: 0.4291 - val_acc: 0.8225\n",
      "Epoch 3/100\n",
      " 42421/565732 [=>............................] - ETA: 511s - loss: 0.4590 - acc: 0.7784"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390875/565732 [===================>..........] - ETA: 171s - loss: 0.4615 - acc: 0.7771"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4607 - acc: 0.7775Epoch 00002: val_acc improved from 0.82252 to 0.82445, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 634s - loss: 0.4607 - acc: 0.7775 - val_loss: 0.4276 - val_acc: 0.8245\n",
      "Epoch 4/100\n",
      "165908/565732 [=======>......................] - ETA: 391s - loss: 0.4609 - acc: 0.7786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509170/565732 [==========================>...] - ETA: 55s - loss: 0.4582 - acc: 0.7792"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291106/565732 [==============>...............] - ETA: 269s - loss: 0.4558 - acc: 0.7805"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4562 - acc: 0.7799Epoch 00004: val_acc improved from 0.82445 to 0.82465, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 637s - loss: 0.4562 - acc: 0.7799 - val_loss: 0.4224 - val_acc: 0.8247\n",
      "Epoch 6/100\n",
      " 67555/565732 [==>...........................] - ETA: 496s - loss: 0.4565 - acc: 0.7789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413944/565732 [====================>.........] - ETA: 149s - loss: 0.4552 - acc: 0.7811"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186558/565732 [========>.....................] - ETA: 371s - loss: 0.4530 - acc: 0.7820"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531472/565732 [===========================>..] - ETA: 33s - loss: 0.4540 - acc: 0.7818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162014/565732 [=======>......................] - ETA: 395s - loss: 0.4531 - acc: 0.7815"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318659/565732 [===============>..............] - ETA: 241s - loss: 0.4534 - acc: 0.7813"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509583/565732 [==========================>...] - ETA: 54s - loss: 0.4529 - acc: 0.7816"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102778/565732 [====>.........................] - ETA: 452s - loss: 0.4526 - acc: 0.7830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302670/565732 [===============>..............] - ETA: 257s - loss: 0.4525 - acc: 0.7827"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453710/565732 [=======================>......] - ETA: 109s - loss: 0.4523 - acc: 0.7826"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4525 - acc: 0.7824Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 635s - loss: 0.4525 - acc: 0.7824 - val_loss: 0.4192 - val_acc: 0.8261\n",
      "Epoch 10/100\n",
      " 66434/565732 [==>...........................] - ETA: 487s - loss: 0.4523 - acc: 0.7829"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215409/565732 [==========>...................] - ETA: 343s - loss: 0.4528 - acc: 0.7825"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420788/565732 [=====================>........] - ETA: 141s - loss: 0.4531 - acc: 0.7822"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4529 - acc: 0.7823Epoch 00009: val_acc improved from 0.82679 to 0.83063, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 637s - loss: 0.4529 - acc: 0.7823 - val_loss: 0.4215 - val_acc: 0.8306\n",
      "Epoch 11/100\n",
      " 16638/565732 [..............................] - ETA: 547s - loss: 0.4562 - acc: 0.7779"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201308/565732 [=========>....................] - ETA: 358s - loss: 0.4503 - acc: 0.7844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359074/565732 [==================>...........] - ETA: 203s - loss: 0.4506 - acc: 0.7832"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535720/565732 [===========================>..] - ETA: 29s - loss: 0.4513 - acc: 0.7823"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131157/565732 [=====>........................] - ETA: 426s - loss: 0.4509 - acc: 0.7819"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300664/565732 [==============>...............] - ETA: 259s - loss: 0.4518 - acc: 0.7818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548287/565732 [============================>.] - ETA: 17s - loss: 0.4517 - acc: 0.7818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319013/565732 [===============>..............] - ETA: 241s - loss: 0.4514 - acc: 0.7823"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4513 - acc: 0.7826Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 635s - loss: 0.4513 - acc: 0.7826 - val_loss: 0.4158 - val_acc: 0.8223\n",
      "Epoch 14/100\n",
      " 91627/565732 [===>..........................] - ETA: 463s - loss: 0.4506 - acc: 0.7836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428812/565732 [=====================>........] - ETA: 134s - loss: 0.4509 - acc: 0.7832"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203019/565732 [=========>....................] - ETA: 354s - loss: 0.4511 - acc: 0.7833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532947/565732 [===========================>..] - ETA: 32s - loss: 0.4507 - acc: 0.7834"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297714/565732 [==============>...............] - ETA: 262s - loss: 0.4487 - acc: 0.7838"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.7836Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 634s - loss: 0.4480 - acc: 0.7836 - val_loss: 0.4188 - val_acc: 0.8211\n",
      "Epoch 17/100\n",
      " 65667/565732 [==>...........................] - ETA: 490s - loss: 0.4501 - acc: 0.7839"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414003/565732 [====================>.........] - ETA: 148s - loss: 0.4476 - acc: 0.7846"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181248/565732 [========>.....................] - ETA: 376s - loss: 0.4473 - acc: 0.7843"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "513949/565732 [==========================>...] - ETA: 50s - loss: 0.4479 - acc: 0.7838"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292817/565732 [==============>...............] - ETA: 267s - loss: 0.4481 - acc: 0.7839"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565692/565732 [============================>.] - ETA: 0s - loss: 0.4481 - acc: 0.7840Epoch 00018: val_acc did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 6.1972416006e-06.\n",
      "565732/565732 [==============================] - 635s - loss: 0.4482 - acc: 0.7840 - val_loss: 0.4172 - val_acc: 0.8214\n",
      "Epoch 20/100\n",
      " 67732/565732 [==>...........................] - ETA: 488s - loss: 0.4462 - acc: 0.7847"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416776/565732 [=====================>........] - ETA: 146s - loss: 0.4470 - acc: 0.7846"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203904/565732 [=========>....................] - ETA: 354s - loss: 0.4475 - acc: 0.7845"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544924/565732 [===========================>..] - ETA: 20s - loss: 0.4475 - acc: 0.7842"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4612 - acc: 0.7808Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4612 - acc: 0.7808 - val_loss: 0.4124 - val_acc: 0.8251\n",
      "Epoch 4/100\n",
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.7817Epoch 00003: val_acc improved from 0.82599 to 0.82816, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 94s - loss: 0.4578 - acc: 0.7818 - val_loss: 0.4095 - val_acc: 0.8282\n",
      "Epoch 5/100\n",
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4552 - acc: 0.7827Epoch 00004: val_acc improved from 0.82816 to 0.82971, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 95s - loss: 0.4552 - acc: 0.7827 - val_loss: 0.4079 - val_acc: 0.8297\n",
      "Epoch 6/100\n",
      "188993/565732 [=========>....................] - ETA: 54s - loss: 0.4557 - acc: 0.7836"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4506 - acc: 0.7861Epoch 00008: val_acc improved from 0.82993 to 0.83161, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 93s - loss: 0.4506 - acc: 0.7861 - val_loss: 0.4038 - val_acc: 0.8316\n",
      "Epoch 10/100\n",
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.7862Epoch 00009: val_acc improved from 0.83161 to 0.83509, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 93s - loss: 0.4500 - acc: 0.7862 - val_loss: 0.4043 - val_acc: 0.8351\n",
      "Epoch 11/100\n",
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.7857Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4502 - acc: 0.7858 - val_loss: 0.4061 - val_acc: 0.8301\n",
      "Epoch 12/100\n",
      " 34300/565732 [>.............................] - ETA: 76s - loss: 0.4518 - acc: 0.7862"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4468 - acc: 0.7873Epoch 00014: val_acc did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 4.47316211648e-05.\n",
      "565732/565732 [==============================] - 94s - loss: 0.4468 - acc: 0.7873 - val_loss: 0.4041 - val_acc: 0.8283\n",
      "Epoch 16/100\n",
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4462 - acc: 0.7881Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4462 - acc: 0.7881 - val_loss: 0.4037 - val_acc: 0.8257\n",
      "Epoch 17/100\n",
      "553602/565732 [============================>.] - ETA: 1s - loss: 0.4457 - acc: 0.7882"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.7892Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4446 - acc: 0.7892 - val_loss: 0.4046 - val_acc: 0.8259\n",
      "Epoch 21/100\n",
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.7894Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 94s - loss: 0.4447 - acc: 0.7894 - val_loss: 0.4051 - val_acc: 0.8253\n",
      "Epoch 00020: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_11 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_14 (Masking)             (None, 6, 7)          0           input_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                   (None, 6, 202)        169680      masking_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_15 (Masking)             (None, 6, 202)        0           lstm_14[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                   (None, 202)           327240      masking_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_12 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)             (None, 202)           0           lstm_15[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)             (None, 9)             0           input_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_6 (Merge)                  (None, 211)           0           dropout_22[0][0]                 \n",
      "                                                                   dropout_23[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 244)           51728       merge_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)             (None, 244)           0           dense_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 122)           29890       dropout_24[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)             (None, 122)           0           dense_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 1)             123         dropout_25[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 578,661\n",
      "Trainable params: 578,661\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.5163 - acc: 0.7416Epoch 00000: val_acc improved from -inf to 0.82794, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 97s - loss: 0.5163 - acc: 0.7417 - val_loss: 0.4341 - val_acc: 0.8279\n",
      "Epoch 2/100\n",
      "354319/565732 [=================>............] - ETA: 30s - loss: 0.4713 - acc: 0.7735"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4557 - acc: 0.7829Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4557 - acc: 0.7829 - val_loss: 0.4103 - val_acc: 0.8287\n",
      "Epoch 6/100\n",
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4540 - acc: 0.7838Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4540 - acc: 0.7838 - val_loss: 0.4073 - val_acc: 0.8307\n",
      "Epoch 7/100\n",
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4523 - acc: 0.7848Epoch 00006: val_acc improved from 0.83126 to 0.83351, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 93s - loss: 0.4523 - acc: 0.7848 - val_loss: 0.4040 - val_acc: 0.8335\n",
      "Epoch 8/100\n",
      "309386/565732 [===============>..............] - ETA: 36s - loss: 0.4521 - acc: 0.7854"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4487 - acc: 0.7861Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4488 - acc: 0.7860 - val_loss: 0.4056 - val_acc: 0.8323\n",
      "Epoch 12/100\n",
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4491 - acc: 0.7858Epoch 00011: val_acc did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 4.47316211648e-05.\n",
      "565732/565732 [==============================] - 94s - loss: 0.4491 - acc: 0.7858 - val_loss: 0.4038 - val_acc: 0.8286\n",
      "Epoch 13/100\n",
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4470 - acc: 0.7870Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4470 - acc: 0.7870 - val_loss: 0.4029 - val_acc: 0.8322\n",
      "Epoch 14/100\n",
      "291893/565732 [==============>...............] - ETA: 39s - loss: 0.4472 - acc: 0.7871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4463 - acc: 0.7868Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4463 - acc: 0.7868 - val_loss: 0.4044 - val_acc: 0.8305\n",
      "Epoch 18/100\n",
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4459 - acc: 0.7871Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4459 - acc: 0.7871 - val_loss: 0.4038 - val_acc: 0.8296\n",
      "Epoch 00017: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_16 (Masking)             (None, 6, 7)          0           input_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                   (None, 6, 202)        169680      masking_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_17 (Masking)             (None, 6, 202)        0           lstm_16[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                   (None, 202)           327240      masking_17[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_14 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)             (None, 202)           0           lstm_17[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)             (None, 9)             0           input_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_7 (Merge)                  (None, 211)           0           dropout_26[0][0]                 \n",
      "                                                                   dropout_27[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 244)           51728       merge_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)             (None, 244)           0           dense_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 122)           29890       dropout_28[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)             (None, 122)           0           dense_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 1)             123         dropout_29[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 578,661\n",
      "Trainable params: 578,661\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.5239 - acc: 0.7378Epoch 00000: val_acc improved from -inf to 0.82900, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 98s - loss: 0.5238 - acc: 0.7378 - val_loss: 0.4332 - val_acc: 0.8290\n",
      "Epoch 2/100\n",
      "178017/565732 [========>.....................] - ETA: 55s - loss: 0.4707 - acc: 0.7752"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4618 - acc: 0.7806Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 94s - loss: 0.4618 - acc: 0.7806 - val_loss: 0.4183 - val_acc: 0.8260\n",
      "Epoch 4/100\n",
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4578 - acc: 0.7815Epoch 00003: val_acc did not improve\n",
      "565732/565732 [==============================] - 94s - loss: 0.4578 - acc: 0.7815 - val_loss: 0.4116 - val_acc: 0.8237\n",
      "Epoch 5/100\n",
      "155036/565732 [=======>......................] - ETA: 58s - loss: 0.4558 - acc: 0.7824"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4535 - acc: 0.7829Epoch 00005: val_acc did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 4.47316211648e-05.\n",
      "565732/565732 [==============================] - 94s - loss: 0.4535 - acc: 0.7829 - val_loss: 0.4064 - val_acc: 0.8266\n",
      "Epoch 7/100\n",
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4519 - acc: 0.7843Epoch 00006: val_acc improved from 0.82900 to 0.82928, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 94s - loss: 0.4519 - acc: 0.7842 - val_loss: 0.4059 - val_acc: 0.8293\n",
      "Epoch 8/100\n",
      " 82320/565732 [===>..........................] - ETA: 69s - loss: 0.4530 - acc: 0.7849"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.7856Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 94s - loss: 0.4505 - acc: 0.7856 - val_loss: 0.4043 - val_acc: 0.8275\n",
      "Epoch 10/100\n",
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4510 - acc: 0.7849Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 93s - loss: 0.4509 - acc: 0.7849 - val_loss: 0.4046 - val_acc: 0.8288\n",
      "Epoch 11/100\n",
      " 13034/565732 [..............................] - ETA: 78s - loss: 0.4537 - acc: 0.7832"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4505 - acc: 0.7854Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 94s - loss: 0.4505 - acc: 0.7854 - val_loss: 0.4020 - val_acc: 0.8287\n",
      "Epoch 13/100\n",
      "412972/565732 [====================>.........] - ETA: 22s - loss: 0.4492 - acc: 0.7860"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4494 - acc: 0.7853Epoch 00013: val_acc improved from 0.83007 to 0.83086, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 95s - loss: 0.4494 - acc: 0.7854 - val_loss: 0.4015 - val_acc: 0.8309\n",
      "Epoch 15/100\n",
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4497 - acc: 0.7855Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 94s - loss: 0.4497 - acc: 0.7855 - val_loss: 0.4012 - val_acc: 0.8289\n",
      "Epoch 16/100\n",
      "249704/565732 [============>.................] - ETA: 45s - loss: 0.4500 - acc: 0.7849"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4485 - acc: 0.7868Epoch 00017: val_acc improved from 0.83086 to 0.83135, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 94s - loss: 0.4485 - acc: 0.7868 - val_loss: 0.4019 - val_acc: 0.8313\n",
      "Epoch 19/100\n",
      "204428/565732 [=========>....................] - ETA: 51s - loss: 0.4480 - acc: 0.7867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4490 - acc: 0.7860Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 94s - loss: 0.4489 - acc: 0.7860 - val_loss: 0.4038 - val_acc: 0.8296\n",
      "Epoch 21/100\n",
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4480 - acc: 0.7866Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 94s - loss: 0.4480 - acc: 0.7866 - val_loss: 0.4034 - val_acc: 0.8305\n",
      "Epoch 22/100\n",
      "107702/565732 [====>.........................] - ETA: 66s - loss: 0.4462 - acc: 0.7870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4478 - acc: 0.7868Epoch 00022: val_acc did not improve\n",
      "\n",
      "Epoch 00022: reducing learning rate to 8.94632394193e-06.\n",
      "565732/565732 [==============================] - 94s - loss: 0.4478 - acc: 0.7868 - val_loss: 0.4011 - val_acc: 0.8306\n",
      "Epoch 24/100\n",
      "546056/565732 [===========================>..] - ETA: 2s - loss: 0.4472 - acc: 0.7870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565607/565732 [============================>.] - ETA: 0s - loss: 0.4473 - acc: 0.7867Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 94s - loss: 0.4473 - acc: 0.7867 - val_loss: 0.4018 - val_acc: 0.8308\n",
      "Epoch 27/100\n",
      "400281/565732 [====================>.........] - ETA: 23s - loss: 0.4484 - acc: 0.7864"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565264/565732 [============================>.] - ETA: 0s - loss: 0.4472 - acc: 0.7873Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 94s - loss: 0.4472 - acc: 0.7873 - val_loss: 0.4021 - val_acc: 0.8308\n",
      "Epoch 00028: early stopping\n",
      "Average best_acc across k-fold: 0.833316873252\n",
      "New configuration: {'hidden_layers': 2, 'dropout_g': 0.72281056926188358, 'initial_nodes': 479, 'dropout': 0.1349121945672542, 'gru_layers': 3, 'batch_size': 358, 'gru_size': 242, 'learning_rate': 0.001207563335135195}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_15 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_18 (Masking)             (None, 6, 7)          0           input_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                   (None, 6, 242)        242000      masking_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_19 (Masking)             (None, 6, 242)        0           lstm_18[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                   (None, 6, 242)        469480      masking_19[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_20 (Masking)             (None, 6, 242)        0           lstm_19[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                   (None, 242)           469480      masking_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_16 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)             (None, 242)           0           lstm_20[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)             (None, 9)             0           input_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_8 (Merge)                  (None, 251)           0           dropout_30[0][0]                 \n",
      "                                                                   dropout_31[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 479)           120708      merge_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)             (None, 479)           0           dense_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 239)           114720      dropout_32[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)             (None, 239)           0           dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 1)             240         dropout_33[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,416,628\n",
      "Trainable params: 1,416,628\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "232342/565732 [===========>..................] - ETA: 77s - loss: 0.3668 - acc: 0.8378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.8485Epoch 00001: val_acc improved from 0.85374 to 0.85497, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 142s - loss: 0.3449 - acc: 0.8485 - val_loss: 0.3306 - val_acc: 0.8550\n",
      "Epoch 3/100\n",
      "219454/565732 [==========>...................] - ETA: 75s - loss: 0.3419 - acc: 0.8494"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8509Epoch 00003: val_acc improved from 0.85497 to 0.85600, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 142s - loss: 0.3399 - acc: 0.8509 - val_loss: 0.3276 - val_acc: 0.8560\n",
      "Epoch 5/100\n",
      " 96660/565732 [====>.........................] - ETA: 102s - loss: 0.3365 - acc: 0.8523"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3389 - acc: 0.8510Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 142s - loss: 0.3389 - acc: 0.8510 - val_loss: 0.3286 - val_acc: 0.8552\n",
      "Epoch 6/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3379 - acc: 0.8515Epoch 00005: val_acc improved from 0.85600 to 0.85613, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 142s - loss: 0.3379 - acc: 0.8515 - val_loss: 0.3279 - val_acc: 0.8561\n",
      "Epoch 7/100\n",
      " 26850/565732 [>.............................] - ETA: 116s - loss: 0.3354 - acc: 0.8536"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457882/565732 [=======================>......] - ETA: 23s - loss: 0.3363 - acc: 0.8521"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3356 - acc: 0.8525Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 142s - loss: 0.3356 - acc: 0.8525 - val_loss: 0.3270 - val_acc: 0.8563\n",
      "Epoch 10/100\n",
      "365160/565732 [==================>...........] - ETA: 43s - loss: 0.3361 - acc: 0.8521"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8533Epoch 00010: val_acc improved from 0.85772 to 0.85800, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 142s - loss: 0.3347 - acc: 0.8533 - val_loss: 0.3247 - val_acc: 0.8580\n",
      "Epoch 12/100\n",
      "222676/565732 [==========>...................] - ETA: 74s - loss: 0.3333 - acc: 0.8538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3339 - acc: 0.8534Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 141s - loss: 0.3339 - acc: 0.8534 - val_loss: 0.3281 - val_acc: 0.8575\n",
      "Epoch 14/100\n",
      "218380/565732 [==========>...................] - ETA: 75s - loss: 0.3321 - acc: 0.8542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3334 - acc: 0.8537Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 143s - loss: 0.3334 - acc: 0.8537 - val_loss: 0.3280 - val_acc: 0.8555\n",
      "Epoch 15/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3333 - acc: 0.8536Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 145s - loss: 0.3332 - acc: 0.8536 - val_loss: 0.3336 - val_acc: 0.8565\n",
      "Epoch 16/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.8537Epoch 00015: val_acc did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.000241512665525.\n",
      "565732/565732 [==============================] - 142s - loss: 0.3327 - acc: 0.8537 - val_loss: 0.3284 - val_acc: 0.8563\n",
      "Epoch 17/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.8553Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 142s - loss: 0.3295 - acc: 0.8553 - val_loss: 0.3264 - val_acc: 0.8570\n",
      "Epoch 18/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.8553Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 142s - loss: 0.3288 - acc: 0.8553 - val_loss: 0.3265 - val_acc: 0.8569\n",
      "Epoch 19/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3281 - acc: 0.8555Epoch 00018: val_acc improved from 0.85800 to 0.85817, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 141s - loss: 0.3281 - acc: 0.8555 - val_loss: 0.3238 - val_acc: 0.8582\n",
      "Epoch 20/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3281 - acc: 0.8556Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 141s - loss: 0.3281 - acc: 0.8556 - val_loss: 0.3252 - val_acc: 0.8577\n",
      "Epoch 21/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8558Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 141s - loss: 0.3276 - acc: 0.8558 - val_loss: 0.3243 - val_acc: 0.8576\n",
      "Epoch 22/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8564Epoch 00021: val_acc did not improve\n",
      "565732/565732 [==============================] - 142s - loss: 0.3277 - acc: 0.8564 - val_loss: 0.3263 - val_acc: 0.8565\n",
      "Epoch 23/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8558Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 141s - loss: 0.3274 - acc: 0.8558 - val_loss: 0.3253 - val_acc: 0.8575\n",
      "Epoch 24/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.8558Epoch 00023: val_acc did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 4.8302533105e-05.\n",
      "565732/565732 [==============================] - 141s - loss: 0.3272 - acc: 0.8558 - val_loss: 0.3248 - val_acc: 0.8578\n",
      "Epoch 25/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8565Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 142s - loss: 0.3263 - acc: 0.8565 - val_loss: 0.3246 - val_acc: 0.8578\n",
      "Epoch 26/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8565Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 142s - loss: 0.3262 - acc: 0.8565 - val_loss: 0.3248 - val_acc: 0.8578\n",
      "Epoch 27/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8565Epoch 00026: val_acc did not improve\n",
      "565732/565732 [==============================] - 141s - loss: 0.3258 - acc: 0.8565 - val_loss: 0.3247 - val_acc: 0.8581\n",
      "Epoch 28/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8568Epoch 00027: val_acc did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 9.66050647548e-06.\n",
      "565732/565732 [==============================] - 141s - loss: 0.3262 - acc: 0.8568 - val_loss: 0.3245 - val_acc: 0.8579\n",
      "Epoch 29/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8565Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 141s - loss: 0.3259 - acc: 0.8565 - val_loss: 0.3251 - val_acc: 0.8577\n",
      "Epoch 30/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3260 - acc: 0.8563Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 141s - loss: 0.3260 - acc: 0.8563 - val_loss: 0.3247 - val_acc: 0.8579\n",
      "Epoch 00029: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_17 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_21 (Masking)             (None, 6, 7)          0           input_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                   (None, 6, 242)        242000      masking_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_22 (Masking)             (None, 6, 242)        0           lstm_21[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                   (None, 6, 242)        469480      masking_22[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_23 (Masking)             (None, 6, 242)        0           lstm_22[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                   (None, 242)           469480      masking_23[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_18 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)             (None, 242)           0           lstm_23[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)             (None, 9)             0           input_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_9 (Merge)                  (None, 251)           0           dropout_34[0][0]                 \n",
      "                                                                   dropout_35[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 479)           120708      merge_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)             (None, 479)           0           dense_26[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_27 (Dense)                 (None, 239)           114720      dropout_36[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)             (None, 239)           0           dense_27[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_28 (Dense)                 (None, 1)             240         dropout_37[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,416,628\n",
      "Trainable params: 1,416,628\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3572 - acc: 0.8424Epoch 00000: val_acc improved from -inf to 0.85455, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 147s - loss: 0.3572 - acc: 0.8424 - val_loss: 0.3331 - val_acc: 0.8545\n",
      "Epoch 2/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3449 - acc: 0.8486Epoch 00001: val_acc did not improve\n",
      "565732/565732 [==============================] - 140s - loss: 0.3449 - acc: 0.8486 - val_loss: 0.3339 - val_acc: 0.8523\n",
      "Epoch 3/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3419 - acc: 0.8497Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 140s - loss: 0.3419 - acc: 0.8498 - val_loss: 0.3309 - val_acc: 0.8543\n",
      "Epoch 4/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3403 - acc: 0.8504Epoch 00003: val_acc improved from 0.85455 to 0.85543, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 140s - loss: 0.3403 - acc: 0.8504 - val_loss: 0.3327 - val_acc: 0.8554\n",
      "Epoch 5/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3391 - acc: 0.8514Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 140s - loss: 0.3391 - acc: 0.8514 - val_loss: 0.3301 - val_acc: 0.8547\n",
      "Epoch 6/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3384 - acc: 0.8513Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 140s - loss: 0.3384 - acc: 0.8513 - val_loss: 0.3322 - val_acc: 0.8547\n",
      "Epoch 7/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3372 - acc: 0.8517Epoch 00006: val_acc improved from 0.85543 to 0.85592, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 141s - loss: 0.3372 - acc: 0.8517 - val_loss: 0.3271 - val_acc: 0.8559\n",
      "Epoch 8/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3367 - acc: 0.8520Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 140s - loss: 0.3367 - acc: 0.8520 - val_loss: 0.3285 - val_acc: 0.8559\n",
      "Epoch 9/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3359 - acc: 0.8526Epoch 00008: val_acc improved from 0.85592 to 0.85638, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 140s - loss: 0.3359 - acc: 0.8526 - val_loss: 0.3271 - val_acc: 0.8564\n",
      "Epoch 10/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8528Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 140s - loss: 0.3351 - acc: 0.8528 - val_loss: 0.3311 - val_acc: 0.8535\n",
      "Epoch 11/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3346 - acc: 0.8527Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 140s - loss: 0.3346 - acc: 0.8527 - val_loss: 0.3332 - val_acc: 0.8548\n",
      "Epoch 12/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8530Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 141s - loss: 0.3343 - acc: 0.8530 - val_loss: 0.3300 - val_acc: 0.8553\n",
      "Epoch 13/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3338 - acc: 0.8535Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 141s - loss: 0.3338 - acc: 0.8536 - val_loss: 0.3316 - val_acc: 0.8545\n",
      "Epoch 14/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3332 - acc: 0.8536Epoch 00013: val_acc did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.000241512665525.\n",
      "565732/565732 [==============================] - 141s - loss: 0.3332 - acc: 0.8536 - val_loss: 0.3339 - val_acc: 0.8538\n",
      "Epoch 15/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3300 - acc: 0.8548Epoch 00014: val_acc improved from 0.85638 to 0.85677, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 140s - loss: 0.3300 - acc: 0.8548 - val_loss: 0.3262 - val_acc: 0.8568\n",
      "Epoch 16/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8548Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 140s - loss: 0.3296 - acc: 0.8548 - val_loss: 0.3256 - val_acc: 0.8565\n",
      "Epoch 17/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8551Epoch 00016: val_acc improved from 0.85677 to 0.85708, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 140s - loss: 0.3293 - acc: 0.8551 - val_loss: 0.3255 - val_acc: 0.8571\n",
      "Epoch 18/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.8549Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 140s - loss: 0.3290 - acc: 0.8550 - val_loss: 0.3254 - val_acc: 0.8570\n",
      "Epoch 19/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8557Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 140s - loss: 0.3286 - acc: 0.8557 - val_loss: 0.3269 - val_acc: 0.8560\n",
      "Epoch 20/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8555Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 167s - loss: 0.3282 - acc: 0.8555 - val_loss: 0.3274 - val_acc: 0.8558\n",
      "Epoch 21/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8557Epoch 00020: val_acc improved from 0.85708 to 0.85755, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 183s - loss: 0.3282 - acc: 0.8557 - val_loss: 0.3245 - val_acc: 0.8576\n",
      "Epoch 22/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8555Epoch 00021: val_acc did not improve\n",
      "565732/565732 [==============================] - 183s - loss: 0.3277 - acc: 0.8555 - val_loss: 0.3289 - val_acc: 0.8549\n",
      "Epoch 23/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.8558Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 183s - loss: 0.3278 - acc: 0.8558 - val_loss: 0.3256 - val_acc: 0.8568\n",
      "Epoch 24/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8560Epoch 00023: val_acc improved from 0.85755 to 0.85801, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 185s - loss: 0.3274 - acc: 0.8560 - val_loss: 0.3247 - val_acc: 0.8580\n",
      "Epoch 25/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8556Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 184s - loss: 0.3277 - acc: 0.8556 - val_loss: 0.3262 - val_acc: 0.8573\n",
      "Epoch 26/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8563- ETA: 0s - loss: 0.3270 - acc: 0.85 - ETA: 0s - loss: 0.3270 - acc:Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 184s - loss: 0.3270 - acc: 0.8563 - val_loss: 0.3255 - val_acc: 0.8569\n",
      "Epoch 27/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8560Epoch 00026: val_acc did not improve\n",
      "565732/565732 [==============================] - 183s - loss: 0.3270 - acc: 0.8560 - val_loss: 0.3254 - val_acc: 0.8575\n",
      "Epoch 28/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.8561Epoch 00027: val_acc improved from 0.85801 to 0.85811, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 183s - loss: 0.3269 - acc: 0.8561 - val_loss: 0.3243 - val_acc: 0.8581\n",
      "Epoch 29/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8555Epoch 00028: val_acc did not improve\n",
      "\n",
      "Epoch 00028: reducing learning rate to 4.8302533105e-05.\n",
      "565732/565732 [==============================] - 184s - loss: 0.3270 - acc: 0.8555 - val_loss: 0.3253 - val_acc: 0.8576\n",
      "Epoch 30/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8563Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 177s - loss: 0.3262 - acc: 0.8563 - val_loss: 0.3259 - val_acc: 0.8570\n",
      "Epoch 31/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8566Epoch 00030: val_acc did not improve\n",
      "565732/565732 [==============================] - 183s - loss: 0.3256 - acc: 0.8566 - val_loss: 0.3265 - val_acc: 0.8569\n",
      "Epoch 32/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8568Epoch 00031: val_acc did not improve\n",
      "565732/565732 [==============================] - 182s - loss: 0.3255 - acc: 0.8568 - val_loss: 0.3263 - val_acc: 0.8572\n",
      "Epoch 33/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.8563Epoch 00032: val_acc did not improve\n",
      "\n",
      "Epoch 00032: reducing learning rate to 9.66050647548e-06.\n",
      "565732/565732 [==============================] - 183s - loss: 0.3259 - acc: 0.8563 - val_loss: 0.3262 - val_acc: 0.8573\n",
      "Epoch 34/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8566Epoch 00033: val_acc did not improve\n",
      "565732/565732 [==============================] - 183s - loss: 0.3251 - acc: 0.8567 - val_loss: 0.3267 - val_acc: 0.8567\n",
      "Epoch 35/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8568Epoch 00034: val_acc did not improve\n",
      "565732/565732 [==============================] - 182s - loss: 0.3251 - acc: 0.8568 - val_loss: 0.3262 - val_acc: 0.8571\n",
      "Epoch 36/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8566Epoch 00035: val_acc did not improve\n",
      "565732/565732 [==============================] - 183s - loss: 0.3254 - acc: 0.8566 - val_loss: 0.3265 - val_acc: 0.8570\n",
      "Epoch 37/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8565Epoch 00036: val_acc did not improve\n",
      "\n",
      "Epoch 00036: reducing learning rate to 1.93210125872e-06.\n",
      "565732/565732 [==============================] - 183s - loss: 0.3255 - acc: 0.8565 - val_loss: 0.3263 - val_acc: 0.8571\n",
      "Epoch 38/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8571Epoch 00037: val_acc did not improve\n",
      "565732/565732 [==============================] - 182s - loss: 0.3252 - acc: 0.8571 - val_loss: 0.3263 - val_acc: 0.8571\n",
      "Epoch 39/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8568Epoch 00038: val_acc did not improve\n",
      "565732/565732 [==============================] - 182s - loss: 0.3252 - acc: 0.8568 - val_loss: 0.3264 - val_acc: 0.8570\n",
      "Epoch 00038: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_19 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_24 (Masking)             (None, 6, 7)          0           input_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                   (None, 6, 242)        242000      masking_24[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_25 (Masking)             (None, 6, 242)        0           lstm_24[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_25 (LSTM)                   (None, 6, 242)        469480      masking_25[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_26 (Masking)             (None, 6, 242)        0           lstm_25[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_26 (LSTM)                   (None, 242)           469480      masking_26[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_20 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)             (None, 242)           0           lstm_26[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)             (None, 9)             0           input_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_10 (Merge)                 (None, 251)           0           dropout_38[0][0]                 \n",
      "                                                                   dropout_39[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_29 (Dense)                 (None, 479)           120708      merge_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)             (None, 479)           0           dense_29[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_30 (Dense)                 (None, 239)           114720      dropout_40[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)             (None, 239)           0           dense_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_31 (Dense)                 (None, 1)             240         dropout_41[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,416,628\n",
      "Trainable params: 1,416,628\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8426Epoch 00000: val_acc improved from -inf to 0.85325, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 191s - loss: 0.3565 - acc: 0.8426 - val_loss: 0.3341 - val_acc: 0.8532\n",
      "Epoch 2/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3443 - acc: 0.8487Epoch 00001: val_acc did not improve\n",
      "565732/565732 [==============================] - 184s - loss: 0.3443 - acc: 0.8487 - val_loss: 0.3407 - val_acc: 0.8507\n",
      "Epoch 3/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.8504Epoch 00002: val_acc improved from 0.85325 to 0.85597, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 185s - loss: 0.3409 - acc: 0.8504 - val_loss: 0.3297 - val_acc: 0.8560\n",
      "Epoch 4/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8509Epoch 00003: val_acc did not improve\n",
      "565732/565732 [==============================] - 185s - loss: 0.3394 - acc: 0.8508 - val_loss: 0.3304 - val_acc: 0.8552\n",
      "Epoch 5/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3388 - acc: 0.8512Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 185s - loss: 0.3388 - acc: 0.8512 - val_loss: 0.3304 - val_acc: 0.8549\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3373 - acc: 0.8514Epoch 00005: val_acc improved from 0.85597 to 0.85691, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 185s - loss: 0.3373 - acc: 0.8514 - val_loss: 0.3297 - val_acc: 0.8569\n",
      "Epoch 7/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8522Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 184s - loss: 0.3364 - acc: 0.8522 - val_loss: 0.3286 - val_acc: 0.8566\n",
      "Epoch 8/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8528Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 184s - loss: 0.3357 - acc: 0.8529 - val_loss: 0.3286 - val_acc: 0.8565\n",
      "Epoch 9/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.8531Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 184s - loss: 0.3351 - acc: 0.8531 - val_loss: 0.3384 - val_acc: 0.8507\n",
      "Epoch 10/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8532Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 185s - loss: 0.3345 - acc: 0.8532 - val_loss: 0.3322 - val_acc: 0.8537\n",
      "Epoch 11/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8533Epoch 00010: val_acc did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 0.000241512665525.\n",
      "565732/565732 [==============================] - 186s - loss: 0.3342 - acc: 0.8533 - val_loss: 0.3299 - val_acc: 0.8568\n",
      "Epoch 12/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3304 - acc: 0.8547Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 184s - loss: 0.3304 - acc: 0.8547 - val_loss: 0.3282 - val_acc: 0.8559\n",
      "Epoch 13/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3296 - acc: 0.8549Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 184s - loss: 0.3296 - acc: 0.8549 - val_loss: 0.3262 - val_acc: 0.8567\n",
      "Epoch 14/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8551Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 184s - loss: 0.3293 - acc: 0.8551 - val_loss: 0.3265 - val_acc: 0.8567\n",
      "Epoch 15/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3290 - acc: 0.8553Epoch 00014: val_acc did not improve\n",
      "\n",
      "Epoch 00014: reducing learning rate to 4.8302533105e-05.\n",
      "565732/565732 [==============================] - 184s - loss: 0.3290 - acc: 0.8553 - val_loss: 0.3270 - val_acc: 0.8564\n",
      "Epoch 16/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3280 - acc: 0.8556Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 185s - loss: 0.3280 - acc: 0.8556 - val_loss: 0.3263 - val_acc: 0.8569\n",
      "Epoch 17/100\n",
      "565640/565732 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.8554Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 186s - loss: 0.3285 - acc: 0.8554 - val_loss: 0.3276 - val_acc: 0.8565\n",
      "Epoch 00016: early stopping\n",
      "Average best_acc across k-fold: 0.857727680906\n",
      "New configuration: {'hidden_layers': 2, 'dropout_g': 0.43150437321852519, 'initial_nodes': 295, 'dropout': 0.48826217420961959, 'gru_layers': 3, 'batch_size': 386, 'gru_size': 62, 'learning_rate': 5.5632602056985624e-05}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_21 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_27 (Masking)             (None, 6, 7)          0           input_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_27 (LSTM)                   (None, 6, 62)         17360       masking_27[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_28 (Masking)             (None, 6, 62)         0           lstm_27[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_28 (LSTM)                   (None, 6, 62)         31000       masking_28[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_29 (Masking)             (None, 6, 62)         0           lstm_28[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_29 (LSTM)                   (None, 62)            31000       masking_29[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_22 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)             (None, 62)            0           lstm_29[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)             (None, 9)             0           input_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_11 (Merge)                 (None, 71)            0           dropout_42[0][0]                 \n",
      "                                                                   dropout_43[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                 (None, 295)           21240       merge_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)             (None, 295)           0           dense_32[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_33 (Dense)                 (None, 147)           43512       dropout_44[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)             (None, 147)           0           dense_33[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_34 (Dense)                 (None, 1)             148         dropout_45[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 144,260\n",
      "Trainable params: 144,260\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4894 - acc: 0.7628Epoch 00000: val_acc improved from -inf to 0.84097, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 136s - loss: 0.4894 - acc: 0.7628 - val_loss: 0.3731 - val_acc: 0.8410\n",
      "Epoch 2/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4187 - acc: 0.8078Epoch 00001: val_acc improved from 0.84097 to 0.84472, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 128s - loss: 0.4187 - acc: 0.8078 - val_loss: 0.3701 - val_acc: 0.8447\n",
      "Epoch 3/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4111 - acc: 0.8115Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 128s - loss: 0.4111 - acc: 0.8115 - val_loss: 0.3695 - val_acc: 0.8447\n",
      "Epoch 4/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4069 - acc: 0.8134Epoch 00003: val_acc improved from 0.84472 to 0.84583, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 123s - loss: 0.4069 - acc: 0.8134 - val_loss: 0.3664 - val_acc: 0.8458\n",
      "Epoch 5/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4049 - acc: 0.8140Epoch 00004: val_acc improved from 0.84583 to 0.84688, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.4049 - acc: 0.8140 - val_loss: 0.3694 - val_acc: 0.8469\n",
      "Epoch 6/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4026 - acc: 0.8156Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.4026 - acc: 0.8156 - val_loss: 0.3670 - val_acc: 0.8465\n",
      "Epoch 7/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4018 - acc: 0.8158Epoch 00006: val_acc improved from 0.84688 to 0.84746, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.4017 - acc: 0.8158 - val_loss: 0.3670 - val_acc: 0.8475\n",
      "Epoch 8/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4003 - acc: 0.816 - ETA: 0s - loss: 0.4003 - acc: 0.8162Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 107s - loss: 0.4003 - acc: 0.8162 - val_loss: 0.3677 - val_acc: 0.8464\n",
      "Epoch 9/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3998 - acc: 0.8167Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 113s - loss: 0.3998 - acc: 0.8167 - val_loss: 0.3679 - val_acc: 0.8474\n",
      "Epoch 10/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3984 - acc: 0.8176Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 113s - loss: 0.3984 - acc: 0.8176 - val_loss: 0.3698 - val_acc: 0.8464\n",
      "Epoch 11/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3976 - acc: 0.8180Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 110s - loss: 0.3976 - acc: 0.8180 - val_loss: 0.3686 - val_acc: 0.8466\n",
      "Epoch 12/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3974 - acc: 0.8183Epoch 00011: val_acc did not improve\n",
      "\n",
      "Epoch 00011: reducing learning rate to 1.11265202577e-05.\n",
      "565732/565732 [==============================] - 104s - loss: 0.3974 - acc: 0.8184 - val_loss: 0.3685 - val_acc: 0.8466\n",
      "Epoch 13/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3963 - acc: 0.8193Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3963 - acc: 0.8193 - val_loss: 0.3682 - val_acc: 0.8462\n",
      "Epoch 14/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3956 - acc: 0.8187Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3956 - acc: 0.8187 - val_loss: 0.3696 - val_acc: 0.8458\n",
      "Epoch 15/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3965 - acc: 0.8188Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3966 - acc: 0.8187 - val_loss: 0.3685 - val_acc: 0.8462\n",
      "Epoch 16/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3959 - acc: 0.8188Epoch 00015: val_acc did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 2.22530397878e-06.\n",
      "565732/565732 [==============================] - 102s - loss: 0.3959 - acc: 0.8188 - val_loss: 0.3693 - val_acc: 0.8465\n",
      "Epoch 17/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3960 - acc: 0.8189Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3961 - acc: 0.8189 - val_loss: 0.3681 - val_acc: 0.8467\n",
      "Epoch 18/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3960 - acc: 0.8189Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3960 - acc: 0.8189 - val_loss: 0.3681 - val_acc: 0.8467\n",
      "Epoch 00017: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_23 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_30 (Masking)             (None, 6, 7)          0           input_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_30 (LSTM)                   (None, 6, 62)         17360       masking_30[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_31 (Masking)             (None, 6, 62)         0           lstm_30[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_31 (LSTM)                   (None, 6, 62)         31000       masking_31[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_32 (Masking)             (None, 6, 62)         0           lstm_31[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                   (None, 62)            31000       masking_32[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_24 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)             (None, 62)            0           lstm_32[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)             (None, 9)             0           input_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_12 (Merge)                 (None, 71)            0           dropout_46[0][0]                 \n",
      "                                                                   dropout_47[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_35 (Dense)                 (None, 295)           21240       merge_12[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)             (None, 295)           0           dense_35[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_36 (Dense)                 (None, 147)           43512       dropout_48[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)             (None, 147)           0           dense_36[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_37 (Dense)                 (None, 1)             148         dropout_49[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 144,260\n",
      "Trainable params: 144,260\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4880 - acc: 0.7635Epoch 00000: val_acc improved from -inf to 0.84101, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.4879 - acc: 0.7636 - val_loss: 0.3734 - val_acc: 0.8410\n",
      "Epoch 2/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4175 - acc: 0.8081Epoch 00001: val_acc improved from 0.84101 to 0.84355, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.4176 - acc: 0.8081 - val_loss: 0.3703 - val_acc: 0.8435\n",
      "Epoch 3/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4093 - acc: 0.8129Epoch 00002: val_acc improved from 0.84355 to 0.84470, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 101s - loss: 0.4093 - acc: 0.8129 - val_loss: 0.3683 - val_acc: 0.8447\n",
      "Epoch 4/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4061 - acc: 0.8144Epoch 00003: val_acc improved from 0.84470 to 0.84508, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.4061 - acc: 0.8144 - val_loss: 0.3669 - val_acc: 0.8451\n",
      "Epoch 5/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8158Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.4031 - acc: 0.8158 - val_loss: 0.3688 - val_acc: 0.8450\n",
      "Epoch 6/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4017 - acc: 0.8164Epoch 00005: val_acc improved from 0.84508 to 0.84569, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 103s - loss: 0.4017 - acc: 0.8164 - val_loss: 0.3695 - val_acc: 0.8457\n",
      "Epoch 7/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4005 - acc: 0.8162Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 103s - loss: 0.4005 - acc: 0.8162 - val_loss: 0.3689 - val_acc: 0.8456\n",
      "Epoch 8/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3993 - acc: 0.8173Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 101s - loss: 0.3993 - acc: 0.8173 - val_loss: 0.3677 - val_acc: 0.8451\n",
      "Epoch 9/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8184Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 101s - loss: 0.3983 - acc: 0.8184 - val_loss: 0.3688 - val_acc: 0.8456\n",
      "Epoch 10/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8183Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3970 - acc: 0.8183 - val_loss: 0.3704 - val_acc: 0.8454\n",
      "Epoch 11/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8185Epoch 00010: val_acc did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 1.11265202577e-05.\n",
      "565732/565732 [==============================] - 103s - loss: 0.3970 - acc: 0.8185 - val_loss: 0.3692 - val_acc: 0.8450\n",
      "Epoch 12/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3957 - acc: 0.8194Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 101s - loss: 0.3956 - acc: 0.8194 - val_loss: 0.3693 - val_acc: 0.8453\n",
      "Epoch 13/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8195Epoch 00012: val_acc improved from 0.84569 to 0.84583, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.3950 - acc: 0.8195 - val_loss: 0.3680 - val_acc: 0.8458\n",
      "Epoch 14/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8194Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 101s - loss: 0.3953 - acc: 0.8194 - val_loss: 0.3690 - val_acc: 0.8454\n",
      "Epoch 15/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8193Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 101s - loss: 0.3953 - acc: 0.8193 - val_loss: 0.3691 - val_acc: 0.8456\n",
      "Epoch 16/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3949 - acc: 0.8197- ETA: 0s - loss: 0.3948 - accEpoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3949 - acc: 0.8197 - val_loss: 0.3692 - val_acc: 0.8457\n",
      "Epoch 17/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3954 - acc: 0.8193Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 101s - loss: 0.3954 - acc: 0.8193 - val_loss: 0.3695 - val_acc: 0.8453\n",
      "Epoch 18/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3948 - acc: 0.8196Epoch 00017: val_acc did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 2.22530397878e-06.\n",
      "565732/565732 [==============================] - 101s - loss: 0.3948 - acc: 0.8196 - val_loss: 0.3686 - val_acc: 0.8453\n",
      "Epoch 19/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8195Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3947 - acc: 0.8194 - val_loss: 0.3688 - val_acc: 0.8455\n",
      "Epoch 20/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8199Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 101s - loss: 0.3953 - acc: 0.8199 - val_loss: 0.3696 - val_acc: 0.8453\n",
      "Epoch 21/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8193Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 101s - loss: 0.3950 - acc: 0.8193 - val_loss: 0.3690 - val_acc: 0.8456\n",
      "Epoch 22/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8196Epoch 00021: val_acc did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 4.45060777565e-07.\n",
      "565732/565732 [==============================] - 102s - loss: 0.3952 - acc: 0.8196 - val_loss: 0.3696 - val_acc: 0.8453\n",
      "Epoch 23/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3945 - acc: 0.8199Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 101s - loss: 0.3945 - acc: 0.8199 - val_loss: 0.3694 - val_acc: 0.8455\n",
      "Epoch 24/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3947 - acc: 0.8199Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 101s - loss: 0.3947 - acc: 0.8199 - val_loss: 0.3693 - val_acc: 0.8455\n",
      "Epoch 00023: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_25 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_33 (Masking)             (None, 6, 7)          0           input_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_33 (LSTM)                   (None, 6, 62)         17360       masking_33[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_34 (Masking)             (None, 6, 62)         0           lstm_33[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_34 (LSTM)                   (None, 6, 62)         31000       masking_34[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_35 (Masking)             (None, 6, 62)         0           lstm_34[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_35 (LSTM)                   (None, 62)            31000       masking_35[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_26 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)             (None, 62)            0           lstm_35[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)             (None, 9)             0           input_26[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_13 (Merge)                 (None, 71)            0           dropout_50[0][0]                 \n",
      "                                                                   dropout_51[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_38 (Dense)                 (None, 295)           21240       merge_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)             (None, 295)           0           dense_38[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_39 (Dense)                 (None, 147)           43512       dropout_52[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)             (None, 147)           0           dense_39[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_40 (Dense)                 (None, 1)             148         dropout_53[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 144,260\n",
      "Trainable params: 144,260\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4864 - acc: 0.7656Epoch 00000: val_acc improved from -inf to 0.83803, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.4864 - acc: 0.7656 - val_loss: 0.3755 - val_acc: 0.8380\n",
      "Epoch 2/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4177 - acc: 0.8087Epoch 00001: val_acc improved from 0.83803 to 0.84294, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.4176 - acc: 0.8086 - val_loss: 0.3683 - val_acc: 0.8429\n",
      "Epoch 3/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4098 - acc: 0.8124Epoch 00002: val_acc improved from 0.84294 to 0.84437, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.4098 - acc: 0.8124 - val_loss: 0.3679 - val_acc: 0.8444\n",
      "Epoch 4/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.8142Epoch 00003: val_acc improved from 0.84437 to 0.84497, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.4057 - acc: 0.8142 - val_loss: 0.3657 - val_acc: 0.8450\n",
      "Epoch 5/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4025 - acc: 0.8162Epoch 00004: val_acc improved from 0.84497 to 0.84501, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.4025 - acc: 0.8162 - val_loss: 0.3675 - val_acc: 0.8450\n",
      "Epoch 6/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.4006 - acc: 0.8171Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.4007 - acc: 0.8171 - val_loss: 0.3672 - val_acc: 0.8446\n",
      "Epoch 7/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3990 - acc: 0.8177Epoch 00006: val_acc improved from 0.84501 to 0.84520, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.3991 - acc: 0.8177 - val_loss: 0.3690 - val_acc: 0.8452\n",
      "Epoch 8/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8175Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3986 - acc: 0.8175 - val_loss: 0.3687 - val_acc: 0.8447\n",
      "Epoch 9/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3974 - acc: 0.8187Epoch 00008: val_acc improved from 0.84520 to 0.84622, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 102s - loss: 0.3974 - acc: 0.8187 - val_loss: 0.3678 - val_acc: 0.8462\n",
      "Epoch 10/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8185Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 103s - loss: 0.3970 - acc: 0.8185 - val_loss: 0.3676 - val_acc: 0.8453\n",
      "Epoch 11/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3953 - acc: 0.8192Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 103s - loss: 0.3953 - acc: 0.8192 - val_loss: 0.3680 - val_acc: 0.8455\n",
      "Epoch 12/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8200Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 103s - loss: 0.3942 - acc: 0.8200 - val_loss: 0.3684 - val_acc: 0.8453\n",
      "Epoch 13/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.8201Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3942 - acc: 0.8201 - val_loss: 0.3692 - val_acc: 0.8457\n",
      "Epoch 14/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3934 - acc: 0.8206Epoch 00013: val_acc did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 1.11265202577e-05.\n",
      "565732/565732 [==============================] - 104s - loss: 0.3933 - acc: 0.8206 - val_loss: 0.3706 - val_acc: 0.8452\n",
      "Epoch 15/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8207Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3925 - acc: 0.8207 - val_loss: 0.3696 - val_acc: 0.8450\n",
      "Epoch 16/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3926 - acc: 0.8205Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3926 - acc: 0.8205 - val_loss: 0.3696 - val_acc: 0.8450\n",
      "Epoch 17/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8211Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3918 - acc: 0.8211 - val_loss: 0.3693 - val_acc: 0.8449\n",
      "Epoch 18/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8211Epoch 00017: val_acc did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 2.22530397878e-06.\n",
      "565732/565732 [==============================] - 102s - loss: 0.3925 - acc: 0.8210 - val_loss: 0.3708 - val_acc: 0.8446\n",
      "Epoch 19/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8212Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3918 - acc: 0.8212 - val_loss: 0.3696 - val_acc: 0.8450\n",
      "Epoch 20/100\n",
      "565490/565732 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8212Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 102s - loss: 0.3917 - acc: 0.8212 - val_loss: 0.3694 - val_acc: 0.8450\n",
      "Epoch 00019: early stopping\n",
      "Average best_acc across k-fold: 0.846503284006\n",
      "New configuration: {'hidden_layers': 1, 'dropout_g': 0.35397519320201726, 'initial_nodes': 76, 'dropout': 0.2984854969359606, 'gru_layers': 1, 'batch_size': 248, 'gru_size': 119, 'learning_rate': 0.040774993653676143}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_27 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_36 (Masking)             (None, 6, 7)          0           input_27[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_36 (LSTM)                   (None, 119)           60452       masking_36[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_28 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)             (None, 119)           0           lstm_36[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)             (None, 9)             0           input_28[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_14 (Merge)                 (None, 128)           0           dropout_54[0][0]                 \n",
      "                                                                   dropout_55[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_41 (Dense)                 (None, 76)            9804        merge_14[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)             (None, 76)            0           dense_41[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_42 (Dense)                 (None, 1)             77          dropout_56[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 70,333\n",
      "Trainable params: 70,333\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3951 - acc: 0.8260Epoch 00000: val_acc improved from -inf to 0.84307, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 64s - loss: 0.3951 - acc: 0.8260 - val_loss: 0.3737 - val_acc: 0.8431\n",
      "Epoch 2/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3917 - acc: 0.8283Epoch 00001: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3917 - acc: 0.8283 - val_loss: 0.3692 - val_acc: 0.8430\n",
      "Epoch 3/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8284Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3918 - acc: 0.8284 - val_loss: 0.3758 - val_acc: 0.8402\n",
      "Epoch 4/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3932 - acc: 0.8278Epoch 00003: val_acc improved from 0.84307 to 0.84587, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 59s - loss: 0.3932 - acc: 0.8278 - val_loss: 0.3701 - val_acc: 0.8459\n",
      "Epoch 5/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8282Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3924 - acc: 0.8282 - val_loss: 0.3804 - val_acc: 0.8439\n",
      "Epoch 6/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3929 - acc: 0.8280Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3929 - acc: 0.8280 - val_loss: 0.3749 - val_acc: 0.8427\n",
      "Epoch 7/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3924 - acc: 0.8280Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3924 - acc: 0.8280 - val_loss: 0.3710 - val_acc: 0.8446\n",
      "Epoch 8/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8279Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3922 - acc: 0.8280 - val_loss: 0.4025 - val_acc: 0.8451\n",
      "Epoch 9/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8284Epoch 00008: val_acc did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 0.00815499871969.\n",
      "565732/565732 [==============================] - 61s - loss: 0.3922 - acc: 0.8284 - val_loss: 0.3746 - val_acc: 0.8430\n",
      "Epoch 10/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3792 - acc: 0.8332Epoch 00009: val_acc improved from 0.84587 to 0.84692, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 59s - loss: 0.3792 - acc: 0.8332 - val_loss: 0.3668 - val_acc: 0.8469\n",
      "Epoch 11/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8338Epoch 00010: val_acc improved from 0.84692 to 0.84707, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 59s - loss: 0.3777 - acc: 0.8338 - val_loss: 0.3704 - val_acc: 0.8471\n",
      "Epoch 12/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8342Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3767 - acc: 0.8342 - val_loss: 0.3634 - val_acc: 0.8470\n",
      "Epoch 13/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8343Epoch 00012: val_acc improved from 0.84707 to 0.84779, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 60s - loss: 0.3768 - acc: 0.8344 - val_loss: 0.3721 - val_acc: 0.8478\n",
      "Epoch 14/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3756 - acc: 0.8343Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3756 - acc: 0.8343 - val_loss: 0.3674 - val_acc: 0.8465\n",
      "Epoch 15/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8343Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3762 - acc: 0.8343 - val_loss: 0.3686 - val_acc: 0.8461\n",
      "Epoch 18/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3752 - acc: 0.8349Epoch 00017: val_acc improved from 0.84809 to 0.84813, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 59s - loss: 0.3752 - acc: 0.8349 - val_loss: 0.3623 - val_acc: 0.8481\n",
      "Epoch 19/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3754 - acc: 0.8345Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3754 - acc: 0.8345 - val_loss: 0.3635 - val_acc: 0.8470\n",
      "Epoch 20/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3749 - acc: 0.8350Epoch 00019: val_acc did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.00163099970669.\n",
      "565732/565732 [==============================] - 59s - loss: 0.3749 - acc: 0.8350 - val_loss: 0.3611 - val_acc: 0.8470\n",
      "Epoch 21/100\n",
      "273792/565732 [=============>................] - ETA: 26s - loss: 0.3732 - acc: 0.8356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8362Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3709 - acc: 0.8362 - val_loss: 0.3669 - val_acc: 0.8472\n",
      "Epoch 26/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3708 - acc: 0.8366Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3708 - acc: 0.8366 - val_loss: 0.3649 - val_acc: 0.8469\n",
      "Epoch 27/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8363Epoch 00026: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3704 - acc: 0.8363 - val_loss: 0.3667 - val_acc: 0.8473\n",
      "Epoch 28/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8362Epoch 00027: val_acc did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 6.52399903629e-05.\n",
      "565732/565732 [==============================] - 59s - loss: 0.3709 - acc: 0.8362 - val_loss: 0.3660 - val_acc: 0.8472\n",
      "Epoch 29/100\n",
      "275528/565732 [=============>................] - ETA: 26s - loss: 0.3711 - acc: 0.8359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3705 - acc: 0.8364Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3706 - acc: 0.8363 - val_loss: 0.3656 - val_acc: 0.8471\n",
      "Epoch 00028: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_29 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_37 (Masking)             (None, 6, 7)          0           input_29[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_37 (LSTM)                   (None, 119)           60452       masking_37[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_30 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_57 (Dropout)             (None, 119)           0           lstm_37[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_58 (Dropout)             (None, 9)             0           input_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_15 (Merge)                 (None, 128)           0           dropout_57[0][0]                 \n",
      "                                                                   dropout_58[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_43 (Dense)                 (None, 76)            9804        merge_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_59 (Dropout)             (None, 76)            0           dense_43[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_44 (Dense)                 (None, 1)             77          dropout_59[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 70,333\n",
      "Trainable params: 70,333\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3961 - acc: 0.8261Epoch 00000: val_acc improved from -inf to 0.84669, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 64s - loss: 0.3961 - acc: 0.8262 - val_loss: 0.3710 - val_acc: 0.8467\n",
      "Epoch 2/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8289Epoch 00001: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3912 - acc: 0.8289 - val_loss: 0.3643 - val_acc: 0.8431\n",
      "Epoch 3/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3912 - acc: 0.8284Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3912 - acc: 0.8284 - val_loss: 0.3717 - val_acc: 0.8416\n",
      "Epoch 4/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3919 - acc: 0.8285Epoch 00003: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3919 - acc: 0.8285 - val_loss: 0.3656 - val_acc: 0.8435\n",
      "Epoch 5/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3914 - acc: 0.8282Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3914 - acc: 0.8282 - val_loss: 0.3782 - val_acc: 0.8415\n",
      "Epoch 6/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8286Epoch 00005: val_acc did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.00815499871969.\n",
      "565732/565732 [==============================] - 61s - loss: 0.3910 - acc: 0.8286 - val_loss: 0.3751 - val_acc: 0.8392\n",
      "Epoch 7/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3778 - acc: 0.8338Epoch 00006: val_acc improved from 0.84669 to 0.84688, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 59s - loss: 0.3778 - acc: 0.8338 - val_loss: 0.3663 - val_acc: 0.8469\n",
      "Epoch 8/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3750 - acc: 0.8346Epoch 00007: val_acc improved from 0.84688 to 0.84873, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 60s - loss: 0.3750 - acc: 0.8346 - val_loss: 0.3667 - val_acc: 0.8487\n",
      "Epoch 9/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8345Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3751 - acc: 0.8345 - val_loss: 0.3698 - val_acc: 0.8465\n",
      "Epoch 10/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3745 - acc: 0.8354- ETA: 0s - loss: 0.3745 - acc: 0.83Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3745 - acc: 0.8354 - val_loss: 0.3588 - val_acc: 0.8475\n",
      "Epoch 11/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3745 - acc: 0.8347Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3745 - acc: 0.8347 - val_loss: 0.3667 - val_acc: 0.8462\n",
      "Epoch 12/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3745 - acc: 0.8351Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3745 - acc: 0.8351 - val_loss: 0.3638 - val_acc: 0.8475\n",
      "Epoch 13/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3742 - acc: 0.8354Epoch 00012: val_acc did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 0.00163099970669.\n",
      "565732/565732 [==============================] - 60s - loss: 0.3742 - acc: 0.8355 - val_loss: 0.3618 - val_acc: 0.8465\n",
      "Epoch 14/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3717 - acc: 0.8364Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3717 - acc: 0.8364 - val_loss: 0.3651 - val_acc: 0.8462\n",
      "Epoch 15/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8372Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3705 - acc: 0.8371 - val_loss: 0.3600 - val_acc: 0.8470\n",
      "Epoch 16/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.8370Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3707 - acc: 0.8370 - val_loss: 0.3622 - val_acc: 0.8456\n",
      "Epoch 17/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.8368Epoch 00016: val_acc did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.000326199945994.\n",
      "565732/565732 [==============================] - 59s - loss: 0.3706 - acc: 0.8367 - val_loss: 0.3609 - val_acc: 0.8466\n",
      "Epoch 18/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.8364Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3706 - acc: 0.8364 - val_loss: 0.3629 - val_acc: 0.8468\n",
      "Epoch 19/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3704 - acc: 0.8365Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 59s - loss: 0.3704 - acc: 0.8365 - val_loss: 0.3641 - val_acc: 0.8464\n",
      "Epoch 00018: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_31 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_38 (Masking)             (None, 6, 7)          0           input_31[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_38 (LSTM)                   (None, 119)           60452       masking_38[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_32 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_60 (Dropout)             (None, 119)           0           lstm_38[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_61 (Dropout)             (None, 9)             0           input_32[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_16 (Merge)                 (None, 128)           0           dropout_60[0][0]                 \n",
      "                                                                   dropout_61[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_45 (Dense)                 (None, 76)            9804        merge_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)             (None, 76)            0           dense_45[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_46 (Dense)                 (None, 1)             77          dropout_62[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 70,333\n",
      "Trainable params: 70,333\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3952 - acc: 0.8266Epoch 00000: val_acc improved from -inf to 0.84626, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 66s - loss: 0.3952 - acc: 0.8266 - val_loss: 0.3675 - val_acc: 0.8463\n",
      "Epoch 2/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3910 - acc: 0.8291Epoch 00001: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3910 - acc: 0.8291 - val_loss: 0.3826 - val_acc: 0.8452\n",
      "Epoch 3/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3918 - acc: 0.8288Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3917 - acc: 0.8288 - val_loss: 0.3651 - val_acc: 0.8435\n",
      "Epoch 4/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3921 - acc: 0.8287Epoch 00003: val_acc did not improve\n",
      "565732/565732 [==============================] - 61s - loss: 0.3921 - acc: 0.8287 - val_loss: 0.3675 - val_acc: 0.8451\n",
      "Epoch 5/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8287Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3925 - acc: 0.8287 - val_loss: 0.3751 - val_acc: 0.8434\n",
      "Epoch 6/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8285Epoch 00005: val_acc did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.00815499871969.\n",
      "565732/565732 [==============================] - 62s - loss: 0.3931 - acc: 0.8285 - val_loss: 0.3775 - val_acc: 0.8434\n",
      "Epoch 7/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3791 - acc: 0.8342Epoch 00006: val_acc improved from 0.84626 to 0.84647, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 60s - loss: 0.3791 - acc: 0.8342 - val_loss: 0.3662 - val_acc: 0.8465\n",
      "Epoch 8/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8339Epoch 00007: val_acc improved from 0.84647 to 0.84683, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 60s - loss: 0.3776 - acc: 0.8339 - val_loss: 0.3584 - val_acc: 0.8468\n",
      "Epoch 9/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8344Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 61s - loss: 0.3768 - acc: 0.8344 - val_loss: 0.3740 - val_acc: 0.8457\n",
      "Epoch 10/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3761 - acc: 0.8345Epoch 00009: val_acc improved from 0.84683 to 0.84691, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 61s - loss: 0.3761 - acc: 0.8345 - val_loss: 0.3673 - val_acc: 0.8469\n",
      "Epoch 11/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8345Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 61s - loss: 0.3758 - acc: 0.8345 - val_loss: 0.3618 - val_acc: 0.8464\n",
      "Epoch 12/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3750 - acc: 0.8350Epoch 00011: val_acc improved from 0.84691 to 0.84766, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 61s - loss: 0.3750 - acc: 0.8350 - val_loss: 0.3729 - val_acc: 0.8477\n",
      "Epoch 13/100\n",
      "565688/565732 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8356Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3751 - acc: 0.8356 - val_loss: 0.3698 - val_acc: 0.8462\n",
      "Epoch 14/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3758 - acc: 0.8351Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3758 - acc: 0.8351 - val_loss: 0.3633 - val_acc: 0.8459\n",
      "Epoch 15/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3751 - acc: 0.8350Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3751 - acc: 0.8350 - val_loss: 0.3644 - val_acc: 0.8467\n",
      "Epoch 16/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3755 - acc: 0.8350Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3755 - acc: 0.8350 - val_loss: 0.3643 - val_acc: 0.8466\n",
      "Epoch 17/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3741 - acc: 0.8355Epoch 00016: val_acc did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.00163099970669.\n",
      "565732/565732 [==============================] - 60s - loss: 0.3741 - acc: 0.8355 - val_loss: 0.3699 - val_acc: 0.8466\n",
      "Epoch 18/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3724 - acc: 0.8359Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3725 - acc: 0.8359 - val_loss: 0.3608 - val_acc: 0.8473\n",
      "Epoch 19/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8364Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3718 - acc: 0.8364 - val_loss: 0.3633 - val_acc: 0.8476\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8366Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3710 - acc: 0.8366 - val_loss: 0.3688 - val_acc: 0.8469\n",
      "Epoch 21/100\n",
      "565440/565732 [============================>.] - ETA: 0s - loss: 0.3710 - acc: 0.8370Epoch 00020: val_acc did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 0.000326199945994.\n",
      "565732/565732 [==============================] - 60s - loss: 0.3710 - acc: 0.8370 - val_loss: 0.3641 - val_acc: 0.8474\n",
      "Epoch 22/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3702 - acc: 0.8370Epoch 00021: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3702 - acc: 0.8370 - val_loss: 0.3628 - val_acc: 0.8475\n",
      "Epoch 23/100\n",
      "565192/565732 [============================>.] - ETA: 0s - loss: 0.3701 - acc: 0.8369Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 60s - loss: 0.3701 - acc: 0.8369 - val_loss: 0.3637 - val_acc: 0.8472\n",
      "Epoch 00022: early stopping\n",
      "Average best_acc across k-fold: 0.848175480703\n",
      "New configuration: {'hidden_layers': 2, 'dropout_g': 0.16210953077108023, 'initial_nodes': 452, 'dropout': 0.098359511819486767, 'gru_layers': 3, 'batch_size': 392, 'gru_size': 330, 'learning_rate': 0.00027077506445052755}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_33 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_39 (Masking)             (None, 6, 7)          0           input_33[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_39 (LSTM)                   (None, 6, 330)        446160      masking_39[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_40 (Masking)             (None, 6, 330)        0           lstm_39[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_40 (LSTM)                   (None, 6, 330)        872520      masking_40[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_41 (Masking)             (None, 6, 330)        0           lstm_40[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_41 (LSTM)                   (None, 330)           872520      masking_41[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_34 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)             (None, 330)           0           lstm_41[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_64 (Dropout)             (None, 9)             0           input_34[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_17 (Merge)                 (None, 339)           0           dropout_63[0][0]                 \n",
      "                                                                   dropout_64[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_47 (Dense)                 (None, 452)           153680      merge_17[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)             (None, 452)           0           dense_47[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_48 (Dense)                 (None, 226)           102378      dropout_65[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)             (None, 226)           0           dense_48[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_49 (Dense)                 (None, 1)             227         dropout_66[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,447,485\n",
      "Trainable params: 2,447,485\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3565 - acc: 0.8429Epoch 00000: val_acc improved from -inf to 0.84993, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 155s - loss: 0.3565 - acc: 0.8429 - val_loss: 0.3421 - val_acc: 0.8499\n",
      "Epoch 2/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3366 - acc: 0.8523Epoch 00002: val_acc improved from 0.85289 to 0.85591, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 147s - loss: 0.3366 - acc: 0.8523 - val_loss: 0.3287 - val_acc: 0.8559\n",
      "Epoch 4/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3340 - acc: 0.8533Epoch 00003: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3340 - acc: 0.8532 - val_loss: 0.3291 - val_acc: 0.8554\n",
      "Epoch 5/100\n",
      "148176/565732 [======>.......................] - ETA: 93s - loss: 0.3312 - acc: 0.8544Epoch 00004: val_acc improved from 0.85591 to 0.85631, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 147s - loss: 0.3328 - acc: 0.8538 - val_loss: 0.3270 - val_acc: 0.8563\n",
      "Epoch 6/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3320 - acc: 0.8540Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3320 - acc: 0.8540 - val_loss: 0.3291 - val_acc: 0.8554\n",
      "Epoch 7/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3314 - acc: 0.8544Epoch 00006: val_acc improved from 0.85631 to 0.85704, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 148s - loss: 0.3314 - acc: 0.8544 - val_loss: 0.3261 - val_acc: 0.8570\n",
      "Epoch 8/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3306 - acc: 0.8547Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3306 - acc: 0.8547 - val_loss: 0.3280 - val_acc: 0.8556\n",
      "Epoch 9/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8550Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3298 - acc: 0.8550 - val_loss: 0.3262 - val_acc: 0.8566\n",
      "Epoch 10/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.8557Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3292 - acc: 0.8557 - val_loss: 0.3282 - val_acc: 0.8554\n",
      "Epoch 11/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.8555Epoch 00010: val_acc improved from 0.85704 to 0.85780, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 148s - loss: 0.3292 - acc: 0.8555 - val_loss: 0.3265 - val_acc: 0.8578\n",
      "Epoch 12/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3283 - acc: 0.8558Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 148s - loss: 0.3283 - acc: 0.8558 - val_loss: 0.3246 - val_acc: 0.8576\n",
      "Epoch 13/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8562Epoch 00012: val_acc improved from 0.85780 to 0.85846, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 147s - loss: 0.3277 - acc: 0.8562 - val_loss: 0.3228 - val_acc: 0.8585\n",
      "Epoch 14/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8561Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3274 - acc: 0.8560 - val_loss: 0.3274 - val_acc: 0.8560\n",
      "Epoch 15/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.8561Epoch 00014: val_acc improved from 0.85846 to 0.85855, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 147s - loss: 0.3272 - acc: 0.8561 - val_loss: 0.3215 - val_acc: 0.8586\n",
      "Epoch 16/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8564Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3266 - acc: 0.8564 - val_loss: 0.3237 - val_acc: 0.8578\n",
      "Epoch 17/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8567Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3262 - acc: 0.8567 - val_loss: 0.3228 - val_acc: 0.8579\n",
      "Epoch 18/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8565Epoch 00017: val_acc did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 5.415501073e-05.\n",
      "565732/565732 [==============================] - 151s - loss: 0.3266 - acc: 0.8565 - val_loss: 0.3233 - val_acc: 0.8576\n",
      "Epoch 19/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.8571Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 148s - loss: 0.3242 - acc: 0.8571 - val_loss: 0.3221 - val_acc: 0.8584\n",
      "Epoch 20/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8577Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3235 - acc: 0.8577 - val_loss: 0.3219 - val_acc: 0.8585\n",
      "Epoch 21/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8581Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3231 - acc: 0.8581 - val_loss: 0.3220 - val_acc: 0.8582\n",
      "Epoch 22/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8577Epoch 00021: val_acc did not improve\n",
      "\n",
      "Epoch 00021: reducing learning rate to 1.0831002146e-05.\n",
      "565732/565732 [==============================] - 147s - loss: 0.3233 - acc: 0.8577 - val_loss: 0.3216 - val_acc: 0.8583\n",
      "Epoch 23/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8580Epoch 00022: val_acc improved from 0.85855 to 0.85870, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 147s - loss: 0.3223 - acc: 0.8580 - val_loss: 0.3218 - val_acc: 0.8587\n",
      "Epoch 24/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.8583Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3224 - acc: 0.8583 - val_loss: 0.3216 - val_acc: 0.8587\n",
      "Epoch 25/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8585Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3225 - acc: 0.8585 - val_loss: 0.3220 - val_acc: 0.8585\n",
      "Epoch 26/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8582Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3219 - acc: 0.8582 - val_loss: 0.3218 - val_acc: 0.8587\n",
      "Epoch 27/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8581Epoch 00026: val_acc improved from 0.85870 to 0.85878, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 148s - loss: 0.3225 - acc: 0.8581 - val_loss: 0.3215 - val_acc: 0.8588\n",
      "Epoch 28/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8586Epoch 00027: val_acc did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 2.16620046558e-06.\n",
      "565732/565732 [==============================] - 148s - loss: 0.3220 - acc: 0.8586 - val_loss: 0.3219 - val_acc: 0.8586\n",
      "Epoch 29/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8582Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 148s - loss: 0.3223 - acc: 0.8582 - val_loss: 0.3218 - val_acc: 0.8587\n",
      "Epoch 30/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8586Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3220 - acc: 0.8586 - val_loss: 0.3218 - val_acc: 0.8587\n",
      "Epoch 31/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8585Epoch 00030: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3221 - acc: 0.8585 - val_loss: 0.3218 - val_acc: 0.8587\n",
      "Epoch 32/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8583Epoch 00031: val_acc did not improve\n",
      "\n",
      "Epoch 00031: reducing learning rate to 4.33240074926e-07.\n",
      "565732/565732 [==============================] - 147s - loss: 0.3220 - acc: 0.8583 - val_loss: 0.3216 - val_acc: 0.8587\n",
      "Epoch 33/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8580Epoch 00032: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3222 - acc: 0.8580 - val_loss: 0.3217 - val_acc: 0.8587\n",
      "Epoch 34/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8583Epoch 00033: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3222 - acc: 0.8583 - val_loss: 0.3217 - val_acc: 0.8587\n",
      "Epoch 35/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8588Epoch 00034: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3220 - acc: 0.8588 - val_loss: 0.3218 - val_acc: 0.8587\n",
      "Epoch 36/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8584Epoch 00035: val_acc did not improve\n",
      "\n",
      "Epoch 00035: reducing learning rate to 8.66480149853e-08.\n",
      "565732/565732 [==============================] - 147s - loss: 0.3221 - acc: 0.8584 - val_loss: 0.3217 - val_acc: 0.8587\n",
      "Epoch 37/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8585Epoch 00036: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3220 - acc: 0.8585 - val_loss: 0.3217 - val_acc: 0.8587\n",
      "Epoch 38/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8584Epoch 00037: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3220 - acc: 0.8584 - val_loss: 0.3218 - val_acc: 0.8587\n",
      "Epoch 00037: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_35 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_42 (Masking)             (None, 6, 7)          0           input_35[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_42 (LSTM)                   (None, 6, 330)        446160      masking_42[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_43 (Masking)             (None, 6, 330)        0           lstm_42[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_43 (LSTM)                   (None, 6, 330)        872520      masking_43[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_44 (Masking)             (None, 6, 330)        0           lstm_43[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_44 (LSTM)                   (None, 330)           872520      masking_44[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_36 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)             (None, 330)           0           lstm_44[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_68 (Dropout)             (None, 9)             0           input_36[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_18 (Merge)                 (None, 339)           0           dropout_67[0][0]                 \n",
      "                                                                   dropout_68[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_50 (Dense)                 (None, 452)           153680      merge_18[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_69 (Dropout)             (None, 452)           0           dense_50[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_51 (Dense)                 (None, 226)           102378      dropout_69[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_70 (Dropout)             (None, 226)           0           dense_51[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_52 (Dense)                 (None, 1)             227         dropout_70[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,447,485\n",
      "Trainable params: 2,447,485\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.8417Epoch 00000: val_acc improved from -inf to 0.85375, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 155s - loss: 0.3587 - acc: 0.8417 - val_loss: 0.3339 - val_acc: 0.8538\n",
      "Epoch 2/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3414 - acc: 0.8499Epoch 00001: val_acc improved from 0.85375 to 0.85580, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 146s - loss: 0.3414 - acc: 0.8499 - val_loss: 0.3284 - val_acc: 0.8558\n",
      "Epoch 3/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8521Epoch 00002: val_acc improved from 0.85580 to 0.85676, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 147s - loss: 0.3371 - acc: 0.8521 - val_loss: 0.3254 - val_acc: 0.8568\n",
      "Epoch 4/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8524Epoch 00003: val_acc improved from 0.85676 to 0.85759, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 146s - loss: 0.3357 - acc: 0.8524 - val_loss: 0.3250 - val_acc: 0.8576\n",
      "Epoch 5/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8531Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3342 - acc: 0.8531 - val_loss: 0.3273 - val_acc: 0.8558\n",
      "Epoch 6/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3327 - acc: 0.8536Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3327 - acc: 0.8536 - val_loss: 0.3257 - val_acc: 0.8573\n",
      "Epoch 7/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3323 - acc: 0.8540Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3323 - acc: 0.8540 - val_loss: 0.3283 - val_acc: 0.8567\n",
      "Epoch 8/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.8542Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3316 - acc: 0.8542 - val_loss: 0.3270 - val_acc: 0.8568\n",
      "Epoch 9/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8544Epoch 00008: val_acc did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 5.415501073e-05.\n",
      "565732/565732 [==============================] - 148s - loss: 0.3310 - acc: 0.8544 - val_loss: 0.3248 - val_acc: 0.8575\n",
      "Epoch 10/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8559Epoch 00009: val_acc improved from 0.85759 to 0.85921, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 146s - loss: 0.3282 - acc: 0.8559 - val_loss: 0.3221 - val_acc: 0.8592\n",
      "Epoch 11/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8561Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3277 - acc: 0.8561 - val_loss: 0.3209 - val_acc: 0.8592\n",
      "Epoch 12/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8559Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3275 - acc: 0.8559 - val_loss: 0.3218 - val_acc: 0.8588\n",
      "Epoch 13/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8566Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3270 - acc: 0.8566 - val_loss: 0.3225 - val_acc: 0.8587\n",
      "Epoch 14/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.8564Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3269 - acc: 0.8564 - val_loss: 0.3226 - val_acc: 0.8587\n",
      "Epoch 15/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3269 - acc: 0.8560Epoch 00014: val_acc improved from 0.85921 to 0.85981, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 146s - loss: 0.3269 - acc: 0.8560 - val_loss: 0.3204 - val_acc: 0.8598\n",
      "Epoch 16/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8567Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 145s - loss: 0.3266 - acc: 0.8567 - val_loss: 0.3209 - val_acc: 0.8594\n",
      "Epoch 17/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8564Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3266 - acc: 0.8564 - val_loss: 0.3203 - val_acc: 0.8595\n",
      "Epoch 18/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.8565Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3265 - acc: 0.8565 - val_loss: 0.3214 - val_acc: 0.8589\n",
      "Epoch 19/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8568Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3262 - acc: 0.8568 - val_loss: 0.3216 - val_acc: 0.8590\n",
      "Epoch 20/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8560Epoch 00019: val_acc improved from 0.85981 to 0.85983, saving model to tmp_best_allnode.h5\n",
      "\n",
      "Epoch 00019: reducing learning rate to 1.0831002146e-05.\n",
      "565732/565732 [==============================] - 146s - loss: 0.3264 - acc: 0.8560 - val_loss: 0.3213 - val_acc: 0.8598\n",
      "Epoch 21/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8567Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3255 - acc: 0.8567 - val_loss: 0.3208 - val_acc: 0.8593\n",
      "Epoch 22/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8566Epoch 00021: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3256 - acc: 0.8566 - val_loss: 0.3211 - val_acc: 0.8591\n",
      "Epoch 23/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8569Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3253 - acc: 0.8569 - val_loss: 0.3208 - val_acc: 0.8594\n",
      "Epoch 24/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8568Epoch 00023: val_acc did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 2.16620046558e-06.\n",
      "565732/565732 [==============================] - 146s - loss: 0.3253 - acc: 0.8568 - val_loss: 0.3205 - val_acc: 0.8592\n",
      "Epoch 25/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8571Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3250 - acc: 0.8571 - val_loss: 0.3206 - val_acc: 0.8594\n",
      "Epoch 26/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8574Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3250 - acc: 0.8574 - val_loss: 0.3206 - val_acc: 0.8593\n",
      "Epoch 27/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3253 - acc: 0.8570Epoch 00026: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3253 - acc: 0.8570 - val_loss: 0.3205 - val_acc: 0.8594\n",
      "Epoch 28/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8570Epoch 00027: val_acc did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 4.33240074926e-07.\n",
      "565732/565732 [==============================] - 146s - loss: 0.3254 - acc: 0.8570 - val_loss: 0.3208 - val_acc: 0.8592\n",
      "Epoch 29/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.8573Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3248 - acc: 0.8573 - val_loss: 0.3206 - val_acc: 0.8593\n",
      "Epoch 30/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8571Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3251 - acc: 0.8571 - val_loss: 0.3206 - val_acc: 0.8593\n",
      "Epoch 31/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8568Epoch 00030: val_acc did not improve\n",
      "565732/565732 [==============================] - 146s - loss: 0.3250 - acc: 0.8568 - val_loss: 0.3206 - val_acc: 0.8594\n",
      "Epoch 00030: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_37 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_45 (Masking)             (None, 6, 7)          0           input_37[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_45 (LSTM)                   (None, 6, 330)        446160      masking_45[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_46 (Masking)             (None, 6, 330)        0           lstm_45[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_46 (LSTM)                   (None, 6, 330)        872520      masking_46[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_47 (Masking)             (None, 6, 330)        0           lstm_46[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_47 (LSTM)                   (None, 330)           872520      masking_47[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_38 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_71 (Dropout)             (None, 330)           0           lstm_47[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_72 (Dropout)             (None, 9)             0           input_38[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_19 (Merge)                 (None, 339)           0           dropout_71[0][0]                 \n",
      "                                                                   dropout_72[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_53 (Dense)                 (None, 452)           153680      merge_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)             (None, 452)           0           dense_53[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_54 (Dense)                 (None, 226)           102378      dropout_73[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_74 (Dropout)             (None, 226)           0           dense_54[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_55 (Dense)                 (None, 1)             227         dropout_74[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,447,485\n",
      "Trainable params: 2,447,485\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3572 - acc: 0.8421Epoch 00000: val_acc improved from -inf to 0.85438, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 157s - loss: 0.3572 - acc: 0.8421 - val_loss: 0.3341 - val_acc: 0.8544\n",
      "Epoch 2/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.8500Epoch 00001: val_acc improved from 0.85438 to 0.85610, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 149s - loss: 0.3405 - acc: 0.8500 - val_loss: 0.3288 - val_acc: 0.8561\n",
      "Epoch 3/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8520Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 148s - loss: 0.3364 - acc: 0.8520 - val_loss: 0.3313 - val_acc: 0.8557\n",
      "Epoch 4/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8528Epoch 00003: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3347 - acc: 0.8528 - val_loss: 0.3295 - val_acc: 0.8561\n",
      "Epoch 5/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3335 - acc: 0.8534Epoch 00004: val_acc improved from 0.85610 to 0.85647, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 148s - loss: 0.3335 - acc: 0.8534 - val_loss: 0.3279 - val_acc: 0.8565\n",
      "Epoch 6/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.8538Epoch 00005: val_acc improved from 0.85647 to 0.85794, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 147s - loss: 0.3322 - acc: 0.8538 - val_loss: 0.3249 - val_acc: 0.8579\n",
      "Epoch 7/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3313 - acc: 0.8545Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3313 - acc: 0.8544 - val_loss: 0.3257 - val_acc: 0.8575\n",
      "Epoch 8/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3307 - acc: 0.8544Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3307 - acc: 0.8544 - val_loss: 0.3253 - val_acc: 0.8573\n",
      "Epoch 9/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.8548Epoch 00008: val_acc improved from 0.85794 to 0.85833, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 148s - loss: 0.3303 - acc: 0.8548 - val_loss: 0.3247 - val_acc: 0.8583\n",
      "Epoch 10/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.8553Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 149s - loss: 0.3291 - acc: 0.8553 - val_loss: 0.3239 - val_acc: 0.8580\n",
      "Epoch 11/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.8554Epoch 00010: val_acc improved from 0.85833 to 0.85874, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 147s - loss: 0.3285 - acc: 0.8554 - val_loss: 0.3234 - val_acc: 0.8587\n",
      "Epoch 12/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3285 - acc: 0.8553Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 148s - loss: 0.3285 - acc: 0.8553 - val_loss: 0.3258 - val_acc: 0.8570\n",
      "Epoch 13/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8555Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3282 - acc: 0.8555 - val_loss: 0.3252 - val_acc: 0.8574\n",
      "Epoch 14/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8558Epoch 00013: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565732/565732 [==============================] - 148s - loss: 0.3275 - acc: 0.8558 - val_loss: 0.3240 - val_acc: 0.8580\n",
      "Epoch 15/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8554Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3276 - acc: 0.8554 - val_loss: 0.3231 - val_acc: 0.8583\n",
      "Epoch 16/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3271 - acc: 0.8560Epoch 00015: val_acc did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 5.415501073e-05.\n",
      "565732/565732 [==============================] - 150s - loss: 0.3271 - acc: 0.8560 - val_loss: 0.3234 - val_acc: 0.8582\n",
      "Epoch 17/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8570Epoch 00016: val_acc improved from 0.85874 to 0.85929, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 148s - loss: 0.3246 - acc: 0.8570 - val_loss: 0.3214 - val_acc: 0.8593\n",
      "Epoch 18/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3238 - acc: 0.8572Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3238 - acc: 0.8572 - val_loss: 0.3221 - val_acc: 0.8588\n",
      "Epoch 19/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3238 - acc: 0.8573Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3238 - acc: 0.8573 - val_loss: 0.3223 - val_acc: 0.8584\n",
      "Epoch 20/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3239 - acc: 0.8575Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3239 - acc: 0.8575 - val_loss: 0.3217 - val_acc: 0.8588\n",
      "Epoch 21/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8578Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 148s - loss: 0.3235 - acc: 0.8578 - val_loss: 0.3215 - val_acc: 0.8590\n",
      "Epoch 22/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8578Epoch 00021: val_acc improved from 0.85929 to 0.85964, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 147s - loss: 0.3233 - acc: 0.8578 - val_loss: 0.3212 - val_acc: 0.8596\n",
      "Epoch 23/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8575Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 148s - loss: 0.3234 - acc: 0.8575 - val_loss: 0.3213 - val_acc: 0.8592\n",
      "Epoch 24/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8578Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3233 - acc: 0.8578 - val_loss: 0.3210 - val_acc: 0.8596\n",
      "Epoch 25/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8576Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 148s - loss: 0.3231 - acc: 0.8576 - val_loss: 0.3213 - val_acc: 0.8590\n",
      "Epoch 26/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.8576Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 148s - loss: 0.3229 - acc: 0.8576 - val_loss: 0.3225 - val_acc: 0.8585\n",
      "Epoch 27/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3229 - acc: 0.8576Epoch 00026: val_acc did not improve\n",
      "\n",
      "Epoch 00026: reducing learning rate to 1.0831002146e-05.\n",
      "565732/565732 [==============================] - 149s - loss: 0.3229 - acc: 0.8576 - val_loss: 0.3220 - val_acc: 0.8590\n",
      "Epoch 28/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8580Epoch 00027: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3222 - acc: 0.8580 - val_loss: 0.3218 - val_acc: 0.8589\n",
      "Epoch 29/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8577Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 148s - loss: 0.3223 - acc: 0.8577 - val_loss: 0.3212 - val_acc: 0.8593\n",
      "Epoch 30/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8581Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3221 - acc: 0.8581 - val_loss: 0.3213 - val_acc: 0.8591\n",
      "Epoch 31/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8580Epoch 00030: val_acc did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 2.16620046558e-06.\n",
      "565732/565732 [==============================] - 148s - loss: 0.3220 - acc: 0.8579 - val_loss: 0.3212 - val_acc: 0.8592\n",
      "Epoch 32/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8583Epoch 00031: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3219 - acc: 0.8583 - val_loss: 0.3216 - val_acc: 0.8588\n",
      "Epoch 33/100\n",
      "565656/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8580Epoch 00032: val_acc did not improve\n",
      "565732/565732 [==============================] - 147s - loss: 0.3221 - acc: 0.8580 - val_loss: 0.3215 - val_acc: 0.8590\n",
      "Epoch 00032: early stopping\n",
      "Average best_acc across k-fold: 0.859417481617\n",
      "New configuration: {'hidden_layers': 2, 'dropout_g': 0.59098338213488699, 'initial_nodes': 169, 'dropout': 0.044198629560733914, 'gru_layers': 2, 'batch_size': 510, 'gru_size': 480, 'learning_rate': 0.0034692491808253506}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_39 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_48 (Masking)             (None, 6, 7)          0           input_39[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_48 (LSTM)                   (None, 6, 480)        936960      masking_48[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_49 (Masking)             (None, 6, 480)        0           lstm_48[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_49 (LSTM)                   (None, 480)           1845120     masking_49[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_40 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_75 (Dropout)             (None, 480)           0           lstm_49[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_76 (Dropout)             (None, 9)             0           input_40[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_20 (Merge)                 (None, 489)           0           dropout_75[0][0]                 \n",
      "                                                                   dropout_76[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_56 (Dense)                 (None, 169)           82810       merge_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_77 (Dropout)             (None, 169)           0           dense_56[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_57 (Dense)                 (None, 84)            14280       dropout_77[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_78 (Dropout)             (None, 84)            0           dense_57[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_58 (Dense)                 (None, 1)             85          dropout_78[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,879,255\n",
      "Trainable params: 2,879,255\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3505 - acc: 0.8457Epoch 00000: val_acc improved from -inf to 0.85610, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 117s - loss: 0.3504 - acc: 0.8457 - val_loss: 0.3286 - val_acc: 0.8561\n",
      "Epoch 2/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.8526Epoch 00001: val_acc improved from 0.85610 to 0.85725, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3364 - acc: 0.8526 - val_loss: 0.3276 - val_acc: 0.8573\n",
      "Epoch 3/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3336 - acc: 0.8539Epoch 00002: val_acc improved from 0.85725 to 0.85842, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3336 - acc: 0.8539 - val_loss: 0.3248 - val_acc: 0.8584\n",
      "Epoch 4/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3316 - acc: 0.8550Epoch 00003: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3316 - acc: 0.8550 - val_loss: 0.3252 - val_acc: 0.8571\n",
      "Epoch 5/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3302 - acc: 0.8557Epoch 00004: val_acc improved from 0.85842 to 0.85917, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3303 - acc: 0.8557 - val_loss: 0.3234 - val_acc: 0.8592\n",
      "Epoch 6/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.8558Epoch 00005: val_acc improved from 0.85917 to 0.85990, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3294 - acc: 0.8558 - val_loss: 0.3208 - val_acc: 0.8599\n",
      "Epoch 7/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3283 - acc: 0.8566Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3283 - acc: 0.8566 - val_loss: 0.3229 - val_acc: 0.8595\n",
      "Epoch 8/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8570Epoch 00007: val_acc improved from 0.85990 to 0.86010, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3275 - acc: 0.8570 - val_loss: 0.3201 - val_acc: 0.8601\n",
      "Epoch 9/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8568Epoch 00008: val_acc improved from 0.86010 to 0.86045, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3274 - acc: 0.8568 - val_loss: 0.3202 - val_acc: 0.8605\n",
      "Epoch 10/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8571Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3264 - acc: 0.8571 - val_loss: 0.3207 - val_acc: 0.8600\n",
      "Epoch 11/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8572Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3256 - acc: 0.8572 - val_loss: 0.3211 - val_acc: 0.8600\n",
      "Epoch 12/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8575Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3250 - acc: 0.8575 - val_loss: 0.3200 - val_acc: 0.8598\n",
      "Epoch 13/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3248 - acc: 0.8580Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3248 - acc: 0.8580 - val_loss: 0.3206 - val_acc: 0.8599\n",
      "Epoch 14/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8579Epoch 00013: val_acc did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.000693849846721.\n",
      "565732/565732 [==============================] - 112s - loss: 0.3247 - acc: 0.8579 - val_loss: 0.3211 - val_acc: 0.8599\n",
      "Epoch 15/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.8595Epoch 00014: val_acc improved from 0.86045 to 0.86141, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3201 - acc: 0.8595 - val_loss: 0.3175 - val_acc: 0.8614\n",
      "Epoch 16/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3188 - acc: 0.8601Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3188 - acc: 0.8601 - val_loss: 0.3181 - val_acc: 0.8608\n",
      "Epoch 17/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.8604Epoch 00016: val_acc improved from 0.86141 to 0.86158, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3184 - acc: 0.8604 - val_loss: 0.3171 - val_acc: 0.8616\n",
      "Epoch 18/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.8605Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3176 - acc: 0.8605 - val_loss: 0.3180 - val_acc: 0.8612\n",
      "Epoch 19/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.8606Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3176 - acc: 0.8606 - val_loss: 0.3174 - val_acc: 0.8616\n",
      "Epoch 20/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.8607Epoch 00019: val_acc improved from 0.86158 to 0.86172, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3173 - acc: 0.8607 - val_loss: 0.3177 - val_acc: 0.8617\n",
      "Epoch 21/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.8610Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3165 - acc: 0.8610 - val_loss: 0.3188 - val_acc: 0.8611\n",
      "Epoch 22/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3165 - acc: 0.8609Epoch 00021: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3165 - acc: 0.8609 - val_loss: 0.3190 - val_acc: 0.8614\n",
      "Epoch 23/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3163 - acc: 0.8609Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3163 - acc: 0.8609 - val_loss: 0.3195 - val_acc: 0.8616\n",
      "Epoch 24/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.8616Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3157 - acc: 0.8616 - val_loss: 0.3193 - val_acc: 0.8613\n",
      "Epoch 25/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3154 - acc: 0.8611Epoch 00024: val_acc did not improve\n",
      "\n",
      "Epoch 00024: reducing learning rate to 0.000138769974001.\n",
      "565732/565732 [==============================] - 109s - loss: 0.3154 - acc: 0.8611 - val_loss: 0.3193 - val_acc: 0.8615\n",
      "Epoch 26/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.8621Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3141 - acc: 0.8621 - val_loss: 0.3197 - val_acc: 0.8616\n",
      "Epoch 27/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.8619Epoch 00026: val_acc improved from 0.86172 to 0.86175, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3139 - acc: 0.8619 - val_loss: 0.3196 - val_acc: 0.8618\n",
      "Epoch 28/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3139 - acc: 0.8620Epoch 00027: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3139 - acc: 0.8620 - val_loss: 0.3201 - val_acc: 0.8616\n",
      "Epoch 29/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.8620Epoch 00028: val_acc did not improve\n",
      "\n",
      "Epoch 00028: reducing learning rate to 2.77539948002e-05.\n",
      "565732/565732 [==============================] - 109s - loss: 0.3137 - acc: 0.8621 - val_loss: 0.3203 - val_acc: 0.8617\n",
      "Epoch 30/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.8622Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 110s - loss: 0.3131 - acc: 0.8622 - val_loss: 0.3204 - val_acc: 0.8615\n",
      "Epoch 31/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.8621Epoch 00030: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3131 - acc: 0.8621 - val_loss: 0.3205 - val_acc: 0.8616\n",
      "Epoch 32/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.8622Epoch 00031: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3130 - acc: 0.8622 - val_loss: 0.3206 - val_acc: 0.8616\n",
      "Epoch 33/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.8626Epoch 00032: val_acc did not improve\n",
      "\n",
      "Epoch 00032: reducing learning rate to 5.55079896003e-06.\n",
      "565732/565732 [==============================] - 109s - loss: 0.3130 - acc: 0.8626 - val_loss: 0.3207 - val_acc: 0.8616\n",
      "Epoch 34/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.8622Epoch 00033: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3130 - acc: 0.8622 - val_loss: 0.3206 - val_acc: 0.8616\n",
      "Epoch 35/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3132 - acc: 0.8621Epoch 00034: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3132 - acc: 0.8621 - val_loss: 0.3206 - val_acc: 0.8616\n",
      "Epoch 36/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.8623Epoch 00035: val_acc did not improve\n",
      "565732/565732 [==============================] - 110s - loss: 0.3130 - acc: 0.8623 - val_loss: 0.3206 - val_acc: 0.8616\n",
      "Epoch 37/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3129 - acc: 0.8626Epoch 00036: val_acc did not improve\n",
      "\n",
      "Epoch 00036: reducing learning rate to 1.11015979201e-06.\n",
      "565732/565732 [==============================] - 110s - loss: 0.3129 - acc: 0.8626 - val_loss: 0.3207 - val_acc: 0.8616\n",
      "Epoch 38/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3131 - acc: 0.8619Epoch 00037: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3131 - acc: 0.8619 - val_loss: 0.3207 - val_acc: 0.8616\n",
      "Epoch 00037: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_41 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_50 (Masking)             (None, 6, 7)          0           input_41[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_50 (LSTM)                   (None, 6, 480)        936960      masking_50[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_51 (Masking)             (None, 6, 480)        0           lstm_50[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_51 (LSTM)                   (None, 480)           1845120     masking_51[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_42 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_79 (Dropout)             (None, 480)           0           lstm_51[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_80 (Dropout)             (None, 9)             0           input_42[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_21 (Merge)                 (None, 489)           0           dropout_79[0][0]                 \n",
      "                                                                   dropout_80[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_59 (Dense)                 (None, 169)           82810       merge_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_81 (Dropout)             (None, 169)           0           dense_59[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_60 (Dense)                 (None, 84)            14280       dropout_81[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_82 (Dropout)             (None, 84)            0           dense_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_61 (Dense)                 (None, 1)             85          dropout_82[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,879,255\n",
      "Trainable params: 2,879,255\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3474 - acc: 0.8473Epoch 00000: val_acc improved from -inf to 0.85270, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 117s - loss: 0.3474 - acc: 0.8473 - val_loss: 0.3344 - val_acc: 0.8527\n",
      "Epoch 2/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3345 - acc: 0.8539Epoch 00001: val_acc improved from 0.85270 to 0.85555, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3345 - acc: 0.8539 - val_loss: 0.3282 - val_acc: 0.8556\n",
      "Epoch 3/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8555Epoch 00002: val_acc improved from 0.85555 to 0.85586, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3315 - acc: 0.8555 - val_loss: 0.3285 - val_acc: 0.8559\n",
      "Epoch 4/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8559Epoch 00003: val_acc improved from 0.85586 to 0.85743, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3299 - acc: 0.8559 - val_loss: 0.3270 - val_acc: 0.8574\n",
      "Epoch 5/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.8562Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3288 - acc: 0.8562 - val_loss: 0.3251 - val_acc: 0.8574\n",
      "Epoch 6/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3273 - acc: 0.8572Epoch 00005: val_acc improved from 0.85743 to 0.85851, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3273 - acc: 0.8572 - val_loss: 0.3236 - val_acc: 0.8585\n",
      "Epoch 7/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8574Epoch 00006: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565732/565732 [==============================] - 108s - loss: 0.3267 - acc: 0.8574 - val_loss: 0.3237 - val_acc: 0.8578\n",
      "Epoch 8/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8571Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3263 - acc: 0.8571 - val_loss: 0.3223 - val_acc: 0.8581\n",
      "Epoch 9/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8578Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3257 - acc: 0.8578 - val_loss: 0.3228 - val_acc: 0.8581\n",
      "Epoch 10/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8580Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3250 - acc: 0.8580 - val_loss: 0.3253 - val_acc: 0.8573\n",
      "Epoch 11/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8582Epoch 00010: val_acc improved from 0.85851 to 0.85944, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3246 - acc: 0.8582 - val_loss: 0.3215 - val_acc: 0.8594\n",
      "Epoch 12/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8585Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3241 - acc: 0.8585 - val_loss: 0.3220 - val_acc: 0.8586\n",
      "Epoch 13/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.8586Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3236 - acc: 0.8586 - val_loss: 0.3220 - val_acc: 0.8585\n",
      "Epoch 14/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3230 - acc: 0.8587Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3230 - acc: 0.8587 - val_loss: 0.3222 - val_acc: 0.8578\n",
      "Epoch 15/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.8586Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3224 - acc: 0.8586 - val_loss: 0.3230 - val_acc: 0.8585\n",
      "Epoch 16/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8590Epoch 00015: val_acc did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.000693849846721.\n",
      "565732/565732 [==============================] - 111s - loss: 0.3219 - acc: 0.8590 - val_loss: 0.3228 - val_acc: 0.8592\n",
      "Epoch 17/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8607Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3175 - acc: 0.8607 - val_loss: 0.3207 - val_acc: 0.8593\n",
      "Epoch 18/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3166 - acc: 0.8612Epoch 00017: val_acc improved from 0.85944 to 0.86007, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3166 - acc: 0.8612 - val_loss: 0.3193 - val_acc: 0.8601\n",
      "Epoch 19/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3161 - acc: 0.8614Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3161 - acc: 0.8614 - val_loss: 0.3193 - val_acc: 0.8600\n",
      "Epoch 20/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8615Epoch 00019: val_acc improved from 0.86007 to 0.86025, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3158 - acc: 0.8615 - val_loss: 0.3203 - val_acc: 0.8602\n",
      "Epoch 21/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3153 - acc: 0.8616Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 110s - loss: 0.3152 - acc: 0.8616 - val_loss: 0.3209 - val_acc: 0.8598\n",
      "Epoch 22/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3148 - acc: 0.8616Epoch 00021: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3148 - acc: 0.8616 - val_loss: 0.3217 - val_acc: 0.8599\n",
      "Epoch 23/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3145 - acc: 0.8618Epoch 00022: val_acc improved from 0.86025 to 0.86032, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 109s - loss: 0.3145 - acc: 0.8618 - val_loss: 0.3210 - val_acc: 0.8603\n",
      "Epoch 24/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3141 - acc: 0.8619Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3141 - acc: 0.8619 - val_loss: 0.3219 - val_acc: 0.8600\n",
      "Epoch 25/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3138 - acc: 0.8621Epoch 00024: val_acc did not improve\n",
      "\n",
      "Epoch 00024: reducing learning rate to 0.000138769974001.\n",
      "565732/565732 [==============================] - 109s - loss: 0.3138 - acc: 0.8621 - val_loss: 0.3222 - val_acc: 0.8600\n",
      "Epoch 26/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.8625Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3126 - acc: 0.8625 - val_loss: 0.3214 - val_acc: 0.8602\n",
      "Epoch 27/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8629Epoch 00026: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3121 - acc: 0.8629 - val_loss: 0.3217 - val_acc: 0.8601\n",
      "Epoch 28/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8627Epoch 00027: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3121 - acc: 0.8627 - val_loss: 0.3222 - val_acc: 0.8602\n",
      "Epoch 29/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.8628Epoch 00028: val_acc did not improve\n",
      "\n",
      "Epoch 00028: reducing learning rate to 2.77539948002e-05.\n",
      "565732/565732 [==============================] - 108s - loss: 0.3120 - acc: 0.8628 - val_loss: 0.3226 - val_acc: 0.8601\n",
      "Epoch 30/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.8632Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3118 - acc: 0.8632 - val_loss: 0.3223 - val_acc: 0.8601\n",
      "Epoch 31/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3113 - acc: 0.8631Epoch 00030: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3113 - acc: 0.8632 - val_loss: 0.3224 - val_acc: 0.8600\n",
      "Epoch 32/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3118 - acc: 0.8630Epoch 00031: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3118 - acc: 0.8630 - val_loss: 0.3224 - val_acc: 0.8600\n",
      "Epoch 33/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3115 - acc: 0.8629Epoch 00032: val_acc did not improve\n",
      "\n",
      "Epoch 00032: reducing learning rate to 5.55079896003e-06.\n",
      "565732/565732 [==============================] - 109s - loss: 0.3116 - acc: 0.8629 - val_loss: 0.3226 - val_acc: 0.8601\n",
      "Epoch 34/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3116 - acc: 0.8630Epoch 00033: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3115 - acc: 0.8630 - val_loss: 0.3225 - val_acc: 0.8601\n",
      "Epoch 00033: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_43 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_52 (Masking)             (None, 6, 7)          0           input_43[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_52 (LSTM)                   (None, 6, 480)        936960      masking_52[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_53 (Masking)             (None, 6, 480)        0           lstm_52[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_53 (LSTM)                   (None, 480)           1845120     masking_53[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_44 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)             (None, 480)           0           lstm_53[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)             (None, 9)             0           input_44[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_22 (Merge)                 (None, 489)           0           dropout_83[0][0]                 \n",
      "                                                                   dropout_84[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_62 (Dense)                 (None, 169)           82810       merge_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_85 (Dropout)             (None, 169)           0           dense_62[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_63 (Dense)                 (None, 84)            14280       dropout_85[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_86 (Dropout)             (None, 84)            0           dense_63[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_64 (Dense)                 (None, 1)             85          dropout_86[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 2,879,255\n",
      "Trainable params: 2,879,255\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.8457Epoch 00000: val_acc improved from -inf to 0.85492, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 116s - loss: 0.3497 - acc: 0.8457 - val_loss: 0.3310 - val_acc: 0.8549\n",
      "Epoch 2/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3357 - acc: 0.8530Epoch 00001: val_acc improved from 0.85492 to 0.85617, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3357 - acc: 0.8530 - val_loss: 0.3302 - val_acc: 0.8562\n",
      "Epoch 3/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8545Epoch 00002: val_acc improved from 0.85617 to 0.85678, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3329 - acc: 0.8545 - val_loss: 0.3255 - val_acc: 0.8568\n",
      "Epoch 4/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8556Epoch 00003: val_acc improved from 0.85678 to 0.85737, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3311 - acc: 0.8556 - val_loss: 0.3245 - val_acc: 0.8574\n",
      "Epoch 5/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8555Epoch 00004: val_acc improved from 0.85737 to 0.85805, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3298 - acc: 0.8555 - val_loss: 0.3244 - val_acc: 0.8580\n",
      "Epoch 6/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.8562Epoch 00005: val_acc improved from 0.85805 to 0.85926, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3288 - acc: 0.8562 - val_loss: 0.3220 - val_acc: 0.8593\n",
      "Epoch 7/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8562Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3277 - acc: 0.8562 - val_loss: 0.3263 - val_acc: 0.8573\n",
      "Epoch 8/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.8569Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3269 - acc: 0.8569 - val_loss: 0.3237 - val_acc: 0.8588\n",
      "Epoch 9/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8569Epoch 00008: val_acc improved from 0.85926 to 0.85971, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3263 - acc: 0.8569 - val_loss: 0.3222 - val_acc: 0.8597\n",
      "Epoch 10/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8576Epoch 00009: val_acc improved from 0.85971 to 0.86004, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3254 - acc: 0.8575 - val_loss: 0.3196 - val_acc: 0.8600\n",
      "Epoch 11/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8574Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3252 - acc: 0.8574 - val_loss: 0.3216 - val_acc: 0.8594\n",
      "Epoch 12/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8580Epoch 00011: val_acc improved from 0.86004 to 0.86047, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3247 - acc: 0.8580 - val_loss: 0.3197 - val_acc: 0.8605\n",
      "Epoch 13/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.8580Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3243 - acc: 0.8580 - val_loss: 0.3222 - val_acc: 0.8602\n",
      "Epoch 14/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8581Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3235 - acc: 0.8581 - val_loss: 0.3236 - val_acc: 0.8588\n",
      "Epoch 15/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3234 - acc: 0.8582Epoch 00014: val_acc improved from 0.86047 to 0.86064, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3234 - acc: 0.8582 - val_loss: 0.3198 - val_acc: 0.8606\n",
      "Epoch 16/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3231 - acc: 0.8580Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3231 - acc: 0.8580 - val_loss: 0.3203 - val_acc: 0.8604\n",
      "Epoch 17/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8587Epoch 00016: val_acc improved from 0.86064 to 0.86067, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3225 - acc: 0.8587 - val_loss: 0.3190 - val_acc: 0.8607\n",
      "Epoch 18/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8588Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3219 - acc: 0.8588 - val_loss: 0.3206 - val_acc: 0.8602\n",
      "Epoch 19/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8590Epoch 00018: val_acc improved from 0.86067 to 0.86068, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3216 - acc: 0.8589 - val_loss: 0.3197 - val_acc: 0.8607\n",
      "Epoch 20/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3212 - acc: 0.8591Epoch 00019: val_acc did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.000693849846721.\n",
      "565732/565732 [==============================] - 111s - loss: 0.3212 - acc: 0.8591 - val_loss: 0.3192 - val_acc: 0.8606\n",
      "Epoch 21/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.8609Epoch 00020: val_acc improved from 0.86068 to 0.86115, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3168 - acc: 0.8609 - val_loss: 0.3182 - val_acc: 0.8612\n",
      "Epoch 22/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.8611Epoch 00021: val_acc improved from 0.86115 to 0.86120, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3157 - acc: 0.8611 - val_loss: 0.3196 - val_acc: 0.8612\n",
      "Epoch 23/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.8617Epoch 00022: val_acc improved from 0.86120 to 0.86141, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3149 - acc: 0.8617 - val_loss: 0.3189 - val_acc: 0.8614\n",
      "Epoch 24/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.8616Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3147 - acc: 0.8616 - val_loss: 0.3193 - val_acc: 0.8613\n",
      "Epoch 25/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.8618Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3143 - acc: 0.8618 - val_loss: 0.3200 - val_acc: 0.8613\n",
      "Epoch 26/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3137 - acc: 0.8621Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3137 - acc: 0.8621 - val_loss: 0.3200 - val_acc: 0.8614\n",
      "Epoch 27/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.8623Epoch 00026: val_acc improved from 0.86141 to 0.86155, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 108s - loss: 0.3134 - acc: 0.8623 - val_loss: 0.3204 - val_acc: 0.8615\n",
      "Epoch 28/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3130 - acc: 0.8625Epoch 00027: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3129 - acc: 0.8625 - val_loss: 0.3208 - val_acc: 0.8612\n",
      "Epoch 29/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3125 - acc: 0.8625Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3126 - acc: 0.8625 - val_loss: 0.3207 - val_acc: 0.8612\n",
      "Epoch 30/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3127 - acc: 0.8621Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3126 - acc: 0.8621 - val_loss: 0.3219 - val_acc: 0.8611\n",
      "Epoch 31/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8628Epoch 00030: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3121 - acc: 0.8628 - val_loss: 0.3220 - val_acc: 0.8613\n",
      "Epoch 32/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3121 - acc: 0.8628Epoch 00031: val_acc did not improve\n",
      "\n",
      "Epoch 00031: reducing learning rate to 0.000138769974001.\n",
      "565732/565732 [==============================] - 108s - loss: 0.3121 - acc: 0.8628 - val_loss: 0.3220 - val_acc: 0.8613\n",
      "Epoch 33/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3104 - acc: 0.8636Epoch 00032: val_acc did not improve\n",
      "565732/565732 [==============================] - 109s - loss: 0.3105 - acc: 0.8636 - val_loss: 0.3222 - val_acc: 0.8613\n",
      "Epoch 34/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3101 - acc: 0.8637Epoch 00033: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3101 - acc: 0.8637 - val_loss: 0.3223 - val_acc: 0.8613\n",
      "Epoch 35/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3103 - acc: 0.8635Epoch 00034: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3103 - acc: 0.8635 - val_loss: 0.3230 - val_acc: 0.8612\n",
      "Epoch 36/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3099 - acc: 0.8638Epoch 00035: val_acc did not improve\n",
      "\n",
      "Epoch 00035: reducing learning rate to 2.77539948002e-05.\n",
      "565732/565732 [==============================] - 108s - loss: 0.3099 - acc: 0.8638 - val_loss: 0.3226 - val_acc: 0.8613\n",
      "Epoch 37/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3094 - acc: 0.8638Epoch 00036: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3094 - acc: 0.8638 - val_loss: 0.3230 - val_acc: 0.8613\n",
      "Epoch 38/100\n",
      "565590/565732 [============================>.] - ETA: 0s - loss: 0.3098 - acc: 0.8638Epoch 00037: val_acc did not improve\n",
      "565732/565732 [==============================] - 108s - loss: 0.3098 - acc: 0.8638 - val_loss: 0.3230 - val_acc: 0.8613\n",
      "Epoch 00037: early stopping\n",
      "Average best_acc across k-fold: 0.861207612251\n",
      "New configuration: {'hidden_layers': 2, 'dropout_g': 0.6105295668105285, 'initial_nodes': 213, 'dropout': 0.43248077703668758, 'gru_layers': 2, 'batch_size': 406, 'gru_size': 176, 'learning_rate': 0.00018569788965728436}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_45 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_54 (Masking)             (None, 6, 7)          0           input_45[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_54 (LSTM)                   (None, 6, 176)        129536      masking_54[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_55 (Masking)             (None, 6, 176)        0           lstm_54[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_55 (LSTM)                   (None, 176)           248512      masking_55[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_46 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_87 (Dropout)             (None, 176)           0           lstm_55[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_88 (Dropout)             (None, 9)             0           input_46[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_23 (Merge)                 (None, 185)           0           dropout_87[0][0]                 \n",
      "                                                                   dropout_88[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_65 (Dense)                 (None, 213)           39618       merge_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_89 (Dropout)             (None, 213)           0           dense_65[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_66 (Dense)                 (None, 106)           22684       dropout_89[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_90 (Dropout)             (None, 106)           0           dense_66[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_67 (Dense)                 (None, 1)             107         dropout_90[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 440,457\n",
      "Trainable params: 440,457\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.4274 - acc: 0.8022Epoch 00000: val_acc improved from -inf to 0.84642, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 90s - loss: 0.4274 - acc: 0.8022 - val_loss: 0.3595 - val_acc: 0.8464\n",
      "Epoch 2/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3957 - acc: 0.8209Epoch 00001: val_acc improved from 0.84642 to 0.84759, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 81s - loss: 0.3957 - acc: 0.8209 - val_loss: 0.3575 - val_acc: 0.8476\n",
      "Epoch 3/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3916 - acc: 0.8224Epoch 00002: val_acc improved from 0.84759 to 0.84819, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 81s - loss: 0.3915 - acc: 0.8224 - val_loss: 0.3603 - val_acc: 0.8482\n",
      "Epoch 4/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3895 - acc: 0.8231Epoch 00003: val_acc improved from 0.84819 to 0.84927, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 81s - loss: 0.3895 - acc: 0.8231 - val_loss: 0.3573 - val_acc: 0.8493\n",
      "Epoch 5/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3878 - acc: 0.8247Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3878 - acc: 0.8247 - val_loss: 0.3588 - val_acc: 0.8482\n",
      "Epoch 6/100\n",
      "565152/565732 [============================>.] - ETA: 0s - loss: 0.3857 - acc: 0.8254Epoch 00005: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565732/565732 [==============================] - 81s - loss: 0.3857 - acc: 0.8254 - val_loss: 0.3645 - val_acc: 0.8446\n",
      "Epoch 7/100\n",
      "565152/565732 [============================>.] - ETA: 0s - loss: 0.3843 - acc: 0.8263Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3844 - acc: 0.8263 - val_loss: 0.3614 - val_acc: 0.8481\n",
      "Epoch 8/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3833 - acc: 0.8272Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3833 - acc: 0.8272 - val_loss: 0.3611 - val_acc: 0.8471\n",
      "Epoch 9/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3821 - acc: 0.8280Epoch 00008: val_acc did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 3.71395784896e-05.\n",
      "565732/565732 [==============================] - 84s - loss: 0.3821 - acc: 0.8280 - val_loss: 0.3648 - val_acc: 0.8441\n",
      "Epoch 10/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3807 - acc: 0.8288Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3807 - acc: 0.8288 - val_loss: 0.3626 - val_acc: 0.8449\n",
      "Epoch 11/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8288Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3798 - acc: 0.8288 - val_loss: 0.3620 - val_acc: 0.8454\n",
      "Epoch 12/100\n",
      "565152/565732 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8293Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3790 - acc: 0.8293 - val_loss: 0.3661 - val_acc: 0.8431\n",
      "Epoch 13/100\n",
      "565152/565732 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8291Epoch 00012: val_acc did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 7.42791598896e-06.\n",
      "565732/565732 [==============================] - 81s - loss: 0.3798 - acc: 0.8291 - val_loss: 0.3637 - val_acc: 0.8446\n",
      "Epoch 14/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8292Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3794 - acc: 0.8292 - val_loss: 0.3633 - val_acc: 0.8448\n",
      "Epoch 15/100\n",
      "565152/565732 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8296Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3789 - acc: 0.8296 - val_loss: 0.3637 - val_acc: 0.8440\n",
      "Epoch 00014: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_47 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_56 (Masking)             (None, 6, 7)          0           input_47[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_56 (LSTM)                   (None, 6, 176)        129536      masking_56[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_57 (Masking)             (None, 6, 176)        0           lstm_56[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_57 (LSTM)                   (None, 176)           248512      masking_57[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_48 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_91 (Dropout)             (None, 176)           0           lstm_57[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_92 (Dropout)             (None, 9)             0           input_48[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_24 (Merge)                 (None, 185)           0           dropout_91[0][0]                 \n",
      "                                                                   dropout_92[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_68 (Dense)                 (None, 213)           39618       merge_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_93 (Dropout)             (None, 213)           0           dense_68[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_69 (Dense)                 (None, 106)           22684       dropout_93[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_94 (Dropout)             (None, 106)           0           dense_69[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_70 (Dense)                 (None, 1)             107         dropout_94[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 440,457\n",
      "Trainable params: 440,457\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565152/565732 [============================>.] - ETA: 0s - loss: 0.4262 - acc: 0.8040Epoch 00000: val_acc improved from -inf to 0.84534, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 91s - loss: 0.4261 - acc: 0.8040 - val_loss: 0.3592 - val_acc: 0.8453\n",
      "Epoch 2/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3940 - acc: 0.8220Epoch 00001: val_acc improved from 0.84534 to 0.84632, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 81s - loss: 0.3940 - acc: 0.8220 - val_loss: 0.3592 - val_acc: 0.8463\n",
      "Epoch 3/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3907 - acc: 0.8233Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3907 - acc: 0.8233 - val_loss: 0.3614 - val_acc: 0.8455\n",
      "Epoch 4/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3877 - acc: 0.8250Epoch 00003: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3877 - acc: 0.8250 - val_loss: 0.3639 - val_acc: 0.8443\n",
      "Epoch 5/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8269Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 82s - loss: 0.3853 - acc: 0.8269 - val_loss: 0.3654 - val_acc: 0.8448\n",
      "Epoch 6/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3840 - acc: 0.8270Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 82s - loss: 0.3840 - acc: 0.8269 - val_loss: 0.3646 - val_acc: 0.8433\n",
      "Epoch 7/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3828 - acc: 0.8277Epoch 00006: val_acc did not improve\n",
      "\n",
      "Epoch 00006: reducing learning rate to 3.71395784896e-05.\n",
      "565732/565732 [==============================] - 86s - loss: 0.3828 - acc: 0.8277 - val_loss: 0.3654 - val_acc: 0.8438\n",
      "Epoch 8/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3809 - acc: 0.8288Epoch 00007: val_acc did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565732/565732 [==============================] - 82s - loss: 0.3809 - acc: 0.8288 - val_loss: 0.3653 - val_acc: 0.8436\n",
      "Epoch 9/100\n",
      "565152/565732 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8292Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 82s - loss: 0.3801 - acc: 0.8292 - val_loss: 0.3663 - val_acc: 0.8434\n",
      "Epoch 10/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.8295Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3801 - acc: 0.8295 - val_loss: 0.3649 - val_acc: 0.8448\n",
      "Epoch 11/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8293Epoch 00010: val_acc did not improve\n",
      "\n",
      "Epoch 00010: reducing learning rate to 7.42791598896e-06.\n",
      "565732/565732 [==============================] - 81s - loss: 0.3799 - acc: 0.8293 - val_loss: 0.3661 - val_acc: 0.8435\n",
      "Epoch 12/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8297Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 82s - loss: 0.3794 - acc: 0.8297 - val_loss: 0.3650 - val_acc: 0.8437\n",
      "Epoch 13/100\n",
      "565152/565732 [============================>.] - ETA: 0s - loss: 0.3793 - acc: 0.8291Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3793 - acc: 0.8291 - val_loss: 0.3648 - val_acc: 0.8439\n",
      "Epoch 00012: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_49 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_58 (Masking)             (None, 6, 7)          0           input_49[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_58 (LSTM)                   (None, 6, 176)        129536      masking_58[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_59 (Masking)             (None, 6, 176)        0           lstm_58[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_59 (LSTM)                   (None, 176)           248512      masking_59[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_50 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_95 (Dropout)             (None, 176)           0           lstm_59[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_96 (Dropout)             (None, 9)             0           input_50[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_25 (Merge)                 (None, 185)           0           dropout_95[0][0]                 \n",
      "                                                                   dropout_96[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_71 (Dense)                 (None, 213)           39618       merge_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_97 (Dropout)             (None, 213)           0           dense_71[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_72 (Dense)                 (None, 106)           22684       dropout_97[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_98 (Dropout)             (None, 106)           0           dense_72[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_73 (Dense)                 (None, 1)             107         dropout_98[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 440,457\n",
      "Trainable params: 440,457\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.4256 - acc: 0.8047Epoch 00000: val_acc improved from -inf to 0.84493, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 91s - loss: 0.4256 - acc: 0.8048 - val_loss: 0.3613 - val_acc: 0.8449\n",
      "Epoch 2/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3964 - acc: 0.8209Epoch 00001: val_acc improved from 0.84493 to 0.84686, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 81s - loss: 0.3964 - acc: 0.8209 - val_loss: 0.3616 - val_acc: 0.8469\n",
      "Epoch 3/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3916 - acc: 0.8223Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 82s - loss: 0.3916 - acc: 0.8223 - val_loss: 0.3654 - val_acc: 0.8464\n",
      "Epoch 4/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3899 - acc: 0.8236Epoch 00003: val_acc improved from 0.84686 to 0.84695, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 81s - loss: 0.3899 - acc: 0.8236 - val_loss: 0.3588 - val_acc: 0.8470\n",
      "Epoch 5/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3872 - acc: 0.8250Epoch 00004: val_acc improved from 0.84695 to 0.84703, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 81s - loss: 0.3871 - acc: 0.8250 - val_loss: 0.3584 - val_acc: 0.8470\n",
      "Epoch 6/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3859 - acc: 0.8256Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 82s - loss: 0.3859 - acc: 0.8256 - val_loss: 0.3655 - val_acc: 0.8449\n",
      "Epoch 7/100\n",
      "565152/565732 [============================>.] - ETA: 0s - loss: 0.3847 - acc: 0.8262Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 82s - loss: 0.3847 - acc: 0.8262 - val_loss: 0.3662 - val_acc: 0.8449\n",
      "Epoch 8/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3831 - acc: 0.8274- ETA: 0s - loss: 0.3830 - acc: 0.82Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3831 - acc: 0.8274 - val_loss: 0.3667 - val_acc: 0.8445\n",
      "Epoch 9/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8279Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3819 - acc: 0.8279 - val_loss: 0.3663 - val_acc: 0.8445\n",
      "Epoch 10/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3808 - acc: 0.8287Epoch 00009: val_acc did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 3.71395784896e-05.\n",
      "565732/565732 [==============================] - 85s - loss: 0.3808 - acc: 0.8287 - val_loss: 0.3622 - val_acc: 0.8466\n",
      "Epoch 11/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3800 - acc: 0.8290Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3800 - acc: 0.8290 - val_loss: 0.3625 - val_acc: 0.8454\n",
      "Epoch 12/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3790 - acc: 0.8299Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3790 - acc: 0.8299 - val_loss: 0.3637 - val_acc: 0.8453\n",
      "Epoch 13/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3787 - acc: 0.8296Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3787 - acc: 0.8296 - val_loss: 0.3644 - val_acc: 0.8449\n",
      "Epoch 14/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3784 - acc: 0.8299Epoch 00013: val_acc did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 7.42791598896e-06.\n",
      "565732/565732 [==============================] - 82s - loss: 0.3784 - acc: 0.8299 - val_loss: 0.3655 - val_acc: 0.8436\n",
      "Epoch 15/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3781 - acc: 0.8303Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 81s - loss: 0.3781 - acc: 0.8303 - val_loss: 0.3638 - val_acc: 0.8443\n",
      "Epoch 16/100\n",
      "565558/565732 [============================>.] - ETA: 0s - loss: 0.3786 - acc: 0.8303Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 82s - loss: 0.3786 - acc: 0.8303 - val_loss: 0.3646 - val_acc: 0.8439\n",
      "Epoch 00015: early stopping\n",
      "Average best_acc across k-fold: 0.84754265948\n",
      "New configuration: {'hidden_layers': 3, 'dropout_g': 0.87503105222470279, 'initial_nodes': 335, 'dropout': 0.022078755694777755, 'gru_layers': 2, 'batch_size': 277, 'gru_size': 340, 'learning_rate': 0.032566709888722553}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_51 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_60 (Masking)             (None, 6, 7)          0           input_51[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_60 (LSTM)                   (None, 6, 340)        473280      masking_60[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_61 (Masking)             (None, 6, 340)        0           lstm_60[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_61 (LSTM)                   (None, 340)           926160      masking_61[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_52 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_99 (Dropout)             (None, 340)           0           lstm_61[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_100 (Dropout)            (None, 9)             0           input_52[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_26 (Merge)                 (None, 349)           0           dropout_99[0][0]                 \n",
      "                                                                   dropout_100[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_74 (Dense)                 (None, 335)           117250      merge_26[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_101 (Dropout)            (None, 335)           0           dense_74[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_75 (Dense)                 (None, 167)           56112       dropout_101[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_102 (Dropout)            (None, 167)           0           dense_75[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_76 (Dense)                 (None, 83)            13944       dropout_102[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_103 (Dropout)            (None, 83)            0           dense_76[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_77 (Dense)                 (None, 1)             84          dropout_103[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 1,586,830\n",
      "Trainable params: 1,586,830\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.4106 - acc: 0.8210Epoch 00000: val_acc improved from -inf to 0.83652, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 140s - loss: 0.4106 - acc: 0.8210 - val_loss: 0.3840 - val_acc: 0.8365\n",
      "Epoch 2/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.4107 - acc: 0.8178Epoch 00001: val_acc did not improve\n",
      "565732/565732 [==============================] - 129s - loss: 0.4107 - acc: 0.8178 - val_loss: 0.3757 - val_acc: 0.8263\n",
      "Epoch 3/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.4364 - acc: 0.8069Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 0.4364 - acc: 0.8069 - val_loss: 0.4136 - val_acc: 0.8281\n",
      "Epoch 4/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.8035Epoch 00003: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 0.4502 - acc: 0.8035 - val_loss: 0.4139 - val_acc: 0.8274\n",
      "Epoch 5/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.5360 - acc: 0.7616Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 0.5360 - acc: 0.7616 - val_loss: 0.4836 - val_acc: 0.8124\n",
      "Epoch 6/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.5532 - acc: 0.7506Epoch 00005: val_acc did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.00651334226131.\n",
      "565732/565732 [==============================] - 133s - loss: 0.5532 - acc: 0.7506 - val_loss: 0.5199 - val_acc: 0.7750\n",
      "Epoch 7/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.5479 - acc: 0.7540Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 0.5480 - acc: 0.7540 - val_loss: 0.5134 - val_acc: 0.7802\n",
      "Epoch 8/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.5451 - acc: 0.7558Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 0.5451 - acc: 0.7558 - val_loss: 0.5106 - val_acc: 0.7917\n",
      "Epoch 9/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.5390 - acc: 0.7601Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 0.5390 - acc: 0.7601 - val_loss: 0.5014 - val_acc: 0.7881\n",
      "Epoch 10/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.5211 - acc: 0.7705Epoch 00009: val_acc did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.00130266845226.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565732/565732 [==============================] - 130s - loss: 0.5211 - acc: 0.7705 - val_loss: 0.4771 - val_acc: 0.8103\n",
      "Epoch 11/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.5113 - acc: 0.7808Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 0.5113 - acc: 0.7808 - val_loss: 0.4775 - val_acc: 0.8103\n",
      "Epoch 12/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 0.5089 - acc: 0.7819Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 0.5089 - acc: 0.7820 - val_loss: 0.4769 - val_acc: 0.8093\n",
      "Epoch 00011: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_53 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_62 (Masking)             (None, 6, 7)          0           input_53[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_62 (LSTM)                   (None, 6, 340)        473280      masking_62[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_63 (Masking)             (None, 6, 340)        0           lstm_62[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_63 (LSTM)                   (None, 340)           926160      masking_63[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_54 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_104 (Dropout)            (None, 340)           0           lstm_63[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_105 (Dropout)            (None, 9)             0           input_54[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_27 (Merge)                 (None, 349)           0           dropout_104[0][0]                \n",
      "                                                                   dropout_105[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_78 (Dense)                 (None, 335)           117250      merge_27[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_106 (Dropout)            (None, 335)           0           dense_78[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_79 (Dense)                 (None, 167)           56112       dropout_106[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_107 (Dropout)            (None, 167)           0           dense_79[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_80 (Dense)                 (None, 83)            13944       dropout_107[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_108 (Dropout)            (None, 83)            0           dense_80[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_81 (Dense)                 (None, 1)             84          dropout_108[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 1,586,830\n",
      "Trainable params: 1,586,830\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0517 - acc: 0.5001Epoch 00000: val_acc improved from -inf to 0.50000, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 141s - loss: 8.0517 - acc: 0.5001 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0588 - acc: 0.5000Epoch 00001: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0591 - acc: 0.5000Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0592 - acc: 0.5000Epoch 00003: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0591 - acc: 0.5000Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0589 - acc: 0.5000Epoch 00005: val_acc did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.00651334226131.\n",
      "565732/565732 [==============================] - 133s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0588 - acc: 0.5000Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0588 - acc: 0.5000Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0592 - acc: 0.5000Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0588 - acc: 0.5000Epoch 00009: val_acc did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.00130266845226.\n",
      "565732/565732 [==============================] - 130s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0590 - acc: 0.5000Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 130s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 12/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0590 - acc: 0.5000Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 131s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 00011: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_55 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_64 (Masking)             (None, 6, 7)          0           input_55[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_64 (LSTM)                   (None, 6, 340)        473280      masking_64[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_65 (Masking)             (None, 6, 340)        0           lstm_64[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_65 (LSTM)                   (None, 340)           926160      masking_65[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_56 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_109 (Dropout)            (None, 340)           0           lstm_65[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_110 (Dropout)            (None, 9)             0           input_56[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_28 (Merge)                 (None, 349)           0           dropout_109[0][0]                \n",
      "                                                                   dropout_110[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_82 (Dense)                 (None, 335)           117250      merge_28[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_111 (Dropout)            (None, 335)           0           dense_82[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_83 (Dense)                 (None, 167)           56112       dropout_111[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_112 (Dropout)            (None, 167)           0           dense_83[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_84 (Dense)                 (None, 83)            13944       dropout_112[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_113 (Dropout)            (None, 83)            0           dense_84[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_85 (Dense)                 (None, 1)             84          dropout_113[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 1,586,830\n",
      "Trainable params: 1,586,830\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0515 - acc: 0.5002Epoch 00000: val_acc improved from -inf to 0.50000, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 143s - loss: 8.0516 - acc: 0.5002 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0592 - acc: 0.5000Epoch 00001: val_acc improved from 0.50000 to 0.50000, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0588 - acc: 0.5000Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0589 - acc: 0.5000Epoch 00003: val_acc improved from 0.50000 to 0.50000, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0592 - acc: 0.5000Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0588 - acc: 0.5000Epoch 00005: val_acc did not improve\n",
      "\n",
      "Epoch 00005: reducing learning rate to 0.00651334226131.\n",
      "565732/565732 [==============================] - 132s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0593 - acc: 0.5000Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0590 - acc: 0.5000Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0588 - acc: 0.5000Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0590 - acc: 0.5000Epoch 00009: val_acc did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.00130266845226.\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0589 - acc: 0.5000Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 12/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0590 - acc: 0.5000Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 13/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0590 - acc: 0.5000Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 14/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0590 - acc: 0.5000Epoch 00013: val_acc did not improve\n",
      "\n",
      "Epoch 00013: reducing learning rate to 0.000260533695109.\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 15/100\n",
      "565634/565732 [============================>.] - ETA: 0s - loss: 8.0590 - acc: 0.5000Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 129s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 00014: early stopping\n",
      "Average best_acc across k-fold: 0.612174436855\n",
      "New configuration: {'hidden_layers': 1, 'dropout_g': 0.32990557293269618, 'initial_nodes': 231, 'dropout': 0.027789022213795277, 'gru_layers': 2, 'batch_size': 363, 'gru_size': 490, 'learning_rate': 0.00083863720107068597}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_57 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_66 (Masking)             (None, 6, 7)          0           input_57[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_66 (LSTM)                   (None, 6, 490)        976080      masking_66[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_67 (Masking)             (None, 6, 490)        0           lstm_66[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_67 (LSTM)                   (None, 490)           1922760     masking_67[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_58 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_114 (Dropout)            (None, 490)           0           lstm_67[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_115 (Dropout)            (None, 9)             0           input_58[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_29 (Merge)                 (None, 499)           0           dropout_114[0][0]                \n",
      "                                                                   dropout_115[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_86 (Dense)                 (None, 231)           115500      merge_29[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_116 (Dropout)            (None, 231)           0           dense_86[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_87 (Dense)                 (None, 1)             232         dropout_116[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 3,014,572\n",
      "Trainable params: 3,014,572\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8463Epoch 00000: val_acc improved from -inf to 0.85416, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 135s - loss: 0.3491 - acc: 0.8463 - val_loss: 0.3321 - val_acc: 0.8542\n",
      "Epoch 2/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8534Epoch 00001: val_acc improved from 0.85416 to 0.85589, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3342 - acc: 0.8534 - val_loss: 0.3287 - val_acc: 0.8559\n",
      "Epoch 3/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8550Epoch 00002: val_acc improved from 0.85589 to 0.85726, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3308 - acc: 0.8550 - val_loss: 0.3265 - val_acc: 0.8573\n",
      "Epoch 4/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3288 - acc: 0.8558Epoch 00003: val_acc improved from 0.85726 to 0.85750, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 126s - loss: 0.3288 - acc: 0.8558 - val_loss: 0.3249 - val_acc: 0.8575\n",
      "Epoch 5/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.8563Epoch 00004: val_acc improved from 0.85750 to 0.85842, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 126s - loss: 0.3278 - acc: 0.8563 - val_loss: 0.3233 - val_acc: 0.8584\n",
      "Epoch 6/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3267 - acc: 0.8566Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3267 - acc: 0.8566 - val_loss: 0.3232 - val_acc: 0.8578\n",
      "Epoch 7/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.8569Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3261 - acc: 0.8569 - val_loss: 0.3223 - val_acc: 0.8584\n",
      "Epoch 8/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8571Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3255 - acc: 0.8571 - val_loss: 0.3231 - val_acc: 0.8584\n",
      "Epoch 9/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8573Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3251 - acc: 0.8573 - val_loss: 0.3238 - val_acc: 0.8579\n",
      "Epoch 10/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3241 - acc: 0.8575- ETAEpoch 00009: val_acc did not improve\n",
      "\n",
      "Epoch 00009: reducing learning rate to 0.000167727435473.\n",
      "565732/565732 [==============================] - 128s - loss: 0.3241 - acc: 0.8575 - val_loss: 0.3216 - val_acc: 0.8584\n",
      "Epoch 11/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8587Epoch 00010: val_acc improved from 0.85842 to 0.85950, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3217 - acc: 0.8587 - val_loss: 0.3200 - val_acc: 0.8595\n",
      "Epoch 12/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3211 - acc: 0.8591Epoch 00011: val_acc improved from 0.85950 to 0.85962, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3210 - acc: 0.8591 - val_loss: 0.3198 - val_acc: 0.8596\n",
      "Epoch 13/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3208 - acc: 0.8590Epoch 00012: val_acc improved from 0.85962 to 0.85974, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3208 - acc: 0.8590 - val_loss: 0.3196 - val_acc: 0.8597\n",
      "Epoch 14/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3207 - acc: 0.8591Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3207 - acc: 0.8591 - val_loss: 0.3199 - val_acc: 0.8594\n",
      "Epoch 15/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3205 - acc: 0.8592Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3205 - acc: 0.8592 - val_loss: 0.3198 - val_acc: 0.8596\n",
      "Epoch 16/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3203 - acc: 0.8593Epoch 00015: val_acc improved from 0.85974 to 0.85988, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3203 - acc: 0.8593 - val_loss: 0.3196 - val_acc: 0.8599\n",
      "Epoch 17/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3201 - acc: 0.8595Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3201 - acc: 0.8596 - val_loss: 0.3201 - val_acc: 0.8597\n",
      "Epoch 18/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3200 - acc: 0.8594Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3200 - acc: 0.8594 - val_loss: 0.3196 - val_acc: 0.8596\n",
      "Epoch 19/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3200 - acc: 0.8594Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3200 - acc: 0.8594 - val_loss: 0.3195 - val_acc: 0.8597\n",
      "Epoch 20/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3196 - acc: 0.8597- ETA: 0s - loss: 0.3197 - acc: 0Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3196 - acc: 0.8597 - val_loss: 0.3196 - val_acc: 0.8599\n",
      "Epoch 21/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3197 - acc: 0.8595Epoch 00020: val_acc did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 3.35454882588e-05.\n",
      "565732/565732 [==============================] - 124s - loss: 0.3197 - acc: 0.8595 - val_loss: 0.3197 - val_acc: 0.8597\n",
      "Epoch 22/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3188 - acc: 0.8602Epoch 00021: val_acc improved from 0.85988 to 0.86004, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3188 - acc: 0.8602 - val_loss: 0.3193 - val_acc: 0.8600\n",
      "Epoch 23/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3188 - acc: 0.8600Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3188 - acc: 0.8600 - val_loss: 0.3194 - val_acc: 0.8599\n",
      "Epoch 24/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3188 - acc: 0.8599Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3188 - acc: 0.8599 - val_loss: 0.3194 - val_acc: 0.8600\n",
      "Epoch 25/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3186 - acc: 0.8600Epoch 00024: val_acc improved from 0.86004 to 0.86009, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3186 - acc: 0.8600 - val_loss: 0.3193 - val_acc: 0.8601\n",
      "Epoch 26/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3188 - acc: 0.8599Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3188 - acc: 0.8599 - val_loss: 0.3196 - val_acc: 0.8598\n",
      "Epoch 27/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3187 - acc: 0.8600Epoch 00026: val_acc did not improve\n",
      "\n",
      "Epoch 00026: reducing learning rate to 6.7090979428e-06.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3187 - acc: 0.8600 - val_loss: 0.3195 - val_acc: 0.8600\n",
      "Epoch 28/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.8601- ETAEpoch 00027: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3185 - acc: 0.8601 - val_loss: 0.3194 - val_acc: 0.8599\n",
      "Epoch 29/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.8603Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3184 - acc: 0.8603 - val_loss: 0.3194 - val_acc: 0.8598\n",
      "Epoch 30/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8604Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3181 - acc: 0.8604 - val_loss: 0.3194 - val_acc: 0.8599\n",
      "Epoch 31/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.8602Epoch 00030: val_acc did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 1.34181955218e-06.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565732/565732 [==============================] - 124s - loss: 0.3182 - acc: 0.8602 - val_loss: 0.3195 - val_acc: 0.8599\n",
      "Epoch 32/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.8601Epoch 00031: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3183 - acc: 0.8601 - val_loss: 0.3194 - val_acc: 0.8599\n",
      "Epoch 33/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.8605Epoch 00032: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3183 - acc: 0.8605 - val_loss: 0.3194 - val_acc: 0.8599\n",
      "Epoch 34/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3184 - acc: 0.8600- ETA: 1s - loss: 0.Epoch 00033: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3184 - acc: 0.8600 - val_loss: 0.3194 - val_acc: 0.8599\n",
      "Epoch 35/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3183 - acc: 0.8602Epoch 00034: val_acc did not improve\n",
      "\n",
      "Epoch 00034: reducing learning rate to 2.68363919531e-07.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3183 - acc: 0.8602 - val_loss: 0.3194 - val_acc: 0.8599\n",
      "Epoch 36/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8601Epoch 00035: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3181 - acc: 0.8601 - val_loss: 0.3194 - val_acc: 0.8599\n",
      "Epoch 00035: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_59 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_68 (Masking)             (None, 6, 7)          0           input_59[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_68 (LSTM)                   (None, 6, 490)        976080      masking_68[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_69 (Masking)             (None, 6, 490)        0           lstm_68[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_69 (LSTM)                   (None, 490)           1922760     masking_69[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_60 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_117 (Dropout)            (None, 490)           0           lstm_69[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_118 (Dropout)            (None, 9)             0           input_60[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_30 (Merge)                 (None, 499)           0           dropout_117[0][0]                \n",
      "                                                                   dropout_118[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_88 (Dense)                 (None, 231)           115500      merge_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_119 (Dropout)            (None, 231)           0           dense_88[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_89 (Dense)                 (None, 1)             232         dropout_119[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 3,014,572\n",
      "Trainable params: 3,014,572\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3491 - acc: 0.8463Epoch 00000: val_acc improved from -inf to 0.85390, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 136s - loss: 0.3491 - acc: 0.8463 - val_loss: 0.3335 - val_acc: 0.8539\n",
      "Epoch 2/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.8533Epoch 00001: val_acc improved from 0.85390 to 0.85453, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3337 - acc: 0.8533 - val_loss: 0.3317 - val_acc: 0.8545\n",
      "Epoch 3/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3304 - acc: 0.8554Epoch 00002: val_acc improved from 0.85453 to 0.85605, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3304 - acc: 0.8554 - val_loss: 0.3275 - val_acc: 0.8560\n",
      "Epoch 4/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8563Epoch 00003: val_acc improved from 0.85605 to 0.85697, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3282 - acc: 0.8563 - val_loss: 0.3260 - val_acc: 0.8570\n",
      "Epoch 5/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3274 - acc: 0.8566Epoch 00004: val_acc improved from 0.85697 to 0.85766, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3274 - acc: 0.8566 - val_loss: 0.3247 - val_acc: 0.8577\n",
      "Epoch 6/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.8570Epoch 00005: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3265 - acc: 0.8570 - val_loss: 0.3250 - val_acc: 0.8576\n",
      "Epoch 7/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8572Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3255 - acc: 0.8572 - val_loss: 0.3245 - val_acc: 0.8573\n",
      "Epoch 8/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3249 - acc: 0.8578Epoch 00007: val_acc improved from 0.85766 to 0.85796, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3249 - acc: 0.8578 - val_loss: 0.3236 - val_acc: 0.8580\n",
      "Epoch 9/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.8577Epoch 00008: val_acc improved from 0.85796 to 0.85855, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 126s - loss: 0.3245 - acc: 0.8577 - val_loss: 0.3225 - val_acc: 0.8586\n",
      "Epoch 10/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3239 - acc: 0.8579Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3239 - acc: 0.8579 - val_loss: 0.3241 - val_acc: 0.8579\n",
      "Epoch 11/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8582Epoch 00010: val_acc improved from 0.85855 to 0.85867, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 126s - loss: 0.3233 - acc: 0.8582 - val_loss: 0.3224 - val_acc: 0.8587\n",
      "Epoch 12/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8579Epoch 00011: val_acc improved from 0.85867 to 0.85887, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3233 - acc: 0.8579 - val_loss: 0.3223 - val_acc: 0.8589\n",
      "Epoch 13/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8587Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3226 - acc: 0.8587 - val_loss: 0.3237 - val_acc: 0.8580\n",
      "Epoch 14/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8588Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3225 - acc: 0.8588 - val_loss: 0.3219 - val_acc: 0.8587\n",
      "Epoch 15/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8591Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3219 - acc: 0.8590 - val_loss: 0.3230 - val_acc: 0.8582\n",
      "Epoch 16/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8588Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3218 - acc: 0.8588 - val_loss: 0.3233 - val_acc: 0.8580\n",
      "Epoch 17/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3214 - acc: 0.8590Epoch 00016: val_acc did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.000167727435473.\n",
      "565732/565732 [==============================] - 128s - loss: 0.3214 - acc: 0.8590 - val_loss: 0.3219 - val_acc: 0.8588\n",
      "Epoch 18/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3185 - acc: 0.8602Epoch 00017: val_acc improved from 0.85887 to 0.85942, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3185 - acc: 0.8602 - val_loss: 0.3204 - val_acc: 0.8594\n",
      "Epoch 19/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.8604Epoch 00018: val_acc improved from 0.85942 to 0.85962, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3180 - acc: 0.8604 - val_loss: 0.3202 - val_acc: 0.8596\n",
      "Epoch 20/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3178 - acc: 0.8606Epoch 00019: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3178 - acc: 0.8606 - val_loss: 0.3204 - val_acc: 0.8595\n",
      "Epoch 21/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.8607Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3176 - acc: 0.8607 - val_loss: 0.3206 - val_acc: 0.8594\n",
      "Epoch 22/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.8607Epoch 00021: val_acc improved from 0.85962 to 0.85986, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3173 - acc: 0.8607 - val_loss: 0.3204 - val_acc: 0.8599\n",
      "Epoch 23/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3171 - acc: 0.8610Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3171 - acc: 0.8610 - val_loss: 0.3210 - val_acc: 0.8594\n",
      "Epoch 24/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3169 - acc: 0.8610Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3169 - acc: 0.8610 - val_loss: 0.3205 - val_acc: 0.8597\n",
      "Epoch 25/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3167 - acc: 0.8609Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3167 - acc: 0.8609 - val_loss: 0.3206 - val_acc: 0.8597\n",
      "Epoch 26/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3164 - acc: 0.8612Epoch 00025: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3164 - acc: 0.8612 - val_loss: 0.3204 - val_acc: 0.8597\n",
      "Epoch 27/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.8612Epoch 00026: val_acc did not improve\n",
      "\n",
      "Epoch 00026: reducing learning rate to 3.35454882588e-05.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3162 - acc: 0.8612 - val_loss: 0.3211 - val_acc: 0.8597\n",
      "Epoch 28/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.8617Epoch 00027: val_acc did not improve\n",
      "565732/565732 [==============================] - 126s - loss: 0.3151 - acc: 0.8617 - val_loss: 0.3208 - val_acc: 0.8598\n",
      "Epoch 29/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3149 - acc: 0.8617Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3149 - acc: 0.8617 - val_loss: 0.3209 - val_acc: 0.8597\n",
      "Epoch 30/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3151 - acc: 0.8614Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3150 - acc: 0.8614 - val_loss: 0.3208 - val_acc: 0.8598\n",
      "Epoch 31/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3150 - acc: 0.8618Epoch 00030: val_acc did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 6.7090979428e-06.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3150 - acc: 0.8618 - val_loss: 0.3211 - val_acc: 0.8596\n",
      "Epoch 32/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3146 - acc: 0.8620Epoch 00031: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3146 - acc: 0.8620 - val_loss: 0.3211 - val_acc: 0.8597\n",
      "Epoch 33/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3147 - acc: 0.8621Epoch 00032: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3147 - acc: 0.8621 - val_loss: 0.3211 - val_acc: 0.8597\n",
      "Epoch 00032: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_61 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_70 (Masking)             (None, 6, 7)          0           input_61[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_70 (LSTM)                   (None, 6, 490)        976080      masking_70[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_71 (Masking)             (None, 6, 490)        0           lstm_70[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_71 (LSTM)                   (None, 490)           1922760     masking_71[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_62 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_120 (Dropout)            (None, 490)           0           lstm_71[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_121 (Dropout)            (None, 9)             0           input_62[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_31 (Merge)                 (None, 499)           0           dropout_120[0][0]                \n",
      "                                                                   dropout_121[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_90 (Dense)                 (None, 231)           115500      merge_31[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_122 (Dropout)            (None, 231)           0           dense_90[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_91 (Dense)                 (None, 1)             232         dropout_122[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 3,014,572\n",
      "Trainable params: 3,014,572\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3511 - acc: 0.8447Epoch 00000: val_acc improved from -inf to 0.85457, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 136s - loss: 0.3511 - acc: 0.8448 - val_loss: 0.3317 - val_acc: 0.8546\n",
      "Epoch 2/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.8529Epoch 00001: val_acc improved from 0.85457 to 0.85742, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3348 - acc: 0.8529 - val_loss: 0.3258 - val_acc: 0.8574\n",
      "Epoch 3/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3315 - acc: 0.8543Epoch 00002: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3315 - acc: 0.8543 - val_loss: 0.3282 - val_acc: 0.8560\n",
      "Epoch 4/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3297 - acc: 0.8555Epoch 00003: val_acc improved from 0.85742 to 0.85755, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3297 - acc: 0.8555 - val_loss: 0.3253 - val_acc: 0.8576\n",
      "Epoch 5/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8560Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3284 - acc: 0.8560 - val_loss: 0.3248 - val_acc: 0.8574\n",
      "Epoch 6/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8560Epoch 00005: val_acc improved from 0.85755 to 0.85882, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3277 - acc: 0.8560 - val_loss: 0.3229 - val_acc: 0.8588\n",
      "Epoch 7/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.8565Epoch 00006: val_acc improved from 0.85882 to 0.85921, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3268 - acc: 0.8565 - val_loss: 0.3214 - val_acc: 0.8592\n",
      "Epoch 8/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8570Epoch 00007: val_acc improved from 0.85921 to 0.85925, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3258 - acc: 0.8569 - val_loss: 0.3215 - val_acc: 0.8593\n",
      "Epoch 9/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8572Epoch 00008: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3256 - acc: 0.8572 - val_loss: 0.3214 - val_acc: 0.8592\n",
      "Epoch 10/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3252 - acc: 0.8574Epoch 00009: val_acc improved from 0.85925 to 0.85974, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3252 - acc: 0.8574 - val_loss: 0.3206 - val_acc: 0.8597\n",
      "Epoch 11/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.8575Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3245 - acc: 0.8575 - val_loss: 0.3208 - val_acc: 0.8596\n",
      "Epoch 12/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.8578Epoch 00011: val_acc improved from 0.85974 to 0.85994, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3242 - acc: 0.8578 - val_loss: 0.3201 - val_acc: 0.8599\n",
      "Epoch 13/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3236 - acc: 0.8579Epoch 00012: val_acc improved from 0.85994 to 0.86020, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3236 - acc: 0.8579 - val_loss: 0.3195 - val_acc: 0.8602\n",
      "Epoch 14/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8581Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3233 - acc: 0.8580 - val_loss: 0.3199 - val_acc: 0.8600\n",
      "Epoch 15/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3233 - acc: 0.8582Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3233 - acc: 0.8582 - val_loss: 0.3200 - val_acc: 0.8601\n",
      "Epoch 16/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8583Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3226 - acc: 0.8583 - val_loss: 0.3201 - val_acc: 0.8597\n",
      "Epoch 17/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8582Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3220 - acc: 0.8582 - val_loss: 0.3200 - val_acc: 0.8600\n",
      "Epoch 18/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8582Epoch 00017: val_acc did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 0.000167727435473.\n",
      "565732/565732 [==============================] - 129s - loss: 0.3219 - acc: 0.8582 - val_loss: 0.3204 - val_acc: 0.8602\n",
      "Epoch 19/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3191 - acc: 0.8599Epoch 00018: val_acc improved from 0.86020 to 0.86050, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3191 - acc: 0.8599 - val_loss: 0.3186 - val_acc: 0.8605\n",
      "Epoch 20/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.8599Epoch 00019: val_acc improved from 0.86050 to 0.86065, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 126s - loss: 0.3182 - acc: 0.8600 - val_loss: 0.3190 - val_acc: 0.8607\n",
      "Epoch 21/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.8599Epoch 00020: val_acc improved from 0.86065 to 0.86104, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3180 - acc: 0.8599 - val_loss: 0.3186 - val_acc: 0.8610\n",
      "Epoch 22/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3179 - acc: 0.8603Epoch 00021: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3179 - acc: 0.8603 - val_loss: 0.3194 - val_acc: 0.8606\n",
      "Epoch 23/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3176 - acc: 0.8605Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3176 - acc: 0.8605 - val_loss: 0.3200 - val_acc: 0.8605\n",
      "Epoch 24/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3174 - acc: 0.8608Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3174 - acc: 0.8608 - val_loss: 0.3193 - val_acc: 0.8608\n",
      "Epoch 25/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3175 - acc: 0.8606Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3175 - acc: 0.8606 - val_loss: 0.3201 - val_acc: 0.8605\n",
      "Epoch 26/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.8608Epoch 00025: val_acc did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 3.35454882588e-05.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3168 - acc: 0.8608 - val_loss: 0.3196 - val_acc: 0.8605\n",
      "Epoch 27/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3160 - acc: 0.8612Epoch 00026: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3160 - acc: 0.8612 - val_loss: 0.3202 - val_acc: 0.8605\n",
      "Epoch 28/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3158 - acc: 0.8611Epoch 00027: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3158 - acc: 0.8611 - val_loss: 0.3202 - val_acc: 0.8606\n",
      "Epoch 29/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3159 - acc: 0.8613Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3158 - acc: 0.8613 - val_loss: 0.3201 - val_acc: 0.8607\n",
      "Epoch 30/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.8615Epoch 00029: val_acc did not improve\n",
      "\n",
      "Epoch 00029: reducing learning rate to 6.7090979428e-06.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3156 - acc: 0.8615 - val_loss: 0.3205 - val_acc: 0.8607\n",
      "Epoch 31/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.8610Epoch 00030: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3156 - acc: 0.8610 - val_loss: 0.3204 - val_acc: 0.8608\n",
      "Epoch 32/100\n",
      "565554/565732 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.8612Epoch 00031: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3156 - acc: 0.8612 - val_loss: 0.3204 - val_acc: 0.8607\n",
      "Epoch 00031: early stopping\n",
      "Average best_acc across k-fold: 0.860329656275\n",
      "New configuration: {'hidden_layers': 2, 'dropout_g': 0.077396233812437196, 'initial_nodes': 22, 'dropout': 0.01, 'gru_layers': 3, 'batch_size': 262, 'gru_size': 11, 'learning_rate': 0.001021747599539526}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_63 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_72 (Masking)             (None, 6, 7)          0           input_63[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_72 (LSTM)                   (None, 6, 11)         836         masking_72[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_73 (Masking)             (None, 6, 11)         0           lstm_72[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_73 (LSTM)                   (None, 6, 11)         1012        masking_73[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_74 (Masking)             (None, 6, 11)         0           lstm_73[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_74 (LSTM)                   (None, 11)            1012        masking_74[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_64 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_123 (Dropout)            (None, 11)            0           lstm_74[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_124 (Dropout)            (None, 9)             0           input_64[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_32 (Merge)                 (None, 20)            0           dropout_123[0][0]                \n",
      "                                                                   dropout_124[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_92 (Dense)                 (None, 22)            462         merge_32[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_125 (Dropout)            (None, 22)            0           dense_92[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_93 (Dense)                 (None, 11)            253         dropout_125[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_126 (Dropout)            (None, 11)            0           dense_93[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_94 (Dense)                 (None, 1)             12          dropout_126[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 3,587\n",
      "Trainable params: 3,587\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3676 - acc: 0.8343Epoch 00000: val_acc improved from -inf to 0.85151, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 138s - loss: 0.3676 - acc: 0.8343 - val_loss: 0.3381 - val_acc: 0.8515\n",
      "Epoch 2/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8511Epoch 00001: val_acc improved from 0.85151 to 0.85491, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3390 - acc: 0.8511 - val_loss: 0.3315 - val_acc: 0.8549\n",
      "Epoch 3/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3344 - acc: 0.8532Epoch 00002: val_acc improved from 0.85491 to 0.85507, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 127s - loss: 0.3343 - acc: 0.8532 - val_loss: 0.3302 - val_acc: 0.8551\n",
      "Epoch 4/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.8541Epoch 00003: val_acc improved from 0.85507 to 0.85644, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 128s - loss: 0.3324 - acc: 0.8541 - val_loss: 0.3284 - val_acc: 0.8564\n",
      "Epoch 5/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3308 - acc: 0.8551Epoch 00004: val_acc improved from 0.85644 to 0.85677, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3308 - acc: 0.8551 - val_loss: 0.3271 - val_acc: 0.8568\n",
      "Epoch 6/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8556Epoch 00005: val_acc improved from 0.85677 to 0.85703, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3298 - acc: 0.8556 - val_loss: 0.3268 - val_acc: 0.8570\n",
      "Epoch 7/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3294 - acc: 0.8557Epoch 00006: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3294 - acc: 0.8557 - val_loss: 0.3269 - val_acc: 0.8568\n",
      "Epoch 8/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8560Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 126s - loss: 0.3289 - acc: 0.8560 - val_loss: 0.3266 - val_acc: 0.8568\n",
      "Epoch 9/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8560Epoch 00008: val_acc improved from 0.85703 to 0.85742, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 128s - loss: 0.3284 - acc: 0.8560 - val_loss: 0.3255 - val_acc: 0.8574\n",
      "Epoch 10/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3280 - acc: 0.8564Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 126s - loss: 0.3280 - acc: 0.8564 - val_loss: 0.3266 - val_acc: 0.8568\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3276 - acc: 0.8565Epoch 00010: val_acc improved from 0.85742 to 0.85781, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3276 - acc: 0.8565 - val_loss: 0.3246 - val_acc: 0.8578\n",
      "Epoch 12/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3275 - acc: 0.8564Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3275 - acc: 0.8564 - val_loss: 0.3248 - val_acc: 0.8578\n",
      "Epoch 13/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8566Epoch 00012: val_acc improved from 0.85781 to 0.85796, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3270 - acc: 0.8566 - val_loss: 0.3251 - val_acc: 0.8580\n",
      "Epoch 14/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.8568Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3268 - acc: 0.8568 - val_loss: 0.3248 - val_acc: 0.8577\n",
      "Epoch 15/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.8569Epoch 00014: val_acc improved from 0.85796 to 0.85800, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3268 - acc: 0.8569 - val_loss: 0.3244 - val_acc: 0.8580\n",
      "Epoch 16/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8570Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3264 - acc: 0.8570 - val_loss: 0.3240 - val_acc: 0.8580\n",
      "Epoch 17/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.8569Epoch 00016: val_acc improved from 0.85800 to 0.85846, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3265 - acc: 0.8569 - val_loss: 0.3237 - val_acc: 0.8585\n",
      "Epoch 18/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8571Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3262 - acc: 0.8571 - val_loss: 0.3241 - val_acc: 0.8580\n",
      "Epoch 19/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.8572Epoch 00018: val_acc improved from 0.85846 to 0.85854, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3259 - acc: 0.8572 - val_loss: 0.3232 - val_acc: 0.8585\n",
      "Epoch 20/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8571Epoch 00019: val_acc improved from 0.85854 to 0.85862, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3257 - acc: 0.8571 - val_loss: 0.3236 - val_acc: 0.8586\n",
      "Epoch 21/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8576Epoch 00020: val_acc did not improve\n",
      "565732/565732 [==============================] - 127s - loss: 0.3257 - acc: 0.8576 - val_loss: 0.3230 - val_acc: 0.8584\n",
      "Epoch 22/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8574Epoch 00021: val_acc improved from 0.85862 to 0.85877, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 128s - loss: 0.3255 - acc: 0.8574 - val_loss: 0.3232 - val_acc: 0.8588\n",
      "Epoch 23/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3254 - acc: 0.8574Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3254 - acc: 0.8574 - val_loss: 0.3230 - val_acc: 0.8587\n",
      "Epoch 24/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8576Epoch 00023: val_acc improved from 0.85877 to 0.85906, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3251 - acc: 0.8576 - val_loss: 0.3226 - val_acc: 0.8591\n",
      "Epoch 25/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8576Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3250 - acc: 0.8576 - val_loss: 0.3230 - val_acc: 0.8588\n",
      "Epoch 26/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.8575Epoch 00025: val_acc improved from 0.85906 to 0.85925, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3247 - acc: 0.8575 - val_loss: 0.3222 - val_acc: 0.8593\n",
      "Epoch 27/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8576Epoch 00026: val_acc improved from 0.85925 to 0.85933, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3246 - acc: 0.8576 - val_loss: 0.3220 - val_acc: 0.8593\n",
      "Epoch 28/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8577Epoch 00027: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3246 - acc: 0.8577 - val_loss: 0.3231 - val_acc: 0.8591\n",
      "Epoch 29/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8577Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3246 - acc: 0.8577 - val_loss: 0.3224 - val_acc: 0.8587\n",
      "Epoch 30/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.8578Epoch 00029: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3245 - acc: 0.8578 - val_loss: 0.3222 - val_acc: 0.8590\n",
      "Epoch 31/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3245 - acc: 0.8577Epoch 00030: val_acc improved from 0.85933 to 0.85942, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3245 - acc: 0.8577 - val_loss: 0.3220 - val_acc: 0.8594\n",
      "Epoch 32/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.8580- ETAEpoch 00031: val_acc improved from 0.85942 to 0.85942, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3243 - acc: 0.8580 - val_loss: 0.3215 - val_acc: 0.8594\n",
      "Epoch 33/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3243 - acc: 0.8580Epoch 00032: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3243 - acc: 0.8580 - val_loss: 0.3220 - val_acc: 0.8593\n",
      "Epoch 34/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.8580Epoch 00033: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3240 - acc: 0.8580 - val_loss: 0.3226 - val_acc: 0.8590\n",
      "Epoch 35/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.8579Epoch 00034: val_acc improved from 0.85942 to 0.85946, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3240 - acc: 0.8579 - val_loss: 0.3215 - val_acc: 0.8595\n",
      "Epoch 36/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.8582Epoch 00035: val_acc improved from 0.85946 to 0.85952, saving model to tmp_best_allnode.h5\n",
      "\n",
      "Epoch 00035: reducing learning rate to 0.000204349518754.\n",
      "565732/565732 [==============================] - 129s - loss: 0.3240 - acc: 0.8582 - val_loss: 0.3212 - val_acc: 0.8595\n",
      "Epoch 37/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3227 - acc: 0.8587Epoch 00036: val_acc improved from 0.85952 to 0.85962, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3227 - acc: 0.8587 - val_loss: 0.3207 - val_acc: 0.8596\n",
      "Epoch 38/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8586Epoch 00037: val_acc improved from 0.85962 to 0.85988, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3225 - acc: 0.8586 - val_loss: 0.3207 - val_acc: 0.8599\n",
      "Epoch 39/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8588Epoch 00038: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3223 - acc: 0.8588 - val_loss: 0.3205 - val_acc: 0.8599\n",
      "Epoch 40/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.8587Epoch 00039: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3224 - acc: 0.8587 - val_loss: 0.3206 - val_acc: 0.8598\n",
      "Epoch 41/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3225 - acc: 0.8586Epoch 00040: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3225 - acc: 0.8586 - val_loss: 0.3209 - val_acc: 0.8598\n",
      "Epoch 42/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8590Epoch 00041: val_acc improved from 0.85988 to 0.85999, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3221 - acc: 0.8590 - val_loss: 0.3205 - val_acc: 0.8600\n",
      "Epoch 43/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8589Epoch 00042: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3223 - acc: 0.8589 - val_loss: 0.3206 - val_acc: 0.8598\n",
      "Epoch 44/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8587Epoch 00043: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3222 - acc: 0.8587 - val_loss: 0.3204 - val_acc: 0.8599\n",
      "Epoch 45/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8586Epoch 00044: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3223 - acc: 0.8586 - val_loss: 0.3207 - val_acc: 0.8597\n",
      "Epoch 46/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8587Epoch 00045: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3223 - acc: 0.8587 - val_loss: 0.3204 - val_acc: 0.8598\n",
      "Epoch 47/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8588Epoch 00046: val_acc did not improve\n",
      "\n",
      "Epoch 00046: reducing learning rate to 4.08699037507e-05.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3222 - acc: 0.8588 - val_loss: 0.3205 - val_acc: 0.8598\n",
      "Epoch 48/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8591Epoch 00047: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3220 - acc: 0.8591 - val_loss: 0.3203 - val_acc: 0.8600\n",
      "Epoch 49/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8588Epoch 00048: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3219 - acc: 0.8588 - val_loss: 0.3204 - val_acc: 0.8598\n",
      "Epoch 50/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3221 - acc: 0.8585Epoch 00049: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3220 - acc: 0.8585 - val_loss: 0.3204 - val_acc: 0.8598\n",
      "Epoch 51/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8588Epoch 00050: val_acc did not improve\n",
      "\n",
      "Epoch 00050: reducing learning rate to 8.17398104118e-06.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3220 - acc: 0.8588 - val_loss: 0.3203 - val_acc: 0.8599\n",
      "Epoch 52/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8588Epoch 00051: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3219 - acc: 0.8588 - val_loss: 0.3203 - val_acc: 0.8599\n",
      "Epoch 53/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8590Epoch 00052: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3218 - acc: 0.8590 - val_loss: 0.3203 - val_acc: 0.8599\n",
      "Epoch 00052: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_65 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_75 (Masking)             (None, 6, 7)          0           input_65[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_75 (LSTM)                   (None, 6, 11)         836         masking_75[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_76 (Masking)             (None, 6, 11)         0           lstm_75[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_76 (LSTM)                   (None, 6, 11)         1012        masking_76[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_77 (Masking)             (None, 6, 11)         0           lstm_76[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_77 (LSTM)                   (None, 11)            1012        masking_77[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_66 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_127 (Dropout)            (None, 11)            0           lstm_77[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_128 (Dropout)            (None, 9)             0           input_66[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_33 (Merge)                 (None, 20)            0           dropout_127[0][0]                \n",
      "                                                                   dropout_128[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_95 (Dense)                 (None, 22)            462         merge_33[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_129 (Dropout)            (None, 22)            0           dense_95[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_96 (Dense)                 (None, 11)            253         dropout_129[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_130 (Dropout)            (None, 11)            0           dense_96[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_97 (Dense)                 (None, 1)             12          dropout_130[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 3,587\n",
      "Trainable params: 3,587\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3630 - acc: 0.8379Epoch 00000: val_acc improved from -inf to 0.85205, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 138s - loss: 0.3630 - acc: 0.8379 - val_loss: 0.3374 - val_acc: 0.8521\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.8523Epoch 00001: val_acc improved from 0.85205 to 0.85491, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3371 - acc: 0.8523 - val_loss: 0.3302 - val_acc: 0.8549\n",
      "Epoch 3/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8543Epoch 00002: val_acc improved from 0.85491 to 0.85640, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3329 - acc: 0.8543 - val_loss: 0.3284 - val_acc: 0.8564\n",
      "Epoch 4/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3310 - acc: 0.8549Epoch 00003: val_acc improved from 0.85640 to 0.85769, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3310 - acc: 0.8549 - val_loss: 0.3257 - val_acc: 0.8577\n",
      "Epoch 5/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3299 - acc: 0.8555Epoch 00004: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3299 - acc: 0.8555 - val_loss: 0.3262 - val_acc: 0.8574\n",
      "Epoch 6/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3295 - acc: 0.8555Epoch 00005: val_acc improved from 0.85769 to 0.85774, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 124s - loss: 0.3295 - acc: 0.8555 - val_loss: 0.3253 - val_acc: 0.8577\n",
      "Epoch 7/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3289 - acc: 0.8558- ETA: 2Epoch 00006: val_acc improved from 0.85774 to 0.85777, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3289 - acc: 0.8558 - val_loss: 0.3246 - val_acc: 0.8578\n",
      "Epoch 8/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8557Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3286 - acc: 0.8556 - val_loss: 0.3251 - val_acc: 0.8572\n",
      "Epoch 9/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3282 - acc: 0.8560Epoch 00008: val_acc did not improve\n",
      "\n",
      "Epoch 00008: reducing learning rate to 0.000204349518754.\n",
      "565732/565732 [==============================] - 129s - loss: 0.3282 - acc: 0.8560 - val_loss: 0.3264 - val_acc: 0.8566\n",
      "Epoch 10/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3268 - acc: 0.8565Epoch 00009: val_acc improved from 0.85777 to 0.85827, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 128s - loss: 0.3268 - acc: 0.8566 - val_loss: 0.3231 - val_acc: 0.8583\n",
      "Epoch 11/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3266 - acc: 0.8568Epoch 00010: val_acc improved from 0.85827 to 0.85886, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 127s - loss: 0.3265 - acc: 0.8568 - val_loss: 0.3226 - val_acc: 0.8589\n",
      "Epoch 12/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.8568Epoch 00011: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3265 - acc: 0.8569 - val_loss: 0.3225 - val_acc: 0.8587\n",
      "Epoch 13/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8572Epoch 00012: val_acc improved from 0.85886 to 0.85897, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3264 - acc: 0.8572 - val_loss: 0.3224 - val_acc: 0.8590\n",
      "Epoch 14/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8568Epoch 00013: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3264 - acc: 0.8568 - val_loss: 0.3223 - val_acc: 0.8589\n",
      "Epoch 15/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3263 - acc: 0.8570Epoch 00014: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3263 - acc: 0.8570 - val_loss: 0.3225 - val_acc: 0.8588\n",
      "Epoch 16/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3264 - acc: 0.8568Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3264 - acc: 0.8568 - val_loss: 0.3221 - val_acc: 0.8588\n",
      "Epoch 17/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3262 - acc: 0.8568Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3262 - acc: 0.8569 - val_loss: 0.3224 - val_acc: 0.8587\n",
      "Epoch 18/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.8571Epoch 00017: val_acc did not improve\n",
      "\n",
      "Epoch 00017: reducing learning rate to 4.08699037507e-05.\n",
      "565732/565732 [==============================] - 127s - loss: 0.3261 - acc: 0.8571 - val_loss: 0.3224 - val_acc: 0.8586\n",
      "Epoch 19/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3259 - acc: 0.8571Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 127s - loss: 0.3259 - acc: 0.8571 - val_loss: 0.3221 - val_acc: 0.8588\n",
      "Epoch 20/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8572Epoch 00019: val_acc improved from 0.85897 to 0.85897, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3255 - acc: 0.8572 - val_loss: 0.3219 - val_acc: 0.8590\n",
      "Epoch 21/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8573Epoch 00020: val_acc improved from 0.85897 to 0.85901, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3258 - acc: 0.8573 - val_loss: 0.3219 - val_acc: 0.8590\n",
      "Epoch 22/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8569Epoch 00021: val_acc improved from 0.85901 to 0.85903, saving model to tmp_best_allnode.h5\n",
      "\n",
      "Epoch 00021: reducing learning rate to 8.17398104118e-06.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3258 - acc: 0.8569 - val_loss: 0.3219 - val_acc: 0.8590\n",
      "Epoch 23/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8572Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3256 - acc: 0.8572 - val_loss: 0.3219 - val_acc: 0.8590\n",
      "Epoch 24/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8572Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3257 - acc: 0.8572 - val_loss: 0.3218 - val_acc: 0.8590\n",
      "Epoch 25/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8572Epoch 00024: val_acc improved from 0.85903 to 0.85906, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 124s - loss: 0.3257 - acc: 0.8572 - val_loss: 0.3218 - val_acc: 0.8591\n",
      "Epoch 26/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8571Epoch 00025: val_acc did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 1.63479617186e-06.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3256 - acc: 0.8571 - val_loss: 0.3218 - val_acc: 0.8590\n",
      "Epoch 27/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8570Epoch 00026: val_acc did not improve\n",
      "565732/565732 [==============================] - 128s - loss: 0.3258 - acc: 0.8570 - val_loss: 0.3218 - val_acc: 0.8590\n",
      "Epoch 28/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8573Epoch 00027: val_acc did not improve\n",
      "565732/565732 [==============================] - 128s - loss: 0.3256 - acc: 0.8573 - val_loss: 0.3218 - val_acc: 0.8590\n",
      "Epoch 29/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8573Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3257 - acc: 0.8572 - val_loss: 0.3218 - val_acc: 0.8590\n",
      "Epoch 30/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3258 - acc: 0.8570Epoch 00029: val_acc did not improve\n",
      "\n",
      "Epoch 00029: reducing learning rate to 3.26959229824e-07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565732/565732 [==============================] - 124s - loss: 0.3258 - acc: 0.8570 - val_loss: 0.3218 - val_acc: 0.8591\n",
      "Epoch 31/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8572Epoch 00030: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3256 - acc: 0.8572 - val_loss: 0.3218 - val_acc: 0.8590\n",
      "Epoch 32/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8572Epoch 00031: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3255 - acc: 0.8572 - val_loss: 0.3218 - val_acc: 0.8591\n",
      "Epoch 33/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8574Epoch 00032: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3257 - acc: 0.8574 - val_loss: 0.3218 - val_acc: 0.8590\n",
      "Epoch 34/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3255 - acc: 0.8572Epoch 00033: val_acc did not improve\n",
      "\n",
      "Epoch 00033: reducing learning rate to 6.53918448279e-08.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3256 - acc: 0.8572 - val_loss: 0.3218 - val_acc: 0.8590\n",
      "Epoch 35/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8571Epoch 00034: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3257 - acc: 0.8571 - val_loss: 0.3218 - val_acc: 0.8590\n",
      "Epoch 36/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8571Epoch 00035: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3256 - acc: 0.8571 - val_loss: 0.3218 - val_acc: 0.8590\n",
      "Epoch 00035: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_67 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_78 (Masking)             (None, 6, 7)          0           input_67[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_78 (LSTM)                   (None, 6, 11)         836         masking_78[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_79 (Masking)             (None, 6, 11)         0           lstm_78[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_79 (LSTM)                   (None, 6, 11)         1012        masking_79[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_80 (Masking)             (None, 6, 11)         0           lstm_79[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_80 (LSTM)                   (None, 11)            1012        masking_80[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_68 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_131 (Dropout)            (None, 11)            0           lstm_80[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_132 (Dropout)            (None, 9)             0           input_68[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_34 (Merge)                 (None, 20)            0           dropout_131[0][0]                \n",
      "                                                                   dropout_132[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_98 (Dense)                 (None, 22)            462         merge_34[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_133 (Dropout)            (None, 22)            0           dense_98[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_99 (Dense)                 (None, 11)            253         dropout_133[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dropout_134 (Dropout)            (None, 11)            0           dense_99[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_100 (Dense)                (None, 1)             12          dropout_134[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 3,587\n",
      "Trainable params: 3,587\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3626 - acc: 0.8384Epoch 00000: val_acc improved from -inf to 0.85116, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 138s - loss: 0.3626 - acc: 0.8384 - val_loss: 0.3381 - val_acc: 0.8512\n",
      "Epoch 2/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3390 - acc: 0.8511Epoch 00001: val_acc improved from 0.85116 to 0.85291, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3390 - acc: 0.8511 - val_loss: 0.3351 - val_acc: 0.8529\n",
      "Epoch 3/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3347 - acc: 0.8532Epoch 00002: val_acc improved from 0.85291 to 0.85522, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3347 - acc: 0.8532 - val_loss: 0.3300 - val_acc: 0.8552\n",
      "Epoch 4/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3325 - acc: 0.8546Epoch 00003: val_acc improved from 0.85522 to 0.85583, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3325 - acc: 0.8546 - val_loss: 0.3282 - val_acc: 0.8558\n",
      "Epoch 5/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3311 - acc: 0.8553Epoch 00004: val_acc improved from 0.85583 to 0.85650, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3311 - acc: 0.8553 - val_loss: 0.3269 - val_acc: 0.8565\n",
      "Epoch 6/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3298 - acc: 0.8552Epoch 00005: val_acc improved from 0.85650 to 0.85676, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3298 - acc: 0.8552 - val_loss: 0.3263 - val_acc: 0.8568\n",
      "Epoch 7/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3292 - acc: 0.8556Epoch 00006: val_acc improved from 0.85676 to 0.85712, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3292 - acc: 0.8556 - val_loss: 0.3257 - val_acc: 0.8571\n",
      "Epoch 8/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3284 - acc: 0.8564Epoch 00007: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3284 - acc: 0.8564 - val_loss: 0.3254 - val_acc: 0.8571\n",
      "Epoch 9/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3278 - acc: 0.8566Epoch 00008: val_acc improved from 0.85712 to 0.85718, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3278 - acc: 0.8566 - val_loss: 0.3254 - val_acc: 0.8572\n",
      "Epoch 10/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8565Epoch 00009: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3277 - acc: 0.8565 - val_loss: 0.3260 - val_acc: 0.8569\n",
      "Epoch 11/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3270 - acc: 0.8569Epoch 00010: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3270 - acc: 0.8569 - val_loss: 0.3253 - val_acc: 0.8567\n",
      "Epoch 12/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3265 - acc: 0.8571Epoch 00011: val_acc improved from 0.85718 to 0.85798, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3265 - acc: 0.8571 - val_loss: 0.3236 - val_acc: 0.8580\n",
      "Epoch 13/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3257 - acc: 0.8572Epoch 00012: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3257 - acc: 0.8572 - val_loss: 0.3242 - val_acc: 0.8578\n",
      "Epoch 14/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3256 - acc: 0.8573Epoch 00013: val_acc improved from 0.85798 to 0.85829, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3256 - acc: 0.8573 - val_loss: 0.3229 - val_acc: 0.8583\n",
      "Epoch 15/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3251 - acc: 0.8576Epoch 00014: val_acc improved from 0.85829 to 0.85884, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3251 - acc: 0.8576 - val_loss: 0.3224 - val_acc: 0.8588\n",
      "Epoch 16/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3250 - acc: 0.8577Epoch 00015: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3249 - acc: 0.8577 - val_loss: 0.3222 - val_acc: 0.8586\n",
      "Epoch 17/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3246 - acc: 0.8582Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3246 - acc: 0.8582 - val_loss: 0.3225 - val_acc: 0.8583\n",
      "Epoch 18/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3244 - acc: 0.8577Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3244 - acc: 0.8577 - val_loss: 0.3232 - val_acc: 0.8583\n",
      "Epoch 19/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3242 - acc: 0.8581Epoch 00018: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3242 - acc: 0.8581 - val_loss: 0.3230 - val_acc: 0.8582\n",
      "Epoch 20/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3238 - acc: 0.8581Epoch 00019: val_acc improved from 0.85884 to 0.85893, saving model to tmp_best_allnode.h5\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.000204349518754.\n",
      "565732/565732 [==============================] - 129s - loss: 0.3238 - acc: 0.8582 - val_loss: 0.3220 - val_acc: 0.8589\n",
      "Epoch 21/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3226 - acc: 0.8587Epoch 00020: val_acc improved from 0.85893 to 0.85932, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3226 - acc: 0.8587 - val_loss: 0.3210 - val_acc: 0.8593\n",
      "Epoch 22/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.8587Epoch 00021: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3224 - acc: 0.8587 - val_loss: 0.3207 - val_acc: 0.8593\n",
      "Epoch 23/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8585Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3223 - acc: 0.8585 - val_loss: 0.3207 - val_acc: 0.8592\n",
      "Epoch 24/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3224 - acc: 0.8590Epoch 00023: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3224 - acc: 0.8589 - val_loss: 0.3206 - val_acc: 0.8593\n",
      "Epoch 25/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3222 - acc: 0.8589Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3222 - acc: 0.8589 - val_loss: 0.3210 - val_acc: 0.8593\n",
      "Epoch 26/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3223 - acc: 0.8585Epoch 00025: val_acc did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 4.08699037507e-05.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3223 - acc: 0.8585 - val_loss: 0.3206 - val_acc: 0.8593\n",
      "Epoch 27/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8590Epoch 00026: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3218 - acc: 0.8590 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 28/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8589Epoch 00027: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3219 - acc: 0.8589 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 29/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8588Epoch 00028: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3220 - acc: 0.8588 - val_loss: 0.3204 - val_acc: 0.8593\n",
      "Epoch 30/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8587Epoch 00029: val_acc did not improve\n",
      "\n",
      "Epoch 00029: reducing learning rate to 8.17398104118e-06.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3217 - acc: 0.8587 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 31/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8590Epoch 00030: val_acc improved from 0.85932 to 0.85936, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 125s - loss: 0.3216 - acc: 0.8590 - val_loss: 0.3205 - val_acc: 0.8594\n",
      "Epoch 32/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8591Epoch 00031: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3217 - acc: 0.8591 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 33/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3218 - acc: 0.8588Epoch 00032: val_acc did not improve\n",
      "565732/565732 [==============================] - 126s - loss: 0.3218 - acc: 0.8588 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 34/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8590Epoch 00033: val_acc improved from 0.85936 to 0.85938, saving model to tmp_best_allnode.h5\n",
      "\n",
      "Epoch 00033: reducing learning rate to 1.63479617186e-06.\n",
      "565732/565732 [==============================] - 128s - loss: 0.3216 - acc: 0.8590 - val_loss: 0.3205 - val_acc: 0.8594\n",
      "Epoch 35/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8593Epoch 00034: val_acc improved from 0.85938 to 0.85939, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 127s - loss: 0.3219 - acc: 0.8593 - val_loss: 0.3205 - val_acc: 0.8594\n",
      "Epoch 36/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8592Epoch 00035: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3215 - acc: 0.8592 - val_loss: 0.3205 - val_acc: 0.8594\n",
      "Epoch 37/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8588Epoch 00036: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3217 - acc: 0.8588 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8591Epoch 00037: val_acc did not improve\n",
      "\n",
      "Epoch 00037: reducing learning rate to 3.26959229824e-07.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3216 - acc: 0.8591 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 39/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8591Epoch 00038: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3217 - acc: 0.8591 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 40/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8591Epoch 00039: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3215 - acc: 0.8591 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 41/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8590Epoch 00040: val_acc did not improve\n",
      "565732/565732 [==============================] - 124s - loss: 0.3216 - acc: 0.8590 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 42/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8590Epoch 00041: val_acc did not improve\n",
      "\n",
      "Epoch 00041: reducing learning rate to 6.53918448279e-08.\n",
      "565732/565732 [==============================] - 125s - loss: 0.3217 - acc: 0.8590 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 43/100\n",
      "565396/565732 [============================>.] - ETA: 0s - loss: 0.3216 - acc: 0.8590Epoch 00042: val_acc did not improve\n",
      "565732/565732 [==============================] - 125s - loss: 0.3216 - acc: 0.8590 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 44/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8590Epoch 00043: val_acc did not improve\n",
      "565732/565732 [==============================] - 126s - loss: 0.3217 - acc: 0.8590 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 45/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8589Epoch 00044: val_acc did not improve\n",
      "565732/565732 [==============================] - 128s - loss: 0.3218 - acc: 0.8589 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 46/100\n",
      "565658/565732 [============================>.] - ETA: 0s - loss: 0.3217 - acc: 0.8591Epoch 00045: val_acc did not improve\n",
      "\n",
      "Epoch 00045: reducing learning rate to 1.30783689656e-08.\n",
      "565732/565732 [==============================] - 126s - loss: 0.3217 - acc: 0.8591 - val_loss: 0.3205 - val_acc: 0.8593\n",
      "Epoch 00045: early stopping\n",
      "Average best_acc across k-fold: 0.859481185044\n",
      "New configuration: {'hidden_layers': 1, 'dropout_g': 0.90000000000000002, 'initial_nodes': 500, 'dropout': 0.01, 'gru_layers': 3, 'batch_size': 32, 'gru_size': 10, 'learning_rate': 1.0000000000000001e-05}\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_69 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_81 (Masking)             (None, 6, 7)          0           input_69[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_81 (LSTM)                   (None, 6, 10)         720         masking_81[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_82 (Masking)             (None, 6, 10)         0           lstm_81[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_82 (LSTM)                   (None, 6, 10)         840         masking_82[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_83 (Masking)             (None, 6, 10)         0           lstm_82[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_83 (LSTM)                   (None, 10)            840         masking_83[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_70 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_135 (Dropout)            (None, 10)            0           lstm_83[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_136 (Dropout)            (None, 9)             0           input_70[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_35 (Merge)                 (None, 19)            0           dropout_135[0][0]                \n",
      "                                                                   dropout_136[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_101 (Dense)                (None, 500)           10000       merge_35[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_137 (Dropout)            (None, 500)           0           dense_101[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_102 (Dense)                (None, 1)             501         dropout_137[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 12,901\n",
      "Trainable params: 12,901\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n",
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.4812 - acc: 0.7740Epoch 00000: val_acc improved from -inf to 0.79030, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 974s - loss: 0.4812 - acc: 0.7740 - val_loss: 0.4478 - val_acc: 0.7903\n",
      "Epoch 2/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.4416 - acc: 0.7938Epoch 00001: val_acc improved from 0.79030 to 0.80500, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 956s - loss: 0.4417 - acc: 0.7938 - val_loss: 0.4239 - val_acc: 0.8050\n",
      "Epoch 3/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.4268 - acc: 0.8026Epoch 00002: val_acc improved from 0.80500 to 0.81948, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 956s - loss: 0.4268 - acc: 0.8026 - val_loss: 0.4006 - val_acc: 0.8195\n",
      "Epoch 4/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.4148 - acc: 0.8103Epoch 00003: val_acc improved from 0.81948 to 0.82926, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 955s - loss: 0.4148 - acc: 0.8103 - val_loss: 0.3844 - val_acc: 0.8293\n",
      "Epoch 5/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.4057 - acc: 0.8153Epoch 00004: val_acc improved from 0.82926 to 0.83495, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 955s - loss: 0.4057 - acc: 0.8153 - val_loss: 0.3734 - val_acc: 0.8350\n",
      "Epoch 6/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3986 - acc: 0.8192Epoch 00005: val_acc improved from 0.83495 to 0.83843, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 955s - loss: 0.3986 - acc: 0.8192 - val_loss: 0.3662 - val_acc: 0.8384\n",
      "Epoch 7/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3950 - acc: 0.8207Epoch 00006: val_acc improved from 0.83843 to 0.84001, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 961s - loss: 0.3950 - acc: 0.8207 - val_loss: 0.3623 - val_acc: 0.8400\n",
      "Epoch 8/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3927 - acc: 0.8220Epoch 00007: val_acc improved from 0.84001 to 0.84119, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 957s - loss: 0.3927 - acc: 0.8220 - val_loss: 0.3596 - val_acc: 0.8412\n",
      "Epoch 9/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.8229Epoch 00008: val_acc improved from 0.84119 to 0.84190, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 961s - loss: 0.3904 - acc: 0.8229 - val_loss: 0.3573 - val_acc: 0.8419\n",
      "Epoch 10/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3889 - acc: 0.8236Epoch 00009: val_acc improved from 0.84190 to 0.84249, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 952s - loss: 0.3889 - acc: 0.8236 - val_loss: 0.3556 - val_acc: 0.8425\n",
      "Epoch 11/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3881 - acc: 0.8242Epoch 00010: val_acc improved from 0.84249 to 0.84296, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 951s - loss: 0.3881 - acc: 0.8242 - val_loss: 0.3544 - val_acc: 0.8430\n",
      "Epoch 12/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3873 - acc: 0.8245Epoch 00011: val_acc improved from 0.84296 to 0.84339, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 952s - loss: 0.3873 - acc: 0.8245 - val_loss: 0.3534 - val_acc: 0.8434\n",
      "Epoch 13/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3862 - acc: 0.8251Epoch 00012: val_acc improved from 0.84339 to 0.84377, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 951s - loss: 0.3862 - acc: 0.8251 - val_loss: 0.3525 - val_acc: 0.8438\n",
      "Epoch 14/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8258Epoch 00013: val_acc improved from 0.84377 to 0.84454, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 952s - loss: 0.3850 - acc: 0.8258 - val_loss: 0.3512 - val_acc: 0.8445\n",
      "Epoch 15/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3846 - acc: 0.8264Epoch 00014: val_acc improved from 0.84454 to 0.84456, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 957s - loss: 0.3846 - acc: 0.8264 - val_loss: 0.3511 - val_acc: 0.8446\n",
      "Epoch 16/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8265Epoch 00015: val_acc improved from 0.84456 to 0.84515, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 953s - loss: 0.3839 - acc: 0.8265 - val_loss: 0.3502 - val_acc: 0.8451\n",
      "Epoch 17/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8266Epoch 00016: val_acc improved from 0.84515 to 0.84556, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 961s - loss: 0.3836 - acc: 0.8266 - val_loss: 0.3495 - val_acc: 0.8456\n",
      "Epoch 18/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3825 - acc: 0.8274Epoch 00017: val_acc did not improve\n",
      "565732/565732 [==============================] - 952s - loss: 0.3825 - acc: 0.8274 - val_loss: 0.3495 - val_acc: 0.8454\n",
      "Epoch 19/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3827 - acc: 0.8272Epoch 00018: val_acc improved from 0.84556 to 0.84611, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 952s - loss: 0.3827 - acc: 0.8272 - val_loss: 0.3482 - val_acc: 0.8461\n",
      "Epoch 20/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8280Epoch 00019: val_acc improved from 0.84611 to 0.84631, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 951s - loss: 0.3819 - acc: 0.8280 - val_loss: 0.3479 - val_acc: 0.8463\n",
      "Epoch 21/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3819 - acc: 0.8276Epoch 00020: val_acc improved from 0.84631 to 0.84633, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 952s - loss: 0.3819 - acc: 0.8276 - val_loss: 0.3481 - val_acc: 0.8463\n",
      "Epoch 22/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3814 - acc: 0.8281Epoch 00021: val_acc improved from 0.84633 to 0.84650, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 952s - loss: 0.3814 - acc: 0.8281 - val_loss: 0.3472 - val_acc: 0.8465\n",
      "Epoch 23/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3804 - acc: 0.8288Epoch 00022: val_acc improved from 0.84650 to 0.84691, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 956s - loss: 0.3804 - acc: 0.8288 - val_loss: 0.3469 - val_acc: 0.8469\n",
      "Epoch 24/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8283Epoch 00023: val_acc improved from 0.84691 to 0.84693, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 956s - loss: 0.3805 - acc: 0.8283 - val_loss: 0.3467 - val_acc: 0.8469\n",
      "Epoch 25/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3802 - acc: 0.8290Epoch 00024: val_acc improved from 0.84693 to 0.84709, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 957s - loss: 0.3802 - acc: 0.8290 - val_loss: 0.3464 - val_acc: 0.8471\n",
      "Epoch 26/100\n",
      "432800/565732 [=====================>........] - ETA: 196s - loss: 0.3791 - acc: 0.8289"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216576/565732 [==========>...................] - ETA: 516s - loss: 0.3803 - acc: 0.8283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8286Epoch 00026: val_acc improved from 0.84709 to 0.84720, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 952s - loss: 0.3798 - acc: 0.8285 - val_loss: 0.3458 - val_acc: 0.8472\n",
      "Epoch 28/100\n",
      " 21888/565732 [>.............................] - ETA: 803s - loss: 0.3817 - acc: 0.8280"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391232/565732 [===================>..........] - ETA: 257s - loss: 0.3789 - acc: 0.8290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3796 - acc: 0.8287Epoch 00027: val_acc improved from 0.84720 to 0.84740, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 951s - loss: 0.3796 - acc: 0.8287 - val_loss: 0.3458 - val_acc: 0.8474\n",
      "Epoch 29/100\n",
      "147840/565732 [======>.......................] - ETA: 616s - loss: 0.3787 - acc: 0.8296"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504384/565732 [=========================>....] - ETA: 90s - loss: 0.3793 - acc: 0.8289"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306560/565732 [===============>..............] - ETA: 381s - loss: 0.3802 - acc: 0.8283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3789 - acc: 0.8291Epoch 00029: val_acc improved from 0.84743 to 0.84771, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 955s - loss: 0.3789 - acc: 0.8291 - val_loss: 0.3451 - val_acc: 0.8477\n",
      "Epoch 31/100\n",
      " 82688/565732 [===>..........................] - ETA: 721s - loss: 0.3817 - acc: 0.8268"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423744/565732 [=====================>........] - ETA: 210s - loss: 0.3796 - acc: 0.8293"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236320/565732 [===========>..................] - ETA: 486s - loss: 0.3792 - acc: 0.8298"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552768/565732 [============================>.] - ETA: 19s - loss: 0.3783 - acc: 0.8300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363712/565732 [==================>...........] - ETA: 298s - loss: 0.3770 - acc: 0.8305"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8298Epoch 00032: val_acc improved from 0.84783 to 0.84822, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 957s - loss: 0.3780 - acc: 0.8298 - val_loss: 0.3443 - val_acc: 0.8482\n",
      "Epoch 34/100\n",
      "154624/565732 [=======>......................] - ETA: 605s - loss: 0.3798 - acc: 0.8285"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407776/565732 [====================>.........] - ETA: 232s - loss: 0.3787 - acc: 0.8294"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526528/565732 [==========================>...] - ETA: 57s - loss: 0.3788 - acc: 0.8296"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211904/565732 [==========>...................] - ETA: 522s - loss: 0.3785 - acc: 0.8299"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326208/565732 [================>.............] - ETA: 353s - loss: 0.3783 - acc: 0.8298"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8301Epoch 00034: val_acc did not improve\n",
      "565732/565732 [==============================] - 951s - loss: 0.3780 - acc: 0.8301 - val_loss: 0.3442 - val_acc: 0.8481\n",
      "Epoch 36/100\n",
      " 10336/565732 [..............................] - ETA: 819s - loss: 0.3786 - acc: 0.8315"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125856/565732 [=====>........................] - ETA: 650s - loss: 0.3768 - acc: 0.8317"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377856/565732 [===================>..........] - ETA: 277s - loss: 0.3774 - acc: 0.8302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494720/565732 [=========================>....] - ETA: 104s - loss: 0.3778 - acc: 0.8302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178560/565732 [========>.....................] - ETA: 571s - loss: 0.3775 - acc: 0.8297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295840/565732 [==============>...............] - ETA: 398s - loss: 0.3775 - acc: 0.8299"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8299Epoch 00036: val_acc did not improve\n",
      "565732/565732 [==============================] - 951s - loss: 0.3776 - acc: 0.8299 - val_loss: 0.3438 - val_acc: 0.8482\n",
      "Epoch 38/100\n",
      " 35744/565732 [>.............................] - ETA: 788s - loss: 0.3780 - acc: 0.8301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223712/565732 [==========>...................] - ETA: 505s - loss: 0.3771 - acc: 0.8306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403680/565732 [====================>.........] - ETA: 238s - loss: 0.3772 - acc: 0.8302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8299Epoch 00037: val_acc did not improve\n",
      "\n",
      "Epoch 00037: reducing learning rate to 1.99999994948e-06.\n",
      "565732/565732 [==============================] - 961s - loss: 0.3775 - acc: 0.8299 - val_loss: 0.3439 - val_acc: 0.8482\n",
      "Epoch 39/100\n",
      "  8096/565732 [..............................] - ETA: 816s - loss: 0.3788 - acc: 0.8318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181376/565732 [========>.....................] - ETA: 566s - loss: 0.3777 - acc: 0.8302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343744/565732 [=================>............] - ETA: 327s - loss: 0.3776 - acc: 0.8303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521120/565732 [==========================>...] - ETA: 65s - loss: 0.3773 - acc: 0.8304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132672/565732 [======>.......................] - ETA: 639s - loss: 0.3772 - acc: 0.8293"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291168/565732 [==============>...............] - ETA: 410s - loss: 0.3777 - acc: 0.8295"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480192/565732 [========================>.....] - ETA: 127s - loss: 0.3767 - acc: 0.8301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8300Epoch 00039: val_acc improved from 0.84832 to 0.84847, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 957s - loss: 0.3770 - acc: 0.8300 - val_loss: 0.3435 - val_acc: 0.8485\n",
      "Epoch 41/100\n",
      " 90112/565732 [===>..........................] - ETA: 703s - loss: 0.3753 - acc: 0.8310"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275264/565732 [=============>................] - ETA: 428s - loss: 0.3768 - acc: 0.8306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453344/565732 [=======================>......] - ETA: 165s - loss: 0.3773 - acc: 0.8302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8301Epoch 00040: val_acc did not improve\n",
      "565732/565732 [==============================] - 953s - loss: 0.3772 - acc: 0.8301 - val_loss: 0.3434 - val_acc: 0.8484\n",
      "Epoch 42/100\n",
      " 38752/565732 [=>............................] - ETA: 800s - loss: 0.3792 - acc: 0.8283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213472/565732 [==========>...................] - ETA: 524s - loss: 0.3775 - acc: 0.8303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389216/565732 [===================>..........] - ETA: 261s - loss: 0.3778 - acc: 0.8303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8305Epoch 00041: val_acc did not improve\n",
      "565732/565732 [==============================] - 955s - loss: 0.3773 - acc: 0.8305 - val_loss: 0.3436 - val_acc: 0.8484\n",
      "Epoch 43/100\n",
      " 63040/565732 [==>...........................] - ETA: 741s - loss: 0.3780 - acc: 0.8299"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224128/565732 [==========>...................] - ETA: 503s - loss: 0.3769 - acc: 0.8304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410272/565732 [====================>.........] - ETA: 229s - loss: 0.3767 - acc: 0.8307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3776 - acc: 0.8302Epoch 00042: val_acc did not improve\n",
      "565732/565732 [==============================] - 951s - loss: 0.3776 - acc: 0.8302 - val_loss: 0.3435 - val_acc: 0.8484\n",
      "Epoch 44/100\n",
      " 15776/565732 [..............................] - ETA: 811s - loss: 0.3775 - acc: 0.8310"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200064/565732 [=========>....................] - ETA: 539s - loss: 0.3777 - acc: 0.8302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370816/565732 [==================>...........] - ETA: 287s - loss: 0.3771 - acc: 0.8307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557856/565732 [============================>.] - ETA: 11s - loss: 0.3773 - acc: 0.8302"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157568/565732 [=======>......................] - ETA: 603s - loss: 0.3760 - acc: 0.8300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345344/565732 [=================>............] - ETA: 325s - loss: 0.3769 - acc: 0.8306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511808/565732 [==========================>...] - ETA: 79s - loss: 0.3773 - acc: 0.8303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131840/565732 [=====>........................] - ETA: 639s - loss: 0.3774 - acc: 0.8305"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296160/565732 [==============>...............] - ETA: 397s - loss: 0.3776 - acc: 0.8298"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468128/565732 [=======================>......] - ETA: 144s - loss: 0.3772 - acc: 0.8301"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8302Epoch 00045: val_acc improved from 0.84847 to 0.84852, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 958s - loss: 0.3772 - acc: 0.8302 - val_loss: 0.3433 - val_acc: 0.8485\n",
      "Epoch 47/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8305Epoch 00046: val_acc improved from 0.84852 to 0.84854, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 952s - loss: 0.3773 - acc: 0.8305 - val_loss: 0.3433 - val_acc: 0.8485\n",
      "Epoch 48/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8304Epoch 00047: val_acc did not improve\n",
      "565732/565732 [==============================] - 957s - loss: 0.3768 - acc: 0.8304 - val_loss: 0.3433 - val_acc: 0.8485\n",
      "Epoch 49/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3767 - acc: 0.8308Epoch 00048: val_acc improved from 0.84854 to 0.84856, saving model to tmp_best_allnode.h5\n",
      "\n",
      "Epoch 00048: reducing learning rate to 8.00000009349e-08.\n",
      "565732/565732 [==============================] - 951s - loss: 0.3767 - acc: 0.8308 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 50/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8302Epoch 00049: val_acc did not improve\n",
      "565732/565732 [==============================] - 957s - loss: 0.3773 - acc: 0.8302 - val_loss: 0.3433 - val_acc: 0.8485\n",
      "Epoch 51/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3775 - acc: 0.8299Epoch 00050: val_acc did not improve\n",
      "565732/565732 [==============================] - 952s - loss: 0.3775 - acc: 0.8299 - val_loss: 0.3433 - val_acc: 0.8485\n",
      "Epoch 52/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8304Epoch 00051: val_acc did not improve\n",
      "565732/565732 [==============================] - 952s - loss: 0.3772 - acc: 0.8304 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 53/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3774 - acc: 0.8304Epoch 00052: val_acc did not improve\n",
      "\n",
      "Epoch 00052: reducing learning rate to 1.59999999028e-08.\n",
      "565732/565732 [==============================] - 951s - loss: 0.3774 - acc: 0.8304 - val_loss: 0.3433 - val_acc: 0.8485\n",
      "Epoch 54/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8300Epoch 00053: val_acc did not improve\n",
      "565732/565732 [==============================] - 956s - loss: 0.3773 - acc: 0.8300 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 55/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8304Epoch 00054: val_acc did not improve\n",
      "565732/565732 [==============================] - 956s - loss: 0.3768 - acc: 0.8304 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 56/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8304Epoch 00055: val_acc improved from 0.84856 to 0.84857, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 951s - loss: 0.3768 - acc: 0.8304 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 57/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3771 - acc: 0.8305Epoch 00056: val_acc did not improve\n",
      "565732/565732 [==============================] - 951s - loss: 0.3771 - acc: 0.8305 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 58/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8301Epoch 00057: val_acc did not improve\n",
      "565732/565732 [==============================] - 988s - loss: 0.3773 - acc: 0.8301 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 59/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8304Epoch 00058: val_acc did not improve\n",
      "565732/565732 [==============================] - 956s - loss: 0.3768 - acc: 0.8304 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 60/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8305Epoch 00059: val_acc improved from 0.84857 to 0.84859, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 961s - loss: 0.3770 - acc: 0.8305 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 61/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8307Epoch 00060: val_acc did not improve\n",
      "\n",
      "Epoch 00060: reducing learning rate to 3.1999999095e-09.\n",
      "565732/565732 [==============================] - 959s - loss: 0.3766 - acc: 0.8307 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 62/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3769 - acc: 0.8304Epoch 00061: val_acc did not improve\n",
      "565732/565732 [==============================] - 983s - loss: 0.3769 - acc: 0.8304 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 63/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8306Epoch 00062: val_acc did not improve\n",
      "565732/565732 [==============================] - 993s - loss: 0.3771 - acc: 0.8307 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 64/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8297Epoch 00063: val_acc did not improve\n",
      "565732/565732 [==============================] - 951s - loss: 0.3772 - acc: 0.8297 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 65/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8304Epoch 00064: val_acc did not improve\n",
      "\n",
      "Epoch 00064: reducing learning rate to 6.39999964136e-10.\n",
      "565732/565732 [==============================] - 951s - loss: 0.3773 - acc: 0.8304 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 66/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8302Epoch 00065: val_acc did not improve\n",
      "565732/565732 [==============================] - 956s - loss: 0.3771 - acc: 0.8302 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 67/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3770 - acc: 0.8303Epoch 00066: val_acc did not improve\n",
      "565732/565732 [==============================] - 952s - loss: 0.3770 - acc: 0.8303 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 68/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3768 - acc: 0.8302Epoch 00067: val_acc did not improve\n",
      "565732/565732 [==============================] - 951s - loss: 0.3768 - acc: 0.8302 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 69/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8307Epoch 00068: val_acc did not improve\n",
      "\n",
      "Epoch 00068: reducing learning rate to 1.27999988386e-10.\n",
      "565732/565732 [==============================] - 952s - loss: 0.3772 - acc: 0.8307 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 70/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3773 - acc: 0.8303Epoch 00069: val_acc did not improve\n",
      "565732/565732 [==============================] - 957s - loss: 0.3773 - acc: 0.8303 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 71/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3772 - acc: 0.8302Epoch 00070: val_acc did not improve\n",
      "565732/565732 [==============================] - 957s - loss: 0.3772 - acc: 0.8302 - val_loss: 0.3433 - val_acc: 0.8486\n",
      "Epoch 00070: early stopping\n",
      "data_list shape = (565732, 6, 7)\n",
      "data_hlf shape = (565732, 9)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_71 (InputLayer)            (None, 6, 7)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_84 (Masking)             (None, 6, 7)          0           input_71[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_84 (LSTM)                   (None, 6, 10)         720         masking_84[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_85 (Masking)             (None, 6, 10)         0           lstm_84[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_85 (LSTM)                   (None, 6, 10)         840         masking_85[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "masking_86 (Masking)             (None, 6, 10)         0           lstm_85[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_86 (LSTM)                   (None, 10)            840         masking_86[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "input_72 (InputLayer)            (None, 9)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_138 (Dropout)            (None, 10)            0           lstm_86[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_139 (Dropout)            (None, 9)             0           input_72[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_36 (Merge)                 (None, 19)            0           dropout_138[0][0]                \n",
      "                                                                   dropout_139[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_103 (Dense)                (None, 500)           10000       merge_36[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_140 (Dropout)            (None, 500)           0           dense_103[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_104 (Dense)                (None, 1)             501         dropout_140[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 12,901\n",
      "Trainable params: 12,901\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "Input data particle shape: (565732, 6, 7)\n",
      "Input data HLF shape: (565732, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 565732 samples, validate on 282866 samples\n",
      "Epoch 1/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.4784 - acc: 0.7798Epoch 00000: val_acc improved from -inf to 0.78692, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 963s - loss: 0.4784 - acc: 0.7798 - val_loss: 0.4523 - val_acc: 0.7869\n",
      "Epoch 2/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.7875Epoch 00001: val_acc improved from 0.78692 to 0.79003, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 951s - loss: 0.4508 - acc: 0.7875 - val_loss: 0.4462 - val_acc: 0.7900\n",
      "Epoch 3/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.4453 - acc: 0.7906Epoch 00002: val_acc improved from 0.79003 to 0.79463, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 954s - loss: 0.4454 - acc: 0.7906 - val_loss: 0.4374 - val_acc: 0.7946\n",
      "Epoch 4/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.4366 - acc: 0.7961Epoch 00003: val_acc improved from 0.79463 to 0.80496, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 987s - loss: 0.4366 - acc: 0.7961 - val_loss: 0.4201 - val_acc: 0.8050\n",
      "Epoch 5/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.4230 - acc: 0.8046Epoch 00004: val_acc improved from 0.80496 to 0.81883, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 975s - loss: 0.4230 - acc: 0.8046 - val_loss: 0.3977 - val_acc: 0.8188\n",
      "Epoch 6/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.4123 - acc: 0.8112Epoch 00005: val_acc improved from 0.81883 to 0.82785, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 975s - loss: 0.4123 - acc: 0.8112 - val_loss: 0.3833 - val_acc: 0.8278\n",
      "Epoch 7/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.4056 - acc: 0.8147Epoch 00006: val_acc improved from 0.82785 to 0.83291, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 956s - loss: 0.4056 - acc: 0.8147 - val_loss: 0.3745 - val_acc: 0.8329\n",
      "Epoch 8/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.4001 - acc: 0.8179Epoch 00007: val_acc improved from 0.83291 to 0.83634, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 955s - loss: 0.4001 - acc: 0.8179 - val_loss: 0.3682 - val_acc: 0.8363\n",
      "Epoch 9/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3970 - acc: 0.8197Epoch 00008: val_acc improved from 0.83634 to 0.83855, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 959s - loss: 0.3970 - acc: 0.8197 - val_loss: 0.3639 - val_acc: 0.8385\n",
      "Epoch 10/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3928 - acc: 0.8218Epoch 00009: val_acc improved from 0.83855 to 0.84044, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 955s - loss: 0.3928 - acc: 0.8218 - val_loss: 0.3600 - val_acc: 0.8404\n",
      "Epoch 11/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3906 - acc: 0.8227Epoch 00010: val_acc improved from 0.84044 to 0.84129, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 949s - loss: 0.3906 - acc: 0.8227 - val_loss: 0.3578 - val_acc: 0.8413\n",
      "Epoch 12/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3886 - acc: 0.8237Epoch 00011: val_acc improved from 0.84129 to 0.84220, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 956s - loss: 0.3886 - acc: 0.8237 - val_loss: 0.3557 - val_acc: 0.8422\n",
      "Epoch 13/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3874 - acc: 0.8240Epoch 00012: val_acc improved from 0.84220 to 0.84246, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 980s - loss: 0.3874 - acc: 0.8240 - val_loss: 0.3548 - val_acc: 0.8425\n",
      "Epoch 14/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8254Epoch 00013: val_acc improved from 0.84246 to 0.84300, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 955s - loss: 0.3863 - acc: 0.8254 - val_loss: 0.3535 - val_acc: 0.8430\n",
      "Epoch 15/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.8262Epoch 00014: val_acc improved from 0.84300 to 0.84368, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 992s - loss: 0.3850 - acc: 0.8262 - val_loss: 0.3520 - val_acc: 0.8437\n",
      "Epoch 16/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3839 - acc: 0.8264Epoch 00015: val_acc improved from 0.84368 to 0.84427, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 988s - loss: 0.3839 - acc: 0.8264 - val_loss: 0.3514 - val_acc: 0.8443\n",
      "Epoch 17/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3838 - acc: 0.8265Epoch 00016: val_acc did not improve\n",
      "565732/565732 [==============================] - 988s - loss: 0.3838 - acc: 0.8265 - val_loss: 0.3514 - val_acc: 0.8441\n",
      "Epoch 18/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3836 - acc: 0.8266Epoch 00017: val_acc improved from 0.84427 to 0.84476, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 994s - loss: 0.3836 - acc: 0.8266 - val_loss: 0.3508 - val_acc: 0.8448\n",
      "Epoch 19/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3826 - acc: 0.8273Epoch 00018: val_acc improved from 0.84476 to 0.84498, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 986s - loss: 0.3826 - acc: 0.8273 - val_loss: 0.3499 - val_acc: 0.8450\n",
      "Epoch 20/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3824 - acc: 0.8273Epoch 00019: val_acc improved from 0.84498 to 0.84545, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 996s - loss: 0.3824 - acc: 0.8273 - val_loss: 0.3496 - val_acc: 0.8454\n",
      "Epoch 21/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3817 - acc: 0.8279Epoch 00020: val_acc improved from 0.84545 to 0.84588, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 992s - loss: 0.3817 - acc: 0.8279 - val_loss: 0.3489 - val_acc: 0.8459\n",
      "Epoch 22/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3816 - acc: 0.8277Epoch 00021: val_acc did not improve\n",
      "565732/565732 [==============================] - 994s - loss: 0.3816 - acc: 0.8277 - val_loss: 0.3487 - val_acc: 0.8458\n",
      "Epoch 23/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8286Epoch 00022: val_acc did not improve\n",
      "565732/565732 [==============================] - 999s - loss: 0.3806 - acc: 0.8286 - val_loss: 0.3485 - val_acc: 0.8457\n",
      "Epoch 24/100\n",
      "565696/565732 [============================>.] - ETA: 0s - loss: 0.3806 - acc: 0.8281Epoch 00023: val_acc improved from 0.84588 to 0.84693, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 993s - loss: 0.3806 - acc: 0.8281 - val_loss: 0.3471 - val_acc: 0.8469\n",
      "Epoch 25/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3805 - acc: 0.8285Epoch 00024: val_acc did not improve\n",
      "565732/565732 [==============================] - 995s - loss: 0.3805 - acc: 0.8285 - val_loss: 0.3474 - val_acc: 0.8464\n",
      "Epoch 26/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3797 - acc: 0.8289Epoch 00025: val_acc improved from 0.84693 to 0.84727, saving model to tmp_best_allnode.h5\n",
      "565732/565732 [==============================] - 976s - loss: 0.3797 - acc: 0.8289 - val_loss: 0.3469 - val_acc: 0.8473\n",
      "Epoch 27/100\n",
      "565728/565732 [============================>.] - ETA: 0s - loss: 0.3794 - acc: 0.8294Epoch 00026: val_acc did not improve\n",
      "565732/565732 [==============================] - 1021s - loss: 0.3794 - acc: 0.8294 - val_loss: 0.3465 - val_acc: 0.8472\n",
      "Epoch 28/100\n",
      "378688/565732 [===================>..........] - ETA: 299s - loss: 0.3792 - acc: 0.8295"
     ]
    }
   ],
   "source": [
    "res_gp = gp_minimize(objective, space, n_calls=25, random_state=0)\n",
    "\n",
    "print(\"Best parameters: {}\".format(res_gp.x))\n",
    "best_hidden_layers = res_gp.x[0]\n",
    "best_initial_nodes = res_gp.x[1]\n",
    "best_dropout = res_gp.x[2]\n",
    "best_gru_layers = res_gp.x[3]\n",
    "best_gru_size = res_gp.x[4]\n",
    "best_dropout_g = res_gp.x[5]\n",
    "best_learning_rate = res_gp.x[6]\n",
    "best_batch_size = res_gp.x[7]\n",
    "\n",
    "bestconf = {\"hidden_layers\": best_hidden_layers,\n",
    "          \"initial_nodes\": best_initial_nodes,\n",
    "          \"dropout\": best_dropout,\n",
    "          \"gru_layers\": best_gru_layers,\n",
    "          \"gru_size\": best_gru_size,\n",
    "          \"dropout_g\": best_dropout_g,\n",
    "          \"learning_rate\": best_learning_rate,\n",
    "          \"batch_size\": best_batch_size}\n",
    "with open(config_file, 'w') as config:\n",
    "    json.dump(bestconf, config)\n",
    "    print(\"Save best configuration to {}\".format(config_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gru_layers': 2, 'learning_rate': 0.0012141768937927327, 'hidden_layers': 3, 'dropout_g': 0.9, 'gru_size': 500, 'dropout': 0.01, 'initial_nodes': 385, 'batch_size': 512}\n",
      "Loaded best configuration from BestConfigInclusiveKeras2017.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshome/thong/.local/lib/python3.5/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n",
      "/nfshome/thong/.local/lib/python3.5/site-packages/ipykernel_launcher.py:39: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 8, 7)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 8, 7)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 8, 500)       762000      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 8, 500)       0           gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 500)          1501500     masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500)          0           gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 9)            0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 509)          0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 385)          196350      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 385)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 192)          74112       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 192)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 96)           18528       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 96)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            97          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,552,587\n",
      "Trainable params: 2,552,587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Input data particle shape: (71196, 8, 7)\n",
      "Input data HLF shape: (71196, 9)\n",
      "Train on 71196 samples, validate on 35598 samples\n",
      "Epoch 1/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3894 - acc: 0.8359Epoch 00001: val_loss improved from inf to 0.33442, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 16s 220us/step - loss: 0.3894 - acc: 0.8359 - val_loss: 0.3344 - val_acc: 0.8643\n",
      "Epoch 2/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3473 - acc: 0.8564Epoch 00002: val_loss improved from 0.33442 to 0.32806, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.3473 - acc: 0.8564 - val_loss: 0.3281 - val_acc: 0.8671\n",
      "Epoch 3/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3409 - acc: 0.8580Epoch 00003: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 196us/step - loss: 0.3408 - acc: 0.8580 - val_loss: 0.3311 - val_acc: 0.8665\n",
      "Epoch 4/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3377 - acc: 0.8599Epoch 00004: val_loss improved from 0.32806 to 0.32045, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.3377 - acc: 0.8599 - val_loss: 0.3205 - val_acc: 0.8672\n",
      "Epoch 5/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3352 - acc: 0.8621Epoch 00005: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.3353 - acc: 0.8620 - val_loss: 0.3207 - val_acc: 0.8666\n",
      "Epoch 6/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3326 - acc: 0.8606Epoch 00006: val_loss improved from 0.32045 to 0.31792, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.3326 - acc: 0.8607 - val_loss: 0.3179 - val_acc: 0.8705\n",
      "Epoch 7/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3281 - acc: 0.8633Epoch 00007: val_loss improved from 0.31792 to 0.31733, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 197us/step - loss: 0.3283 - acc: 0.8633 - val_loss: 0.3173 - val_acc: 0.8682\n",
      "Epoch 8/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3302 - acc: 0.8622Epoch 00008: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.3301 - acc: 0.8622 - val_loss: 0.3227 - val_acc: 0.8715\n",
      "Epoch 9/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3261 - acc: 0.8659Epoch 00009: val_loss improved from 0.31733 to 0.31388, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 201us/step - loss: 0.3261 - acc: 0.8659 - val_loss: 0.3139 - val_acc: 0.8692\n",
      "Epoch 10/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3235 - acc: 0.8664Epoch 00010: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.3235 - acc: 0.8664 - val_loss: 0.3142 - val_acc: 0.8683\n",
      "Epoch 11/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3240 - acc: 0.8654Epoch 00011: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.3239 - acc: 0.8655 - val_loss: 0.3140 - val_acc: 0.8725\n",
      "Epoch 12/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3215 - acc: 0.8665Epoch 00012: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.3215 - acc: 0.8665 - val_loss: 0.3273 - val_acc: 0.8599\n",
      "Epoch 13/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3219 - acc: 0.8668Epoch 00013: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 200us/step - loss: 0.3218 - acc: 0.8668 - val_loss: 0.3184 - val_acc: 0.8715\n",
      "Epoch 14/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3198 - acc: 0.8670Epoch 00014: val_loss improved from 0.31388 to 0.31202, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.3197 - acc: 0.8670 - val_loss: 0.3120 - val_acc: 0.8719\n",
      "Epoch 15/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3206 - acc: 0.8673Epoch 00015: val_loss improved from 0.31202 to 0.31161, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.3205 - acc: 0.8673 - val_loss: 0.3116 - val_acc: 0.8739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3180 - acc: 0.8678Epoch 00016: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.3180 - acc: 0.8678 - val_loss: 0.3120 - val_acc: 0.8718\n",
      "Epoch 17/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3173 - acc: 0.8686Epoch 00017: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 197us/step - loss: 0.3173 - acc: 0.8686 - val_loss: 0.3273 - val_acc: 0.8681\n",
      "Epoch 18/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3177 - acc: 0.8680Epoch 00018: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 196us/step - loss: 0.3178 - acc: 0.8680 - val_loss: 0.3139 - val_acc: 0.8740\n",
      "Epoch 19/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.8679Epoch 00019: val_loss improved from 0.31161 to 0.30925, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 201us/step - loss: 0.3182 - acc: 0.8679 - val_loss: 0.3093 - val_acc: 0.8730\n",
      "Epoch 20/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3156 - acc: 0.8688Epoch 00020: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.3156 - acc: 0.8688 - val_loss: 0.3118 - val_acc: 0.8726\n",
      "Epoch 21/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.8674Epoch 00021: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.3168 - acc: 0.8674 - val_loss: 0.3136 - val_acc: 0.8734\n",
      "Epoch 22/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3143 - acc: 0.8696Epoch 00022: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.3142 - acc: 0.8696 - val_loss: 0.3095 - val_acc: 0.8728\n",
      "Epoch 23/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3120 - acc: 0.8701Epoch 00023: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.3119 - acc: 0.8701 - val_loss: 0.3129 - val_acc: 0.8745\n",
      "Epoch 24/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3140 - acc: 0.8690Epoch 00024: val_loss did not improve\n",
      "\n",
      "Epoch 00024: reducing learning rate to 0.0002428353764116764.\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.3140 - acc: 0.8690 - val_loss: 0.3160 - val_acc: 0.8752\n",
      "Epoch 25/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3062 - acc: 0.8729Epoch 00025: val_loss improved from 0.30925 to 0.30634, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 200us/step - loss: 0.3061 - acc: 0.8729 - val_loss: 0.3063 - val_acc: 0.8741\n",
      "Epoch 26/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3045 - acc: 0.8733Epoch 00026: val_loss improved from 0.30634 to 0.30582, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 200us/step - loss: 0.3045 - acc: 0.8734 - val_loss: 0.3058 - val_acc: 0.8751\n",
      "Epoch 27/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3023 - acc: 0.8735Epoch 00027: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.3023 - acc: 0.8735 - val_loss: 0.3062 - val_acc: 0.8756\n",
      "Epoch 28/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3008 - acc: 0.8746Epoch 00028: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 197us/step - loss: 0.3007 - acc: 0.8746 - val_loss: 0.3060 - val_acc: 0.8751\n",
      "Epoch 29/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3001 - acc: 0.8751Epoch 00029: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 197us/step - loss: 0.3001 - acc: 0.8750 - val_loss: 0.3085 - val_acc: 0.8747\n",
      "Epoch 30/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.3004 - acc: 0.8752Epoch 00030: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 196us/step - loss: 0.3004 - acc: 0.8752 - val_loss: 0.3060 - val_acc: 0.8740\n",
      "Epoch 31/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2992 - acc: 0.8752Epoch 00031: val_loss did not improve\n",
      "\n",
      "Epoch 00031: reducing learning rate to 4.856707528233528e-05.\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.2993 - acc: 0.8751 - val_loss: 0.3066 - val_acc: 0.8749\n",
      "Epoch 32/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2974 - acc: 0.8757Epoch 00032: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.2974 - acc: 0.8757 - val_loss: 0.3062 - val_acc: 0.8756\n",
      "Epoch 33/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2979 - acc: 0.8747Epoch 00033: val_loss improved from 0.30582 to 0.30561, saving model to tmp_best_allnode.h5\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.2979 - acc: 0.8747 - val_loss: 0.3056 - val_acc: 0.8747\n",
      "Epoch 34/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2965 - acc: 0.8764Epoch 00034: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.2965 - acc: 0.8764 - val_loss: 0.3060 - val_acc: 0.8743\n",
      "Epoch 35/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2972 - acc: 0.8754Epoch 00035: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 198us/step - loss: 0.2972 - acc: 0.8755 - val_loss: 0.3061 - val_acc: 0.8750\n",
      "Epoch 36/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.8763Epoch 00036: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 197us/step - loss: 0.2957 - acc: 0.8763 - val_loss: 0.3062 - val_acc: 0.8752\n",
      "Epoch 37/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.8752Epoch 00037: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.2966 - acc: 0.8752 - val_loss: 0.3060 - val_acc: 0.8749\n",
      "Epoch 38/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2960 - acc: 0.8761Epoch 00038: val_loss did not improve\n",
      "\n",
      "Epoch 00038: reducing learning rate to 9.713415056467057e-06.\n",
      "71196/71196 [==============================] - 14s 199us/step - loss: 0.2960 - acc: 0.8761 - val_loss: 0.3059 - val_acc: 0.8746\n",
      "Epoch 39/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2950 - acc: 0.8766Epoch 00039: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 200us/step - loss: 0.2950 - acc: 0.8766 - val_loss: 0.3061 - val_acc: 0.8751\n",
      "Epoch 40/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2945 - acc: 0.8762Epoch 00040: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 197us/step - loss: 0.2944 - acc: 0.8762 - val_loss: 0.3062 - val_acc: 0.8751\n",
      "Epoch 41/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.8765Epoch 00041: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 200us/step - loss: 0.2951 - acc: 0.8766 - val_loss: 0.3062 - val_acc: 0.8754\n",
      "Epoch 42/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2946 - acc: 0.8765Epoch 00042: val_loss did not improve\n",
      "\n",
      "Epoch 00042: reducing learning rate to 1.9426830476731994e-06.\n",
      "71196/71196 [==============================] - 14s 197us/step - loss: 0.2946 - acc: 0.8765 - val_loss: 0.3062 - val_acc: 0.8751\n",
      "Epoch 43/100\n",
      "71168/71196 [============================>.] - ETA: 0s - loss: 0.2952 - acc: 0.8764Epoch 00043: val_loss did not improve\n",
      "71196/71196 [==============================] - 14s 203us/step - loss: 0.2952 - acc: 0.8764 - val_loss: 0.3062 - val_acc: 0.8751\n",
      "Epoch 00043: early stopping\n"
     ]
    }
   ],
   "source": [
    "#with open(config_file) as f:\n",
    "#config_file = 'BestConfigInclusiveKeras2017.json'\n",
    "with open(config_file) as f:\n",
    "    bestconf = json.load(f)\n",
    "    print(bestconf)\n",
    "print(\"Loaded best configuration from {}\".format(config_file))\n",
    "model = build_inclusive_model(\"tmp\", num_hiddens=bestconf['hidden_layers'], initial_node=bestconf['initial_nodes'], \n",
    "                          dropout=bestconf['dropout'], gru_layers=bestconf['gru_layers'], \n",
    "                          gru_size=bestconf['gru_size'], dropout_g=bestconf['dropout_g'])\n",
    "model.compile(optimizer=optimizers.Adam(lr=bestconf['learning_rate']), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "for train_index, test_index in skf.split(data_hlf, label):\n",
    "    train_data = [data_list[train_index].astype(np.float32), data_hlf[train_index].astype(np.float32)], label[train_index].astype(np.float32)\n",
    "    test_data = [data_list[test_index].astype(np.float32), data_hlf[test_index].astype(np.float32)], label[test_index].astype(np.float32)\n",
    "    \n",
    "    train_weight = weight[train_index].astype(np.float32)*allnode_weight[train_index].astype(np.float32)\n",
    "    test_weight = weight[test_index].astype(np.float32)*allnode_weight[test_index].astype(np.float32)\n",
    "    \n",
    "    best_acc = train(EPOCHS, model, \n",
    "                     batch_size=bestconf['batch_size'], \n",
    "                     train_data=train_data, val_data=test_data,\n",
    "                     train_weight=train_weight,\n",
    "                     val_weight=test_weight)\n",
    "    break\n",
    "model.save(\"Keras2017_allnode.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Keras2017_allnode.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 8, 7)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 8, 7)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 8, 500)       762000      masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, 8, 500)       0           gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, 500)          1501500     masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500)          0           gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 9)            0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 509)          0           dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 385)          196350      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 385)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 192)          74112       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 192)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 96)           18528       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 96)           0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            97          dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,552,587\n",
      "Trainable params: 2,552,587\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.34132378, -0.15389732, -0.57296987,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.76263941, -0.30212   ,  1.34049123,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.49275546,  0.53356723, -0.83245077,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.79778174,  0.        ,  1.60887642,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred = model.predict(x=[data_list[0].reshape(1,8,7),data_hlf[0].reshape(1,9)])\n",
    "# print(pred)\n",
    "data_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1730  |       0.9602      |    0.2721 +/- 0.0116     |\n",
      "|   0.2667  |       0.9352      |    0.2031 +/- 0.0067     |\n",
      "|   0.3795  |       0.9002      |    0.1468 +/- 0.0053     |\n",
      "|   0.7752  |       0.7001      |    0.0400 +/- 0.0022     |\n",
      "|   0.9062  |       0.5001      |    0.0133 +/- 0.0009     |\n",
      "|   0.9659  |       0.3001      |    0.0030 +/- 0.0005     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f2d7c6550b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d7c6bf6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d5c7e0128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_list = np.load(\"data_list_2017_allnode.npy\")\n",
    "# data_hlf = np.load(\"data_hlf_2017_allnode.npy\")\n",
    "# label = np.load(\"label_2017_allnode.npy\")\n",
    "# skf = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "# skf.get_n_splits(data_hlf, label)\n",
    "def fill_array(array_to_fill, value, index, batch_size):\n",
    "    array_to_fill[index*batch_size:min((index+1)*batch_size, array_to_fill.shape[0]),...] = value  \n",
    "\n",
    "TPR_thresholds = [0.96, 0.935, 0.9, 0.7, 0.5, 0.3]\n",
    "\n",
    "fprs = []\n",
    "base_tpr = np.linspace(0, 1, 5000)\n",
    "thresholds = []\n",
    "volatile=True\n",
    "best_batch_size = 1000\n",
    "for train_index, test_index in skf.split(data_hlf, label):\n",
    "    train_data = [data_list[train_index].astype(np.float32), data_hlf[train_index].astype(np.float32)], label[train_index].astype(np.float32)\n",
    "    test_data = [data_list[test_index].astype(np.float32), data_hlf[test_index].astype(np.float32)], label[test_index].astype(np.float32)\n",
    "    train_weight = weight[train_index].astype(np.float32)*allnode_weight[train_index].astype(np.float32)\n",
    "    test_weight = weight[test_index].astype(np.float32) *allnode_weight[test_index].astype(np.float32)\n",
    "    test_label = label[test_index]\n",
    "    pred = model.predict(x=[test_data[0][0],test_data[0][1]])\n",
    "    \n",
    "    fpr, tpr, threshold = roc_curve(test_data[1], pred, sample_weight=test_weight)\n",
    "    fpr = np.interp(base_tpr, tpr, fpr)\n",
    "    threshold = np.interp(base_tpr, tpr, threshold)\n",
    "    fpr[0] = 0.0\n",
    "    fprs.append(fpr)\n",
    "    thresholds.append(threshold)\n",
    "\n",
    "thresholds = np.array(thresholds)\n",
    "mean_thresholds = thresholds.mean(axis=0)\n",
    "\n",
    "fprs = np.array(fprs)\n",
    "mean_fprs = fprs.mean(axis=0)\n",
    "std_fprs = fprs.std(axis=0)\n",
    "fprs_right = np.minimum(mean_fprs + std_fprs, 1)\n",
    "fprs_left = np.maximum(mean_fprs - std_fprs,0)\n",
    "\n",
    "mean_area = auc(mean_fprs, base_tpr, reorder=True)\n",
    "\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(base_tpr>TPR_threshold)\n",
    "    NNtable.add_row([mean_thresholds[thres_idx], base_tpr[thres_idx], \"{:.4f} +/- {:.4f}\".format(mean_fprs[thres_idx], std_fprs[thres_idx])])\n",
    "print(NNtable)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(mean_fprs, base_tpr,label=\"AUC = {:.4f}\".format(mean_area), color='deepskyblue')\n",
    "plt.fill_betweenx(base_tpr, fprs_left, fprs_right, color='deepskyblue', alpha=0.4)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel('Background contamination', fontsize=16)\n",
    "plt.ylabel('Signal efficiency', fontsize=16)\n",
    "plt.title('2017', fontsize=20)\n",
    "#plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "#plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist(pred[test_data[1]==0], weights=test_weight[test_data[1]==0], bins=60, label='ttH background',alpha=0.5, normed=True)\n",
    "plt.hist(pred[test_data[1]==1], weights=test_weight[test_data[1]==1], bins=60, label='HH signal', alpha=0.5, normed=True)\n",
    "#plt.axvline(thresholds[thres_idx], ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Threshold\", fontsize=18)\n",
    "\n",
    "# with h5py.File(\"ReallyInclusive_ROC.h5\",\"w\") as out:\n",
    "#     out['FPR'] = mean_fprs\n",
    "#     out['dFPR'] = std_fprs\n",
    "#     out['TPR'] = base_tpr\n",
    "#     out['Thresholds'] = mean_thresholds\n",
    "#     print(\"Saved ROC.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check on only SM node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1632  |       0.9602      |    0.2806 +/- 0.0041     |\n",
      "|   0.2537  |       0.9352      |    0.2111 +/- 0.0011     |\n",
      "|   0.3635  |       0.9002      |    0.1536 +/- 0.0010     |\n",
      "|   0.7562  |       0.7001      |    0.0442 +/- 0.0023     |\n",
      "|   0.8939  |       0.5001      |    0.0158 +/- 0.0016     |\n",
      "|   0.9587  |       0.3001      |    0.0041 +/- 0.0004     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fd891a22940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAHDCAYAAADV3Ut7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAIABJREFUeJzs3XmcHWWd7/HPr/fO1tkTskDYBcERiTAsKgyggCMoggOO7CMIMoyi3HG/XBwHcbnOVRAEZVHZFZkwIpvgKCMgURTZCSFACJB933p57h9VHTpN9+mTpPssnc/79TqvOqeqTp1fipj6+tTzPBUpJSRJkqpNTbkLkCRJ2hyGGEmSVJUMMZIkqSoZYiRJUlUyxEiSpKpkiJEkSVXJECNJkqqSIUZSv4mIMRHxTxHxi4iYFRFrImJZRDwQEadHRI//5kTE/hFxR0Qszr/zWER8KiJqe9h3ZEScHxHXRcSTEdEWESkiDi1QVyridWJ/ngtJAy+c7E5Sf4mITwCXAa8C9wMvAROAY4AW4OfAcanLPzwRcXS+fi1wE7AY+ACwK/CzlNJx3X7j7cCj+ce5QH3+G4ellO7tpa4Leil5OHAe0AZMTSm9tml/YknlZIiR1G8i4u+AocAvU0odXdZPBP4ATAWOTSn9PF8/AphFFnAOSCnNzNc3AfcB+wEnpJRu7HKsUcA7gEdTSosj4hrgZAqEmAL1nglcDvwipXTM5v2pJZWLt5Mk9ZuU0n0ppdu7Bph8/WtkYQHgoC6bjgXGATd2Bph8/7XAl/KPZ3U71pKU0q9TSov7oeQz8uUP+uFYkkrMECOpVFrzZVuXdX+XL+/sYf/fAquB/SOisb+LiYi9yVp05gB39/fxJQ08Q4ykARcRdcBJ+ceugWXXfPls9++klNqAF4A6YIcBKKuzFebK5H11qSoZYiSVwteBPYA7Ukp3dVnfki+X9fK9zvUj+7OYiBgGnEDWKnRVfx5bUukYYiQNqIg4F/gM8DRQKcOYTyAbmTTDEUlS9TLESBowEXEO8P+AJ4GDe+iM29nS0kLPOtcv7efSOm8lXdHPx5VUQoYYSQMiIj4FfA94nCzA9NTi8Uy+3KWH79cB25Pd8pndj3W9HZhO1t/GDr1SFTPESOp3EfGvwHeAP5MFmPm97Hpfvjy8h23vBoYAv08prevH8s7Mlz+0Q69U3QwxkvpVRHyZrCPvH4FDUkoLC+z+M2AhcHxETO9yjCbg3/KPl/VjbUOBj2KHXmlQcMZeSf0mIk4GrgHayW4l9TTqaE5K6Zou3/kgWZhZC9xI9tiBo8gfOwB8pHuLSUR8CxibfzwQ2JHs1tCr+brbUkq39VDf6cAPgVtTSh/erD+kpIpRV+4CJA0q2+fLWuBTvezz32RBB4CU0m0R8R7gi8CHgSayRxGcB3y3l1s+xwLbdVv33i7v5wBvCjHYoVcaVGyJkSRJVck+MZIkqSoZYiRJUlUyxEiSpKpkiJEkSVVpUIxOGjt2bJo2bVq5y5AkSf3gj3/848KU0ri+9hsUIWbatGnMnDmz3GVIkqR+EBEvFrOft5MkSVJVMsRIkqSqZIiRJElVyRAjSZKqkiFGkiRVJUOMJEmqSoYYSZJUlQwxkiSpKhliJElSVTLESJKkqmSIkSRJVckQI0mSqlJJQ0xEXBUR8yPi8V62R0R8NyJmRcRjEfGOUtYnSZKqR6lbYq4BDi+w/Qhg5/x1BnBZCWqSJElVqK6UP5ZS+m1ETCuwy9HAj1NKCXgoIkZGxDYppVdLUqAkqeRSgkT+yt93dFvf0XWf7p9Tvn8P33/Tb/X0+8Wu68fjVXJtm3K8nZqhubaHDSVS0hBThMnAy10+z83XGWIkVaSUoC3Bug5Y2wHr8vcbPndZ1/l5fYL1HdDaZdn1/foErfl+rV3f58fq/Ny5rvMCvuHCnl/UewoCXS/2vV34uwcFumzr6PzcLVR03dZTyOi+b9fPql4P7AUHtJTv9ystxBQtIs4gu+XEtttuW+ZqJJVDSlkw6B4W1vXweUU7LGvbeN+17bCmI3utzZe9fX99enMI6Xzf3xfiWqAmoDay97XR5dXlc11kfQIi3z/y70f+fsPnznWR/aPf+Znu27t+7uFYnd+Jbi/ijb4Jb/rN/HNNL+u7HmOjYxY6dvRQQ9f1PRyrq+jlfaH9Cq3s6zf6PH7wpr9Enec6pY3/fEUdr4+Vm1tvT/tNaijyiwOk0kLMK8DULp+n5OveJKV0BXAFwPTp0w3zUpVa2w5L2rq8Wt94vzRfLmqFxfn6ZW2wvD0LJSvaoH0Lf7+WLAzU5cGgvktA6PqqDRhWCyNr8+01Xbax8b71na8aaMyX9QHNNTCkNl/WwPA6aKmDpoCmWmisgXqgtuaNIFMDNNS8cczOcBNk2zr36ayx83NfFzxpMKi0EDMDOCcibgT2BZbZH0aqbB0pCxsLW7PXotY33i9ug+V56FiWB5Cl+XJFO6xsz1oyCmkIaKp549VcA6Prsv8H2FyTXfhruwSQhi4Boj6y7c01MLQWxtdnoWFEbRZIhtZm+zXVbBw8OgPEhmDAGy0dnQGh1pAglV1JQ0xE3AAcBIyNiLnA/yb7Px6klC4H7gCOBGYBq4FTS1mftLVLKQscC1s3fi1YD6+tz993CSudrSUdvRyvljfCR2PnK7IgMb4hez+k9o19htZk4WJELUxqhKmNMKExa7Xo3GdEXd5ikbc6SNp6lXp00gl9bE/AJ0tUjrTV6Awnc9fCc2vghbXw8jqYm7/mr8/CyNICt2dqyFo0mmuzUDG0BrZvylozhuWv4XkA2aYRJjfAdk15y0ddFkDq8paSDbdenG5T0haotNtJkjbR2naYtx7mrcuCyew18OI6eGVdtu71vNWk+22bIAsiQ/OWkDH1MLkxCyNDaqGlBlrqYZsGeEtztm1E3odjSJfbMd5WkVQuhhipgqWU3b6ZtQaeWQ0vrX0joLyaB5dFbW/+Xi1Z0BiSh5TdhsDIuuw1ojYLIhMbYEJDtm54XRZoWupgdH3WWjI0DzOSVKkMMVIFWNsOf10Ff1oJT6yCp1bB82uzkLKuWwtK5yiX5poscGzfDC21WRgZlYeTHZtgeH12e6elNrsFNDQfFTOyznAiaXAwxEgl0tqR9T95eR08tRoeXZEFlhfWZp1mu/ZFGVYLY+vgrUOzIb1j62FUfRZSdmjO+qK01GVDbxvjjT4nw2qzdZK0NTDESP2s8xbQn1bAb5fB06uzlpXZazfulxJkt3ZG18P04Vnfk7H1WUvJ+Ibs84T8NTzvNDu63hE5ktTJECNtgZVt8NgqmLkCHlmRBZbnVsOyLs0qQ/K+JrsOgXH1WVAZWwdvH5YNH26ueaOfSudInqHe7pGkPhlipCKkBC+ty27//Hkl/GF5tnxp3RuzhTdG1ooyqRHekY/q+Zu8hWXbRti2KQssnaN/bFGRpC1jiJG6SSnrs/KXlVkLy0PLs/eruszo1pLf2tl3OExpgh2a4B3Dsv4qY+qzFpdhtU79LkkDyRCjrV5HgidXwX1L4c7FWWhZkg9briULJdOasmnupzbBO4fBzkOyCd3G5Z1tnbRNkkrPEKOtTmsHPLwcfrUYfrcM/rgCVuetLENrsts/04dnrSt7DIW3Dcumvx/fYF8VSaokhhhtFV5fD79aBLcvgnuWZA8frCGfjXZIFlL2Hg5vH5rNuzK5MRvSLEmqXIYYDUrz1sHdi+HhFfC7pfDE6mz98FrYtTm7HbTvcDhmXHY7aJj/S5CkquM/3RoU1nfAA8uyPi2/WgyPr8rWN0Y2Wug9LdmtoaPGwD4jYKStLJJU9Qwxqlor2+D3y+FnC+DG+dktolqyEUJHj4Hdh8KHx2YdcFuce0WSBh1DjKpGStnEcrcvhLuXZKOIWlP2sMI9hsKeQ+GgkfDe0VnriyRpcDPEqKJ1pGyulp+8DrctzJ49FMDkBjhwBOzUDPuOgCPGGFwkaWtjiFHFSQn+sAJumZ8Fl+fXQl3A7kNgv+Gw9zA4YWI2osjJ5CRp62WIUcVY3gbfnQtXvppN518L7NgMx43NOuOeMhHGNpS7SklSpTDEqOyeWw0XvwTXz4c1HTCtEf5+NBzYAv84IbtN5HOGJEndGWJUNg8ug2++DDMWZiFl12Z4Zz53y7tGZiOKJEnqjZcJlVR7glsXwHfmwoPLYUhNFlzeOwpO2iZ72nO9zyGSJBXBEKOSWNkG183P+rw8uRpG18GRo7PJ544eCxMdWSRJ2kSGGA2o1g64fB58ZQ4sbcueVXTCOPjIeDhyDDTY6iJJ2kyGGA2IlW1w7evwjZeykUZTGuADE+D9o+Hvxzp7riRpyxli1K+eXgXfnwc/fDUbaTSxHo4dBx8bD+8bDU2GF0lSPzHEqF88vwYueCHr9xJk87vsNwI+OBYOGQUj/JsmSepnXlq0RWavgW+9nE1QB/C2oXDYqGx+l7cMseVFkjRwDDHaLC+sga+8ADfMhwS8dWg20ujEibBdo+FFkjTwDDHaJCvasj4vF8yBtpQ9PfrQkfCPE+Ftw6DWmXUlSSViiFFRUsoeC/Avz8GiNti+CY4fBydNzJ4kXedQaUlSiRli1KenV8FZz8FvlsLkBjhzGzhxAuzX4jONJEnlY4hRr1a3w7+/CBe/DHUBh43Mh0tPhCH2eZEklZkhRj16eS287zF4anX2YMa/HwOnTIQ9hpW7MkmSMoYYvck9i+EjT2YtMYeNhE9MhiNGQ7OtL5KkCmKI0QYdCf7txWzk0eg6+Mg4+Kdt4F0j7fsiSao8hhgBsKQVTngS7loCOzTBsWOzkUdv9faRJKlCGWLEn1fA0Y/DK+tg/xFwygT48HgYXV/uyiRJ6p0hZiv315VwwKPZ7aITxsMZk+AAh05LkqqAIWYrdt8S+PDj2QMb/2kifGkajLH1RZJUJZxndSt1z2I44rHsMQGnTIRPTTXASJKqiy0xW6E7FsIHn4ARtXDqRPjcdgYYSVL1sSVmK3PbgmwOmBG12eijT9sCI0mqUoaYrch358IxT8DQWvjYBDhvCkxqLHdVkiRtHm8nbSW++RL8r9nZHDAnT4Bzp8BIW2AkSVXMELMV+Lc58OU5sH0TfH7b7DZSg21wkqQqZ4gZ5P7j5SzA7NoMX9gWPjLeACNJGhwMMYPY9a/Dec/Ddo1w1iQ4foIBRpI0eHhJG6RumQ8nPQWTGuBTk+HjkwwwkqTBxZaYQeiuxfCPT8E2jXD2JDhzMjTXlrsqSZL6lyFmkPndUvjQ49ncL/88KZsHpt4WGEnSIOTlbRB5ZR0c9wQMq4VPTsoeJWCAkSQNVl7iBom17XD4X2BxG3xsPHx+O/vASJIGNy9zg0BKcOrT8Phq+MBo+F/bZg92lCRpMDPEDAJXvgo3LoD9RmQPc5zoowQkSVsBQ0yVe241fGoWTGuCL24L7xxR7ookSSoNQ0wVa+2Ajz6VvT9+HBwxprz1SJJUSoaYKvbZ52HmCnj/aDhvKtTYD0aStBUxxFSp2xbAd1+BPYfCWZNhXEO5K5IkqbQMMVXohdVw4lMwrh6OGgPvbil3RZIklZ4hpgqd/gysS/D3Y+CYsVDnf0VJ0lbIy1+VueF1uH8ZHNgCewyBvYaXuyJJksqj5CEmIg6PiGciYlZEfK6H7dtGxP0R8WhEPBYRR5a6xkq1sg0++Vx2G+ltQ+GosRB25pUkbaVKGmIioha4FDgC2B04ISJ277bbl4CbU0p7AccD3y9ljZXsvOdhSRscNgo+MAZ2GlLuiiRJKp9St8TsA8xKKc1OKa0HbgSO7rZPAjqnbGsB5pWwvor16Aq46lXYfQgc0AIHjyp3RZIklVepQ8xk4OUun+fm67q6APhYRMwF7gD+uacDRcQZETEzImYuWLBgIGqtGB0JTn46e6Dje0fBB8c6J4wkSZXYsfcE4JqU0hTgSOAnEfGmOlNKV6SUpqeUpo8bN67kRZbSJa/AX1fBO4fDh8fBJJ+NJElSyUPMK8DULp+n5Ou6Oh24GSCl9CDQBIwtSXUVaN5a+MJsmNKQtcLs47ORJEkCSh9iHgF2jojtI6KBrOPujG77vAQcAhARu5GFmMF9v6iATz4HazrgfaPhPSOzW0qSJKnEISal1AacA9wFPEU2CumJiLgwIo7Kd/sM8PGI+AtwA3BKSimVss5K8fo6+NVi+JthMLY+W0qSpExdqX8wpXQHWYfdruu+0uX9k8ABpa6rEn3tpWxm3r2HwSGjYHjJ/2tJklS5vDlRoRa3ZkOqd2mGCQ2wr31hJEnaiCGmQp3/PKzqgP1HwK5DYIStMJIkbcQQU4GeXQ3XvpY9WmBaczYvjCRJ2pghpgKd/Ww2md27WrInVdsXRpKkNzPEVJj7lsCvl8K7W2BMPbzV5yNJktQjQ0wFSQk+PQsaIhtOvf8IaKotd1WSJFUmQ0wF+cE8eGwVHDkammockSRJUiGGmAqxvgMufBFG1cGOzXDwSBhZX+6qJEmqXIaYCvGNl+DV9XDEaJjUkD1iQJIk9c4QUwFWtMJ35sKURpjcAEeNhXr/y0iSVJCXygrwr7NhcRscMhJ2aIadHJEkSVKfDDFl1toBtyyEHZtgciN8wIntJEkqiiGmzH70KixszUYiDa/N+sNIkqS+GWLKqKMDvvEytNRmrTCHjIKIclclSVJ1MMSU0S8Wwgtr4bBRMLIO3j6s3BVJklQ9DDFldOvCbDmpAfYd7ogkSZI2hZfNMlnTBncuhp2aYGgd7OKIJEmSNokhpkzuX5YNq37HcBhSk/WJkSRJxTPElMmV8yCAbRpg/xaosUOvJEmbxBBTBu0dcN9S2H1I1gpjh15JkjadIaYM/rgClrfDtCaYPgJG+6BHSZI2mSGmDH70WnYraYdm2NUOvZIkbRZDTImlBDMWZq0wLbWwfVO5K5IkqToZYkrskRXwWivs0gwHtMCQ2nJXJElSdTLElNgN87Plzs3wFm8lSZK02QwxJXbHIpjYACPqYBvnhpEkabMZYkrojyvg2TWwxxDYthEaPfuSJG02L6Ml9P1XsuVuQ2DPoeWtRZKkameIKaHfLYOJ9dmzkiZ6K0mSpC1iiCmR19fB7DXZvDBNNTDVECNJ0hYxxJTI9fOhnWx+mLcMgXrPvCRJW8RLaYncsgCaa2BCfTZHjCRJ2jKGmBJY1QYzV8DbhkJTrfPDSJLUH4oKMRGxy0AXMpjdthBaU/aIgXcOh2Zn6ZUkaYsV2xLzdET8OiKOi4i6Aa1oELp1IdQCkxphJ28lSZLUL4oNMacBzcBNwNyI+PeI2H7gyhpcfr8se2J1U022lCRJW66oEJNSuialtD/wduDnwNnAcxFxZ0QcHRH2renF7DXZAx+3a8wmuGvwTEmS1C826ZKaUnospfRJYBJwJjABuBV4KSIuiIgJA1BjVfvPhdlyahO8c0R5a5EkaTDZ3HaBacDb8uV64HHgPGBWRHyoXyobJP5rUTa0emI9TG4odzWSJA0eRYeYiGiIiH+MiN8CfwU+AHwdmJpSOhzYDrgT+L8DUmkVau/IhlZv3wS75cOrJUlS/yhqpFFEfBs4CRgF3AUcBdyRUkqd+6SUlkTE/wN+OxCFVqM/LIfl7dkTq/caVu5qJEkaXIodLn0icBVweUrphQL7PQ2cusVVDRK35v1hdmiGMfXlrUWSpMGm2BAzJaW0vq+dUkoLgWu3rKTB47+Xwag6GFkHYw0xkiT1q2L7xLwjIj7S04Z8Arx9+7GmQaGtAx5bCTs0ZQ999IGPkiT1r2IvrV8H3trLtt2Ai/qnnMHj7iWwLsF2TT4rSZKkgVBsiHkb8FAv2/6Qb1cXt+X9YbZtzFpiJElS/yo2xDQV2LcWGNo/5Qwev10KkxpgdD1s4/wwkiT1u2JDzFNkw6p7chTwTP+UMzis74AX1sKU/IGPdfaHkSSp3xU7Ouly4AcRsRy4EpgLTAbOAE4ne5aScncvhvUJpjZmz0ySJEn9r6gQk1K6MiJ2BT5N9niBDZuA76SUrhiI4qrV3Uuy5ZRGmGiIkSRpQBTbEkNK6bMRcRlwKDAGWAjcm1KaPVDFVauHl8OYOhhaCxPtDyNJ0oAoOsQApJSeB54foFoGhZTguTVZK8y4ehji85IkSRoQmxRiImIisC3ZaKWNpJR8ZhIwdx0saYN9hvuoAUmSBlKxD4CcDPwEeE/nqnyZ8veJbKj1Vu8Py7NlS13WEiNJkgZGsS0xlwF7Av8L+CuwbsAqqnKPrMiWo2t9XpIkSQOp2BDzLuDclNJPBrKYweCh5TCkBkbWwwQ79UqSNGCKnYZtDTB/IAsZDLp26o3wdpIkSQOp2BBzJXDiQBYyGCxohdfXZ48bGF8PTfYSkiRpwBR7O+kV4MSI+DXwK2Bx9x1SSlf1Z2HV6NEV0E42KsmHPkqSNLA25bEDANOAg3vYngBDzMpsObLO/jCSJA20YkPM9v31gxFxOPD/yIZk/zCl9PUe9vkIcAFZOPpLSumj/fX7A+lPK7P7c+PqYbKPG5AkaUAV++ykF/vjxyKiFrgUOIzsIZKPRMSMlNKTXfbZGfg8cEBKaUlEjO+P3y6Fv6zMHjMQAaM3aRpBSZK0qYrt2AtARLwtIs6JiP+dz95LROwUEcOLPMQ+wKyU0uyU0nrgRuDobvt8HLg0pbQEIKVUFaOiVrfDvLxT76g6GGaIkSRpQBU7Y28j8FPgGN6Yofd24DXgG8CzwOeKONRk4OUun+cC+3bbZ5f8N/+H7JbTBSmlO3uo6QzgDIBtt922mD/GgJq9Bla2ZxPcObRakqSBV2xLzNfInl59IjCBNx47ANlopff1Y011wM7AQcAJwJURMbL7TimlK1JK01NK08eNG9ePP795ZuYz9bbUwduGlrcWSZK2BsWGmBOAL6WUrufNw6tfIBu1VIxXgKldPk/J13U1F5iRUmpNKb1A1sqzc5HHL5s/5SOTxtdDiy0xkiQNuGJDzBjgqQLHKHYsziPAzhGxfUQ0AMcDM7rtcxtZKwwRMZbs9tLsIo9fNo+vgsaAYbXZUpIkDaxiQ8wLwH69bNsHeKaYg6SU2oBzgLvIQtHNKaUnIuLCiDgq3+0uYFFEPAncD5yfUlpUZJ1lkVLWJ2ZyI6SA0bbESJI04IodQ/Nj4AsRMQf4eb4uRcTBwKfJ5nQpSkrpDuCObuu+0uV9As7LX1VhRRvMb4W9hmUtMUN83IAkSQOu2JaYbwC/BH4CLMnXPQDcC9yZUvreANRWNZ5bA2s6slFJY22FkSSpJIqd7K4dOD4iLiUbiTQeWEQWYP57AOurCn9Yni1bamEbHzcgSVJJbNKUbCml3wG/G6BaqtZfVmXLEbXZZHeSJGngbdKMverZnLXZiRxelz38UZIkDbxeQ0xEtEfEPvn7jvxzb6+20pVceRa0ZuGlJrIgI0mSBl6hS+6FZBPPdb5PA19O9elI8Or6bJK7BAyxbUuSpJLoNcSklP5Pl/cXlKSaKrSkFRa1wi7N0FxjS4wkSaVSVLtBRNRHRI9PBIqIoRGx1Q4snrsO1qdsfpgpxc5bLEmStlix7QY/yvf9aA/bfgCsB07rr6Kqybz12bKpBnYdUt5aJEnamhTbg+Mg4D972TYDOKRfqqlCc9Zmy+G1WWuMJEkqjWJDzHhgfi/bFgAT+qec6vNkPkfMyLosyEiSpNIoNsTMB/bsZdueZLP3bpVeXJfdSmqugVFbbc8gSZJKr9gQ81/AlyPibV1XRsSewBeB2/u7sGrx4loYXQf1NdDiyCRJkkqm2MvuV4DDgD9GxCNk88dMBvYBXgC+NDDlVb5562GCD36UJKnkimqJSSktBN4JXAQE8PZ8+TXgnfn2rc669myemDH1MMpWGEmSSqroS29KaSlZi8xXBq6c6vLnldBOdhvJSe4kSSotJ8nfAk+vzpYjnOhOkqSS67X9ICLuA85OKT2dvy8kpZS2urli5qzLliPqoMXh1ZIklVShmyDR5X0NhR8AGQW2DVrPrs5OzPDarF+MJEkqnUIh5mhgBUBK6aCSVFNlFrdm88PURzbZnSRJKp1CfWKWkI1IIiLui4i3lKak6rGkLQsxDTUQW2VblCRJ5VMoxKwHOm+SHASMGPBqqsyC1ux5SaNthZEkqeQKXX6fA74QEbfkn48s1BqTUvpxv1ZWBRa0wg5NsE1DuSuRJGnrUyjEfBH4KXAEWafeQvPDJGCrCjFLWmFFO4yud44YSZLKodfLb0rp9ogYDUwhe7TAccCfS1VYpZu5IluOrIMxhhhJkkqu0Dwx5wI3ppRejIhrgT+klF4uXWmVbV4+R8zwGhjp8GpJkkquUMfe7wDT8vcnAdsMeDVVZEFrthxaB0Od91iSpJIrdPldCkzM3weFJ7vb6sxeky2H5kOsJUlSaRXqzfE/wLUR8Zf882URsbyXfbe6xw7MXpsNr26uzR4AKUmSSqtQG8LHgRuADrJWmDqyeWN6em11g4znrc8e/DimHmqd6E6SpJIrNDrpdeBsgIjoAM5IKf2hVIVVukX5RHeOTJIkqTyKvQRvD7w6kIVUm6VtMKHBZyZJklQuRXVJTSm9CLRGxFER8a2IuDoitgOIiPdExKQBrbLCrGqH1R0+vVqSpHIqqh0hIkYBdwD7kj3ZehjwPeBFsr4zi4FzB6jGivPS2mw5si6bsVeSJJVesYODvwlMBQ4AxpANue50L7BVjUx6ZnW2bKmFIQ6vliSpLIrt0XE08NmU0oMRUdtt20tkAWer8Xw+R8zw2uwlSZJKr9h2hGHAK71sa2LjlplB78X8kQNDarMRSpIkqfSKDTHPAO/tZdt7gL/2TznV4dnVWQvMiDqo83aSJEllUeztpO8Dl0TEMuD6fN3IiDgVOAc4YyCKq1QLWmFoLUzc6qb4kySpchQVYlJKV0TEDsD/AS7MV99DNpvvN1JK1w1QfRVpfmv2zCT7w0iSVD5FT9WWUvpcRFwGHAaMBxYB96SUZg9UcZVqUSvs0gzb2BIjSVLZbNJ8s/mkdz8coFqqwup2WJNPdOccMZIklY/dUjfR06uy5Zh6bydJklROhphN9FQ+0d2YOhjn7SRJksrGELOJnu4MMQ3ZCCVJklQehphN9NwaaK6B7ZvKXYkkSVs3Q8wmWtgKjTXOESNJUrnMDiOTAAAgAElEQVQZYjbRwtasJWaEt5IkSSqrXodYR0QHkIo8TkopbdJw7WrV2RIzaqv400qSVLkKXYovpPgQs9VY1g5T62CoIUaSpLLq9VKcUrqghHVUhfYEK9thSE322AFJklQ+Xoo3wYq2bNlcC/WeOUmSyqromyIR0QAcAewKdB9gnFJKX+3PwirR4s4QE9AQ5a1FkqStXVEhJiImAQ8A08j6yXRewrv2mRn0Iea19dmypQ4abImRJKmsir0UfxNYAGxLFmD2BXYAvgbMyt8Pep0hZmSds/VKklRuxd5OehfwWWBe/rkjpTQH+EpE1ALfBY7u//Iqy+t5iNnO2XolSSq7YltixgDzUkodwCpgVJdt9wEH9XNdFamzT8ykxvLWIUmSig8xc4Gx+fvngfd22bYPsLY/i6pUC9ZDLTDOOWIkSSq7Yi/H9wPvAW4DfgBcGhFvB1qB9+XrBr0X18KwWhhifxhJksqu2BDzJWA0QErpsoioA/4BGAJ8g2x230Hv5XUwvBYmejtJkqSyK+p2UkppYUrp2S6fv5dSOjCl9I6U0hdSSkXfToqIwyPimYiYFRGfK7DfhyMiRcT0Yo890Ba0Zq0ww22JkSSp7Eo620k+kulSsknzdgdOiIjde9hvOPAvwMOlrK8vS9qgqcaJ7iRJqgSbMmPve4ATyOaK6WnG3kOKOMw+wKyU0uz8mDeSDc1+stt+XwUuBs4vtr6BtrYdVrRnz0xqdKI7SZLKrqjLcUScSda591hgJNmEd11fxV7WJwMvd/k8N1/X9bfeAUxNKf2yj5rOiIiZETFzwYIFRf785us6W2+zIUaSpLIrtiXmM8D1wGkppfUDVUxE1AD/Fzilr31TSlcAVwBMnz499bH7FluUzxEzrBaa7BMjSVLZbUoLytX9EGBeAaZ2+TwlX9dpOLAH8JuImAP8LTCjEjr3Lsj/5HbqlSSpMhQbYv5I/zwf6RFg54jYPn8q9vHAjM6NKaVlKaWxKaVpKaVpwEPAUSmlmf3w21tkad4SM9yJ7iRJqgjFhphzgU9FxLu35MdSSm3AOcBdwFPAzSmlJyLiwog4akuOPdAWtmbLMfXlrUOSJGWKbVe4HRgB3B8Rq4El3banlNJ2xRwopXQHcEe3dV/pZd+DiqxvwC3PW2JavJ0kSVJFKDbE/BoY8M6zlWx5e7a0T4wkSZWhqBCTUjplgOuoeMvaoC4cmSRJUqVwxpMiLWvPZuod6hmTJKkiFNUSExEnFdjcASwDHk0pze2XqirQ8rwlptmWGEmSKkKxfWKu4Y0+MV2fHNR1XUdE3AScOpAT4pXLkjZoqLFPjCRJlaLYmyMHAC8ClwDvAd6SL78PvAS8H/gc8CHggn6vsgIsbsseNzDUECNJUkUotiXms8CNKaUvdFn3LPC7iFgBnJFS+lBEtAD/CHyhp4NUs2Vt2SMHfG6SJEmVodhL8nvJhln35D6g8wnWv6XbAx0HiyVtMKQmCzKSJKn8ig0x64C9e9m2N9DZB6YGWLWlRVWate2wtiO7ldRoS4wkSRWh2NtJtwD/JyLagZ8B84HxwHFkfWCuyvd7O/BMP9dYdp2PHBhW4+0kSZIqRbEh5jyyJ0x/I391dT3wmfz948CD/VNa5ViZz9bbZEuMJEkVo9gZe9cAH4uIC4F9gW2AV4E/pJSe6bLfLwekyjLrDDHNNVAThfeVJEmlUWxLDAAppWfJRiVtVVZ1ZEs79UqSVDl6DTERsS3wakqpNX9fUErppX6trIKsz0NMva0wkiRVjEItMS8A+wF/AObQ91OsB207xfr8T95kfxhJkipGoRBzGvB8l/d9hZhBq7Mlxk69kiRVjl5DTErp2i7vrylJNRWqsyVmiCFGkqSKsVmX5YhoiYjpETGlvwuqROvylhjniJEkqXL0elmOiPdFxNd7WP8FssnuHgZejIjrI2KTRjlVmxX5EOvR9eWtQ5IkvaFQ+PgE3frBRMRhwL8BfwV+COwGnAn8Efj2ANVYdsvasuWkhvLWIUmS3lAoxOwFfLXbulOBtcD7UkqvAUQEwEcZxCFmaRvUBYwxxEiSVDEK9fIYzxujkzodBjzQGWByvwR26e/CKsnytmx49ahBfdNMkqTqUijErACGdn6IiJ2BMcBD3fZbziCeIwZgWTs0RvYUa0mSVBkKhZingaO7fD6arI/M3d322x54vZ/rqiidLTG1ztgrSVLFKHSD5DvArRExmiyknELWofd/uu13JPCXAamuQqxsh/qarDVGkiRVhl5bYlJKtwGfAt4JnER2G+m4lNKGEUsRMRE4FLhjgOssq9UdUI8tMZIkVZKCXVVTSt8Fvltg+2vA2P4uqtKsaYdabydJklRRnIO2CGs6sidYG2IkSaochpgirGyHhgAzjCRJlcMQU4Tl7dnopHpTjCRJFcMQ04f2BGs7oKEGwhAjSVLFMMT0YWX+8McGA4wkSRXFENOH5fnDH5s8U5IkVRQvzX1YkoeYYT5yQJKkimKI6UNniBliiJEkqaIYYvrQeTupxRAjSVJFMcT0YXnesXd0fXnrkCRJGzPE9GFNHmJGFnxAgyRJKjVDTB9WdWRLbydJklRZDDF9WJW3xLTYEiNJUkUxxPRhVXv2zKQx9omRJKmiGGL6sKo9e2bSUG8nSZJUUQwxfVjTkYWYZs+UJEkVxUtzH9Z2QF1Ajc9OkiSpohhi+rC6PQsxtYYYSZIqiiGmDyvzPjF2iZEkqbIYYvqwqgPqa7ydJElSpTHE9KGzY2+DIUaSpIpiiOnDqvYswNQZYiRJqiiGmD6saoeGmuwlSZIqh5fmPqzLbyfV2xIjSVJFMcT0oXOemHrPlCRJFcVLcx/WJ/vDSJJUiQwxfWg1xEiSVJEMMQWkBG3J2XolSapEhpgCOvKls/VKklR5DDEFtKVsaUuMJEmVxxBTgCFGkqTKZYgpYEOIKW8ZkiSpB4aYAjpDjA9/lCSp8pQ8xETE4RHxTETMiojP9bD9vIh4MiIei4hfR8R2pa6xU7shRpKkilXSEBMRtcClwBHA7sAJEbF7t90eBaanlN4G/Az4Rilr7KrVPjGSJFWsUrfE7APMSinNTimtB24Eju66Q0rp/pTS6vzjQ8CUEte4wbp8jLXPTZIkqfKUOsRMBl7u8nluvq43pwO/6mlDRJwRETMjYuaCBQv6scQ3rM9DTIMhRpKkilOxHXsj4mPAdOCbPW1PKV2RUpqeUpo+bty4AalhfX47yccOSJJUeepK/HuvAFO7fJ6Sr9tIRBwKfBF4T0ppXYlqe5POlpgwxEiSVHFK3RLzCLBzRGwfEQ3A8cCMrjtExF7AD4CjUkrzS1zfRjo79pY66UmSpL6VNMSklNqAc4C7gKeAm1NKT0TEhRFxVL7bN4FhwC0R8eeImNHL4QacQ6wlSapcJW9kSCndAdzRbd1Xurw/tNQ19cYZeyVJqlwV27G3ErTny1rPkiRJFcfLcwGdLTHOEyNJUuUxxBTQbsdeSZIqliGmgM6WmEY7xUiSVHEMMQU4Y68kSZXLEFNA54y9TZ4lSZIqjpfnAjr7xNgSI0lS5THEFLAhxHiWJEmqOF6eC3CyO0mSKpchpoDOye4aPUuSJFUcL88FbGiJ8SxJklRxvDwX4GR3kiRVLkNMAZ0hpt6zJElSxfHyXIDPTpIkqXIZYgrobImxY68kSZXHy3MBnaOT6stahSRJ6okhpoCOzo69niVJkiqOl+cCHJ0kSVLlMsQU0AEEtsRIklSJvDwX0J6yEOPgJEmSKo8hpoDOEFNnipEkqeIYYgroAGoCag0xkiRVHENMAd5OkiSpchliCrAlRpKkymWIKaAtb4lJ5S5EkiS9iSGmgLaUtcJ4kiRJqjxenwvoSNkJ8tlJkiRVHi/PBTjEWpKkymWIKaADiMg690qSpMpiiCmgwyHWkiRVLENMAR1kJ8gQI0lS5THEFNDZJ0aSJFUeQ0wBnX1inOxOkqTKY4gpoLNPTIez3UmSVHEMMQV04O0kSZIqlSGmgPaUDa92iLUkSZXHEFNA54y9niRJkiqP1+cCOspdgCRJ6pUhpoCO/AGQPnZAkqTKY4gpoJ18xl5DjCRJFccQU0B7yueJKXchkiTpTQwxBSTyjr22xEiSVHEMMQV0TnbnSZIkqfJ4fS6gc6JeW2IkSao8hpgCNrTEGGIkSao4deUuoJIlHJkkqbqsXbuWBQsWsHbtWtra2spdjrSR+vp6xo8fz4gRI/rleIaYAjpbYiSpGixbtozXX3+dcePGMXHiROrq6gj/n5gqREqJNWvW8MorrwD0S5DxdlIBztgrqZosXLiQKVOmMGrUKOrr6w0wqigRwZAhQ5g8eTLz58/vl2MaYgpI2BIjqXqsX7+e5ubmcpchFdTc3Exra2u/HMsQU4AhRlK1sfVFla4//44aYgroMMVIklSxDDEFmGEkSapchpgCDDGSVDk+/vGPExF8+tOf7nH7KaecwpQpU3rc9pvf/IaI4N57791ofWtrK9///vc54IADGDlyJI2NjWy//facdtpp/OlPf+r3P0MhV155JW95y1tobGxk11135fLLLy/6u5dddtmG72677bZ8+ctfLtjvpLW1lT333JOI4Ic//GGP+9xxxx28+93vZtiwYYwYMYLp06dz3333bbTPn//8Zw4//PAN+xx11FHMmjWr6Lq3lCGmgJT63keSNPDWrFnDzTffDMD111/fL3PgrFq1ikMOOYTPfOYz7LPPPlx33XXcfffdfOlLX+KFF17gkEMO2eLfKNaVV17JmWeeyYc//GHuvPNOjjvuOM4++2wuu+yyPr970UUX8clPfpL3v//9/Nd//Rf//M//zLe//W3OOuusXr/zrW99i4ULF/a6/Qc/+AFHH300e++9N7/4xS+45ZZbOO6441i9evWGfZ577jne9a53sWzZMq677jquvvpq5syZw7vf/e5+G33Up5RS1b/23nvvNBDe8UhKezw8IIeWpH735JNPlruEAXP99dcnIB155JEJSLfffvub9jn55JPT5MmTe/z+/fffn4B0zz33bFh3+umnp4aGhvT73/++x+/ceuut/VN8H1pbW9O4cePSSSedtNH6U089NY0ZMyatX7++1++uWbMmDRs2LJ188skbrf/mN7+ZIiI9/vjjb/rO888/n4YMGZKuu+66BKQrr7xyo+0vvPBCampqSt/5zncK1n366aenlpaWtGTJkg3rXn755dTY2JjOP//8gt/t6+8qMDMVcf23JaYA54mRpMpw7bXXMmrUKK655hqam5u59tprt+h4r776Ktdeey0f//jH2W+//Xrc50Mf+tAW/UaxHnzwQRYsWMDHPvaxjdafeOKJLFq0iAceeKDX7z7++OOsXLmSI444YqP1hx9+OCklbrvttjd956yzzuL4449n//337/GYV111FTU1NXziE58oWPdDDz3Efvvtx8iRIzesmzJlCnvssQe/+MUvCn63vxhiCkjJxw5IUrnNmzePe++9l3/4h39g3LhxfPCDH+T2229nyZIlm33M+++/n7a2No466qjNPkZKiba2tj5fHR2F/y/xE088AcAee+yx0fq3vvWtADz55JO9fre2thaAhoaGjdY3NjYCWcjp6rrrrmPmzJlcfPHFvR7zgQce4C1veQs33ngjO+64I3V1dey0005ceumlb/rt7r/b+dvPP/88a9eu7fU3+ouPHSjALjGSBoNPPQd/XlneGt4+DP5j58377k9/+lPa29s56aSTADj55JO54YYbuOmmm/psLejNyy+/DMB22223eUWRtQ6deuqpfe538sknc8011/S6ffHixQCMGjVqo/WjR4/eaHtPdt55Z2pqanjooYc2ajl68MEH3/TdJUuWcN5553HxxRczduxYVq7s+S/FvHnzmDdvHueffz7//u//zo477sgtt9zCOeecQ1tbG//yL/8CwK677srvf/97Wltbqa+vB2DFihU88cQTpJRYsmQJ22yzTa+19wdDTAEJm6okqdyuvfZadt555w23fQ499FAmTZrEtddeu9khpj984AMf4JFHHulzv7Fjxw5YDcOGDeO0007jkksuYa+99uLwww/n0Ucf5Qtf+AK1tbXU1LxxFTv//PPZcccdOf300wses6OjgxUrVnDNNddwzDHHAPB3f/d3zJkzh4suuohzzz2XiODcc8/llltu4ROf+AQXXnghbW1tfOYzn9kQjrr+9kAxxBTQkaDG20mSqtzmtoBUgpkzZ/Lkk0/yr//6ryxdunTD+mOOOYZLLrmEZ599ll122QWAuro62tvbezxO5/q6uuyyN3XqVABefPFFdt11182qbfTo0bS0tPS5X18X884WmO4tF52tKJ0tMr359re/zaJFi/joRz9KSommpiYuvPBCvvGNb2w43sMPP8zVV1/Nfffdx7JlywBYvnw5kI38Wrp0KS0tLUQEY8aM4bnnnuOwww7b6Hfe+973cuedd/Lqq68yadIkDjzwQC699FI+//nPc9VVVwFZwDz55JP56U9/2mfd/aHkDQ0RcXhEPBMRsyLicz1sb4yIm/LtD0fEtFLX2Ml5YiSpvDo78F588cWMGjVqw+uSSy4B4Mc//vGGfcePH8/ChQtZv379m44zb948ACZMmADAQQcdRG1tLbfffvsW1VZfX9/n67TTTit4nM6+L519Yzp19oXZfffdC35/xIgR3Hrrrbz++us89thjzJ8/n5NOOomFCxdy4IEHAvDUU0/R0dHBQQcdtOEc/s3f/A0A5557LqNGjdoQbjrr6U3XUHb22Wczf/58Hn/8cV566SXuuece5s2bx7777rvhFtNAKmlLTETUApcChwFzgUciYkZKqWuvpdOBJSmlnSLieOBi4B9KWWcn+8RIUvmsX7+eG264gX333Zevf/3rb9r+6U9/mp/85Cd89atfJSI4+OCDueiii5gxYwbHHnvsRvv+/Oc/Z5ttttnQ6jJp0iROOeUUrrjiCj760Y/2OELptttu44Mf/GCv9fXX7aT99tuPsWPHct1113HooYduWN/ZmnHAAQf0+RsA48aNY9y4cQB87WtfY+zYsRx33HFANlrp/vvv32j/1157jRNOOIHPfvazvP/972fYsGFANirrRz/6EXfddddG5/HOO+9kypQpTJw4caPjNDY2bgg+f/3rX7n33ns3CpcDqdS3k/YBZqWUZgNExI3A0UDXEHM0cEH+/mfAJRER+bjxkupIUIJbepKkHvzyl79k0aJFfPvb3+aggw560/YzzzyTs846i9/85jccfPDBHHrooRx22GGccsopPP300+y7776sWLGCG2+8kf/8z//k6quv3qgV4T/+4z949tlnOeSQQ/jEJz7BoYceyrBhw5g9e/aGUTyFQsyYMWMYM2bMFv856+vr+epXv8rZZ5/N5MmTOfTQQ7nvvvu46qqr+N73vrfRCKDTTz+da6+9dqPJ/m666SYWL17MrrvuypIlS7j11lu5+eab+fnPf87w4cMBmDhx4pvCx5w5c4Csg27X83vkkUdy8MEHc+aZZ7Jw4UJ22GEHbrnlFu6++26uvvrqDfvNnTuXyy67jP3335/GxkZmzpzJRRddxDHHHMMJJ5ywxeelKMVMJtNfL+BY4IddPp8IXNJtn8eBKV0+Pw+MLXTcgZrsbveHU/qbPwzIoSWp3w22ye6OPvroNHz48LRq1aoety9dujQ1NzdvNNHb6tWr0xe/+MW08847p4aGhjRs2LB04IEHpttuu63HY6xfvz5dcsklab/99kvDhw9P9fX1adq0aen0009Pf/nLXwbij9Wryy+/fEPdO+20U7r00kvftM/JJ5+cskv3G2666aa0xx57pObm5jR8+PB02GGHpQceeKDP33vhhRd6nOwupZSWLVuWzj777DR+/PhUX1+f9txzz3TddddttM9rr72WDjnkkDRmzJjU0NCQdtttt/Stb30rtba29vnb/TXZXaQSNnBExLHA4Smlf8o/nwjsm1I6p8s+j+f7zM0/P5/vs7Dbsc4AzgDYdttt937xxRf7vd6HlsGyNnjflgdtSRpwTz31FLvttlu5y5D61Nff1Yj4Y0ppel/HKfXNkleAqV0+T8nX9bhPRNQBLcCi7gdKKV2RUpqeUpreeQ+wv/1tiwFGkqRKVeoQ8wiwc0RsHxENwPHAjG77zABOzt8fC9yXStlcJEmSqkJJO/amlNoi4hzgLqAWuCql9EREXEh2/2sG8CPgJxExC1hMFnQkSZI2UvLJ7lJKdwB3dFv3lS7v1wLHlbouSZJUXRxALEmSqpIhRpIGEbsQqtL1599RQ4wkDRINDQ2sWbOm3GVIBa1Zs6bfHklgiJGkQWLs2LHMnTuXxYsX09raaquMKkpKidWrV/PKK68wfvz4fjmmT7GWpEGipaWFxsZGFixYwKJFizaaml6qBPX19UyYMIERI0b0y/EMMZI0iDQ1NTF16tS+d5QGAW8nSZKkqmSIkSRJVckQI0mSqpIhRpIkVSVDjCRJqkqGGEmSVJViMEyGFBELgBcH6PBjgYUDdGxtzHNdWp7v0vFcl47nunQG8lxvl1Ia19dOgyLEDKSImJlSml7uOrYGnuvS8nyXjue6dDzXpVMJ59rbSZIkqSoZYiRJUlUyxPTtinIXsBXxXJeW57t0PNel47kunbKfa/vESJKkqmRLjCRJqkqGGEmSVJUMMbmIODwinomIWRHxuR62N0bETfn2hyNiWumrHByKONfnRcSTEfFYRPw6IrYrR52DQV/nust+H46IFBEOTd0CxZzviPhI/vf7iYi4vtQ1DhZF/DuybUTcHxGP5v+WHFmOOgeDiLgqIuZHxOO9bI+I+G7+3+KxiHhHyYpLKW31L6AWeB7YAWgA/gLs3m2fs4HL8/fHAzeVu+5qfBV5rg8GhuTvz/JcD9y5zvcbDvwWeAiYXu66q/VV5N/tnYFHgVH55/HlrrsaX0We6yuAs/L3uwNzyl13tb6AdwPvAB7vZfuRwK+AAP4WeLhUtdkSk9kHmJVSmp1SWg/cCBzdbZ+jgWvz9z8DDomIKGGNg0Wf5zqldH9KaXX+8SFgSolrHCyK+XsN8FXgYmBtKYsbhIo53x8HLk0pLQFIKc0vcY2DRTHnOgEj8vctwLwS1jeopJR+CywusMvRwI9T5iFgZERsU4raDDGZycDLXT7Pzdf1uE9KqQ1YBowpSXWDSzHnuqvTyRK+Nl2f5zpv9p2aUvplKQsbpIr5u70LsEtE/E9EPBQRh5esusGlmHN9AfCxiJgL3AH8c2lK2ypt6r/r/aauFD8ibY6I+BgwHXhPuWsZjCKiBvi/wCllLmVrUkd2S+kgshbG30bEnimlpWWtanA6AbgmpfTtiNgP+ElE7JFS6ih3Yeo/tsRkXgGmdvk8JV/X4z4RUUfWPLmoJNUNLsWcayLiUOCLwFEppXUlqm2w6etcDwf2AH4TEXPI7mXPsHPvZivm7/ZcYEZKqTWl9ALwLFmo0aYp5lyfDtwMkFJ6EGgie2Ch+l9R/64PBENM5hFg54jYPiIayDruzui2zwzg5Pz9scB9Ke/RpE3S57mOiL2AH5AFGPsMbL6C5zqltCylNDalNC2lNI2s/9FRKaWZ5Sm36hXz78htZK0wRMRYsttLs0tZ5CBRzLl+CTgEICJ2IwsxC0pa5dZjBnBSPkrpb4FlKaVXS/HD3k4i6+MSEecAd5H1er8qpfRERFwIzEwpzQB+RNYcOYusg9Px5au4ehV5rr8JDANuyftOv5RSOqpsRVepIs+1+kmR5/su4L0R8STQDpyfUrJFdxMVea4/A1wZEZ8m6+R7iv/Hc/NExA1k4Xts3sfofwP1ACmly8n6HB0JzAJWA6eWrDb/m0qSpGrk7SRJklSVDDGSJKkqGWIkSVJVMsRIkqSqZIiRJElVyRCjrVpEnJI/vbnz1R4Rr0TEzRGx6wD+7pyI+OlAHb9SRcQFEVFxQyIjYlpe2w5l+v3Ov4fTBvA3PhURx/SwviL/m0jFMMRImeOA/cie1vp5YC/g1xHRUtaqVCrTyOa+KEuIAX5J9vdvICcI+xTwphAD/DD/banqONmdlPlzSmlW/v5/ImIecA+wP1X4AMqIaPRxDdUjpbSAMs0mm1KaS/Y4BKnq2BIj9Wx5vqzvXBERO0XETyLihYhYExGzI+KyiBjV/csR8Z6IuCcilkXEqoj4S0Sc3tuPRURtRFwREcvz50Z1rj8hIp6OiLUR8deIOCoifhMRv+myz0H5rYhjIuLKiFgAvN5l++ER8WBe87KIuK37rbL89tY1PdSVIuKCLp8vyNftHBG/jIiVEfFiRHwlf6Bk1+/uFRG/y2t/JSK+DERv56CH3/54RPwpr3tJRPx3ROzfZfs2EfHjiFgYEesi4rHIHhra9Ridt2n+NiKuy8/vvIj4bkQ0dZ4/4P78K/d0ubV4UL79+Ii4LyIW5H/eRyPiZLrJv/NvEfGZ/Jyszs/R+Px1c37+X46If+2lzmld1s2JiJ/mv/9U/vdoZkQc2O2774yIn0XE3PxcPRMR/x4RzV2PBWwH/GOXP981+bY33U6KiBERcUl+rtblx/x0xP9v7/xjtSzLOP65wIEea5NfSoZomXNqpUk2LUFaulyoCI2ZKEuZy1VSZqbmLw4ZiZkHmdIcoYhhhr+YhUroHCwCt9TlDpGeIilHEjudM2vEL/Hqj+/9Hh7u87znfYFzds5Z12d79u65nuv+8d7Ps93Xc13X/dxmBZ3Kc3dR0m1NxxIzO6LW/Q2C7iA8MUEgBpo29hyIQgo/ArYCqwo6R6Pt5q8F2pPezeiT2x3ueDObCDwF/A64GmgFTkGTSCfSZPNYqmO8u7+W5OcBj6J9Sa4DRgD3oj1gWkqqug95jaYlHczsfBSqeAm4BG3n8ANgjZmd5u4HuknbMmARMBe4EJiFxmZRand4anML2nNsJ/A9YHQ9lZvZT9Bn4x9EYdx9TX4AAAbMSURBVJ730QaVo4G1ZnY4sBoYgu7B28DlaGuQBndfkFX5czTGk9E4N6J7OBN4DfgmMB/4FtqXB2BD+v0o8CQwJ/VjHLDQzA5Ln1wvMg1YD3wDOArdr0fQZpvPAwtQ6HKOmTW7+3M1hmIscCJwG7ADuANYbmbHFXa+Hg38AXgY+A961m5P/a5sjzIJPaevp/8OVTw/yRh9Fjg91dMMTEA7no9A411kHrAcmJr6+mO0pUInQy8Iuh13jyOO/9sDuALtq5Ifm4EzapQ9BDg76X8qyQzYBLwCDOii7CZgCZqE1wAbgeMznbVoQrSCbExqb1VBNj7JlpW08wrwZ+CQguwjwG6gKevPwyXlHWgsnDcm2ZWZXjOwsnA+G9gFHFOQHY4MOq8xrh9Dk2BTFzrXpH6Mz+QvIuNzYHZ/Z2V6y4GWkjE8t0bfBqT7/jPg9ZKxasnGuinJb82em63AopLn8LjsnrQDQwqyTye9qVX6Z6n+y5HBNSx/5krKNBbvCXABe/caKuotRMbo8GzMFmd69yODy8r6GEcc3XlEOCkIxCTgDOAzwMXoLfw50+63AJjZIDO72RTe2Y4Mgd+myycWfo8FFrr7+zXaPBoZMA3AZ919Y6GtgWjCesrdO1z97v4q8FaV+pYVT5K34nRgqbu/V6jjLeQlOqdG/7ri2ex8Pft6Wc4CXnb3twvtbgN+XUfd5yJjIfemFBkHbHb3VZl8CfIWnFyjv83U7xU6wcweM7PN6J7vBq5i7z0v8kJxrIE30u9vKoJ0/S/AMXU0v87d27N+U+x7Cv3cZWYbkZGxG3meDDihjjZyxiED6BeZfAkwiM5JwGVjOxh5ooKgR4lwUhCI9b43sRczW4lCFI0oDANwJzADhWPWItf9KOBpUvgGGJZ+60mU/GTSv8nd/5ldG47ycbaWlMt1K+QrW4agiaxsxcsWqoS36qQtO9/J3jEA+BAybHKq9b1IPWM4lOr/q3K9SFl/B9fqiJl9ACV4/xe4CXnMdgFfB6aXFGnPznd1IT+U2uzTb3ffmdJSimUXIcPvdhRW2oaM8fl1tpEzFGhz912ZfH/GNu9jEPQIYcQEQQnuvt3M/ooMjQpfAR5x9x9WBGmSK9Kafj9cRzMrUI7CXWa2w93nZfXsBo4sKXcU8Peybmfn7Uk2skR3JPtOPjvQW3YHZjaMA+cdyt/E63k7L47hm1V02ij3hIwsXO8OzkLG3lh3X1MRpvypXiclJ09EIb95BfknDqLaNmComQ3KDJnuHtsgOGginBQEJZhZA3A8+yY/NiDDosiV2XkLyj24qriSoxrufjdwPXCvmX2nIN+D8lm+nK0IGYNyWmqSwjevAlNSeKpSx7Fo6fiqgvrfgI9nVUyop50qrAPONLOOkEkKb11YR9kXUTjja13orAZGmdnnMvlU5L3a0LlIl1S8B4dl8ob023HfTavRJu5n/T3FYJSMnj+XV5To7qTz/ytjNZobpmTyy5AHad3+dTEIeo4+8TYRBH2A09KKGkOhkGuQ2/y+gs4K4Ktm1oxyGiYjY6ADd3czuxaFmF4ysweQIXQScKS7z8wbdvcmM9sDzDWzAe5+T7o0E1gJLDOzBSjE1Ijc+rXybSrchnIWlpvZT9HqpFnAu8A9Bb1fAg+Z2VyU9Hoq5RNhvcxFK3RWmpZoV1Ynba9V0N03pn5cZ2YfRKuz9qAQyRvuvhStxPk28LSZ3YJCT5cB5wFXJyNwf2gB3gOmm1lb6u+bKGz4b2C+mc1Eycm3Im9Rr38I0d3fNbOXge+a2TuoX9Mp9wRuAMaa2QXoGWp1900les+jXK0HzGwE8EfgSygP6E53by0pEwS9QnhigkA8gd4w1wKVZbPnu/sTBZ0ZaEKdDSxFy2YvzSty92fQZApaIvwr5FXYVK3xFAqYAdxtZjck2QtoYj4JJe3eiJYdb0FGSE3cfQXyqBwBPJ7+25+As939HwXVxchomoySb7+Ikp0PiDTRfQFNqotRfsYK4KE6y1+PjKAz0XL1R4HPk8Joyct0DjLy5gDPIMNrmndeXl1Pe/9ChuupyBPxe2CM6yN0k5C340mUF7UQJbn2FS5FHrf5yLjbggy8nO8jw+xx9P8ayypLCekT0H27ERnBE9Ay/1u6tedBcJBYYeFDEAR9HDMbhbxAs939jt7uTxAEQW8SRkwQ9FHSR/CaUI5IK/p42Q0oOfYUd+/JfXaCIAj6PJETEwR9lz1oRcj9aNnxNvRdmilhwARBEIQnJgiCIAiCfkok9gZBEARB0C8JIyYIgiAIgn5JGDFBEARBEPRLwogJgiAIgqBfEkZMEARBEAT9kv8BBXZubN84TF0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd918077f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd891a17e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# skf.get_n_splits(data_hlf, label)\n",
    "\n",
    "fprs = []\n",
    "base_tpr_sm = np.linspace(0, 1, 5000)\n",
    "thresholds = []\n",
    "volatile=True\n",
    "best_batch_size = 1000\n",
    "\n",
    "for train_index, test_index in skf.split(data_hlf, label):\n",
    "    train_data = [data_list[train_index].astype(np.float32), data_hlf[train_index].astype(np.float32)], label[train_index].astype(np.float32)\n",
    "    test_data = [data_list[test_index].astype(np.float32), data_hlf[test_index].astype(np.float32)], label[test_index].astype(np.float32)\n",
    "    train_weight = weight[train_index].astype(np.float32)*nodeweight[train_index,12].astype(np.float32)\n",
    "    test_weight = weight[test_index].astype(np.float32) *nodeweight[test_index,12].astype(np.float32)\n",
    "    \n",
    "    pred = model.predict(x=[test_data[0][0],test_data[0][1]])\n",
    "    fpr, tpr, threshold = roc_curve(test_data[1], pred, sample_weight=test_weight)\n",
    "    fpr = np.interp(base_tpr_sm, tpr, fpr)\n",
    "    threshold = np.interp(base_tpr_sm, tpr, threshold)\n",
    "    fpr[0] = 0.0\n",
    "    fprs.append(fpr)\n",
    "    thresholds.append(threshold)\n",
    "\n",
    "\n",
    "thresholds = np.array(thresholds)\n",
    "mean_thresholds = thresholds.mean(axis=0)\n",
    "\n",
    "fprs = np.array(fprs)\n",
    "mean_fprs_sm = fprs.mean(axis=0)\n",
    "std_fprs = fprs.std(axis=0)\n",
    "fprs_right = np.minimum(mean_fprs_sm + std_fprs, 1)\n",
    "fprs_left = np.maximum(mean_fprs_sm - std_fprs,0)\n",
    "\n",
    "mean_area_sm = auc(mean_fprs_sm, base_tpr_sm, reorder=True)\n",
    "\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(base_tpr_sm>TPR_threshold)\n",
    "    NNtable.add_row([mean_thresholds[thres_idx], base_tpr_sm[thres_idx], \"{:.4f} +/- {:.4f}\".format(mean_fprs_sm[thres_idx], std_fprs[thres_idx])])\n",
    "print(NNtable)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(mean_fprs_sm, base_tpr_sm,label=\"AUC = {:.4f}\".format(mean_area_sm), color='deepskyblue')\n",
    "plt.fill_betweenx(base_tpr_sm, fprs_left, fprs_right, color='deepskyblue', alpha=0.4)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel('Background contamination', fontsize=16)\n",
    "plt.ylabel('Signal efficiency', fontsize=16)\n",
    "plt.title('2017', fontsize=20)\n",
    "#plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "#plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist(pred[test_data[1]==0], weights=test_weight[test_data[1]==0], bins=60, label='ttH background',alpha=0.5, normed=True)\n",
    "plt.hist(pred[test_data[1]==1], weights=test_weight[test_data[1]==1], bins=60, label='HH signal', alpha=0.5, normed=True)\n",
    "#plt.axvline(thresholds[thres_idx], ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Threshold\", fontsize=18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BSM node 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 0\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1618  |       0.9704      |    0.2831 +/- 0.0077     |\n",
      "|   0.2528  |       0.9498      |    0.2114 +/- 0.0036     |\n",
      "|   0.3627  |       0.9196      |    0.1539 +/- 0.0011     |\n",
      "|   0.7560  |       0.7536      |    0.0442 +/- 0.0032     |\n",
      "|   0.8938  |       0.5777      |    0.0158 +/- 0.0006     |\n",
      "|   0.9587  |       0.3837      |    0.0041 +/- 0.0001     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 1\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1624  |       0.9840      |    0.2814 +/- 0.0029     |\n",
      "|   0.2533  |       0.9716      |    0.2116 +/- 0.0087     |\n",
      "|   0.3631  |       0.9502      |    0.1540 +/- 0.0045     |\n",
      "|   0.7560  |       0.8278      |    0.0444 +/- 0.0007     |\n",
      "|   0.8938  |       0.6925      |    0.0158 +/- 0.0007     |\n",
      "|   0.9587  |       0.5177      |    0.0042 +/- 0.0003     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 2\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1626  |       0.9656      |    0.2816 +/- 0.0107     |\n",
      "|   0.2532  |       0.9436      |    0.2114 +/- 0.0062     |\n",
      "|   0.3626  |       0.9110      |    0.1538 +/- 0.0025     |\n",
      "|   0.7561  |       0.7271      |    0.0443 +/- 0.0008     |\n",
      "|   0.8938  |       0.5387      |    0.0158 +/- 0.0006     |\n",
      "|   0.9587  |       0.3411      |    0.0042 +/- 0.0003     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 3\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1629  |       0.9580      |    0.2810 +/- 0.0071     |\n",
      "|   0.2534  |       0.9324      |    0.2112 +/- 0.0040     |\n",
      "|   0.3635  |       0.8962      |    0.1534 +/- 0.0007     |\n",
      "|   0.7562  |       0.6969      |    0.0442 +/- 0.0011     |\n",
      "|   0.8938  |       0.4947      |    0.0158 +/- 0.0008     |\n",
      "|   0.9587  |       0.2967      |    0.0042 +/- 0.0003     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 4\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1620  |       0.9780      |    0.2818 +/- 0.0060     |\n",
      "|   0.2529  |       0.9614      |    0.2116 +/- 0.0071     |\n",
      "|   0.3630  |       0.9358      |    0.1540 +/- 0.0068     |\n",
      "|   0.7561  |       0.7876      |    0.0442 +/- 0.0011     |\n",
      "|   0.8939  |       0.6297      |    0.0157 +/- 0.0009     |\n",
      "|   0.9587  |       0.4399      |    0.0042 +/- 0.0001     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 5\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1620  |       0.9590      |    0.2821 +/- 0.0080     |\n",
      "|   0.2533  |       0.9328      |    0.2117 +/- 0.0055     |\n",
      "|   0.3635  |       0.8960      |    0.1537 +/- 0.0046     |\n",
      "|   0.7561  |       0.7053      |    0.0444 +/- 0.0011     |\n",
      "|   0.8939  |       0.5095      |    0.0157 +/- 0.0001     |\n",
      "|   0.9587  |       0.3181      |    0.0042 +/- 0.0002     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 6\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1631  |       0.9294      |    0.2809 +/- 0.0058     |\n",
      "|   0.2532  |       0.8922      |    0.2115 +/- 0.0012     |\n",
      "|   0.3634  |       0.8416      |    0.1538 +/- 0.0010     |\n",
      "|   0.7561  |       0.5991      |    0.0441 +/- 0.0021     |\n",
      "|   0.8939  |       0.3573      |    0.0158 +/- 0.0011     |\n",
      "|   0.9587  |       0.1712      |    0.0042 +/- 0.0004     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 7\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1621  |       0.9680      |    0.2817 +/- 0.0055     |\n",
      "|   0.2535  |       0.9468      |    0.2110 +/- 0.0024     |\n",
      "|   0.3634  |       0.9156      |    0.1534 +/- 0.0052     |\n",
      "|   0.7561  |       0.7431      |    0.0442 +/- 0.0028     |\n",
      "|   0.8938  |       0.5637      |    0.0158 +/- 0.0015     |\n",
      "|   0.9587  |       0.3705      |    0.0041 +/- 0.0006     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 8\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1621  |       0.9744      |    0.2816 +/- 0.0025     |\n",
      "|   0.2533  |       0.9562      |    0.2113 +/- 0.0028     |\n",
      "|   0.3628  |       0.9286      |    0.1537 +/- 0.0018     |\n",
      "|   0.7560  |       0.7686      |    0.0444 +/- 0.0009     |\n",
      "|   0.8939  |       0.6003      |    0.0159 +/- 0.0006     |\n",
      "|   0.9587  |       0.4067      |    0.0042 +/- 0.0000     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 9\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1626  |       0.9382      |    0.2809 +/- 0.0073     |\n",
      "|   0.2536  |       0.9040      |    0.2113 +/- 0.0051     |\n",
      "|   0.3635  |       0.8578      |    0.1535 +/- 0.0024     |\n",
      "|   0.7560  |       0.6385      |    0.0444 +/- 0.0013     |\n",
      "|   0.8938  |       0.4157      |    0.0159 +/- 0.0005     |\n",
      "|   0.9586  |       0.2318      |    0.0042 +/- 0.0002     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 10\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1632  |       0.9570      |    0.2815 +/- 0.0110     |\n",
      "|   0.2532  |       0.9312      |    0.2115 +/- 0.0078     |\n",
      "|   0.3627  |       0.8954      |    0.1538 +/- 0.0051     |\n",
      "|   0.7560  |       0.7057      |    0.0442 +/- 0.0010     |\n",
      "|   0.8938  |       0.5109      |    0.0157 +/- 0.0005     |\n",
      "|   0.9587  |       0.3205      |    0.0042 +/- 0.0007     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/ranking.py:104: DeprecationWarning: The 'reorder' parameter has been deprecated in version 0.20 and will be removed in 0.22. It is recommended not to set 'reorder' and ensure that x is monotonic increasing or monotonic decreasing.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 11\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1628  |       0.9434      |    0.2809 +/- 0.0063     |\n",
      "|   0.2533  |       0.9120      |    0.2113 +/- 0.0025     |\n",
      "|   0.3633  |       0.8688      |    0.1537 +/- 0.0059     |\n",
      "|   0.7561  |       0.6443      |    0.0443 +/- 0.0023     |\n",
      "|   0.8939  |       0.4209      |    0.0158 +/- 0.0015     |\n",
      "|   0.9587  |       0.2270      |    0.0042 +/- 0.0003     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2d17c506a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_thresholds = [0.1632, 0.2537, 0.3635, 0.7562, 0.8939, 0.9587]\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# skf.get_n_splits(data_hlf, label)\n",
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "for NODE in range(0,12):\n",
    "    fprs = []\n",
    "    base_tpr_node2 = np.linspace(0, 1, 5000)\n",
    "    thresholds = []\n",
    "    volatile=True\n",
    "    best_batch_size = 1000\n",
    "\n",
    "    for train_index, test_index in skf.split(data_hlf, label):\n",
    "        train_data = [data_list[train_index].astype(np.float32), data_hlf[train_index].astype(np.float32)], label[train_index].astype(np.float32)\n",
    "        test_data = [data_list[test_index].astype(np.float32), data_hlf[test_index].astype(np.float32)], label[test_index].astype(np.float32)\n",
    "        train_weight = weight[train_index].astype(np.float32)*nodeweight[train_index,NODE].astype(np.float32)\n",
    "        test_weight = weight[test_index].astype(np.float32) *nodeweight[test_index,NODE].astype(np.float32)\n",
    "\n",
    "        pred = model.predict(x=[test_data[0][0],test_data[0][1]])\n",
    "        fpr, tpr, threshold = roc_curve(test_data[1], pred, sample_weight=test_weight)\n",
    "        fpr = np.interp(base_tpr_node2, tpr, fpr)\n",
    "        threshold = np.interp(base_tpr_node2, tpr, threshold)\n",
    "        fpr[0] = 0.0\n",
    "        fprs.append(fpr)\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "\n",
    "    thresholds = np.array(thresholds)\n",
    "    mean_thresholds = thresholds.mean(axis=0)\n",
    "\n",
    "    fprs = np.array(fprs)\n",
    "    mean_fprs_node2 = fprs.mean(axis=0)\n",
    "    std_fprs = fprs.std(axis=0)\n",
    "    fprs_right = np.minimum(mean_fprs_node2 + std_fprs, 1)\n",
    "    fprs_left = np.maximum(mean_fprs_node2 - std_fprs,0)\n",
    "\n",
    "    mean_area_node2 = auc(mean_fprs_node2, base_tpr_node2, reorder=True)\n",
    "\n",
    "    print(\"Node {}\".format(NODE))\n",
    "    NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "    NNtable.float_format = \".4\"\n",
    "    for value_threshold in value_thresholds:\n",
    "        thres_idx = np.argmax(mean_thresholds<value_threshold)\n",
    "        NNtable.add_row([mean_thresholds[thres_idx], base_tpr_node2[thres_idx], \"{:.4f} +/- {:.4f}\".format(mean_fprs_node2[thres_idx], std_fprs[thres_idx])])\n",
    "    print(NNtable)\n",
    "\n",
    "# plt.figure(figsize=(9,7))\n",
    "    plt.plot(mean_fprs_node2, base_tpr_node2,label=\"Node {} (AUC = {:.4f})\".format(NODE, mean_area_node2))\n",
    "    plt.fill_betweenx(base_tpr_node2, fprs_left, fprs_right, alpha=0.4)\n",
    "    plt.legend(loc='best', fontsize=16)\n",
    "    plt.xlabel('Background contamination', fontsize=16)\n",
    "    plt.ylabel('Signal efficiency', fontsize=16)\n",
    "# plt.title('2017', fontsize=20)\n",
    "plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(9,7))\n",
    "# plt.hist(pred[test_data[1]==0], weights=test_weight[test_data[1]==0], bins=60, label='ttH background',alpha=0.5, normed=True)\n",
    "# plt.hist(pred[test_data[1]==1], weights=test_weight[test_data[1]==1], bins=60, label='HH signal', alpha=0.5, normed=True)\n",
    "# #plt.axvline(thresholds[thres_idx], ls='--',color='tab:gray')\n",
    "# plt.legend(loc='best')\n",
    "# plt.xlabel(\"Threshold\", fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fd89119c550>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd81023ee10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Benchmark Plot\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(mean_fprs, base_tpr,label=\"Evaluated on cocktail (AOC = {:.4f})\".format(mean_area), color='darkgreen')\n",
    "plt.plot(mean_fprs_node2, base_tpr_node2,label=\"Evaluated on Node 2 (AOC = {:.4f})\".format(mean_area_node2), color='deepskyblue')\n",
    "plt.plot(mean_fprs_sm, base_tpr_sm, label=\"Evaluated on SM (AUC = {:.4f})\".format(mean_area_sm), color='darkviolet')\n",
    "\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel('Background contamination', fontsize=16)\n",
    "plt.ylabel('Signal efficiency', fontsize=16)\n",
    "plt.title('2017', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a79fa1e791ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msm_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msm_in_node2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sm_in' is not defined"
     ]
    }
   ],
   "source": [
    "sm_in.close()\n",
    "sm_in_node2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fd891191ba8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd818392c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm_in =  h5py.File(\"Standard_ROC.h5\",\"r\")\n",
    "sm_in_node2 = h5py.File(\"SM_on_node2_ROC.h5\",\"r\") \n",
    "sm_mean_fprs_node2 = sm_in_node2['FPR']\n",
    "sm_base_tpr_node2 = sm_in_node2['TPR']\n",
    "sm_mean_fprs = sm_in['FPR']\n",
    "sm_mean_base_tpr = sm_in['TPR']\n",
    "plt.figure(figsize=(9,7))\n",
    "#plt.plot(mean_fprs, base_tpr,label=\"New model eval. on cocktail (AOC = {:.4f})\".format(mean_area), color='black')\n",
    "plt.plot(mean_fprs_sm, base_tpr_sm, label=\"New model eval. on SM (AUC = {:.4f})\".format(mean_area_sm), color='darkviolet')\n",
    "plt.plot(sm_mean_fprs, sm_mean_base_tpr, label=\"Old model eval. on SM\", color='darkorange')\n",
    "plt.plot(mean_fprs_node2, base_tpr_node2,label=\"New model eval. on node 2 (AOC = {:.4f})\".format(mean_area_node2), color='deepskyblue')\n",
    "plt.plot(sm_mean_fprs_node2, sm_base_tpr_node2, label=\"Old model eval. on node 2\", color='seagreen')\n",
    "\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.xlabel('Background contamination', fontsize=16)\n",
    "plt.ylabel('Signal efficiency', fontsize=16)\n",
    "#plt.xscale('log')\n",
    "#plt.xlim((1e-4,1.))\n",
    "plt.ylim((0.8,1))\n",
    "plt.title('2017', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.187317e-06]]\n"
     ]
    }
   ],
   "source": [
    "hlf_test = np.array([ -1.1048 , -1.43288 , 0.805159 , -0.433313,  -1.24418 , -0.262744 , 2.12138 , -0.633294 , -1.20308])\n",
    "hlf_test = hlf_test.reshape((1,9))\n",
    "pf_test = np.array([\n",
    "[0.718883 , 1.10485 , -0.00305263 , 0 , 0 , 1 , 0],\n",
    "[-0.76369 , 0.00234654 , 0.0949781 , 0 , 0 , 0 , 1],\n",
    "[0  ,  0  ,  0  ,  0  ,  0  ,  0  ,  0],\n",
    "[0  ,  0  ,  0  ,  0  ,  0  ,  0  ,  0],\n",
    "[0  ,  0  ,  0  ,  0  ,  0  ,  0  ,  0],\n",
    "[0  ,  0  ,  0  ,  0  ,  0  ,  0  ,  0]   \n",
    "])\n",
    "pf_test = pf_test.reshape((1,6,7))\n",
    "tmp = model.predict(x=[pf_test,hlf_test])\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7290138,  0.6127338, -1.4543222,  0.       ,  0.       ,\n",
       "         1.       ,  0.       ],\n",
       "       [-0.5375532,  0.       , -1.1775469,  0.       ,  0.       ,\n",
       "         0.       ,  1.       ],\n",
       "       [ 0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ],\n",
       "       [ 0.       ,  0.       ,  0.       ,  0.       ,  0.       ,\n",
       "         0.       ,  0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79466844]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x=[test_data[0][0][5].reshape(1,6,7), test_data[0][1][5].reshape(1,9)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normed_sig_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7997195d4a17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0marray_to_fill\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_to_fill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparticles_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed_sig_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormed_bkg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mhlf_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed_sig_hlf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormed_bkg_hlf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed_sig_hlf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed_bkg_hlf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normed_sig_list' is not defined"
     ]
    }
   ],
   "source": [
    "def fill_array(array_to_fill, value, index, batch_size):\n",
    "    array_to_fill[index*batch_size:min((index+1)*batch_size, array_to_fill.shape[0]),...] = value\n",
    "\n",
    "particles_val = np.concatenate((normed_sig_list, normed_bkg_list))\n",
    "hlf_val = np.concatenate((normed_sig_hlf, normed_bkg_hlf))\n",
    "y_val = np.concatenate((np.ones(len(normed_sig_hlf)),np.zeros(len(normed_bkg_hlf))))\n",
    "print(particles_val.shape)\n",
    "print(hlf_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "test_batch_size = 300\n",
    "\n",
    "all_pred = np.zeros(shape=(len(y_val),2))\n",
    "test_loader = DataLoader(ParticleHLF(particles_val, hlf_val, y_val), batch_size = test_batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "volatile=True\n",
    "\n",
    "for batch_idx, (particles_data, hlf_data, y_data) in enumerate(test_loader):\n",
    "    particles_data = particles_data.numpy()\n",
    "    arr = np.sum(particles_data!=0, axis=1)[:,0] # the number of particles in the whole batch\n",
    "    arr = [1 if x==0 else x for x in arr]\n",
    "    arr = np.array(arr)\n",
    "    sorted_indices_la= np.argsort(-arr)\n",
    "    particles_data = torch.from_numpy(particles_data[sorted_indices_la]).float()\n",
    "    hlf_data = hlf_data[sorted_indices_la]\n",
    "    particles_data = Variable(particles_data, volatile=volatile).cuda()\n",
    "    hlf_data = Variable(hlf_data, volatile).cuda()\n",
    "    t_seq_length= [arr[i] for i in sorted_indices_la]\n",
    "    particles_data = torch.nn.utils.rnn.pack_padded_sequence(particles_data, t_seq_length, batch_first=True)\n",
    "\n",
    "    outputs = model(particles_data, hlf_data)\n",
    "\n",
    "    # Unsort the predictions (to match the original data order)\n",
    "    # https://stackoverflow.com/questions/34159608/how-to-unsort-a-np-array-given-the-argsort\n",
    "    b = np.argsort(sorted_indices_la)\n",
    "    unsorted_pred = outputs[b].data.cpu().numpy()\n",
    "    fill_array(all_pred, unsorted_pred, batch_idx, test_batch_size)\n",
    "    \n",
    "fpr, tpr, thresholds = roc_curve(y_val, np.exp(all_pred)[:,1])\n",
    "area = auc(fpr, tpr)\n",
    "\n",
    "TPR_thresholds = [0.96, 0.935, 0.9, 0.7, 0.5, 0.3]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    NNtable.add_row([thresholds[thres_idx], tpr[thres_idx],  fpr[thres_idx]])\n",
    "print(NNtable)\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(fpr,tpr,label=\"NN AUC = {}\".format(area))\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Background contamination')\n",
    "plt.ylabel('Signal efficiency')\n",
    "#plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "#plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist(np.exp(all_pred)[y_val==0,1], bins=60, label='ttH background',alpha=0.5, normed=True)\n",
    "plt.hist(np.exp(all_pred)[y_val==1,1], bins=60, label='HH signal', alpha=0.5, normed=True)\n",
    "#plt.axvline(thresholds[thres_idx], ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Threshold\", fontsize=18)\n",
    "\n",
    "sig_frame = pd.DataFrame.from_records(signp)\n",
    "bkg_frame = pd.DataFrame.from_records(bkgnp)\n",
    "\n",
    "sig_frame['NN_score'] = pd.Series(np.exp(all_pred[:len(normed_sig_hlf),1]), index=sig_frame.index)\n",
    "bkg_frame['NN_score'] = pd.Series(np.exp(all_pred[len(normed_sig_hlf):,1]), index=bkg_frame.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check mistagged samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2544 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-ac914d122e28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m }\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mbackground_mistag_thres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0msignal_mistag_thres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpr\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.97\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Threshold for bkg mistag: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_mistag_thres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2544 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "features_to_check = {\n",
    "    'sumEt': [40, 0, 2500],\n",
    "    'MET': [40,0,500],\n",
    "    'phiMET': [40, -3.15, 3.15],\n",
    "    'dPhi1': [40, -3.15, 3,15],\n",
    "    'dPhi2': [40, -3.15, 3,15],\n",
    "    'PhoJetMinDr': [40, 0, 5],\n",
    "    'njets': [12, 0, 12],\n",
    "    'Xtt0': [40, 0, 1000],\n",
    "    'Xtt1': [40, 0, 1000],\n",
    "    'pte1': [40, 0, 500],\n",
    "    'pte2': [40, 0, 500],\n",
    "    'ptmu1': [40, 0, 500],\n",
    "    'ptmu2': [40, 0, 500],\n",
    "    'ptdipho': [40, 0, 1500],\n",
    "    'etae1': [40, -3.15, 3.15],\n",
    "    'etae2': [40, -3.15, 3.15],\n",
    "    'etamu1': [40, -3.15, 3.15],\n",
    "    'etamu2': [40, -3.15, 3.15],\n",
    "    'etadipho': [40, -5, 5],\n",
    "    'phie1': [40, -3.15, 3.15],\n",
    "    'phie2': [40, -3.15, 3.15],\n",
    "    'phimu1': [40, -3.15, 3.15],\n",
    "    'phimu2': [40, -3.15, 3.15],\n",
    "    'phidipho': [40, -3.15, 3.15],\n",
    "    'fabs_CosThetaStar_CS': [40, 0, 1],\n",
    "    'fabs_CosTheta_bb': [40, 0, 1]\n",
    "}\n",
    "\n",
    "background_mistag_thres = thresholds[np.argmax(fpr>0.005)]\n",
    "signal_mistag_thres = thresholds[np.argmax(tpr>0.97)]\n",
    "print(\"Threshold for bkg mistag: {}\".format(background_mistag_thres))\n",
    "print(\"Threshold for signal mistag: {}\".format(signal_mistag_thres))\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist(np.exp(all_pred)[y_val==0,1], bins=60, label='ttH background',alpha=0.5, normed=True)\n",
    "plt.hist(np.exp(all_pred)[y_val==1,1], bins=60, label='HH signal', alpha=0.5, normed=True)\n",
    "plt.axvline(background_mistag_thres, ls='--',color='tab:gray')\n",
    "plt.axvline(signal_mistag_thres, ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Threshold\", fontsize=18)\n",
    "\n",
    "\n",
    "for feat in features_to_check:\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.hist(bkg_frame[feat], bins=features_to_check[feat][0], \n",
    "             range=(features_to_check[feat][1], features_to_check[feat][2]),\n",
    "             label='Full background',\n",
    "             histtype='stepfilled',\n",
    "             alpha=0.4,\n",
    "             normed=True\n",
    "             )\n",
    "    plt.hist(sig_frame[feat], bins=features_to_check[feat][0], \n",
    "             range=(features_to_check[feat][1], features_to_check[feat][2]),\n",
    "              label='Full signal',\n",
    "              histtype='stepfilled',\n",
    "             alpha=0.4,\n",
    "             normed=True,\n",
    "            )\n",
    "    plt.hist(sig_frame[feat][sig_frame['NN_score']<signal_mistag_thres], bins=features_to_check[feat][0], \n",
    "             range=(features_to_check[feat][1], features_to_check[feat][2]),\n",
    "             label='Signal predicted as bkg',\n",
    "            histtype='step',\n",
    "            linewidth=3,\n",
    "             normed=True,\n",
    "            )\n",
    "    plt.hist(bkg_frame[feat][bkg_frame['NN_score']>background_mistag_thres], bins=features_to_check[feat][0], \n",
    "             range=(features_to_check[feat][1], features_to_check[feat][2]),\n",
    "             histtype='step',\n",
    "             linewidth=3,\n",
    "             label='Bkg predicted as signal',\n",
    "             normed=True\n",
    "            )\n",
    "    plt.xlabel(feat, fontsize=15)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2),\n",
    "          ncol=2, fancybox=True, fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=25\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(bkg_aux_frame['mass_jj'], bins=100, histtype='step', label='No cut',normed=True)\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(bkg_aux_frame['mass_jj'][bkg_frame['NN_score']>thresholds[thres_idx]], \n",
    "             bins=100, histtype='step', \n",
    "             label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             normed=True)\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Background events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{bb}$',fontsize=fontsize)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(sig_aux_frame['mass_jj'], bins=100, histtype='step', label='No cut', normed=True)\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(sig_aux_frame['mass_jj'][sig_frame['NN_score']>thresholds[thres_idx]], \n",
    "             bins=100, histtype='step', label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             normed=True)\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Signal events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{bb}$',fontsize=fontsize)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(bkg_aux_frame['mass_gg'], bins=100, range=(115,135), histtype='step', label='No cut', normed=True)\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(bkg_aux_frame['mass_gg'][bkg_frame['NN_score']>thresholds[thres_idx]], \n",
    "             bins=100, histtype='step', \n",
    "             range=(115,135),\n",
    "             label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             normed=True)\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Background events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{\\gamma \\gamma}$',fontsize=fontsize)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(sig_aux_frame['mass_gg'], bins=100, range=(115,135), histtype='step', label='No cut', normed=True)\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(sig_aux_frame['mass_gg'][sig_frame['NN_score']>thresholds[thres_idx]], \n",
    "             bins=100, \n",
    "             histtype='step', \n",
    "             range=(115,135),\n",
    "             label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             normed=True)\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Signal events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{\\gamma \\gamma}$',fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"DNN_ROC.h5\",\"r\") as rocfile:\n",
    "    fpr_dnn = rocfile['FPR'][:]\n",
    "    dfpr_dnn = rocfile['dFPR'][:]\n",
    "    tpr_dnn = rocfile['TPR'][:]\n",
    "    thres_dnn = rocfile['Thresholds'][:]\n",
    "    area_dnn = auc(fpr_dnn, tpr_dnn)\n",
    "    fprs_dnn_right = np.minimum(fpr_dnn + dfpr_dnn, 1)\n",
    "    fprs_dnn_left = np.maximum(fpr_dnn - dfpr_dnn,0)\n",
    "    darea_dnn = (1-auc(tpr_dnn,fprs_dnn_left))-area_dnn\n",
    "    \n",
    "with h5py.File(\"BDT_ROC.h5\",\"r\") as rocfile:\n",
    "    fpr_bdt = rocfile['FPR'][:]\n",
    "    tpr_bdt = rocfile['TPR'][:]\n",
    "    thres_bdt = rocfile['Thresholds'][:]\n",
    "    area_bdt = auc(fpr_bdt, tpr_bdt)\n",
    "\n",
    "with h5py.File(\"ReallyInclusive_ROC.h5\",\"r\") as rocfile:\n",
    "    fpr_inc = rocfile['FPR'][:]\n",
    "    dfpr_inc = rocfile['dFPR'][:]\n",
    "    tpr_inc = rocfile['TPR'][:]\n",
    "    thres_inc = rocfile['Thresholds'][:]\n",
    "    area_inc = auc(fpr_inc, tpr_inc)\n",
    "    fprs_inc_right = np.minimum(fpr_inc + dfpr_dnn, 1)\n",
    "    fprs_inc_left = np.maximum(fpr_inc - dfpr_dnn,0)\n",
    "    darea_inc = (1-auc(tpr_inc,fprs_inc_left))-area_inc\n",
    "\n",
    "### Compare\n",
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "plt.plot(fpr_bdt,tpr_bdt,color='blue',label=\"BDT AUC = {:.4f}\".format(area_bdt))\n",
    "\n",
    "\n",
    "plt.plot(fpr_dnn,tpr_dnn,color='green',label=r\"VanillaDNN AUC = {:.4f}$\\pm${:.4f}\".format(area_dnn, darea_dnn))\n",
    "plt.fill_betweenx(tpr_dnn, fprs_dnn_left, fprs_dnn_right, color='green', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.plot(fpr_inc,tpr_inc,color='orange',label=r\"InclusiveNet AUC = {:.4f}$\\pm${:.4f}\".format(area_inc, darea_inc))\n",
    "plt.fill_betweenx(tpr_inc, fprs_inc_left, fprs_inc_right, color='orange', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Background contamination',fontsize=18)\n",
    "plt.ylabel('Signal efficiency',fontsize=18)\n",
    "# # plt.xlim(0.01,0.6)\n",
    "# # plt.ylim(0.2,1)\n",
    "# plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "# plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "plt.legend(loc='best',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_thresholds = [0.96, 0.935, 0.9, 0.7, 0.5]\n",
    "\n",
    "table = PrettyTable(['Signal Efficiency','BDT','VanillaDNN','InclusiveNet'])\n",
    "table.get_string(title=\"Background contamination at different signal efficiencies\")\n",
    "table.float_format = \".2\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_dnn = np.argmax(tpr_dnn>TPR_threshold)\n",
    "    thres_bdt = np.argmax(tpr_bdt>TPR_threshold)\n",
    "    thres_inc = np.argmax(tpr_inc>TPR_threshold)\n",
    "\n",
    "    table.add_row([\"{:.2f}%\".format(100*(tpr_dnn[thres_dnn])),  \"{:.2f}%\".format(100*fpr_bdt[thres_bdt]), \"({:.2f} +/- {:.2f})%\".format(100*fpr_dnn[thres_dnn], 100*dfpr_dnn[thres_dnn]), \"({:.2f} +/- {:.2f})%\".format(100*fpr_inc[thres_inc], 100*dfpr_inc[thres_inc])])\n",
    "print(table.get_string(title=\"Background contamination at different signal efficiencies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = ['pte1','pte2','ptmu1','ptmu2','ptdipho',\n",
    "            'etae1','etae2','etamu1','etamu2','etadipho',\n",
    "            'phie1','phie2','phimu1','phimu2','phidipho',\n",
    "            'MET','phiMET']\n",
    "\n",
    "for fea in features:\n",
    "    plt.figure()\n",
    "    plt.hist(sig_frame[fea][sig_frame[fea]!=0], bins=40, normed=True, \n",
    "             histtype='stepfilled', alpha=0.3, label='GluGluToHHTo2B2G Signal')\n",
    "    plt.hist(bkg_frame[fea][bkg_frame[fea]!=0], bins=40, normed=True, \n",
    "             histtype='stepfilled', alpha=0.45, label='ttHToGG Background')\n",
    "    plt.xlabel(fea,fontsize=15)\n",
    "    plt.legend(loc='best',fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.456e+03, 2.635e+03, 2.481e+03, 2.183e+03, 2.038e+03, 1.273e+03,\n",
       "        9.560e+02, 7.720e+02, 5.780e+02, 4.210e+02, 3.390e+02, 2.480e+02,\n",
       "        1.850e+02, 1.560e+02, 1.150e+02, 7.500e+01, 8.000e+01, 7.400e+01,\n",
       "        5.200e+01, 3.200e+01, 3.700e+01, 3.300e+01, 2.400e+01, 2.400e+01,\n",
       "        1.000e+01, 1.500e+01, 1.300e+01, 6.000e+00, 2.000e+00, 1.200e+01,\n",
       "        8.000e+00, 3.000e+00, 3.000e+00, 5.000e+00, 3.000e+00, 3.000e+00,\n",
       "        2.000e+00, 3.000e+00, 1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00,\n",
       "        0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 1.000e+00, 0.000e+00,\n",
       "        0.000e+00, 2.000e+00]),\n",
       " array([-1.24886137, -0.98233134, -0.71580131, -0.44927128, -0.18274124,\n",
       "         0.08378879,  0.35031882,  0.61684886,  0.88337889,  1.14990892,\n",
       "         1.41643896,  1.68296899,  1.94949902,  2.21602906,  2.48255909,\n",
       "         2.74908912,  3.01561916,  3.28214919,  3.54867922,  3.81520926,\n",
       "         4.08173929,  4.34826932,  4.61479936,  4.88132939,  5.14785942,\n",
       "         5.41438946,  5.68091949,  5.94744952,  6.21397955,  6.48050959,\n",
       "         6.74703962,  7.01356965,  7.28009969,  7.54662972,  7.81315975,\n",
       "         8.07968979,  8.34621982,  8.61274985,  8.87927989,  9.14580992,\n",
       "         9.41233995,  9.67886999,  9.94540002, 10.21193005, 10.47846009,\n",
       "        10.74499012, 11.01152015, 11.27805019, 11.54458022, 11.81111025,\n",
       "        12.07764029]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f429f2ab090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data_hlf[:,0],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
