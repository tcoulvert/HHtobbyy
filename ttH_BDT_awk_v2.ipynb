{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu3.fnal.gov      Wed Feb  5 13:08:06 2025  555.42.06\n",
      "[0] Tesla P100-PCIE-12GB | 43Â°C,   1 % |     0 / 12288 MB |\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib widget\n",
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "from scipy.special import logit as inverse_sigmoid\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import h5py\n",
    "import hist\n",
    "import mplhep as hep\n",
    "import xgboost as xgb\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, fbeta_score\n",
    "from scipy.integrate import trapezoid\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "# Module packages\n",
    "from data_processing_BDT_v2 import process_data\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Locations and Model Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v2/Run3_2022_merged_v1/sim\"\n",
    "\n",
    "FILEPATHS_DICT = {\n",
    "    'ggF HH': [\n",
    "        lpc_fileprefix+f\"/preEE/GluGlutoHHto2B2G_kl_1p00_kt_1p00_c2_0p00/nominal/*\", \n",
    "        lpc_fileprefix+f\"/postEE/GluGluToHH/nominal/*merged.parquet\"\n",
    "    ],\n",
    "    # 'VBF HH': [\n",
    "    #     lpc_fileprefix+f\"/Run3_2022preEE_merged_v4/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", \n",
    "    #     lpc_fileprefix+f\"/Run3_2022postEE_merged_v4/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\"\n",
    "    # ],\n",
    "    'ttH': [\n",
    "        # ttH\n",
    "        lpc_fileprefix+f\"/preEE/ttHtoGG_M_125/nominal/*merged.parquet\", \n",
    "        lpc_fileprefix+f\"/postEE/ttHToGG/nominal/*merged.parquet\",\n",
    "        # # bbH\n",
    "        # lpc_fileprefix+f\"/preEE/BBHto2G_M_125/nominal/*merged.parquet\", \n",
    "        # lpc_fileprefix+f\"/postEE/BBHto2G_M_125/nominal/*merged.parquet\",\n",
    "    ],\n",
    "    'VH': [\n",
    "        # VH\n",
    "        lpc_fileprefix+f\"/preEE/VHtoGG_M_125/nominal/*merged.parquet\", \n",
    "        lpc_fileprefix+f\"/postEE/VHToGG/nominal/*merged.parquet\",\n",
    "        # # ZH\n",
    "        # lpc_fileprefix+f\"/preEE/ZH_Hto2G_Zto2Q_M-125/nominal/*merged.parquet\", \n",
    "        # lpc_fileprefix+f\"/postEE/ZH_Hto2G_Zto2Q_M-125/nominal/*merged.parquet\",\n",
    "        # # W-H\n",
    "        # lpc_fileprefix+f\"/preEE/WminusH_Hto2G_Wto2Q_M-125/nominal/*merged.parquet\", \n",
    "        # lpc_fileprefix+f\"/postEE/WminusH_Hto2G_Wto2Q_M-125/nominal/*merged.parquet\",\n",
    "        # # W+H\n",
    "        # lpc_fileprefix+f\"/preEE/WplusH_Hto2G_Wto2Q_M-125/nominal/*merged.parquet\", \n",
    "        # lpc_fileprefix+f\"/postEE/WplusH_Hto2G_Wto2Q_M-125/nominal/*merged.parquet\",\n",
    "    ],\n",
    "    'non-res + ggFH + VBFH': [\n",
    "        # GG + 3Jets\n",
    "        lpc_fileprefix+f\"/preEE/GGJets/nominal/*merged.parquet\", \n",
    "        lpc_fileprefix+f\"/postEE/GGJets/nominal/*merged.parquet\",\n",
    "        # GJet pT 20-40\n",
    "        lpc_fileprefix+f\"/preEE/GJetPt20To40/nominal/*merged.parquet\", \n",
    "        lpc_fileprefix+f\"/postEE/GJetPt20To40/nominal/*merged.parquet\",\n",
    "        # GJet pT 40-inf\n",
    "        # lpc_fileprefix+f\"/preEE/GJetPt40/nominal/*merged.parquet\", \n",
    "        lpc_fileprefix+f\"/postEE/GJetPt40/nominal/*merged.parquet\",\n",
    "        # ggF H\n",
    "        lpc_fileprefix+f\"/preEE/GluGluHToGG_M_125/nominal/*merged.parquet\", \n",
    "        lpc_fileprefix+f\"/postEE/GluGluHToGG/nominal/*merged.parquet\",\n",
    "        # VBF H\n",
    "        lpc_fileprefix+f\"/preEE/VBFHToGG_M_125/nominal/*merged.parquet\", \n",
    "        lpc_fileprefix+f\"/postEE/VBFHToGG/nominal/*merged.parquet\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v9'\n",
    "MOD_VALS = (5, 5)\n",
    "VARS = 'v2_vars'\n",
    "# CURRENT_TIME = '2025-02-04_19-13-48'\n",
    "if 'CURRENT_TIME' in globals():\n",
    "    OUTPUT_DIRPATH = os.path.join(CURRENT_DIRPATH, f\"MultiClassBDT_model_outputs/{VERSION}/{VARS}\", CURRENT_TIME)\n",
    "else:\n",
    "    OUTPUT_DIRPATH = os.path.join(CURRENT_DIRPATH, f\"MultiClassBDT_model_outputs/{VERSION}/{VARS}\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "OTHER_BKG_RESCALE = 50\n",
    "OPTIMIZE_SPACE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_weights(event_weights, labels, order=None, weighttype='rescaled_and_shifted', sig_rescale_factor=None):\n",
    "    if weighttype == 'abs':\n",
    "        return np.abs(event_weights)\n",
    "    \n",
    "    if order is not None:\n",
    "        sig_idx, big_bkg_idx = -1, -1\n",
    "        for i, sample_name in enumerate(order):\n",
    "            if re.search('ggF HH', sample_name) is not None:\n",
    "                sig_idx = i\n",
    "                continue\n",
    "            if re.search('non-res', sample_name) is not None:\n",
    "                big_bkg_idx = i\n",
    "                continue\n",
    "    else:\n",
    "        sig_idx, big_bkg_idx = 0, len(order)-1\n",
    "    \n",
    "    if sig_rescale_factor is None:\n",
    "        sig_sum = np.sum(event_weights[labels[:, sig_idx] == 1])\n",
    "        bkg_sum = np.sum(event_weights[labels[:, sig_idx] == 0])\n",
    "        \n",
    "        sig_rescale_factor = bkg_sum / sig_sum\n",
    "\n",
    "    scaled_weights = np.where(\n",
    "        labels[:, sig_idx] == 0, \n",
    "        np.where(\n",
    "            np.argmax(labels, axis=1) != big_bkg_idx,  \n",
    "            event_weights * OTHER_BKG_RESCALE,  # if not big bkg, rescale\n",
    "            event_weights  # otherwise do nothing\n",
    "        ),\n",
    "        event_weights * sig_rescale_factor  # if sig, rescale to equal sum of all bkgs\n",
    "    )\n",
    "\n",
    "    abs_weights = np.abs(scaled_weights)\n",
    "\n",
    "    if weighttype == 'rescaled':\n",
    "        return abs_weights\n",
    "    elif weighttype == 'rescaled_and_shifted':\n",
    "        mean_weights = np.mean(scaled_weights)\n",
    "        rescaled_weights = abs_weights / mean_weights\n",
    "        return rescaled_weights\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"The only options for weighttype are 'abs', 'rescaled', and 'rescaled_and_shifted'. You provided {weighttype}\"\n",
    "        )\n",
    "\n",
    "# def training_weights(event_weights, labels, order=None, sig_rescale_factor=None):\n",
    "#     if order is None:\n",
    "#         order = [i for v in range(np.shape(labels)[0])]\n",
    "#     sum_dict, max_sum, max_i = {}, 0, 0\n",
    "#     for i, sample_name in enumerate(order):\n",
    "#         sum_dict[i] = np.sum(event_weights[labels[:, i] == 1])\n",
    "#         if np.sum(event_weights[labels[:, i] == 1]) > max_sum:\n",
    "#             max_sum, max_i = np.sum(event_weights[labels[:, i] == 1]), i\n",
    "\n",
    "#     label_i = np.sum(\n",
    "#         np.tile([i for i in range(np.shape(labels)[1])], (np.shape(labels)[0], 1)) * labels,\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "#     weight_factors = []\n",
    "#     for i in range(len(label_i)):\n",
    "#         weight_factors.append(\n",
    "#             max_sum / sum_dict[label_i[i]] if label_i[i] != max_i else 1\n",
    "#         )\n",
    "#     weights = event_weights * np.array(weight_factors)\n",
    "\n",
    "#     mean_weight = np.mean(weights)\n",
    "#     abs_weights = np.abs(weights)\n",
    "#     scaled_weights = abs_weights / mean_weight\n",
    "\n",
    "#     return scaled_weights\n",
    "\n",
    "\n",
    "def xgb_labels(labels):\n",
    "    label_i = np.sum(\n",
    "        np.tile([i for i in range(np.shape(labels)[1])], (np.shape(labels)[0], 1)) * labels,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return label_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Input Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order = ['ggF HH', 'ttH', 'single-H', 'non-res']\n",
    "order = ['ggF HH', 'ttH', 'VH', 'non-res + ggFH + VBFH']\n",
    "\n",
    "(\n",
    "    sig_rescale_factor,\n",
    "    data_df_dict, data_test_df_dict, \n",
    "    data_hlf_dict, label_dict,\n",
    "    data_hlf_test_dict, label_test_dict, \n",
    "    hlf_vars_columns_dict,\n",
    "    data_aux_dict, data_test_aux_dict\n",
    ") = process_data(\n",
    "    FILEPATHS_DICT, OUTPUT_DIRPATH, order=order, mod_vals=MOD_VALS,\n",
    "    save=False if 'CURRENT_TIME' in globals() else True,\n",
    "    std_json_dirpath=OUTPUT_DIRPATH if 'CURRENT_TIME' in globals() else None,\n",
    "    other_bkg_rescale=OTHER_BKG_RESCALE\n",
    ")\n",
    "\n",
    "# Make xgb-like labels (NOT one-hot encoded, but integer encoded for each class)\n",
    "xgb_label_dict = {\n",
    "    f\"fold_{fold_idx}\": copy.deepcopy(xgb_labels(label_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "xgb_label_test_dict = {\n",
    "    f\"fold_{fold_idx}\": copy.deepcopy(xgb_labels(label_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "# Make weight dicts:\n",
    "#   - the top two are with the training rescale (i.e. rescale sig eventWeight to match bkg and then shift for gradients)\n",
    "#   - the bottom two are the standard eventWeights (i.e. xs * lumi * genWeight) for proper plotting\n",
    "weight_train_dict = {\n",
    "    f\"fold_{fold_idx}\": copy.deepcopy(training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}'], order=order, sig_rescale_factor=sig_rescale_factor)) for fold_idx in range(len(data_aux_dict))\n",
    "}\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(training_weights(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_test_dict[f'fold_{fold_idx}'], order=order, sig_rescale_factor=sig_rescale_factor)) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "weights_plot_train_dict = {\n",
    "    f\"fold_{fold_idx}\": copy.deepcopy(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_aux_dict))\n",
    "}\n",
    "weights_plot_test = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Num train: 2583704 -> 80757 sig & 206979 ttH bkg & 95961 single-H bkg & 2200007 non-res bkg\n",
      "Num val: 645926 -> 20207 sig & 51661 ttH bkg & 24057 single-H bkg & 550001 non-res bkg\n",
      "Num test: 808435 -> 25219 sig & 64846 ttH bkg & 29965 single-H bkg & & 688405 non-res bkg\n",
      "============================================================\n",
      "fold 1\n",
      "Num train: 2584964 -> 80554 sig & 207230 ttH bkg & 95679 single-H bkg & 2201501 non-res bkg\n",
      "Num val: 646241 -> 20172 sig & 51948 ttH bkg & 24182 single-H bkg & 549939 non-res bkg\n",
      "Num test: 806860 -> 25457 sig & 64308 ttH bkg & 30122 single-H bkg & & 686973 non-res bkg\n",
      "============================================================\n",
      "fold 2\n",
      "Num train: 2583336 -> 80550 sig & 206947 ttH bkg & 96209 single-H bkg & 2199630 non-res bkg\n",
      "Num val: 645834 -> 20392 sig & 51367 ttH bkg & 23932 single-H bkg & 550143 non-res bkg\n",
      "Num test: 808895 -> 25241 sig & 65172 ttH bkg & 29842 single-H bkg & & 688640 non-res bkg\n",
      "============================================================\n",
      "fold 3\n",
      "Num train: 2585540 -> 80899 sig & 206852 ttH bkg & 95741 single-H bkg & 2202048 non-res bkg\n",
      "Num val: 646386 -> 20173 sig & 51749 ttH bkg & 24240 single-H bkg & 550224 non-res bkg\n",
      "Num test: 806139 -> 25111 sig & 64885 ttH bkg & 30002 single-H bkg & & 686141 non-res bkg\n",
      "============================================================\n",
      "fold 4\n",
      "Num train: 2584263 -> 80959 sig & 207150 ttH bkg & 95868 single-H bkg & 2200286 non-res bkg\n",
      "Num val: 646066 -> 20069 sig & 52061 ttH bkg & 24063 single-H bkg & 549873 non-res bkg\n",
      "Num test: 807736 -> 25155 sig & 64275 ttH bkg & 30052 single-H bkg & & 688254 non-res bkg\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "bdt_train_dict, bdt_val_dict, bdt_test_dict = {}, {}, {}\n",
    "\n",
    "train_data_dict, val_data_dict = {}, {}\n",
    "xgb_label_train_dict, xgb_label_val_dict = {}, {}\n",
    "weights_plot_train, weights_plot_val= {}, {}\n",
    "train_idxs_dict, val_idxs_dict = {}, {}\n",
    "for fold_idx in range(len(data_df_dict)):\n",
    "    if re.search('no_std', VARS) is not None:\n",
    "        print('no standardization')\n",
    "        train_val_data_dict = {key: value.to_numpy() for key, value in data_df_dict.items()}\n",
    "        test_data_dict = {key: value.to_numpy() for key, value in data_test_df_dict.items()}\n",
    "    else:\n",
    "        train_val_data_dict = data_hlf_dict\n",
    "        test_data_dict = data_hlf_test_dict\n",
    "    (\n",
    "        X_train, X_val, \n",
    "        y_train, y_val, \n",
    "        weight_train, weight_val, \n",
    "        weight_plot_train, weight_plot_val,\n",
    "        train_idxs, val_idxs\n",
    "    ) = train_test_split(\n",
    "        train_val_data_dict[f\"fold_{fold_idx}\"], xgb_label_dict[f\"fold_{fold_idx}\"], \n",
    "        weight_train_dict[f\"fold_{fold_idx}\"], weights_plot_train_dict[f\"fold_{fold_idx}\"],\n",
    "        range(len(train_val_data_dict[f\"fold_{fold_idx}\"])),\n",
    "        test_size=0.2, random_state=21\n",
    "    )\n",
    "\n",
    "    train_data_dict[f\"fold_{fold_idx}\"] = copy.deepcopy(X_train)\n",
    "    val_data_dict[f\"fold_{fold_idx}\"] = copy.deepcopy(X_val)\n",
    "\n",
    "    xgb_label_train_dict[f\"fold_{fold_idx}\"] = copy.deepcopy(y_train)\n",
    "    xgb_label_val_dict[f\"fold_{fold_idx}\"] = copy.deepcopy(y_val)\n",
    "\n",
    "    weights_plot_train[f\"fold_{fold_idx}\"] = copy.deepcopy(weight_plot_train)\n",
    "    weights_plot_val[f\"fold_{fold_idx}\"] = copy.deepcopy(weight_plot_val)\n",
    "\n",
    "    train_idxs_dict[f\"fold_{fold_idx}\"] = copy.deepcopy(train_idxs)\n",
    "    val_idxs_dict[f\"fold_{fold_idx}\"] = copy.deepcopy(val_idxs)\n",
    "\n",
    "    bdt_train_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "        data=X_train, label=y_train, \n",
    "        weight=weight_train,\n",
    "        missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "    )\n",
    "    bdt_val_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "        data=X_val, label=y_val, \n",
    "        weight=weight_val,\n",
    "        missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "    )\n",
    "    \n",
    "    bdt_test_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "        data=test_data_dict[f\"fold_{fold_idx}\"], label=xgb_label_test_dict[f\"fold_{fold_idx}\"], \n",
    "        weight=np.abs(weight_test_dict[f\"fold_{fold_idx}\"]),\n",
    "        missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "    )\n",
    "\n",
    "    print(f\"fold {fold_idx}\")\n",
    "    print(f\"Num train: {len(y_train)} -> {sum(y_train == 0)} sig & {sum(y_train == 1)} ttH bkg & {sum(y_train == 2)} single-H bkg & {sum(y_train == 3)} non-res bkg\")\n",
    "    print(f\"Num val: {len(y_val)} -> {sum(y_val == 0)} sig & {sum(y_val == 1)} ttH bkg & {sum(y_val == 2)} single-H bkg & {sum(y_val == 3)} non-res bkg\")\n",
    "    print(f\"Num test: {len(label_test_dict[f'fold_{fold_idx}'])} -> {sum(label_test_dict[f'fold_{fold_idx}'] == np.array([1, 0, 0, 0]))[0]} sig & {sum(label_test_dict[f'fold_{fold_idx}'] == np.array([0, 1, 0, 0]))[1]} ttH bkg & {sum(label_test_dict[f'fold_{fold_idx}'] == np.array([0, 0, 1, 0]))[2]} single-H bkg & & {sum(label_test_dict[f'fold_{fold_idx}'] == np.array([0, 0, 0, 1]))[3]} non-res bkg\")\n",
    "    print('='*60)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameterization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57986259/multiclass-classification-with-xgboost-classifier\n",
    "# https://forecastegy.com/posts/xgboost-multiclass-classification-python/\n",
    "# https://indico.cern.ch/event/915265/contributions/3848138/attachments/2048174/3432202/kunlinRan_bbyy_20200531.pdf\n",
    "\n",
    "# https://xgboost.readthedocs.io/en/stable/tutorials/intercept.html - for looking at logits level BDT output\n",
    "# https://indico.cern.ch/event/915265/contributions/3848138/attachments/2048174/3432202/kunlinRan_bbyy_20200531.pdf - ATLAS HHbbgg BDT\n",
    "\n",
    "\n",
    "param = {}\n",
    "\n",
    "# Booster parameters\n",
    "param['eta']              = 0.05 # learning rate\n",
    "num_trees = round(25 / param['eta'])  # number of trees to make\n",
    "# param['max_depth']        = 8  # maximum depth of a tree\n",
    "# param['subsample']        = 1 # fraction of events to train tree on\n",
    "# param['colsample_bytree'] = 0.33 # fraction of features to train tree on\n",
    "param['max_depth']        = 10  # maximum depth of a tree\n",
    "param['subsample']        = 0.6 # fraction of events to train tree on\n",
    "param['colsample_bytree'] = 0.6 # fraction of features to train tree on\n",
    "param['num_class']        = len(order) # num classes for multi-class training\n",
    "\n",
    "# Learning task parameters\n",
    "param['objective']   = 'multi:softprob'   # objective function\n",
    "param['eval_metric'] = 'merror'           # evaluation metric for cross validation\n",
    "param = list(param.items()) + [('eval_metric', 'mlogloss')]\n",
    "# param[\"disable_default_eval_metric\"] = True\n",
    "# param = list(param.items())\n",
    "\n",
    "\n",
    "def thresholded_weighted_merror(predt: np.ndarray, dtrain: xgb.DMatrix, threshold=0.95):\n",
    "    \"\"\"Used when there's no custom objective.\"\"\"\n",
    "    # No need to do transform, XGBoost handles it internally.\n",
    "    weights = dtrain.get_weight()\n",
    "    thresh_weight_merror = np.where(\n",
    "        np.logical_and(\n",
    "            np.max(predt, axis=1) >= threshold,\n",
    "            np.argmax(predt, axis=1) == dtrain.get_label()\n",
    "        ),\n",
    "        0,\n",
    "        weights\n",
    "    )\n",
    "    return f'WeightedMError@{threshold:.2f}', np.sum(thresh_weight_merror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparams(\n",
    "    dtrain_dict: dict, dval_dict: dict, dtest_dict: dict, param, verbose: bool=False, verbose_eval=False\n",
    "):  \n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    space  = [\n",
    "        Real(1e-3, 0.1, \"log-uniform\", name='eta'),\n",
    "        Integer(1, 20, \"uniform\", name='max_depth'),\n",
    "        Real(1e-3, 1., \"log-uniform\", name='subsample'),\n",
    "        Real(0.1, 1., \"uniform\", name='colsample_bytree'),\n",
    "        # Real(1e-4, 1., \"log-uniform\", name='alpha'),\n",
    "        # Real(1e-4, 1., \"log-uniform\", name='lambda'),\n",
    "    ]\n",
    "\n",
    "    score_arrs = []\n",
    "\n",
    "    @use_named_args(space)\n",
    "    def objective(**X):\n",
    "        if verbose:\n",
    "            print(\"New configuration: {}\".format(X))\n",
    "\n",
    "        for key, val in X.items():\n",
    "            param[key] = val\n",
    "        num_trees = round(25 / X['eta'])\n",
    "\n",
    "        # randomly sample a fold to evaluate\n",
    "        fold_idx = rng.integers(0, 4)\n",
    "\n",
    "        evallist = [(dtrain_dict[f\"fold_{fold_idx}\"], 'train'), (dtest_dict[f\"fold_{fold_idx}\"], 'test'), (dval_dict[f\"fold_{fold_idx}\"], 'val')]\n",
    "        booster = xgb.train(\n",
    "            param, dtrain_dict[f\"fold_{fold_idx}\"], num_boost_round=num_trees, \n",
    "            evals=evallist, early_stopping_rounds=10, verbose_eval=verbose_eval,\n",
    "        )\n",
    "\n",
    "        eval_str = booster.eval(dval_dict[f\"fold_{fold_idx}\"], name='val', iteration=booster.best_iteration)\n",
    "\n",
    "        best_mlogloss = float(eval_str[eval_str.find('val-mlogloss:')+len('val-mlogloss:'):])\n",
    "        score_arrs.append(best_mlogloss)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Best val. mlogloss on fold{fold_idx} = {best_mlogloss}\")\n",
    "\n",
    "        return -best_mlogloss\n",
    "    \n",
    "    res_gp = gp_minimize(objective, space)\n",
    "    print(\"Best parameters: {}\".format(res_gp.x))\n",
    "\n",
    "    param['eta'] = float(res_gp.x[0])\n",
    "    param['max_depth'] = int(res_gp.x[1])\n",
    "    param['subsample'] = float(res_gp.x[2])\n",
    "    param['colsample_bytree'] = float(res_gp.x[3])\n",
    "    # param['alpha'] = float(res_gp.x[4])\n",
    "    # param['lambda'] = float(res_gp.x[5])\n",
    "    return param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.11388\ttrain-mlogloss:1.31372\ttest-merror:0.13119\ttest-mlogloss:1.31520\tval-merror:0.13150\tval-mlogloss:1.31546\n",
      "[25]\ttrain-merror:0.08758\ttrain-mlogloss:0.51885\ttest-merror:0.10502\ttest-mlogloss:0.54260\tval-merror:0.10573\tval-mlogloss:0.54457\n",
      "[50]\ttrain-merror:0.08050\ttrain-mlogloss:0.31887\ttest-merror:0.10187\ttest-mlogloss:0.35728\tval-merror:0.10242\tval-mlogloss:0.35965\n",
      "[75]\ttrain-merror:0.07477\ttrain-mlogloss:0.25066\ttest-merror:0.09945\ttest-mlogloss:0.30119\tval-merror:0.09959\tval-mlogloss:0.30367\n",
      "[100]\ttrain-merror:0.06974\ttrain-mlogloss:0.22082\ttest-merror:0.09824\ttest-mlogloss:0.28077\tval-merror:0.09776\tval-mlogloss:0.28340\n",
      "[125]\ttrain-merror:0.06504\ttrain-mlogloss:0.20394\ttest-merror:0.09749\ttest-mlogloss:0.27261\tval-merror:0.09697\tval-mlogloss:0.27547\n",
      "[150]\ttrain-merror:0.06053\ttrain-mlogloss:0.19155\ttest-merror:0.09668\ttest-mlogloss:0.26870\tval-merror:0.09641\tval-mlogloss:0.27169\n",
      "[175]\ttrain-merror:0.05635\ttrain-mlogloss:0.18206\ttest-merror:0.09646\ttest-mlogloss:0.26708\tval-merror:0.09642\tval-mlogloss:0.27028\n",
      "[200]\ttrain-merror:0.05274\ttrain-mlogloss:0.17429\ttest-merror:0.09627\ttest-mlogloss:0.26654\tval-merror:0.09632\tval-mlogloss:0.26981\n",
      "[214]\ttrain-merror:0.05065\ttrain-mlogloss:0.17026\ttest-merror:0.09681\ttest-mlogloss:0.26654\tval-merror:0.09636\tval-mlogloss:0.26980\n",
      "[204]\ttest-merror:0.096810\ttest-mlogloss:0.266540\n",
      "====================================================================================================\n",
      "fold 1\n",
      "[0]\ttrain-merror:0.12989\ttrain-mlogloss:1.31568\ttest-merror:0.15036\ttest-mlogloss:1.31737\tval-merror:0.14925\tval-mlogloss:1.31742\n",
      "[25]\ttrain-merror:0.08702\ttrain-mlogloss:0.51617\ttest-merror:0.10679\ttest-mlogloss:0.54251\tval-merror:0.10504\tval-mlogloss:0.54039\n",
      "[50]\ttrain-merror:0.08044\ttrain-mlogloss:0.31906\ttest-merror:0.10319\ttest-mlogloss:0.36130\tval-merror:0.10259\tval-mlogloss:0.35855\n",
      "[75]\ttrain-merror:0.07470\ttrain-mlogloss:0.25036\ttest-merror:0.10086\ttest-mlogloss:0.30436\tval-merror:0.09984\tval-mlogloss:0.30114\n",
      "[100]\ttrain-merror:0.07008\ttrain-mlogloss:0.22069\ttest-merror:0.09924\ttest-mlogloss:0.28459\tval-merror:0.09833\tval-mlogloss:0.28101\n",
      "[125]\ttrain-merror:0.06527\ttrain-mlogloss:0.20380\ttest-merror:0.09855\ttest-mlogloss:0.27649\tval-merror:0.09660\tval-mlogloss:0.27298\n",
      "[150]\ttrain-merror:0.06095\ttrain-mlogloss:0.19181\ttest-merror:0.09833\ttest-mlogloss:0.27289\tval-merror:0.09592\tval-mlogloss:0.26917\n",
      "[175]\ttrain-merror:0.05687\ttrain-mlogloss:0.18263\ttest-merror:0.09776\ttest-mlogloss:0.27122\tval-merror:0.09577\tval-mlogloss:0.26739\n",
      "[200]\ttrain-merror:0.05324\ttrain-mlogloss:0.17466\ttest-merror:0.09767\ttest-mlogloss:0.27044\tval-merror:0.09556\tval-mlogloss:0.26653\n",
      "[223]\ttrain-merror:0.05003\ttrain-mlogloss:0.16813\ttest-merror:0.09761\ttest-mlogloss:0.27043\tval-merror:0.09572\tval-mlogloss:0.26666\n",
      "[213]\ttest-merror:0.097613\ttest-mlogloss:0.270428\n",
      "====================================================================================================\n",
      "fold 2\n",
      "[0]\ttrain-merror:0.10811\ttrain-mlogloss:1.31324\ttest-merror:0.12591\ttest-mlogloss:1.31485\tval-merror:0.12400\tval-mlogloss:1.31473\n",
      "[25]\ttrain-merror:0.08703\ttrain-mlogloss:0.51647\ttest-merror:0.10753\ttest-mlogloss:0.54216\tval-merror:0.10331\tval-mlogloss:0.53853\n",
      "[50]\ttrain-merror:0.08070\ttrain-mlogloss:0.31789\ttest-merror:0.10380\ttest-mlogloss:0.35956\tval-merror:0.10029\tval-mlogloss:0.35418\n",
      "[75]\ttrain-merror:0.07506\ttrain-mlogloss:0.25067\ttest-merror:0.10146\ttest-mlogloss:0.30387\tval-merror:0.09825\tval-mlogloss:0.29772\n",
      "[100]\ttrain-merror:0.06994\ttrain-mlogloss:0.22170\ttest-merror:0.09997\ttest-mlogloss:0.28441\tval-merror:0.09642\tval-mlogloss:0.27773\n",
      "[125]\ttrain-merror:0.06513\ttrain-mlogloss:0.20390\ttest-merror:0.09872\ttest-mlogloss:0.27561\tval-merror:0.09579\tval-mlogloss:0.26883\n",
      "[150]\ttrain-merror:0.06065\ttrain-mlogloss:0.19201\ttest-merror:0.09841\ttest-mlogloss:0.27159\tval-merror:0.09499\tval-mlogloss:0.26480\n",
      "[175]\ttrain-merror:0.05655\ttrain-mlogloss:0.18230\ttest-merror:0.09846\ttest-mlogloss:0.26980\tval-merror:0.09520\tval-mlogloss:0.26272\n",
      "[200]\ttrain-merror:0.05305\ttrain-mlogloss:0.17470\ttest-merror:0.09842\ttest-mlogloss:0.26897\tval-merror:0.09536\tval-mlogloss:0.26196\n",
      "[218]\ttrain-merror:0.05022\ttrain-mlogloss:0.16941\ttest-merror:0.09864\ttest-mlogloss:0.26908\tval-merror:0.09522\tval-mlogloss:0.26198\n",
      "[208]\ttest-merror:0.098635\ttest-mlogloss:0.269082\n",
      "====================================================================================================\n",
      "fold 3\n",
      "[0]\ttrain-merror:0.14149\ttrain-mlogloss:1.31710\ttest-merror:0.16037\ttest-mlogloss:1.31875\tval-merror:0.16326\tval-mlogloss:1.31896\n",
      "[25]\ttrain-merror:0.08821\ttrain-mlogloss:0.52133\ttest-merror:0.10603\ttest-mlogloss:0.54587\tval-merror:0.10827\tval-mlogloss:0.54768\n",
      "[50]\ttrain-merror:0.08103\ttrain-mlogloss:0.31972\ttest-merror:0.10222\ttest-mlogloss:0.35814\tval-merror:0.10345\tval-mlogloss:0.36062\n",
      "[75]\ttrain-merror:0.07530\ttrain-mlogloss:0.25131\ttest-merror:0.09959\ttest-mlogloss:0.30102\tval-merror:0.10038\tval-mlogloss:0.30405\n",
      "[100]\ttrain-merror:0.07012\ttrain-mlogloss:0.22128\ttest-merror:0.09693\ttest-mlogloss:0.28019\tval-merror:0.09940\tval-mlogloss:0.28372\n",
      "[125]\ttrain-merror:0.06539\ttrain-mlogloss:0.20418\ttest-merror:0.09652\ttest-mlogloss:0.27185\tval-merror:0.09838\tval-mlogloss:0.27576\n",
      "[150]\ttrain-merror:0.06086\ttrain-mlogloss:0.19194\ttest-merror:0.09565\ttest-mlogloss:0.26774\tval-merror:0.09879\tval-mlogloss:0.27185\n",
      "[175]\ttrain-merror:0.05679\ttrain-mlogloss:0.18236\ttest-merror:0.09577\ttest-mlogloss:0.26590\tval-merror:0.09834\tval-mlogloss:0.27034\n",
      "[200]\ttrain-merror:0.05278\ttrain-mlogloss:0.17422\ttest-merror:0.09599\ttest-mlogloss:0.26507\tval-merror:0.09818\tval-mlogloss:0.26966\n",
      "[221]\ttrain-merror:0.05012\ttrain-mlogloss:0.16839\ttest-merror:0.09620\ttest-mlogloss:0.26496\tval-merror:0.09808\tval-mlogloss:0.26967\n",
      "[212]\ttest-merror:0.096102\ttest-mlogloss:0.264977\n",
      "====================================================================================================\n",
      "fold 4\n",
      "[0]\ttrain-merror:0.13914\ttrain-mlogloss:1.31645\ttest-merror:0.16033\ttest-mlogloss:1.31827\tval-merror:0.16253\tval-mlogloss:1.31828\n",
      "[25]\ttrain-merror:0.08760\ttrain-mlogloss:0.51951\ttest-merror:0.10726\ttest-mlogloss:0.54570\tval-merror:0.10837\tval-mlogloss:0.54687\n",
      "[50]\ttrain-merror:0.08045\ttrain-mlogloss:0.31766\ttest-merror:0.10343\ttest-mlogloss:0.35949\tval-merror:0.10417\tval-mlogloss:0.36047\n",
      "[75]\ttrain-merror:0.07475\ttrain-mlogloss:0.24965\ttest-merror:0.10104\ttest-mlogloss:0.30366\tval-merror:0.10209\tval-mlogloss:0.30466\n",
      "[100]\ttrain-merror:0.06978\ttrain-mlogloss:0.22004\ttest-merror:0.09926\ttest-mlogloss:0.28385\tval-merror:0.10080\tval-mlogloss:0.28507\n",
      "[125]\ttrain-merror:0.06473\ttrain-mlogloss:0.20298\ttest-merror:0.09866\ttest-mlogloss:0.27598\tval-merror:0.10027\tval-mlogloss:0.27719\n",
      "[150]\ttrain-merror:0.06003\ttrain-mlogloss:0.19056\ttest-merror:0.09854\ttest-mlogloss:0.27240\tval-merror:0.10009\tval-mlogloss:0.27351\n",
      "[175]\ttrain-merror:0.05600\ttrain-mlogloss:0.18143\ttest-merror:0.09797\ttest-mlogloss:0.27072\tval-merror:0.09983\tval-mlogloss:0.27203\n",
      "[200]\ttrain-merror:0.05225\ttrain-mlogloss:0.17356\ttest-merror:0.09826\ttest-mlogloss:0.27036\tval-merror:0.09920\tval-mlogloss:0.27142\n",
      "[216]\ttrain-merror:0.04995\ttrain-mlogloss:0.16894\ttest-merror:0.09802\ttest-mlogloss:0.27032\tval-merror:0.09938\tval-mlogloss:0.27119\n",
      "[206]\ttest-merror:0.098021\ttest-mlogloss:0.270323\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "if 'CURRENT_TIME' in globals():\n",
    "    OUTPUT_DIRPATH, OLD_TIME = os.path.split(OUTPUT_DIRPATH)\n",
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "OUTPUT_DIRPATH = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME)\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "\n",
    "    print('OPTIMIZING SPACE')\n",
    "\n",
    "    param_dict = {}\n",
    "    for name, value in param:\n",
    "        if value == 'merror':\n",
    "            continue\n",
    "        param_dict[name] = value\n",
    "        \n",
    "    param = optimize_hyperparams(bdt_train_dict, bdt_val_dict, bdt_test_dict, param_dict, verbose=True, verbose_eval=50)\n",
    "\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_best_params.json'), 'w') as f:\n",
    "        json.dump(param, f)\n",
    "        print(param)\n",
    "\n",
    "    param['eval_metric'] = 'merror'\n",
    "    param = list(param.items()) + [('eval_metric', 'mlogloss')]\n",
    "\n",
    "evals_result_dict = {f\"fold_{fold_idx}\": dict() for fold_idx in range(len(bdt_train_dict))}\n",
    "for fold_idx in range(len(bdt_train_dict)):\n",
    "    print(f\"fold {fold_idx}\")\n",
    "    # Train bdt\n",
    "    evallist = [(bdt_train_dict[f\"fold_{fold_idx}\"], 'train'), (bdt_test_dict[f\"fold_{fold_idx}\"], 'test'), (bdt_val_dict[f\"fold_{fold_idx}\"], 'val')]\n",
    "    booster = xgb.train(\n",
    "        param, bdt_train_dict[f\"fold_{fold_idx}\"], num_boost_round=num_trees, \n",
    "        evals=evallist, early_stopping_rounds=10, verbose_eval=25, evals_result=evals_result_dict[f\"fold_{fold_idx}\"],\n",
    "        # custom_metric=thresholded_weighted_merror\n",
    "    )\n",
    "\n",
    "    booster.save_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "    \n",
    "    # Print perf on test dataset\n",
    "    print(booster.eval(bdt_test_dict[f\"fold_{fold_idx}\"], name='test', iteration=booster.best_iteration))\n",
    "    print('='*100)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_eval_result.json'), 'w') as f:\n",
    "    json.dump(evals_result_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance (ROC) Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tpr = np.linspace(0, 1, 5000)  # copied from IN evaluate.py file\n",
    "roc_baseline = np.zeros(\n",
    "    (len(bdt_train_dict), len(base_tpr), len(order)), \n",
    "    dtype=float\n",
    ")\n",
    "area_baseline = np.zeros(\n",
    "    (len(bdt_train_dict), len(order)), \n",
    "    dtype=float\n",
    ")\n",
    "\n",
    "BDT_perf = {\n",
    "    sample_name: copy.deepcopy({\n",
    "        'base_tpr': base_tpr,\n",
    "        'class_order': copy.deepcopy(order),\n",
    "        # test data #\n",
    "        'preds': [],\n",
    "        'fprs_density': copy.deepcopy(roc_baseline), 'thresholds_density': copy.deepcopy(roc_baseline), 'areas_density': copy.deepcopy(area_baseline),\n",
    "        'fprs_weighted': copy.deepcopy(roc_baseline), 'thresholds_weighted': copy.deepcopy(roc_baseline), 'areas_weighted': copy.deepcopy(area_baseline),\n",
    "        'fprs_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'thresholds_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'areas_sum_density': copy.deepcopy(area_baseline[0, ...]),\n",
    "        'fprs_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'thresholds_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'areas_sum_weighted': copy.deepcopy(area_baseline[0, ...]),\n",
    "        # train data #\n",
    "        'train_preds': [], \n",
    "        'train_fprs_density': copy.deepcopy(roc_baseline), 'train_thresholds_density': copy.deepcopy(roc_baseline), 'train_areas_density': copy.deepcopy(area_baseline),\n",
    "        'train_fprs_weighted': copy.deepcopy(roc_baseline), 'train_thresholds_weighted': copy.deepcopy(roc_baseline), 'train_areas_weighted': copy.deepcopy(area_baseline),\n",
    "        'train_fprs_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'train_thresholds_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'train_areas_sum_density': copy.deepcopy(area_baseline[0, ...]),\n",
    "        'train_fprs_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'train_thresholds_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'train_areas_sum_weighted': copy.deepcopy(area_baseline[0, ...]),\n",
    "        # val data #\n",
    "        'val_preds': [],\n",
    "        'val_fprs_density': copy.deepcopy(roc_baseline), 'val_thresholds_density': copy.deepcopy(roc_baseline), 'val_areas_density': copy.deepcopy(area_baseline),\n",
    "        'val_fprs_weighted': copy.deepcopy(roc_baseline), 'val_thresholds_weighted': copy.deepcopy(roc_baseline), 'val_areas_weighted': copy.deepcopy(area_baseline),\n",
    "        'val_fprs_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'val_thresholds_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'val_areas_sum_density': copy.deepcopy(area_baseline[0, ...]),\n",
    "        'val_fprs_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'val_thresholds_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'val_areas_sum_weighted': copy.deepcopy(area_baseline[0, ...]),\n",
    "    }) for sample_name in order\n",
    "}\n",
    "\n",
    "for j, sample_name in enumerate(order):\n",
    "\n",
    "    for fold_idx in range(len(bdt_train_dict)):\n",
    "        booster = xgb.Booster(param)\n",
    "\n",
    "        try:\n",
    "            booster.load_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "        except:\n",
    "            raise FileNotFoundError(f\"No model file at fold {fold_idx}.\")\n",
    "    \n",
    "        for pred_type, dataset in [\n",
    "            ('train_', bdt_train_dict[f\"fold_{fold_idx}\"]),\n",
    "            ('val_', bdt_val_dict[f\"fold_{fold_idx}\"]),\n",
    "            ('', bdt_test_dict[f\"fold_{fold_idx}\"])\n",
    "        ]:\n",
    "            \n",
    "            BDT_perf[sample_name][pred_type + 'preds'].append(\n",
    "                booster.predict(\n",
    "                    dataset, \n",
    "                    iteration_range=(0, booster.best_iteration+1)\n",
    "                ).tolist()\n",
    "            )\n",
    "\n",
    "            for i, sample_name_ in enumerate(order):\n",
    "                \n",
    "                if sample_name_ == sample_name:\n",
    "                    event_mask = dataset.get_label() > -1\n",
    "                    pred_rescale = np.ones_like(event_mask)\n",
    "                else:\n",
    "                    event_mask = np.logical_or(dataset.get_label() == j, dataset.get_label() == i)\n",
    "                    pred_rescale = np.array(BDT_perf[sample_name][pred_type + 'preds'][-1])[:, j][event_mask] + np.array(BDT_perf[sample_name][pred_type + 'preds'][-1])[:, i][event_mask]\n",
    "                class_preds = np.array(BDT_perf[sample_name][pred_type + 'preds'][-1])[:, j][event_mask] / pred_rescale\n",
    "                class_truths = np.where(dataset.get_label() == j, 1, 0)[event_mask]\n",
    "                \n",
    "                for roc_type in ['density', 'weighted']:\n",
    "\n",
    "                    if roc_type == 'weighted':\n",
    "                        if re.search('train', pred_type) is not None:\n",
    "                            roc_weights = weights_plot_train[f\"fold_{fold_idx}\"][event_mask]\n",
    "                        elif re.search('val', pred_type) is not None:\n",
    "                            roc_weights = weights_plot_val[f\"fold_{fold_idx}\"][event_mask]\n",
    "                        else:\n",
    "                            roc_weights = weights_plot_test[f\"fold_{fold_idx}\"][event_mask]\n",
    "                    else:\n",
    "                        roc_weights = None\n",
    "\n",
    "                    fpr_bdt, tpr_bdt, threshold_bdt = roc_curve(class_truths, class_preds, sample_weight=roc_weights)\n",
    "                    fpr_bdt = np.interp(base_tpr, tpr_bdt, fpr_bdt)\n",
    "                    threshold_bdt = np.interp(base_tpr, tpr_bdt, threshold_bdt)\n",
    "\n",
    "                    BDT_perf[sample_name][pred_type + 'fprs_' + roc_type][fold_idx][:, i] = fpr_bdt\n",
    "                    BDT_perf[sample_name][pred_type + 'thresholds_' + roc_type][fold_idx][:, i] = threshold_bdt\n",
    "                    BDT_perf[sample_name][pred_type + 'areas_' + roc_type][fold_idx][i] = float(trapezoid(base_tpr, fpr_bdt))\n",
    "    \n",
    "    for pred_type, dataset_dict in [\n",
    "        ('train_', bdt_train_dict),\n",
    "        ('val_', bdt_val_dict),\n",
    "        ('', bdt_test_dict)\n",
    "    ]:\n",
    "\n",
    "        flat_preds = np.concatenate(BDT_perf[sample_name][f'{pred_type}preds'], axis=0)\n",
    "        flat_truths = np.concatenate([dataset_dict[f\"fold_{fold_idx}\"].get_label() for fold_idx in range(len(dataset_dict))], axis=0)\n",
    "\n",
    "        for i, sample_name_ in enumerate(order):\n",
    "            \n",
    "            if sample_name_ == sample_name:\n",
    "                event_mask = flat_truths > -1\n",
    "                pred_rescale = np.ones_like(event_mask)\n",
    "            else:\n",
    "                event_mask = np.logical_or(flat_truths == j, flat_truths == i)\n",
    "                pred_rescale = flat_preds[:, j][event_mask] + flat_preds[:, i][event_mask]\n",
    "            class_preds = flat_preds[:, j][event_mask] / pred_rescale\n",
    "            class_truths = np.where(flat_truths == j, 1, 0)[event_mask]\n",
    "            \n",
    "            for roc_type in ['density', 'weighted']:\n",
    "\n",
    "                if roc_type == 'weighted':\n",
    "                    if re.search('train', pred_type) is not None:\n",
    "                        roc_weights = np.concatenate([weights_plot_train[f\"fold_{fold_idx}\"] for fold_idx in range(len(weights_plot_train))], axis=0)[event_mask]\n",
    "                    elif re.search('val', pred_type) is not None:\n",
    "                        roc_weights = np.concatenate([weights_plot_val[f\"fold_{fold_idx}\"] for fold_idx in range(len(weights_plot_val))], axis=0)[event_mask]\n",
    "                    else:\n",
    "                        roc_weights = np.concatenate([weights_plot_test[f\"fold_{fold_idx}\"] for fold_idx in range(len(weights_plot_test))], axis=0)[event_mask]\n",
    "                else:\n",
    "                    roc_weights = None\n",
    "\n",
    "                fpr_bdt, tpr_bdt, threshold_bdt = roc_curve(class_truths, class_preds, sample_weight=roc_weights)\n",
    "                fpr_bdt = np.interp(base_tpr, tpr_bdt, fpr_bdt)\n",
    "                threshold_bdt = np.interp(base_tpr, tpr_bdt, threshold_bdt)\n",
    "\n",
    "                BDT_perf[sample_name][pred_type + 'fprs_sum_' + roc_type][:, i] = fpr_bdt\n",
    "                BDT_perf[sample_name][pred_type + 'thresholds_sum_' + roc_type][:, i] = threshold_bdt\n",
    "                BDT_perf[sample_name][pred_type + 'areas_sum_' + roc_type][i] = float(trapezoid(base_tpr, fpr_bdt))\n",
    "    \n",
    "    for key in BDT_perf[sample_name].keys():\n",
    "        if type(BDT_perf[sample_name][key]) is list:\n",
    "            continue\n",
    "        BDT_perf[sample_name][key] = BDT_perf[sample_name][key].tolist()\n",
    "\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+f\"_BDT_perf.json\"), 'w') as f:\n",
    "    json.dump(BDT_perf, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(list_of_lists):\n",
    "    max_length = np.max([len(list_i) for list_i in list_of_lists])\n",
    "    for list_i in list_of_lists:\n",
    "        while len(list_i) < max_length:\n",
    "            list_i.append(list_i[-1])\n",
    "\n",
    "    return list_of_lists\n",
    "\n",
    "def plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='png'):\n",
    "    plot_prefix = plot_prefix + ('_' if plot_prefix != '' else '')\n",
    "    plot_postfix = plot_postfix + ('_' if plot_postfix != '' else '')\n",
    "    plot_name = plot_prefix + plot_name + plot_postfix + f'.{format}'\n",
    "\n",
    "    plot_filepath = os.path.join(plot_dirpath, plot_name)\n",
    "    return plot_filepath\n",
    "\n",
    "def plot_train_val_losses(\n",
    "    losses_arrs, labels, plot_name, plot_dirpath, \n",
    "    plot_prefix='', plot_postfix='', linestyles=None,\n",
    "    losses_std_arrs=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    \n",
    "    if type(losses_arrs[0]) is float:\n",
    "        losses_arrs = [losses_arrs]\n",
    "    if linestyles is None:\n",
    "        linestyles = ['solid'] * len(losses_arrs)\n",
    "    if labels is None:\n",
    "        labels = [i for i in range(len(losses_arrs))]\n",
    "\n",
    "    if losses_std_arrs is not None:\n",
    "        for i in range(len(losses_std_arrs)):\n",
    "            plt.fill_between(\n",
    "                range(len(losses_std_arrs[i])), \n",
    "                losses_arrs[i]+losses_std_arrs[i], losses_arrs[i]-losses_std_arrs[i],\n",
    "                alpha=0.7\n",
    "            )\n",
    "\n",
    "    for i in range(len(losses_arrs)):\n",
    "        plt.plot(\n",
    "            range(len(losses_arrs[i])), \n",
    "            losses_arrs[i], \n",
    "            label=f\"{labels[i]} losses\", linestyle=linestyles[i],\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "def plot_rocs(\n",
    "    fprs, tprs, labels, plot_name, plot_dirpath,\n",
    "    plot_prefix='', plot_postfix='', close=True, log=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    \n",
    "    for fpr, tpr, label in zip(fprs, tprs, labels):\n",
    "        linestyle = 'solid' if re.search('IN', label) is not None else ('dashed' if re.search('BDT', label) is not None else 'dotted')\n",
    "        plt.plot(fpr, tpr, label=label, linestyle=linestyle)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    if log is not None and re.search('x', log) is not None:\n",
    "        plt.xscale('log')\n",
    "    elif log is not None and re.search('y', log) is not None:\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    if close:\n",
    "        plt.close()\n",
    "\n",
    "def plot_output_scores(\n",
    "    sigs_and_bkgs, order, plot_name, plot_dirpath,\n",
    "    plot_prefix='', plot_postfix='', bins=1000, weights=None, log=False, arctanh=False\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "\n",
    "    if arctanh:\n",
    "        end_point = 6.\n",
    "    else:\n",
    "        end_point = 1.\n",
    "    hist_axis = hist.axis.Regular(bins, 0., end_point, name='var', growth=False, underflow=False, overflow=False)\n",
    "    hists, labels = [], []\n",
    "    for sample_name in order:\n",
    "        if sample_name not in sigs_and_bkgs:\n",
    "            continue\n",
    "        hists.append(\n",
    "            hist.Hist(hist_axis, storage='weight').fill(\n",
    "                var=sigs_and_bkgs[sample_name], \n",
    "                weight=weights[sample_name] if weights is not None else np.ones_like(sigs_and_bkgs[sample_name])\n",
    "            )\n",
    "        )\n",
    "        labels.append(sample_name)\n",
    "    hep.histplot(\n",
    "        hists,\n",
    "        yerr=(True if weights is not None else False),\n",
    "        alpha=0.2, density=(False if weights is not None else True), histtype='step',\n",
    "        label=labels\n",
    "    )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Output score')\n",
    "    if log:\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "def plot_s_over_root_b(\n",
    "    sig, bkg, label, plot_name, plot_dirpath,\n",
    "    plot_prefix='', plot_postfix='', bins=1000, weights={'sig': None, 'bkg': None},\n",
    "    lines=None, lines_labels=None, line_colors=None, arctanh=False\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "\n",
    "    if arctanh:\n",
    "        end_point = 6.\n",
    "        hist_axis = hist.axis.Regular(bins, 0., end_point, name='var', growth=False, underflow=False, overflow=False)\n",
    "    else:\n",
    "        end_point = 1.\n",
    "        hist_axis = hist.axis.Regular(bins, 0., end_point, name='var', growth=False, underflow=False, overflow=False)\n",
    "    sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig))\n",
    "    bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg, weight=weights['bkg'] if weights['bkg'] is not None else np.ones_like(bkg))\n",
    "    s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "    plt.plot(\n",
    "        np.arange(0., end_point, end_point*(1/bins)), s_over_root_b_points, \n",
    "        label=f'{label} - s/âb', alpha=0.8\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        for i in range(len(lines)):\n",
    "            plt.vlines(\n",
    "                lines[i], 0, np.max(s_over_root_b_points), \n",
    "                label='s/âb'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                alpha=0.5, colors=line_colors[i]\n",
    "            )\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Output score')\n",
    "    plt.ylabel('s/âb')\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    conf_matrix, class_labels, plot_name, plot_dirpath, \n",
    "    plot_prefix='', plot_postfix=''\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=class_labels)\n",
    "    disp.plot(im_kw={'norm': 'log'})\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "def plot_feature_importance(\n",
    "    feature_scores, feature_labels, plot_name, plot_dirpath, \n",
    "    plot_prefix='', plot_postfix='', fscore_method='total_gain', log=True\n",
    "):\n",
    "    plt.figure(figsize=(18,14))\n",
    "\n",
    "    plt.barh(\n",
    "        np.arange(len(feature_scores)), feature_scores, align='center'\n",
    "    )\n",
    "    plt.yticks(np.arange(len(feature_scores)), feature_labels, fontsize=8)\n",
    "    plt.ylabel('Features')\n",
    "    plt.xlabel(f'F score ({fscore_method})')\n",
    "    if log:\n",
    "        plt.xscale('log')\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(sigs, bkgs, weights, bins=10000, arctanh=False):\n",
    "    hist_list_fold = []\n",
    "    cut_boundaries_fold = []\n",
    "    cut_s_over_root_bs_fold = []\n",
    "    sig_weights_fold = []\n",
    "    bkg_weights_fold = []\n",
    "    if len(np.shape(sigs)) == 1:\n",
    "        sigs, bkgs = [sigs], [bkgs] \n",
    "    if arctanh:\n",
    "        end_point = 6.\n",
    "    else:\n",
    "        end_point = 1.\n",
    "    for sig, bkg in zip(sigs, bkgs):\n",
    "        hist_axis = hist.axis.Regular(bins, 0., end_point, name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg, weight=weights['bkg'])\n",
    "        hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "        fold_idx_cuts_bins_inclusive = []\n",
    "        fold_idx_sig_weights = []\n",
    "        fold_idx_bkg_weights = []\n",
    "        fold_idx_prev_s_over_root_b = []\n",
    "        prev_s_over_root_b = 0\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b) or s < 0.25:\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                fold_idx_sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        fold_idx_sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_cuts_bins_inclusive.append(0)\n",
    "        fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "        fold_idx_score_cuts = [end_point * (bin_i / bins) for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "        cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "        cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "        sig_weights_fold.append(fold_idx_sig_weights)\n",
    "        bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "    return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "\n",
    "def p_to_xyz(p, split=True):  # makes a tetrahedron with height 1 and vertices {(0, 0, 0),  (â3/2, 0, â3/2),  (0, â3/2, â3/2),  (â3/2, â3/2, 0)}\n",
    "    rt3o2 = np.sqrt(3) / 2\n",
    "\n",
    "    x = rt3o2 * (0*p[:, 0] + p[:, 1] + p[:, 2] + 0*p[:, 3])\n",
    "    y = rt3o2 * (0*p[:, 0] + 0*p[:, 1] + p[:, 2] + p[:, 3])\n",
    "    z = rt3o2 * (0*p[:, 0] + p[:, 1] + 0*p[:, 2] + p[:, 3])\n",
    "\n",
    "    if split:\n",
    "        return x, y, z\n",
    "    else:\n",
    "        return np.column_stack((x, y, z))\n",
    "\n",
    "def optimize_cuts(\n",
    "    preds: np.ndarray, binary_labels: np.ndarray, weights: np.ndarray,\n",
    "    init_guess=[1e-9, 2e-3, 1e-2], param_names=['r1', 'r2', 'r3'], param_range=[(1e-11, 1e-4), (1e-3, 5e-2), (0., 1.)], \n",
    "    n_steps=int(5e2), verbose: bool=False, min_sig: float=0.2, prefactor: float=1e3, rng_seed: int=21\n",
    "):\n",
    "    xyz_preds = p_to_xyz(preds, split=False)\n",
    "\n",
    "    space  = [Real(float(param_range[i][0]), float(param_range[i][1]), (\"log-uniform\" if param_name == 'r4' else \"uniform\"), name=param_name) for i, param_name in enumerate(param_names)]\n",
    "\n",
    "    def space_transform(X):\n",
    "        triangle_vertices = X['r1']**(1/3) * np.array([\n",
    "            [np.sqrt(3)/2,         0,            np.sqrt(3)/2], \n",
    "            [0,                np.sqrt(3)/2,     np.sqrt(3)/2], \n",
    "            [np.sqrt(3)/2,     np.sqrt(3)/2,                0]\n",
    "        ])\n",
    "\n",
    "        sampled_point = (\n",
    "            (1 - np.sqrt((1 - X['r2']))) * triangle_vertices[0, :]\n",
    "        ) + (\n",
    "            np.sqrt((1 - X['r2']))*(1 - X['r3']) * triangle_vertices[1, :]\n",
    "        ) + (\n",
    "            np.sqrt((1 - X['r2']))*X['r3'] * triangle_vertices[2, :]\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(sampled_point)\n",
    "\n",
    "        return sampled_point\n",
    "\n",
    "    @use_named_args(space)\n",
    "    def objective(**X):\n",
    "        if verbose:\n",
    "            print(\"New configuration: {}\".format(X))\n",
    "\n",
    "        thresholds = space_transform(X)\n",
    "        sample_mask = np.all(xyz_preds < thresholds, axis=1)\n",
    "\n",
    "        # print(f\"total sig = {np.sum(weights[binary_labels == 1])}\")\n",
    "        # print(f\"total bkg = {np.sum(weights[binary_labels == 0])}\")\n",
    "\n",
    "        num_sig = np.abs(\n",
    "            np.sum(\n",
    "                weights[\n",
    "                    np.logical_and(\n",
    "                        binary_labels == 1,\n",
    "                        sample_mask\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        num_bkg = np.abs(\n",
    "            np.sum(\n",
    "                weights[\n",
    "                    np.logical_and(\n",
    "                        binary_labels == 0,\n",
    "                        sample_mask\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        s_over_root_b = num_sig / np.sqrt(num_bkg)\n",
    "\n",
    "        if num_sig == 0 and num_bkg == 0:\n",
    "            both_0 = prefactor*1e1\n",
    "            if verbose:\n",
    "                print(f\"both sig and bkg 0 at this hyperplane => {both_0}\")\n",
    "            return both_0\n",
    "        elif num_sig < min_sig:\n",
    "            small_sig = prefactor*0\n",
    "            if verbose:\n",
    "                print(f\"too little sig ({num_sig}) at this hyperplane => {small_sig}\")\n",
    "            return small_sig\n",
    "        elif num_bkg == 0:\n",
    "            zero_bkg = -prefactor*num_sig\n",
    "            if verbose:\n",
    "                print(f\"zero bkg at this hyperplane (likely due to finite data rather than real bkg-free zone) => {zero_bkg}\")\n",
    "            return zero_bkg\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"s/âb = {s_over_root_b}, s = {num_sig}, b = {num_bkg}\")\n",
    "\n",
    "        return -prefactor*s_over_root_b\n",
    "    \n",
    "    res_gp = gp_minimize(\n",
    "        objective, space, random_state=rng_seed, \n",
    "        n_calls=(n_steps + (1 if len(np.shape(init_guess)) == 1 else np.shape(init_guess)[0])), \n",
    "        n_initial_points=n_steps, x0=init_guess\n",
    "    )\n",
    "\n",
    "    opt_params = [float(res_gp.x[i]) for i in range(len(space))]\n",
    "    opt_cuts = [float(opt_cut) for opt_cut in space_transform({param_names[i]: res_gp.x[i] for i in range(len(param_names))})]\n",
    "    if verbose:\n",
    "        print(\"Best parameters: {}\".format(opt_cuts))\n",
    "        print(f\"Best s/âb = {-res_gp.fun / prefactor}\")\n",
    "\n",
    "    return opt_cuts, opt_params\n",
    "\n",
    "\n",
    "def multi_optimize_cut_boundaries(preds: list, binary_labels: np.ndarray, weights: np.ndarray, num_categories: int=3, min_sig: float=0.2):\n",
    "    init_param_range = [(1e-8, 1e-7), (1e-6, 1e-5), (1e-2, 1e-1)]\n",
    "    init_guess = [5e-8, 5e-6, 5e-2]\n",
    "    clf_dict = {}\n",
    "    param_clf_dict = {}\n",
    "    for cat in range(num_categories):\n",
    "\n",
    "        clf_dict[cat] = []\n",
    "        param_clf_dict[cat] = []\n",
    "\n",
    "        if cat == 0:\n",
    "            sliced_preds = np.array(preds)\n",
    "            sliced_labels = binary_labels\n",
    "            sliced_weights = weights\n",
    "            param_range = init_param_range\n",
    "            guess = init_guess\n",
    "\n",
    "        else:\n",
    "            slice_array = np.ones_like(binary_labels, dtype=bool)\n",
    "            for prev_cat in range(cat):\n",
    "                slice_array = np.logical_and(\n",
    "                    slice_array,\n",
    "                    np.logical_not(\n",
    "                        np.all(\n",
    "                            p_to_xyz(np.array(preds), split=False) < clf_dict[prev_cat], \n",
    "                            axis=1\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            sliced_preds = np.array(preds)[slice_array]\n",
    "            sliced_labels = binary_labels[slice_array]\n",
    "            sliced_weights = weights[slice_array]\n",
    "            # param_range = [(param_clf_dict[cat-1][0], init_param_range[0][1]), (param_clf_dict[cat-1][1], init_param_range[1][1]), init_param_range[2]]\n",
    "            # guess = [param_clf_dict[cat-1][0] + 1e-11, param_clf_dict[cat-1][1] + 1e-11, 0.5 * init_param_range[2][1]]\n",
    "            param_range = init_param_range\n",
    "            guess = init_guess\n",
    "            \n",
    "        opt_cuts, opt_params = optimize_cuts(\n",
    "            sliced_preds, sliced_labels, sliced_weights, verbose=False,\n",
    "            param_range=param_range, init_guess=guess, n_steps=200, min_sig=min_sig, rng_seed=None\n",
    "        )\n",
    "\n",
    "        clf_dict[cat] = opt_cuts\n",
    "        param_clf_dict[cat] = opt_params\n",
    "\n",
    "    return clf_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Names for order #\n",
    "    \"ggF HH\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"ttH\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"single-H\": r\"ggF $H\\rightarrow \\gamma\\gamma$ + VBF $H\\rightarrow \\gamma\\gamma$ + V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"non-res\": r\"$\\gamma\\gamma+3j$ + $\\gamma+j$, 20GeV<$p_T$\",\n",
    "    \"VH\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"non-res + ggFH + VBFH\": r\"$\\gamma\\gamma+3j$ + $\\gamma+j$, 20GeV<$p_T$ + ggF $H\\rightarrow \\gamma\\gamma$ + VBF $H\\rightarrow \\gamma\\gamma$\"\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 150., 2000, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    # 'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # 'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Integer(0, 10, name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(25, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(25, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # Yibo's BDT variables\n",
    "    'lead_mvaID': hist.axis.Regular(50, -1., 1., name='var', label=r'$\\gamma_{lead}$ MVA ID', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_mvaID': hist.axis.Regular(50, -1., 1., name='var', label=r'$\\gamma_{sublead}$ MVA ID', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_gg': hist.axis.Regular(50, -1., 1., name='var', label=r'cos$(\\theta_{gg})$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_pt_over_Mgg': hist.axis.Regular(50, 0., 1., name='var', label=r'$p_{T,\\gamma_1} / M_{\\gamma\\gamma}$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_pt_over_Mgg': hist.axis.Regular(50, 0., 1., name='var', label=r'$p_{T,\\gamma_2} / M_{\\gamma\\gamma}$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_sigmaE_over_E': hist.axis.Regular(50, 0., 1., name='var', label=r'$\\sigma {E,\\gamma_1} / E_{\\gamma\\gamma}$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_sigmaE_over_E': hist.axis.Regular(50, 0., 1., name='var', label=r'$\\sigma {E,\\gamma_2} / E_{\\gamma\\gamma}$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt_over_Mjj': hist.axis.Regular(50, 0., 1., name='var', label=r'$p_{T,j1} / M_{jj}$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_pt_over_Mjj': hist.axis.Regular(50, 0., 1., name='var', label=r'$p_{T,j2} / M_{jj}$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_btagPNetB': hist.axis.Regular(50, -1., 1., name='var', label=r'$j_{lead}$ PNet btag score', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_btagPNetB': hist.axis.Regular(50, -1., 1., name='var', label=r'$j_{sublead}$ PNet btag score', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_sigmapT_over_pT': hist.axis.Regular(50, 0., 1., name='var', label=r'$\\sigma p_{T,j1} / p_{T,jj}$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_sigmapT_over_pT': hist.axis.Regular(50, 0., 1., name='var', label=r'$\\sigma p_{T,j2} / p_{T,jj}$', growth=False, underflow=False, overflow=False),\n",
    "    'dipho_mass_over_Mggjj': hist.axis.Regular(50, 0., 1., name='var', label=r'$M_{\\gamma\\gamma} / M_{\\gamma\\gamma jj}$', growth=False, underflow=False, overflow=False), \n",
    "    'dijet_mass_over_Mggjj': hist.axis.Regular(50, 0., 1., name='var', label=r'$M_{jj} / M_{\\gamma\\gamma jj}$', growth=False, underflow=False, overflow=False),\n",
    "    # My variables for non-reso reduction #\n",
    "    'lead_pfRelIso03_all_quadratic': hist.axis.Regular(50, -1., 1., name='var', label=r'$\\gamma_{lead}$ PF RelIso03 all quad.', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_pfRelIso03_all_quadratic': hist.axis.Regular(50, -1., 1., name='var', label=r'$\\gamma_{sublead}$ PF RelIso03 all quad.', growth=False, underflow=False, overflow=False),\n",
    "    # Michael's DNN variables #\n",
    "    'DeltaR_j1g1': hist.axis.Regular(50, 0., 5., name='var', label=r'$\\Delta R(bjet_{lead}, \\gamma_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaR_j1g2': hist.axis.Regular(50, 0., 5., name='var', label=r'$\\Delta R(bjet_{lead}, \\gamma_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaR_j2g1': hist.axis.Regular(50, 0., 5., name='var', label=r'$\\Delta R(bjet_{sublead}, \\gamma_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaR_j2g2': hist.axis.Regular(50, 0., 5., name='var', label=r'$\\Delta R(bjet_{sublead}, \\gamma_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    'HHbbggCandidate_pt': hist.axis.Regular(100, 0., 700., name='var', label=r'HH $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'HHbbggCandidate_eta': hist.axis.Regular(50, -5., 5., name='var', label=r'HH $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'HHbbggCandidate_phi': hist.axis.Regular(50, -3.2, 3.2, name='var', label=r'HH $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'HHbbggCandidate_mass': hist.axis.Regular(25, 0., 700., name='var', label=r'$M_{HH}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # ATLAS variables #\n",
    "    'pt_balance': hist.axis.Regular(100, 0., 2., name='var', label=r'$p_{T,HH} / (p_{T,\\gamma1} + p_{T,\\gamma2} + p_{T,j1} + p_{T,j2})$', growth=False, underflow=False, overflow=False), \n",
    "    # VH variables #\n",
    "    'DeltaPhi_jj': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,j_2)$', growth=False, underflow=False, overflow=False),\n",
    "    'DeltaEta_jj': hist.axis.Regular(20, 0., 10., name='var', label=r'$\\Delta\\eta (j_1,j_2)$', growth=False, underflow=False, overflow=False),\n",
    "    'isr_jet_pt': hist.axis.Regular(100, 0., 200., name='var', label=r'ISR jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'DeltaPhi_isr_jet_z': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_{ISR},jj)$', growth=False, underflow=False, overflow=False),\n",
    "    'dijet_pt': hist.axis.Regular(100, 0., 500., name='var', label=r'jj $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($\\Sigma E_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t0}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t1}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma$ ln($p_{T}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'ln($M_{jj}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # Yibo's BDT variables\n",
    "    'lead_mvaID': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\gamma_{lead}$ MVA ID', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_mvaID': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\gamma_{sublead}$ MVA ID', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_gg': hist.axis.Regular(50, -1., 1., name='var', label=r'cos$(\\theta_{gg})$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_pt_over_Mgg': hist.axis.Regular(50, -4., 4., name='var', label=r'$p_{T,\\gamma_1} / M_{\\gamma\\gamma}$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_pt_over_Mgg': hist.axis.Regular(50, -4., 4., name='var', label=r'$p_{T,\\gamma_2} / M_{\\gamma\\gamma}$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_sigmaE_over_E': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\sigma {E,\\gamma_1} / E_{\\gamma\\gamma}$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_sigmaE_over_E': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\sigma {E,\\gamma_2} / E_{\\gamma\\gamma}$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt_over_Mjj': hist.axis.Regular(50, -4., 4., name='var', label=r'$p_{T,j1} / M_{jj}$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_pt_over_Mjj': hist.axis.Regular(50, -4., 4., name='var', label=r'$p_{T,j2} / M_{jj}$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_btagPNetB': hist.axis.Regular(50, -4., 4., name='var', label=r'$j_{lead}$ PNet btag score', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_btagPNetB': hist.axis.Regular(50, -4., 4., name='var', label=r'$j_{sublead}$ PNet btag score', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_sigmapT_over_pT': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\sigma p_{T,j1} / p_{T,jj}$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_sigmapT_over_pT': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\sigma p_{T,j2} / p_{T,jj}$', growth=False, underflow=False, overflow=False),\n",
    "    'dipho_mass_over_Mggjj': hist.axis.Regular(50, -4., 4., name='var', label=r'$M_{\\gamma\\gamma} / M_{\\gamma\\gamma jj}$', growth=False, underflow=False, overflow=False), \n",
    "    'dijet_mass_over_Mggjj': hist.axis.Regular(50, -4., 4., name='var', label=r'$M_{jj} / M_{\\gamma\\gamma jj}$', growth=False, underflow=False, overflow=False),\n",
    "    # My variables for non-reso reduction #\n",
    "    'lead_pfRelIso03_all_quadratic': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\gamma_{lead}$ PF RelIso03 all quad.', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_pfRelIso03_all_quadratic': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\gamma_{sublead}$ PF RelIso03 all quad.', growth=False, underflow=False, overflow=False),\n",
    "    # Michael's DNN variables #\n",
    "    'DeltaR_j1g1': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\Delta R(bjet_{lead}, \\gamma_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaR_j1g2': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\Delta R(bjet_{lead}, \\gamma_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaR_j2g1': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, \\gamma_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaR_j2g2': hist.axis.Regular(50, -4., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, \\gamma_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    'HHbbggCandidate_pt': hist.axis.Regular(100, -4., 4., name='var', label=r'HH ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'HHbbggCandidate_eta': hist.axis.Regular(50, -4., 4., name='var', label=r'HH $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'HHbbggCandidate_phi': hist.axis.Regular(50, -4., 4., name='var', label=r'HH $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'HHbbggCandidate_mass': hist.axis.Regular(50, -4., 4., name='var', label=r'ln($M_{HH}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # ATLAS variables #\n",
    "    'pt_balance': hist.axis.Regular(100, -4., 4., name='var', label=r'ln($p_{T,HH} / (p_{T,\\gamma1} + p_{T,\\gamma2} + p_{T,j1} + p_{T,j2})$)', growth=False, underflow=False, overflow=False), \n",
    "    # VH variables #\n",
    "    'DeltaPhi_jj': hist.axis.Regular(20, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,j_2)$', growth=False, underflow=False, overflow=False),\n",
    "    'DeltaEta_jj': hist.axis.Regular(20, -4., 4., name='var', label=r'$\\Delta\\eta (j_1,j_2)$', growth=False, underflow=False, overflow=False),\n",
    "    'isr_jet_pt': hist.axis.Regular(100, -4., 4., name='var', label=r'ISR jet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'DeltaPhi_isr_jet_z': hist.axis.Regular(20, -4., 4., name='var', label=r'$\\Delta\\phi (j_{ISR},jj)$', growth=False, underflow=False, overflow=False),\n",
    "    'dijet_pt': hist.axis.Regular(100, -4., 4., name='var', label=r'jj ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "\n",
    "def make_input_plot(\n",
    "    output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, \n",
    "    plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True\n",
    "):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss per Epoch Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"losses\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "if 'evals_result_dict' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_eval_result.json\"), 'r') as f:\n",
    "        evals_result_dict = json.load(f)\n",
    "\n",
    "# plot train/val/test losses\n",
    "all_train, all_val, all_test = [], [], []\n",
    "for fold_idx in range(len(evals_result_dict)):\n",
    "    all_train.append(evals_result_dict[f\"fold_{fold_idx}\"]['train']['mlogloss'])\n",
    "    all_val.append(evals_result_dict[f\"fold_{fold_idx}\"]['val']['mlogloss'])\n",
    "    all_test.append(evals_result_dict[f\"fold_{fold_idx}\"]['test']['mlogloss'])\n",
    "\n",
    "plot_train_val_losses(\n",
    "    all_train + all_val, [f'train fold {i}' for i in range(len(all_train))]+[f'val fold {i}' for i in range(len(all_val))],\n",
    "    'train_val_losses_vs_epoch', plot_dirpath, \n",
    "    linestyles=['solid']*len(all_train) + ['dashed']*len(all_val),\n",
    ")\n",
    "plot_train_val_losses(\n",
    "    all_train + all_test, [f'train fold {i}' for i in range(len(all_train))]+[f'test fold {i}' for i in range(len(all_test))],\n",
    "    'train_test_losses_vs_epoch', plot_dirpath,\n",
    "    linestyles=['solid']*len(all_train) + ['dotted']*len(all_test),\n",
    ")\n",
    "avg_train, avg_val, avg_test = np.mean(pad_list(all_train), axis=0), np.mean(pad_list(all_val), axis=0), np.mean(pad_list(all_test), axis=0)\n",
    "std_train, std_val, std_test = np.std(pad_list(all_train), axis=0), np.std(pad_list(all_val), axis=0), np.std(pad_list(all_test), axis=0)\n",
    "plot_train_val_losses(\n",
    "    [avg_train, avg_val, avg_test], ['train avg', 'val avg', 'test avg'],\n",
    "    'train_val_test_avg_vs_epoch', plot_dirpath,\n",
    "    losses_std_arrs=[std_train, std_val, std_test],\n",
    "    linestyles=['solid', 'dashed', 'dotted'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"ROCs\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "base_tpr = np.array(BDT_perf['ggF HH']['base_tpr'])\n",
    "\n",
    "# plot ROCs\n",
    "for j, sample_name in enumerate(order):\n",
    "\n",
    "    for fold_idx in range(len(bdt_train_dict)):\n",
    "\n",
    "        for roc_type in ['density', 'weighted']:\n",
    "\n",
    "            fprs = [np.array(BDT_perf[sample_name][f'fprs_{roc_type}'][fold_idx])[:, i] for i in range(len(order))]\n",
    "            tprs = [base_tpr for _ in range(len(order))]\n",
    "            labels = [\n",
    "                f\"{sample_name} vs. {'all' if i == j else sample_name_}, AUC = {BDT_perf[sample_name][f'areas_{roc_type}'][fold_idx][i]:.4f}\" \n",
    "                for i, sample_name_ in enumerate(order)\n",
    "            ]\n",
    "\n",
    "            plot_rocs(fprs, tprs, labels, f\"BDT_roc_{sample_name}_{roc_type}_testData_fold{fold_idx}\", plot_dirpath)\n",
    "\n",
    "    for roc_type in ['sum_density', 'sum_weighted']:\n",
    "\n",
    "        fprs = [np.array(BDT_perf[sample_name][f'fprs_{roc_type}'])[:, i] for i in range(len(order))]\n",
    "        tprs = [base_tpr for _ in range(len(order))]\n",
    "        labels = [\n",
    "            f\"{sample_name} vs. {'all' if i == j else sample_name_}, AUC = {BDT_perf[sample_name][f'areas_{roc_type}'][i]:.4f}\" \n",
    "            for i, sample_name_ in enumerate(order)\n",
    "        ]\n",
    "\n",
    "        plot_rocs(fprs, tprs, labels, f\"BDT_roc_{sample_name}_{roc_type}_testData_sum\", plot_dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Score Dist Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"output_scores\")\n",
    "# plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"output_scores_arctanh\")\n",
    "# plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"output_scores_resample\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "# plot Output scores\n",
    "for j, sample_name in enumerate(order):\n",
    "\n",
    "    for i, sample_name_ in enumerate(order):\n",
    "\n",
    "        for fold_idx in range(len(bdt_train_dict)):\n",
    "            \n",
    "            sigs_and_bkgs = {\n",
    "                sample_name__: np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j][bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == k]\n",
    "                for k, sample_name__ in enumerate(order)\n",
    "            }\n",
    "            score_weights = {\n",
    "                sample_name__: weights_plot_test[f\"fold_{fold_idx}\"][bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == k]\n",
    "                for k, sample_name__ in enumerate(order)\n",
    "            }\n",
    "\n",
    "            if sample_name_ != sample_name:\n",
    "                event_j_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == j\n",
    "                pred_j_rescale = np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j][event_j_mask] + np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, i][event_j_mask]\n",
    "                event_i_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == i\n",
    "                pred_i_rescale = np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j][event_i_mask] + np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, i][event_i_mask]\n",
    "\n",
    "                for sample_name__ in order:\n",
    "                    if sample_name__ == sample_name:\n",
    "                        sigs_and_bkgs[sample_name__] = sigs_and_bkgs[sample_name__] / pred_j_rescale\n",
    "                    elif sample_name__ == sample_name_:\n",
    "                        sigs_and_bkgs[sample_name__] = sigs_and_bkgs[sample_name__] / pred_i_rescale\n",
    "                    else:\n",
    "                        del sigs_and_bkgs[sample_name__]\n",
    "                        del score_weights[sample_name__]\n",
    "\n",
    "            if re.search('arctanh', plot_dirpath) is not None:\n",
    "                for key, value in sigs_and_bkgs.items():\n",
    "                    sigs_and_bkgs[key] = np.arctanh(value)\n",
    "\n",
    "            plot_output_scores(\n",
    "                sigs_and_bkgs, order, \n",
    "                f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_outputScoreWeighted_testData_fold{fold_idx}\", \n",
    "                plot_dirpath, weights=score_weights, log=True,\n",
    "                arctanh=True if re.search('arctanh', plot_dirpath) is not None else False\n",
    "            )\n",
    "            plot_output_scores(\n",
    "                sigs_and_bkgs, order, \n",
    "                f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_outputScoreDensity_testData_fold{fold_idx}\", \n",
    "                plot_dirpath,\n",
    "                arctanh=True if re.search('arctanh', plot_dirpath) is not None else False\n",
    "            )\n",
    "\n",
    "        flat_preds = np.concatenate([BDT_perf[sample_name]['preds'][fold_idx] for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "        if re.search('arctanh', plot_dirpath) is not None:\n",
    "            flat_preds = np.arctanh(flat_preds)\n",
    "        flat_truths = np.concatenate([bdt_test_dict[f\"fold_{fold_idx}\"].get_label() for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "        flat_weights = np.concatenate([weights_plot_test[f\"fold_{fold_idx}\"] for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "\n",
    "        sigs_and_bkgs = {\n",
    "            sample_name__: flat_preds[:, j][flat_truths == k]\n",
    "            for k, sample_name__ in enumerate(order)\n",
    "        }\n",
    "        score_weights = {\n",
    "            sample_name__: flat_weights[flat_truths == k]\n",
    "            for k, sample_name__ in enumerate(order)\n",
    "        }\n",
    "        \n",
    "        if sample_name_ != sample_name:\n",
    "            event_j_mask = flat_truths == j\n",
    "            pred_j_rescale = (flat_preds[:, j] + flat_preds[:, i])[event_j_mask]\n",
    "            event_i_mask = flat_truths == i\n",
    "            pred_i_rescale = (flat_preds[:, j] + flat_preds[:, i])[event_i_mask]\n",
    "\n",
    "            for sample_name__ in order:\n",
    "                if sample_name__ == sample_name:\n",
    "                    sigs_and_bkgs[sample_name__] = sigs_and_bkgs[sample_name__] / pred_j_rescale\n",
    "                elif sample_name__ == sample_name_:\n",
    "                    sigs_and_bkgs[sample_name__] = sigs_and_bkgs[sample_name__] / pred_i_rescale\n",
    "                else:\n",
    "                    del sigs_and_bkgs[sample_name__]\n",
    "                    del score_weights[sample_name__]\n",
    "        \n",
    "        plot_output_scores(\n",
    "            sigs_and_bkgs, order, \n",
    "            f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_outputScoreWeighted_testData_sum\", \n",
    "            plot_dirpath, weights=score_weights, log=True,\n",
    "            arctanh=True if re.search('arctanh', plot_dirpath) is not None else False\n",
    "        )\n",
    "        plot_output_scores(\n",
    "            sigs_and_bkgs, order, \n",
    "            f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_outputScoreDensity_testData_sum\", \n",
    "            plot_dirpath,\n",
    "            arctanh=True if re.search('arctanh', plot_dirpath) is not None else False\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### s/âb Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "============================================================\n",
      "Cat1: 0.9900 < ggF HH score â¤ 1.0000 AND 120 GeV < m_HH < 130 GeV\n",
      "------------------------------------------------------------\n",
      "Cat1: Num ggF HH = 0.2411\n",
      "------------------------------------------------------------\n",
      "Cat1: Num ttH = 0.1064\n",
      "------------------------------------------------------------\n",
      "Cat1: Num VH = 0.0877\n",
      "------------------------------------------------------------\n",
      "Cat1: Num non-res + ggFH + VBFH = 2.9833\n",
      "------------------------------------------------------------\n",
      "Cat1: Num GluGluHToGG = 0.289770489794679\n",
      "------------------------------------------------------------\n",
      "Cat1: Num VBFHToGG = 0.019242626148601\n",
      "------------------------------------------------------------\n",
      "Cat1: Num GGJets = 2.5972642295299244\n",
      "------------------------------------------------------------\n",
      "Cat1: Num GJetPt20To40 = 0.0\n",
      "------------------------------------------------------------\n",
      "Cat1: Num GJetPt40 = 0.0\n",
      "------------------------------------------------------------\n",
      "Cat1: S = 0.2411, B = 3.1774, S/âB = 0.1353\n",
      "============================================================\n",
      "============================================================\n",
      "Cat2: 0.9314 < ggF HH score â¤ 0.9900 AND 120 GeV < m_HH < 130 GeV\n",
      "------------------------------------------------------------\n",
      "Cat2: Num ggF HH = 0.2400\n",
      "------------------------------------------------------------\n",
      "Cat2: Num ttH = 0.7197\n",
      "------------------------------------------------------------\n",
      "Cat2: Num VH = 0.4688\n",
      "------------------------------------------------------------\n",
      "Cat2: Num non-res + ggFH + VBFH = 44.9206\n",
      "------------------------------------------------------------\n",
      "Cat2: Num GluGluHToGG = 3.3384363648420132\n",
      "------------------------------------------------------------\n",
      "Cat2: Num VBFHToGG = 0.2827480080121564\n",
      "------------------------------------------------------------\n",
      "Cat2: Num GGJets = 32.25808223796804\n",
      "------------------------------------------------------------\n",
      "Cat2: Num GJetPt20To40 = 0.0\n",
      "------------------------------------------------------------\n",
      "Cat2: Num GJetPt40 = 8.182650257098565\n",
      "------------------------------------------------------------\n",
      "Cat2: S = 0.2400, B = 46.1091, S/âB = 0.0353\n",
      "============================================================\n",
      "============================================================\n",
      "Cat3: 0.4742 < ggF HH score â¤ 0.9314 AND 120 GeV < m_HH < 130 GeV\n",
      "------------------------------------------------------------\n",
      "Cat3: Num ggF HH = 0.2374\n",
      "------------------------------------------------------------\n",
      "Cat3: Num ttH = 3.6074\n",
      "------------------------------------------------------------\n",
      "Cat3: Num VH = 2.9721\n",
      "------------------------------------------------------------\n",
      "Cat3: Num non-res + ggFH + VBFH = 674.7704\n",
      "------------------------------------------------------------\n",
      "Cat3: Num GluGluHToGG = 23.919952467671244\n",
      "------------------------------------------------------------\n",
      "Cat3: Num VBFHToGG = 2.0449880219395085\n",
      "------------------------------------------------------------\n",
      "Cat3: Num GGJets = 437.77837120932503\n",
      "------------------------------------------------------------\n",
      "Cat3: Num GJetPt20To40 = 0.4337822168479513\n",
      "------------------------------------------------------------\n",
      "Cat3: Num GJetPt40 = 204.74358729890776\n",
      "------------------------------------------------------------\n",
      "Cat3: S = 0.2374, B = 681.3500, S/âB = 0.0091\n"
     ]
    }
   ],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"s_over_rootb\")\n",
    "# plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"s_over_rootb_arctanh\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "# plot s/âb curves\n",
    "for j, sample_name in enumerate(order):\n",
    "\n",
    "    for i, sample_name_ in enumerate(order):\n",
    "\n",
    "        for fold_idx in range(len(BDT_perf['ggF HH']['preds'])):\n",
    "\n",
    "            if sample_name_ == sample_name:\n",
    "                sig_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == j\n",
    "                bkg_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() != j\n",
    "\n",
    "                sig_rescale = np.ones_like(sig_mask)\n",
    "                bkg_rescale = np.ones_like(bkg_mask)\n",
    "            else:\n",
    "                sig_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == j\n",
    "                bkg_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == i\n",
    "\n",
    "                sig_rescale = (\n",
    "                    np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j] \n",
    "                    + np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, i]\n",
    "                )\n",
    "                bkg_rescale = (\n",
    "                    np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j] \n",
    "                    + np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, i]\n",
    "                )\n",
    "\n",
    "            sigs_and_bkgs = {\n",
    "                'sig': (np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j] / sig_rescale)[sig_mask],\n",
    "                'bkg': (np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j] / bkg_rescale)[bkg_mask]\n",
    "            }\n",
    "            if re.search('arctanh', plot_dirpath) is not None:\n",
    "                sigs_and_bkgs['sig'] = np.arctanh(sigs_and_bkgs['sig'])\n",
    "                sigs_and_bkgs['bkg'] = np.arctanh(sigs_and_bkgs['bkg'])\n",
    "            score_weights = {\n",
    "                'sig': weights_plot_test[f\"fold_{fold_idx}\"][sig_mask],\n",
    "                'bkg': weights_plot_test[f\"fold_{fold_idx}\"][bkg_mask]\n",
    "            }\n",
    "\n",
    "\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                plot_s_over_root_b(\n",
    "                    sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], f\"{sample_name} vs. {sample_name_ if sample_name_ != sample_name else 'all'}\", \n",
    "                    f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_sOverRootb_testData_fold{fold_idx}\", \n",
    "                    plot_dirpath, weights=score_weights,\n",
    "                    arctanh=True if re.search('arctanh', plot_dirpath) is not None else False  \n",
    "                )\n",
    "\n",
    "                (\n",
    "                    cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "                ) = optimize_cut_boundaries(\n",
    "                    sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], score_weights,\n",
    "                    arctanh=True if re.search('arctanh', plot_dirpath) is not None else False\n",
    "                )\n",
    "\n",
    "                BDT_cut_labels = [\n",
    "                    f\"cut={cut_boundaries_fold[0][cut_idx]:.4f}: s/âb={cut_s_over_root_bs_fold[0][cut_idx]:.5f}, s={sig_weights_fold[0][cut_idx]['value']:.5f}Â±{sig_weights_fold[0][cut_idx]['w2']:.5f}, b={bkg_weights_fold[0][cut_idx]['value']:.5f}Â±{bkg_weights_fold[0][cut_idx]['w2']:.5f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[0]))\n",
    "                ]\n",
    "                line_labels = BDT_cut_labels[:10]\n",
    "                lines = cut_boundaries_fold[0][:10]\n",
    "                line_colors = cmap_petroff10\n",
    "\n",
    "                plot_s_over_root_b(\n",
    "                    sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], f\"{sample_name} vs. {sample_name_ if sample_name_ != sample_name else 'all'}\", \n",
    "                    f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_sOverRootb_withCuts_testData_fold{fold_idx}_{sample_name}\", plot_dirpath, \n",
    "                    weights=score_weights,\n",
    "                    lines=lines, lines_labels=line_labels, line_colors=line_colors,\n",
    "                    arctanh=True if re.search('arctanh', plot_dirpath) is not None else False\n",
    "                )\n",
    "            \n",
    "        flat_preds = np.concatenate([BDT_perf[sample_name]['preds'][fold_idx] for fold_idx in range(len(BDT_perf['ggF HH']['preds']))], axis=0)\n",
    "        if re.search('arctanh', plot_dirpath) is not None:\n",
    "            flat_preds = np.arctanh(flat_preds)\n",
    "        flat_truths = np.concatenate([bdt_test_dict[f\"fold_{fold_idx}\"].get_label() for fold_idx in range(len(BDT_perf['ggF HH']['preds']))], axis=0)\n",
    "        flat_weights = np.concatenate([weights_plot_test[f\"fold_{fold_idx}\"] for fold_idx in range(len(BDT_perf['ggF HH']['preds']))], axis=0)\n",
    "        flat_sample_names = np.concatenate([data_test_aux_dict[f\"fold_{fold_idx}\"]['sample_name'] for fold_idx in range(len(BDT_perf['ggF HH']['preds']))], axis=0)\n",
    "\n",
    "        if sample_name_ == sample_name:\n",
    "            sig_mask = flat_truths == j\n",
    "            bkg_mask = flat_truths != j\n",
    "\n",
    "            sig_rescale = np.ones_like(sig_mask)\n",
    "            bkg_rescale = np.ones_like(bkg_mask)\n",
    "        else:\n",
    "            sig_mask = flat_truths == j\n",
    "            bkg_mask = flat_truths == i\n",
    "\n",
    "            sig_rescale = flat_preds[:, j] + flat_preds[:, i]\n",
    "            bkg_rescale = flat_preds[:, j] + flat_preds[:, i]\n",
    "\n",
    "        sigs_and_bkgs = {\n",
    "            'sig': (flat_preds[:, j] / sig_rescale)[sig_mask],\n",
    "            'bkg': (flat_preds[:, j] / bkg_rescale)[bkg_mask]\n",
    "        }\n",
    "        score_weights = {\n",
    "            'sig': flat_weights[sig_mask],\n",
    "            'bkg': flat_weights[bkg_mask]\n",
    "        }\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            plot_s_over_root_b(\n",
    "                sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], f\"{sample_name} vs. {sample_name_ if sample_name_ != sample_name else 'all'}\", \n",
    "                f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_sOverRootb_testData_sum\", \n",
    "                plot_dirpath, weights=score_weights,\n",
    "                arctanh=True if re.search('arctanh', plot_dirpath) is not None else False\n",
    "            )\n",
    "\n",
    "            (\n",
    "                cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "            ) = optimize_cut_boundaries(\n",
    "                sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], score_weights,\n",
    "                arctanh=True if re.search('arctanh', plot_dirpath) is not None else False\n",
    "            )\n",
    "\n",
    "            BDT_cut_labels = [\n",
    "                f\"cut={cut_boundaries_fold[0][cut_idx]:.4f}: s/âb={cut_s_over_root_bs_fold[0][cut_idx]:.5f}, s={sig_weights_fold[0][cut_idx]['value']:.5f}Â±{sig_weights_fold[0][cut_idx]['w2']:.5f}, b={bkg_weights_fold[0][cut_idx]['value']:.5f}Â±{bkg_weights_fold[0][cut_idx]['w2']:.5f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[0]))\n",
    "            ]\n",
    "            line_labels = BDT_cut_labels[:10]\n",
    "            lines = cut_boundaries_fold[0][:10]\n",
    "            line_colors = cmap_petroff10\n",
    "\n",
    "            plot_s_over_root_b(\n",
    "                sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], f\"{sample_name} vs. {sample_name_ if sample_name_ != sample_name else 'all'}\", \n",
    "                f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_sOverRootb_withCuts_testData_sum\", plot_dirpath, \n",
    "                weights=score_weights,\n",
    "                lines=lines, lines_labels=line_labels, line_colors=line_colors,\n",
    "                arctanh=True if re.search('arctanh', plot_dirpath) is not None else False\n",
    "            )\n",
    "\n",
    "        if j == 0 and i == 0:\n",
    "            flat_mass = np.concatenate([data_test_aux_dict[f\"fold_{fold_idx}\"]['mass'] for fold_idx in range(len(BDT_perf['ggF HH']['preds']))], axis=0)\n",
    "            if re.search('arctanh', plot_dirpath) is not None:\n",
    "                cat_lines = [6.0] + lines[:3]\n",
    "            else:\n",
    "                cat_lines = [1.0] + lines[:3]\n",
    "            cat_num_samples = {}\n",
    "            for k, cat in enumerate(['Cat1', 'Cat2', 'Cat3']):\n",
    "                cat_num_samples[cat] = {}\n",
    "                print('='*60)\n",
    "                print('='*60)\n",
    "                print(f\"{cat}: {cat_lines[k+1]:.4f} < ggF HH score â¤ {cat_lines[k]:.4f} AND 120 GeV < m_HH < 130 GeV\")\n",
    "                print('-'*60)\n",
    "                for m, sample_name in enumerate(order):\n",
    "                    sample_bool = np.logical_and(  # event passes conditions and is the right type (i.e. sample)\n",
    "                        np.logical_and(  # event passes category and mass conditions\n",
    "                            np.logical_and(  # prediction is within category bounds\n",
    "                                flat_preds[:, 0] <= cat_lines[k],\n",
    "                                flat_preds[:, 0] > cat_lines[k+1]\n",
    "                            ),\n",
    "                            np.logical_and(  # diphoton mass is within 120-130 window\n",
    "                                flat_mass < 130,\n",
    "                                flat_mass > 120\n",
    "                            ),\n",
    "                        ),\n",
    "                        flat_truths == m\n",
    "                    )\n",
    "                    cat_num_samples[cat][sample_name] = np.sum(\n",
    "                        flat_weights[sample_bool]\n",
    "                    )\n",
    "                    print(f\"{cat}: Num {sample_name} = {cat_num_samples[cat][sample_name]:.4f}\")\n",
    "                    print('-'*60)\n",
    "                    if sample_name == order[-1]:\n",
    "                        for smpl in ['GluGluHToGG', 'VBFHToGG', 'GGJets', 'GJetPt20To40', 'GJetPt40']:\n",
    "                            smpl_num = np.sum(\n",
    "                                flat_weights[\n",
    "                                    np.logical_and(\n",
    "                                        sample_bool,\n",
    "                                        flat_sample_names == smpl\n",
    "                                    )\n",
    "                                ]\n",
    "                            )\n",
    "                            print(f\"{cat}: Num {smpl} = {smpl_num}\")\n",
    "                            print('-'*60)\n",
    "\n",
    "                print(f\"{cat}: S = {cat_num_samples[cat][order[0]]:.4f}, B = {np.sum([cat_num_samples[cat][order[v]] for v in range(1, len(order))]):.4f}, S/âB = {(cat_num_samples[cat][order[0]] / np.sqrt(np.sum([cat_num_samples[cat][order[v]] for v in range(1, len(order))]))):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "============================================================\n",
      "Category 0 3D outputs < [0.00014055027654462777, 0.0019279517132367056, 0.0017874033775492724] AND 120 GeV < m_HH < 130 GeV\n",
      "------------------------------------------------------------\n",
      "0: Num ggF HH = 0.4181\n",
      "------------------------------------------------------------\n",
      "0: Num ttH = 0.1792\n",
      "------------------------------------------------------------\n",
      "0: Num VH = 0.1668\n",
      "------------------------------------------------------------\n",
      "0: Num non-res + ggFH + VBFH = 3.0598\n",
      "------------------------------------------------------------\n",
      "0: S = 0.4181, B = 3.4058, S/âB = 0.2265\n",
      "============================================================\n",
      "============================================================\n",
      "Category 1 3D outputs NOT< [0.00014055027654462777, 0.0019279517132367056, 0.0017874033775492724] AND 3D outputs < [0.00015953082067176068, 0.0031904568899924963, 0.0030309420216650075] AND 120 GeV < m_HH < 130 GeV\n",
      "------------------------------------------------------------\n",
      "1: Num ggF HH = 0.0754\n",
      "------------------------------------------------------------\n",
      "1: Num ttH = 0.0950\n",
      "------------------------------------------------------------\n",
      "1: Num VH = 0.0466\n",
      "------------------------------------------------------------\n",
      "1: Num non-res + ggFH + VBFH = 1.8324\n",
      "------------------------------------------------------------\n",
      "1: S = 0.0754, B = 1.9740, S/âB = 0.0537\n",
      "============================================================\n",
      "============================================================\n",
      "Category 2 3D outputs NOT< [0.00015953082067176068, 0.0031904568899924963, 0.0030309420216650075] AND 3D outputs < [0.00012428479978053485, 0.0036933592767097153, 0.003569109152819045] AND 120 GeV < m_HH < 130 GeV\n",
      "------------------------------------------------------------\n",
      "2: Num ggF HH = 0.0135\n",
      "------------------------------------------------------------\n",
      "2: Num ttH = 0.0069\n",
      "------------------------------------------------------------\n",
      "2: Num VH = 0.0089\n",
      "------------------------------------------------------------\n",
      "2: Num non-res + ggFH + VBFH = 0.2208\n",
      "------------------------------------------------------------\n",
      "2: S = 0.0135, B = 0.2365, S/âB = 0.0277\n"
     ]
    }
   ],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"s_over_rootb_multiOptim\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(projection='3d')\n",
    "# # x_preds, y_preds, z_preds = p_to_xyz(np.concatenate([BDT_perf['ggF HH']['preds'][fold_idx] for fold_idx in range(len(bdt_test_dict))], axis=0))\n",
    "# for i, sample_name in enumerate(order):\n",
    "#     if i == 0:\n",
    "#         downsample = 100\n",
    "#     elif i == 1:\n",
    "#         downsample = 200\n",
    "#     elif i == 2:\n",
    "#         downsample = 400\n",
    "#     elif i == 3:\n",
    "#         downsample = 500\n",
    "\n",
    "#     x_preds, y_preds, z_preds = p_to_xyz(np.array(BDT_perf['ggF HH']['preds'][0])[bdt_test_dict[f\"fold_0\"].get_label() == i][::downsample])\n",
    "#     ax.scatter(x_preds, y_preds, z_preds, marker='.', label=sample_name)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # plot s/âb curves\n",
    "# for fold_idx in range(len(bdt_train_dict)):\n",
    "\n",
    "#     with warnings.catch_warnings():\n",
    "#         warnings.simplefilter(\"ignore\")\n",
    "#         clf_dict = multi_optimize_cut_boundaries(\n",
    "#             BDT_perf['ggF HH']['preds'][fold_idx], \n",
    "#             bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == 0, \n",
    "#             weights_plot_test[f\"fold_{fold_idx}\"],\n",
    "#             min_sig=0.07\n",
    "#         )\n",
    "\n",
    "#     cat_dict = {}\n",
    "#     for cat in range(len(clf_dict)):\n",
    "#         prev_cat_slice = np.ones_like(weights_plot_test[f\"fold_{fold_idx}\"], dtype=bool)\n",
    "#         if cat > 0:\n",
    "#             for prev_cat in range(cat):\n",
    "#                 prev_cat_slice = np.logical_and(\n",
    "#                     prev_cat_slice,\n",
    "#                     np.logical_not(\n",
    "#                         np.all(\n",
    "#                             p_to_xyz(np.array(BDT_perf['ggF HH']['preds'][fold_idx]), split=False) < clf_dict[prev_cat], \n",
    "#                             axis=1\n",
    "#                         )\n",
    "#                     )\n",
    "#                 )\n",
    "#         cat_dict[cat] = np.logical_and(\n",
    "#             prev_cat_slice,\n",
    "#             np.all(\n",
    "#                 p_to_xyz(np.array(BDT_perf['ggF HH']['preds'][fold_idx]), split=False) < clf_dict[cat],\n",
    "#                 axis=1\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     masses = data_test_aux_dict[f\"fold_{fold_idx}\"]['mass']\n",
    "#     cat_num_samples = {}\n",
    "#     for cat in range(len(clf_dict)):\n",
    "#         cat_num_samples[cat] = {}\n",
    "#         print('='*60)\n",
    "#         print('='*60)\n",
    "#         print(f\"Fold {fold_idx}: Category {cat} (SVM) AND 120 GeV < m_HH < 130 GeV\")\n",
    "#         print('-'*60)\n",
    "#         for m, sample_name in enumerate(order):\n",
    "#             cat_num_samples[cat][sample_name] = np.sum(\n",
    "#                 weights_plot_test[f\"fold_{fold_idx}\"][\n",
    "#                     np.logical_and(  # event passes conditions and is the right type (i.e. sample)\n",
    "#                         np.logical_and(  # event passes category and mass conditions\n",
    "#                             cat_dict[cat],  # event passes category selections\n",
    "#                             np.logical_and(  # diphoton mass is within 120-130 window\n",
    "#                                 masses < 130,\n",
    "#                                 masses > 120\n",
    "#                             ),\n",
    "#                         ),\n",
    "#                         bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == m\n",
    "#                     )\n",
    "#                 ]\n",
    "#             )\n",
    "#             print(f\"{cat}: Num {sample_name} = {cat_num_samples[cat][sample_name]:.4f}\")\n",
    "#             print('-'*60)\n",
    "#         print(f\"{cat}: S = {cat_num_samples[cat][order[0]]:.4f}, B = {np.sum([cat_num_samples[cat][order[v]] for v in range(1, len(order))]):.4f}, S/âB = {(cat_num_samples[cat][order[0]] / np.sqrt(np.sum([cat_num_samples[cat][order[v]] for v in range(1, len(order))]))):.4f}\")\n",
    "\n",
    "# print('='*60)\n",
    "# print('='*60)\n",
    "# print('='*60)\n",
    "# print('='*60)\n",
    "\n",
    "flat_preds = np.concatenate([BDT_perf['ggF HH']['preds'][fold_idx] for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "flat_truths = np.concatenate([bdt_test_dict[f\"fold_{fold_idx}\"].get_label() for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "flat_weights = np.concatenate([weights_plot_test[f\"fold_{fold_idx}\"] for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "# flat_weights = np.concatenate([weight_test_dict[f\"fold_{fold_idx}\"] for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    clf_dict = multi_optimize_cut_boundaries(\n",
    "        flat_preds, flat_truths == 0, flat_weights\n",
    "    )\n",
    "\n",
    "    # plot_s_over_root_b(\n",
    "    #     sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], f\"{sample_name} vs. {sample_name_ if sample_name_ != sample_name else 'all'}\", \n",
    "    #     f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_sOverRootb_withCuts_testData_sum\", plot_dirpath, \n",
    "    #     weights=score_weights,\n",
    "    #     lines=lines, lines_labels=line_labels, line_colors=line_colors\n",
    "    # )\n",
    "\n",
    "flat_mass = np.concatenate([data_test_aux_dict[f\"fold_{fold_idx}\"]['mass'] for fold_idx in range(len(data_test_aux_dict))], axis=0)\n",
    "cat_dict = {}\n",
    "for cat in range(len(clf_dict)):\n",
    "    prev_cat_slice = np.ones_like(flat_weights, dtype=bool)\n",
    "    if cat > 0:\n",
    "        for prev_cat in range(cat):\n",
    "            prev_cat_slice = np.logical_and(\n",
    "                prev_cat_slice,\n",
    "                np.logical_not(\n",
    "                    np.all(\n",
    "                        p_to_xyz(flat_preds, split=False) < clf_dict[prev_cat], \n",
    "                        axis=1\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    cat_dict[cat] = np.logical_and(\n",
    "        prev_cat_slice,\n",
    "        np.all(\n",
    "            p_to_xyz(flat_preds, split=False) < clf_dict[cat],\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "\n",
    "cat_num_samples = {}\n",
    "for cat in range(len(clf_dict)):\n",
    "    cat_num_samples[cat] = {}\n",
    "    print('='*60)\n",
    "    print('='*60)\n",
    "    print(f\"Category {cat} {f'3D outputs NOT< {clf_dict[cat-1]} AND ' if cat > 0 else ''}3D outputs < {clf_dict[cat]} AND 120 GeV < m_HH < 130 GeV\")\n",
    "    print('-'*60)\n",
    "    for m, sample_name in enumerate(order):\n",
    "        cat_num_samples[cat][sample_name] = np.sum(\n",
    "            flat_weights[\n",
    "                np.logical_and(  # event passes conditions and is the right type (i.e. sample)\n",
    "                    np.logical_and(  # event passes category and mass conditions\n",
    "                        cat_dict[cat],  # event passes category selections\n",
    "                        np.logical_and(  # diphoton mass is within 120-130 window\n",
    "                            flat_mass < 130,\n",
    "                            flat_mass > 120\n",
    "                        ),\n",
    "                    ),\n",
    "                    flat_truths == m\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        print(f\"{cat}: Num {sample_name} = {cat_num_samples[cat][sample_name]:.4f}\")\n",
    "        print('-'*60)\n",
    "    print(f\"{cat}: S = {cat_num_samples[cat][order[0]]:.4f}, B = {np.sum([cat_num_samples[cat][order[v]] for v in range(1, len(order))]):.4f}, S/âB = {(cat_num_samples[cat][order[0]] / np.sqrt(np.sum([cat_num_samples[cat][order[v]] for v in range(1, len(order))]))):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0: FÎ² (Î²=1) score = \n",
      "[2.06338461e-04 8.30193782e-02 3.28043684e-02 9.65794559e-01]\n",
      "fold 1: FÎ² (Î²=1) score = \n",
      "[2.10784747e-04 9.39446874e-02 4.12013594e-02 9.65935539e-01]\n",
      "fold 2: FÎ² (Î²=1) score = \n",
      "[2.02518550e-04 8.32114836e-02 4.27516083e-02 9.65134483e-01]\n",
      "fold 3: FÎ² (Î²=1) score = \n",
      "[2.10114188e-04 9.35402315e-02 4.17540316e-02 9.66324729e-01]\n",
      "fold 4: FÎ² (Î²=1) score = \n",
      "[2.07214332e-04 9.17431115e-02 3.86979539e-02 9.65827188e-01]\n",
      "Sum over folds: FÎ² (Î²=1) score = \n",
      "[2.07368190e-04 8.89039373e-02 3.92836130e-02 9.65803912e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 900x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"confusion_matrix\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "beta = 1\n",
    "normalize = 'true'  # 'true' for normalize over rows, None for absoulte yields\n",
    "\n",
    "for fold_idx in range(len(BDT_perf['ggF HH']['preds'])):\n",
    "\n",
    "    pred_classes = np.argmax(BDT_perf['ggF HH']['preds'][fold_idx], axis=1)\n",
    "\n",
    "    conf_matrix = confusion_matrix(\n",
    "        bdt_test_dict[f\"fold_{fold_idx}\"].get_label(), \n",
    "        pred_classes,\n",
    "        sample_weight=weights_plot_test[f\"fold_{fold_idx}\"],\n",
    "        normalize=normalize\n",
    "    )\n",
    "\n",
    "    plot_confusion_matrix(\n",
    "        conf_matrix, order, f\"confusion_matrix_fold{fold_idx}{'_norm_'+normalize if normalize is not None else ''}\", plot_dirpath\n",
    "    )\n",
    "\n",
    "    f1_scores = fbeta_score(\n",
    "        bdt_test_dict[f\"fold_{fold_idx}\"].get_label(), \n",
    "        pred_classes,\n",
    "        beta=beta,\n",
    "        sample_weight=weights_plot_test[f\"fold_{fold_idx}\"], average=None\n",
    "    )\n",
    "    print(f\"fold {fold_idx}: FÎ² (Î²={beta}) score = \\n{f1_scores}\")\n",
    "\n",
    "full_pred_classes = np.argmax(\n",
    "    np.concatenate(\n",
    "        [\n",
    "            BDT_perf['ggF HH']['preds'][fold_idx] for fold_idx in range(len(BDT_perf['ggF HH']['preds']))\n",
    "        ]\n",
    "    ), axis=1\n",
    ")\n",
    "full_labels = np.concatenate(\n",
    "    [\n",
    "        bdt_test_dict[f\"fold_{fold_idx}\"].get_label() for fold_idx in range(len(BDT_perf['ggF HH']['preds']))\n",
    "    ]\n",
    ")\n",
    "full_weights = np.concatenate(\n",
    "    [\n",
    "        weights_plot_test[f\"fold_{fold_idx}\"] for fold_idx in range(len(BDT_perf['ggF HH']['preds']))\n",
    "    ]\n",
    ")\n",
    "\n",
    "conf_matrix = confusion_matrix(\n",
    "    full_labels, \n",
    "    full_pred_classes,\n",
    "    sample_weight=full_weights,\n",
    "    normalize=normalize\n",
    ")\n",
    "\n",
    "plot_confusion_matrix(\n",
    "    conf_matrix, order, f\"confusion_matrix_sum{'_norm_'+normalize if normalize is not None else ''}\", plot_dirpath\n",
    ")\n",
    "\n",
    "f1_scores = fbeta_score(\n",
    "    full_labels, \n",
    "    full_pred_classes,\n",
    "    beta=beta,\n",
    "    sample_weight=full_weights, average=None\n",
    ")\n",
    "print(f\"Sum over folds: FÎ² (Î²={beta}) score = \\n{f1_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"variable_importance\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "for fold_idx in range(len(bdt_train_dict)):\n",
    "    booster = xgb.Booster(param, model_file=os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "\n",
    "    labels = copy.deepcopy([key for key in hlf_vars_columns_dict[f'fold_{fold_idx}'].keys()])\n",
    "    labels.sort()\n",
    "    \n",
    "    booster.feature_names = labels\n",
    "    score_dict = booster.get_score(importance_type='total_gain')\n",
    "\n",
    "    sorted_scores, sorted_labels = [], []\n",
    "    for label, score in score_dict.items():\n",
    "        sorted_scores.append(score)\n",
    "        sorted_labels.append(label)\n",
    "\n",
    "    sorted_labels = np.array(sorted_labels)[np.argsort(sorted_scores)]\n",
    "    sorted_scores = np.sort(sorted_scores)\n",
    "\n",
    "    plot_feature_importance(\n",
    "        sorted_scores, sorted_labels, f'xgb_importance_fold{fold_idx}', plot_dirpath\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Variable Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "------------------------------------------------------------\n",
      "mass                            1.000000\n",
      "DeltaEta_jj                    -0.015599\n",
      "DeltaPhi_isr_jet_z             -0.000935\n",
      "DeltaPhi_jj                     0.000087\n",
      "leadBjet_leadLepton            -0.042631\n",
      "HHbbggCandidate_eta             0.001218\n",
      "HHbbggCandidate_pt              0.076420\n",
      "dijet_mass                      0.016484\n",
      "dijet_pt                        0.096118\n",
      "eta                             0.002634\n",
      "isr_jet_pt                      0.012029\n",
      "lead_bjet_pt                    0.088244\n",
      "lead_bjet_btagPNetB             0.026766\n",
      "lead_bjet_pt_over_Mjj           0.082386\n",
      "lead_mvaID                      0.031807\n",
      "lead_sigmaE_over_E             -0.093449\n",
      "lepton1_mvaID                  -0.042631\n",
      "lepton1_pfIsoId                -0.042633\n",
      "lepton1_pt                     -0.042591\n",
      "n_jets                          0.016477\n",
      "CosThetaStar_CS                 0.003129\n",
      "CosThetaStar_gg                 0.001016\n",
      "CosThetaStar_jj                -0.000857\n",
      "DeltaPhi_j1MET                 -0.000657\n",
      "DeltaPhi_j2MET                  0.000158\n",
      "DeltaR_jg_min                  -0.054863\n",
      "chi_t0                          0.005927\n",
      "chi_t1                          0.005179\n",
      "lead_bjet_sigmapT_over_pT      -0.034364\n",
      "lead_bjet_eta                   0.000130\n",
      "sublead_bjet_sigmapT_over_pT    0.003122\n",
      "sublead_bjet_eta                0.000955\n",
      "pt                              0.181611\n",
      "pt_balance                     -0.106563\n",
      "puppiMET_pt                    -0.008439\n",
      "puppiMET_sumEt                  0.306124\n",
      "sublead_bjet_pt                 0.006992\n",
      "sublead_bjet_btagPNetB         -0.000692\n",
      "sublead_bjet_pt_over_Mjj        0.001426\n",
      "sublead_mvaID                   0.033957\n",
      "sublead_sigmaE_over_E          -0.119030\n",
      "Name: mass, dtype: float64\n",
      "============================================================\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "fold 1\n",
      "------------------------------------------------------------\n",
      "mass                            1.000000\n",
      "DeltaEta_jj                    -0.015490\n",
      "DeltaPhi_isr_jet_z             -0.000894\n",
      "DeltaPhi_jj                     0.000258\n",
      "leadBjet_leadLepton            -0.042637\n",
      "HHbbggCandidate_eta             0.001877\n",
      "HHbbggCandidate_pt              0.076572\n",
      "dijet_mass                      0.017509\n",
      "dijet_pt                        0.096233\n",
      "eta                             0.003017\n",
      "isr_jet_pt                      0.012541\n",
      "lead_bjet_pt                    0.088636\n",
      "lead_bjet_btagPNetB             0.026108\n",
      "lead_bjet_pt_over_Mjj           0.082509\n",
      "lead_mvaID                      0.031803\n",
      "lead_sigmaE_over_E             -0.092732\n",
      "lepton1_mvaID                  -0.042637\n",
      "lepton1_pfIsoId                -0.042638\n",
      "lepton1_pt                     -0.042589\n",
      "n_jets                          0.017038\n",
      "CosThetaStar_CS                 0.003047\n",
      "CosThetaStar_gg                 0.001091\n",
      "CosThetaStar_jj                 0.000040\n",
      "DeltaPhi_j1MET                 -0.000273\n",
      "DeltaPhi_j2MET                 -0.000648\n",
      "DeltaR_jg_min                  -0.054998\n",
      "chi_t0                          0.005852\n",
      "chi_t1                          0.005470\n",
      "lead_bjet_sigmapT_over_pT      -0.034262\n",
      "lead_bjet_eta                   0.001022\n",
      "sublead_bjet_sigmapT_over_pT    0.002571\n",
      "sublead_bjet_eta                0.000793\n",
      "pt                              0.182160\n",
      "pt_balance                     -0.106850\n",
      "puppiMET_pt                    -0.008098\n",
      "puppiMET_sumEt                  0.306519\n",
      "sublead_bjet_pt                 0.007629\n",
      "sublead_bjet_btagPNetB         -0.000944\n",
      "sublead_bjet_pt_over_Mjj        0.001399\n",
      "sublead_mvaID                   0.034486\n",
      "sublead_sigmaE_over_E          -0.119426\n",
      "Name: mass, dtype: float64\n",
      "============================================================\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "fold 2\n",
      "------------------------------------------------------------\n",
      "mass                            1.000000\n",
      "DeltaEta_jj                    -0.015332\n",
      "DeltaPhi_isr_jet_z             -0.001154\n",
      "DeltaPhi_jj                     0.000091\n",
      "leadBjet_leadLepton            -0.042630\n",
      "HHbbggCandidate_eta             0.001669\n",
      "HHbbggCandidate_pt              0.077385\n",
      "dijet_mass                      0.017613\n",
      "dijet_pt                        0.096531\n",
      "eta                             0.002857\n",
      "isr_jet_pt                      0.011986\n",
      "lead_bjet_pt                    0.088711\n",
      "lead_bjet_btagPNetB             0.026698\n",
      "lead_bjet_pt_over_Mjj           0.082414\n",
      "lead_mvaID                      0.032045\n",
      "lead_sigmaE_over_E             -0.093078\n",
      "lepton1_mvaID                  -0.042630\n",
      "lepton1_pfIsoId                -0.042631\n",
      "lepton1_pt                     -0.042592\n",
      "n_jets                          0.017017\n",
      "CosThetaStar_CS                 0.003489\n",
      "CosThetaStar_gg                 0.000944\n",
      "CosThetaStar_jj                -0.000351\n",
      "DeltaPhi_j1MET                 -0.000773\n",
      "DeltaPhi_j2MET                 -0.000547\n",
      "DeltaR_jg_min                  -0.054246\n",
      "chi_t0                          0.006752\n",
      "chi_t1                          0.005107\n",
      "lead_bjet_sigmapT_over_pT      -0.034506\n",
      "lead_bjet_eta                   0.000092\n",
      "sublead_bjet_sigmapT_over_pT    0.002760\n",
      "sublead_bjet_eta                0.000344\n",
      "pt                              0.182623\n",
      "pt_balance                     -0.106257\n",
      "puppiMET_pt                    -0.007807\n",
      "puppiMET_sumEt                  0.307356\n",
      "sublead_bjet_pt                 0.008273\n",
      "sublead_bjet_btagPNetB         -0.000874\n",
      "sublead_bjet_pt_over_Mjj        0.001666\n",
      "sublead_mvaID                   0.034144\n",
      "sublead_sigmaE_over_E          -0.118862\n",
      "Name: mass, dtype: float64\n",
      "============================================================\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "fold 3\n",
      "------------------------------------------------------------\n",
      "mass                            1.000000\n",
      "DeltaEta_jj                    -0.015958\n",
      "DeltaPhi_isr_jet_z             -0.001193\n",
      "DeltaPhi_jj                     0.000266\n",
      "leadBjet_leadLepton            -0.042584\n",
      "HHbbggCandidate_eta             0.001165\n",
      "HHbbggCandidate_pt              0.077512\n",
      "dijet_mass                      0.017233\n",
      "dijet_pt                        0.096681\n",
      "eta                             0.002849\n",
      "isr_jet_pt                      0.012479\n",
      "lead_bjet_pt                    0.088984\n",
      "lead_bjet_btagPNetB             0.026395\n",
      "lead_bjet_pt_over_Mjj           0.082852\n",
      "lead_mvaID                      0.031693\n",
      "lead_sigmaE_over_E             -0.093010\n",
      "lepton1_mvaID                  -0.042584\n",
      "lepton1_pfIsoId                -0.042586\n",
      "lepton1_pt                     -0.042548\n",
      "n_jets                          0.016844\n",
      "CosThetaStar_CS                 0.003120\n",
      "CosThetaStar_gg                 0.001081\n",
      "CosThetaStar_jj                -0.000833\n",
      "DeltaPhi_j1MET                 -0.000619\n",
      "DeltaPhi_j2MET                 -0.000468\n",
      "DeltaR_jg_min                  -0.054872\n",
      "chi_t0                          0.006454\n",
      "chi_t1                          0.005703\n",
      "lead_bjet_sigmapT_over_pT      -0.034399\n",
      "lead_bjet_eta                   0.000236\n",
      "sublead_bjet_sigmapT_over_pT    0.002640\n",
      "sublead_bjet_eta                0.001125\n",
      "pt                              0.183151\n",
      "pt_balance                     -0.106201\n",
      "puppiMET_pt                    -0.008022\n",
      "puppiMET_sumEt                  0.307018\n",
      "sublead_bjet_pt                 0.007884\n",
      "sublead_bjet_btagPNetB         -0.000895\n",
      "sublead_bjet_pt_over_Mjj        0.001935\n",
      "sublead_mvaID                   0.034243\n",
      "sublead_sigmaE_over_E          -0.119122\n",
      "Name: mass, dtype: float64\n",
      "============================================================\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "fold 4\n",
      "------------------------------------------------------------\n",
      "mass                            1.000000\n",
      "DeltaEta_jj                    -0.015718\n",
      "DeltaPhi_isr_jet_z             -0.000734\n",
      "DeltaPhi_jj                     0.000269\n",
      "leadBjet_leadLepton            -0.042680\n",
      "HHbbggCandidate_eta             0.001022\n",
      "HHbbggCandidate_pt              0.077004\n",
      "dijet_mass                      0.017531\n",
      "dijet_pt                        0.096718\n",
      "eta                             0.002337\n",
      "isr_jet_pt                      0.012393\n",
      "lead_bjet_pt                    0.089300\n",
      "lead_bjet_btagPNetB             0.026510\n",
      "lead_bjet_pt_over_Mjj           0.082999\n",
      "lead_mvaID                      0.031953\n",
      "lead_sigmaE_over_E             -0.093090\n",
      "lepton1_mvaID                  -0.042680\n",
      "lepton1_pfIsoId                -0.042681\n",
      "lepton1_pt                     -0.042642\n",
      "n_jets                          0.016880\n",
      "CosThetaStar_CS                 0.003253\n",
      "CosThetaStar_gg                 0.000937\n",
      "CosThetaStar_jj                -0.000387\n",
      "DeltaPhi_j1MET                 -0.000234\n",
      "DeltaPhi_j2MET                 -0.000471\n",
      "DeltaR_jg_min                  -0.054934\n",
      "chi_t0                          0.006573\n",
      "chi_t1                          0.004979\n",
      "lead_bjet_sigmapT_over_pT      -0.034862\n",
      "lead_bjet_eta                   0.000123\n",
      "sublead_bjet_sigmapT_over_pT    0.003173\n",
      "sublead_bjet_eta                0.000492\n",
      "pt                              0.182784\n",
      "pt_balance                     -0.106648\n",
      "puppiMET_pt                    -0.008082\n",
      "puppiMET_sumEt                  0.306075\n",
      "sublead_bjet_pt                 0.007131\n",
      "sublead_bjet_btagPNetB         -0.001004\n",
      "sublead_bjet_pt_over_Mjj        0.000931\n",
      "sublead_mvaID                   0.034147\n",
      "sublead_sigmaE_over_E          -0.119259\n",
      "Name: mass, dtype: float64\n",
      "============================================================\n",
      "============================================================\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_corr_dict = {}\n",
    "for fold_idx in range(len(data_aux_dict)):\n",
    "    merged_pd = copy.deepcopy(data_df_dict[f\"fold_{fold_idx}\"])\n",
    "    for i, var_name in enumerate(['mass']):\n",
    "        merged_pd.insert(i, var_name, data_aux_dict[f\"fold_{fold_idx}\"].loc[:, var_name])\n",
    "    data_corr_dict[f\"fold_{fold_idx}\"] = merged_pd.corr()\n",
    "\n",
    "    print(f\"fold {fold_idx}\")\n",
    "    print('-'*60)\n",
    "    print(data_corr_dict[f\"fold_{fold_idx}\"].iloc[0, :])\n",
    "    print(f\"{'='*60}\\n{'='*60}\\n{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass Sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 38\u001b[0m\n\u001b[1;32m     32\u001b[0m train_hists[sample_name], val_hists[sample_name], test_hists[sample_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(), \u001b[38;5;28mlist\u001b[39m(), \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m score_cut \u001b[38;5;129;01min\u001b[39;00m score_cuts:\n\u001b[1;32m     35\u001b[0m     train_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     36\u001b[0m         xgb_label_train_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m i\n\u001b[1;32m     37\u001b[0m     ) \u001b[38;5;241m&\u001b[39m (\n\u001b[0;32m---> 38\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBDT_perf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mggF HH\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_preds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m score_cut\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     40\u001b[0m     val_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     41\u001b[0m         xgb_label_val_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m i\n\u001b[1;32m     42\u001b[0m     ) \u001b[38;5;241m&\u001b[39m (\n\u001b[1;32m     43\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(BDT_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mggF HH\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_preds\u001b[39m\u001b[38;5;124m'\u001b[39m][fold_idx])[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m score_cut\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     test_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     46\u001b[0m         xgb_label_test_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m i\n\u001b[1;32m     47\u001b[0m     ) \u001b[38;5;241m&\u001b[39m (\n\u001b[1;32m     48\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(BDT_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mggF HH\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreds\u001b[39m\u001b[38;5;124m'\u001b[39m][fold_idx])[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m score_cut\n\u001b[1;32m     49\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"mass_sculpting\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+f\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "score_cuts = [0.0, 0.7, 0.99]\n",
    "label_arr = {\n",
    "    MC_NAMES_PRETTY[sample_name]: [f'score above {score_cut}' for score_cut in score_cuts] for sample_name in order\n",
    "}\n",
    "\n",
    "# Loop over and plot the per-fold variables\n",
    "for fold_idx in range(len(hlf_vars_columns_dict)):\n",
    "\n",
    "    for var_idx, var_name in enumerate(['mass', 'dijet_mass']):\n",
    "\n",
    "        plot_dirpath_ = os.path.join(plot_dirpath, var_name)\n",
    "        if not os.path.exists(plot_dirpath_):\n",
    "            os.makedirs(plot_dirpath_)\n",
    "\n",
    "        train_hists, val_hists, test_hists = {}, {}, {}\n",
    "        for i, sample_name in enumerate(order):\n",
    "\n",
    "            train_hists[sample_name], val_hists[sample_name], test_hists[sample_name] = list(), list(), list()\n",
    "            for score_cut in score_cuts:\n",
    "\n",
    "                train_mask = (\n",
    "                    xgb_label_train_dict[f'fold_{fold_idx}'] == i\n",
    "                ) & (\n",
    "                    np.array(BDT_perf['ggF HH']['train_preds'][fold_idx])[:, 0] > score_cut\n",
    "                )\n",
    "                val_mask = (\n",
    "                    xgb_label_val_dict[f'fold_{fold_idx}'] == i\n",
    "                ) & (\n",
    "                    np.array(BDT_perf['ggF HH']['val_preds'][fold_idx])[:, 0] > score_cut\n",
    "                )\n",
    "                test_mask = (\n",
    "                    xgb_label_test_dict[f'fold_{fold_idx}'] == i\n",
    "                ) & (\n",
    "                    np.array(BDT_perf['ggF HH']['preds'][fold_idx])[:, 0] > score_cut\n",
    "                )\n",
    "            \n",
    "                train_np = (\n",
    "                    data_aux_dict[f'fold_{fold_idx}'].iloc[train_idxs_dict[f'fold_{fold_idx}']]\n",
    "                ).loc[train_mask, var_name].to_numpy()\n",
    "                val_np = (\n",
    "                    data_aux_dict[f'fold_{fold_idx}'].iloc[val_idxs_dict[f'fold_{fold_idx}']]\n",
    "                ).loc[val_mask, var_name].to_numpy()\n",
    "                test_np = data_test_aux_dict[f'fold_{fold_idx}'].loc[test_mask, var_name].to_numpy()\n",
    "            \n",
    "                train_hists[sample_name].append(hist.Hist(VARIABLES[var_name]).fill(var=train_np))\n",
    "                val_hists[sample_name].append(hist.Hist(VARIABLES[var_name]).fill(var=val_np))\n",
    "                test_hists[sample_name].append(hist.Hist(VARIABLES[var_name]).fill(var=test_np))\n",
    "    \n",
    "            for j, (plot_type, histdict) in enumerate([('train_', train_hists), ('val_', val_hists), ('test_', test_hists)]):\n",
    "                make_input_plot(\n",
    "                    plot_dirpath_, var_name,\n",
    "                    histdict[sample_name], \n",
    "                    fold_idx=fold_idx, labels=label_arr[MC_NAMES_PRETTY[sample_name]], \n",
    "                    plot_prefix=plot_type+f'{sample_name}_scoreCut_'\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling for Mass Sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_var(sample_var, sample_weight, n_events, n_samples_per_event=1, bins=100, seed=None):\n",
    "    resample_rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    np_hist, bin_edges = np.histogram(sample_var, bins=bins, weights=sample_weight, density=True)\n",
    "    np_hist /= np.sum(np_hist)\n",
    "\n",
    "    bin_choices = resample_rng.choice(np.arange(len(np_hist)), size=n_events*n_samples_per_event, p=np_hist)\n",
    "\n",
    "    value_choices = (bin_edges[bin_choices+1] - bin_edges[bin_choices]) * resample_rng.random(size=n_events*n_samples_per_event) + bin_edges[bin_choices]\n",
    "\n",
    "    return value_choices\n",
    "\n",
    "def resample_grow_np(var, bool_arr, n_duplicates_per_event):\n",
    "    new_rows_shape = tuple([n_duplicates_per_event]+[1 for _ in range(1, len(np.shape(var)))])\n",
    "    new_rows = np.tile(\n",
    "        var[bool_arr],\n",
    "        new_rows_shape\n",
    "    )\n",
    "    return np.concatenate([var, new_rows])\n",
    "def resample_grow_pd(var, bool_arr, n_duplicates_per_event):\n",
    "    new_rows = pd.DataFrame(\n",
    "        np.tile(\n",
    "            ( var.loc[bool_arr] ).to_numpy(),\n",
    "            (n_duplicates_per_event, 1)\n",
    "        ),\n",
    "        columns=var.columns\n",
    "    )\n",
    "    return pd.concat([var, new_rows], ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resample_grow_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m booster\u001b[38;5;241m.\u001b[39mload_model(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIRPATH, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCURRENT_TIME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_BDT_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.model\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     28\u001b[0m nonres_bool \u001b[38;5;241m=\u001b[39m (data_test_aux_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGGJets\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m (data_test_aux_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGJetPt20To40\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m|\u001b[39m (data_test_aux_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msample_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGJetPt40\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m data_hlf_test \u001b[38;5;241m=\u001b[39m \u001b[43mresample_grow_np\u001b[49m(data_hlf_test_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], nonres_bool, resample_)\n\u001b[1;32m     31\u001b[0m data_test_aux \u001b[38;5;241m=\u001b[39m resample_grow_pd(data_test_aux_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], nonres_bool, resample_)\n\u001b[1;32m     32\u001b[0m weight_test \u001b[38;5;241m=\u001b[39m resample_grow_np(weight_test_dict[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m], nonres_bool, resample_)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resample_grow_np' is not defined"
     ]
    }
   ],
   "source": [
    "resample_ = 10\n",
    "RESAMPLE = (resample_) * 10  # Set to False for no resampling, otherwise sets the number of times to duplicate gjet data for resampling\n",
    "\n",
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"mass_sculpting_resample\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "score_cuts = [0.0, 0.7, 0.99, 0.9984]\n",
    "label_arr = [f'score above {score_cut}' for score_cut in score_cuts]\n",
    "plot_vars = ['mass', 'dijet_mass', 'HHbbggCandidate_mass']\n",
    "\n",
    "BDT_perf_resample = [\n",
    "    {\n",
    "        f'preds{score_cut}': copy.deepcopy({plot_var: list() for plot_var in plot_vars}) for score_cut in score_cuts\n",
    "    } for fold_idx in range(len(bdt_train_dict))\n",
    "]\n",
    "GJet_preds = []\n",
    "mean_values = {\n",
    "    'gj': list(),\n",
    "    'gg': list(),\n",
    "    'tth': list()\n",
    "}\n",
    "\n",
    "for fold_idx in range(len(bdt_train_dict)):\n",
    "    booster = xgb.Booster(param)\n",
    "    booster.load_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "\n",
    "    nonres_bool = (data_test_aux_dict[f\"fold_{fold_idx}\"].loc[:, 'sample_name'] == \"GGJets\") | (data_test_aux_dict[f\"fold_{fold_idx}\"].loc[:, 'sample_name'] == \"GJetPt20To40\") | (data_test_aux_dict[f\"fold_{fold_idx}\"].loc[:, 'sample_name'] == \"GJetPt40\")\n",
    "\n",
    "    data_hlf_test = resample_grow_np(data_hlf_test_dict[f\"fold_{fold_idx}\"], nonres_bool, resample_)\n",
    "    data_test_aux = resample_grow_pd(data_test_aux_dict[f\"fold_{fold_idx}\"], nonres_bool, resample_)\n",
    "    weight_test = resample_grow_np(weight_test_dict[f\"fold_{fold_idx}\"], nonres_bool, resample_)\n",
    "    weights_plot = resample_grow_np(weights_plot_test[f\"fold_{fold_idx}\"], nonres_bool, resample_)\n",
    "    xgb_label_test = resample_grow_np(xgb_label_test_dict[f\"fold_{fold_idx}\"], nonres_bool, resample_)\n",
    "\n",
    "    gg_bool = (data_test_aux.loc[:, 'sample_name'] == \"GGJets\")\n",
    "    tth_bool = (data_test_aux.loc[:, 'sample_name'] == \"ttHToGG\")\n",
    "    gj_bool = (data_test_aux.loc[:, 'sample_name'] == \"GJetPt20To40\") | (data_test_aux.loc[:, 'sample_name'] == \"GJetPt40\")\n",
    "    hh_bool = (data_test_aux.loc[:, 'sample_name'] == \"GluGluToHH\")\n",
    "    nonres_bool = (data_test_aux.loc[:, 'sample_name'] == \"GGJets\") | (data_test_aux.loc[:, 'sample_name'] == \"GJetPt20To40\") | (data_test_aux.loc[:, 'sample_name'] == \"GJetPt40\")\n",
    "\n",
    "\n",
    "    for var_idx, plot_var in enumerate(plot_vars):\n",
    "\n",
    "        plot_dirpath_ = os.path.join(plot_dirpath, plot_var)\n",
    "        if not os.path.exists(plot_dirpath_):\n",
    "            os.makedirs(plot_dirpath_)\n",
    "\n",
    "        for _ in range(RESAMPLE // resample_):\n",
    "\n",
    "            for particle_type in ['lead', 'sublead']:\n",
    "\n",
    "                # gg_mvaID = data_hlf_test[\n",
    "                #     gg_bool, \n",
    "                #     hlf_vars_columns_dict[f\"fold_{fold_idx}\"][f\"{particle_type}_mvaID\"]\n",
    "                # ]\n",
    "                # data_hlf_test[\n",
    "                #     gj_bool, \n",
    "                #     hlf_vars_columns_dict[f\"fold_{fold_idx}\"][f\"{particle_type}_mvaID\"]\n",
    "                # ] = resample_from_var(\n",
    "                #     gg_mvaID, \n",
    "                #     weights_plot[gg_bool],\n",
    "                #     np.sum(gj_bool),\n",
    "                #     bins=190\n",
    "                # )\n",
    "\n",
    "                tth_pNetB = data_hlf_test[\n",
    "                    tth_bool, \n",
    "                    hlf_vars_columns_dict[f\"fold_{fold_idx}\"][f\"{particle_type}_bjet_btagPNetB\"]\n",
    "                ]\n",
    "                data_hlf_test[\n",
    "                    nonres_bool, \n",
    "                    hlf_vars_columns_dict[f\"fold_{fold_idx}\"][f\"{particle_type}_bjet_btagPNetB\"]\n",
    "                ] = resample_from_var(\n",
    "                    tth_pNetB, \n",
    "                    weights_plot[tth_bool],\n",
    "                    np.sum(nonres_bool),\n",
    "                    bins=100\n",
    "                )\n",
    "\n",
    "                # tth_sigmaE = data_hlf_test[\n",
    "                #     tth_bool, \n",
    "                #     hlf_vars_columns_dict[f\"fold_{fold_idx}\"][f\"{particle_type}_sigmaE_over_E\"]\n",
    "                # ]\n",
    "                # data_hlf_test[\n",
    "                #     gj_bool, \n",
    "                #     hlf_vars_columns_dict[f\"fold_{fold_idx}\"][f\"{particle_type}_sigmaE_over_E\"]\n",
    "                # ] = resample_from_var(\n",
    "                #     tth_sigmaE, \n",
    "                #     np.abs(weights_plot[tth_bool]),\n",
    "                #     np.sum(gj_bool),\n",
    "                #     bins=100\n",
    "                # )\n",
    "            # hh_dijet = data_hlf_test[\n",
    "            #     hh_bool, \n",
    "            #     hlf_vars_columns_dict[f\"fold_{fold_idx}\"][\"dijet_mass\"]\n",
    "            # ]\n",
    "            # data_hlf_test[\n",
    "            #     nonres_bool, \n",
    "            #     hlf_vars_columns_dict[f\"fold_{fold_idx}\"][\"dijet_mass\"]\n",
    "            # ] = resample_from_var(\n",
    "            #     hh_dijet, \n",
    "            #     np.abs(weights_plot[hh_bool]),\n",
    "            #     np.sum(nonres_bool),\n",
    "            #     bins=100\n",
    "            # )\n",
    "\n",
    "            nonres_ggf_preds = booster.predict(\n",
    "                xgb.DMatrix(\n",
    "                    data=data_hlf_test[nonres_bool], label=xgb_label_test[nonres_bool], \n",
    "                    weight=np.abs(weight_test)[nonres_bool],\n",
    "                    missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "                ), \n",
    "                iteration_range=(0, booster.best_iteration+1)\n",
    "            )[:, 0]\n",
    "\n",
    "            # gg_ggf_preds = booster.predict(\n",
    "            #     xgb.DMatrix(\n",
    "            #         data=data_hlf_test[gg_bool], label=xgb_label_test[gg_bool], \n",
    "            #         weight=np.abs(weight_test)[gg_bool],\n",
    "            #         missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "            #     ), \n",
    "            #     iteration_range=(0, booster.best_iteration+1)\n",
    "            # )[:, 0]\n",
    "            # tth_ggf_preds = booster.predict(\n",
    "            #     xgb.DMatrix(\n",
    "            #         data=data_hlf_test[tth_bool], label=xgb_label_test[tth_bool], \n",
    "            #         weight=np.abs(weight_test)[tth_bool],\n",
    "            #         missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "            #     ), \n",
    "            #     iteration_range=(0, booster.best_iteration+1)\n",
    "            # )[:, 0]\n",
    "\n",
    "            if np.sum([len(GJet_preds[i]) for i in range(len(GJet_preds))]) < 100000:\n",
    "                GJet_preds.append(nonres_ggf_preds[nonres_ggf_preds > 0.9])\n",
    "\n",
    "            for score_cut in score_cuts:\n",
    "                if len(BDT_perf_resample[fold_idx][f'preds{score_cut}'][plot_var]) >= 10000:\n",
    "                    continue\n",
    "\n",
    "                BDT_perf_resample[fold_idx][f'preds{score_cut}'][plot_var].append(\n",
    "                    data_test_aux.loc[nonres_bool, plot_var].to_numpy()[nonres_ggf_preds > score_cut]\n",
    "                )\n",
    "        if fold_idx == 0 and plot_var == 'mass':\n",
    "            plt.figure()\n",
    "            plt.hist(np.concatenate(GJet_preds), bins=400, range=(0.9, 1.))\n",
    "            plt.savefig(os.path.join(plot_dirpath_, \"GJet_output_dist_with_resample0p9\"))\n",
    "\n",
    "        test_hists = [hist.Hist(VARIABLES[plot_var]).fill(var=np.concatenate(BDT_perf_resample[fold_idx][f'preds{score_cut}'][plot_var])) for score_cut in score_cuts]\n",
    "        make_input_plot(\n",
    "            plot_dirpath_, plot_var,\n",
    "            test_hists, \n",
    "            fold_idx=fold_idx, labels=label_arr, \n",
    "            plot_prefix='test_non-res_scoreCut_'\n",
    "        )\n",
    "\n",
    "for var_idx, plot_var in enumerate(plot_vars):\n",
    "\n",
    "    plot_dirpath_ = os.path.join(plot_dirpath, plot_var)\n",
    "    if not os.path.exists(plot_dirpath_):\n",
    "        os.makedirs(plot_dirpath_)\n",
    "\n",
    "    test_hists = [hist.Hist(VARIABLES[plot_var]).fill(\n",
    "        var=np.concatenate(\n",
    "            [np.concatenate(BDT_perf_resample[fold_idx][f'preds{score_cut}'][plot_var]) for fold_idx in range(len(BDT_perf_resample))]\n",
    "        )\n",
    "    ) for score_cut in score_cuts]\n",
    "    make_input_plot(\n",
    "        plot_dirpath_, plot_var,\n",
    "        test_hists, \n",
    "        fold_idx=None, labels=label_arr, \n",
    "        plot_prefix='test_non-res_scoreCut_'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"pre_std\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\",\n",
    "    # MC_NAMES_PRETTY[\"single-H\"]+\" train\", MC_NAMES_PRETTY[\"single-H\"]+\" val\", MC_NAMES_PRETTY[\"single-H\"]+\" test\",\n",
    "    # MC_NAMES_PRETTY[\"non-res\"]+\" train\", MC_NAMES_PRETTY[\"non-res\"]+\" val\", MC_NAMES_PRETTY[\"non-res\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"VH\"]+\" train\", MC_NAMES_PRETTY[\"VH\"]+\" val\", MC_NAMES_PRETTY[\"VH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"non-res + ggFH + VBFH\"]+\" train\", MC_NAMES_PRETTY[\"non-res + ggFH + VBFH\"]+\" val\", MC_NAMES_PRETTY[\"non-res + ggFH + VBFH\"]+\" test\",\n",
    "]\n",
    "# Loop over and plot the per-fold variables\n",
    "for fold_idx in range(len(hlf_vars_columns_dict)):\n",
    "\n",
    "    for var_name in hlf_vars_columns_dict['fold_0']:\n",
    "        if var_name in {'puppiMET_eta'}:\n",
    "            continue\n",
    "\n",
    "        plot_dirpath_ = os.path.join(plot_dirpath, var_name)\n",
    "        if not os.path.exists(plot_dirpath_):\n",
    "            os.makedirs(plot_dirpath_)\n",
    "\n",
    "        train_hists, val_hists, test_hists = {}, {}, {}\n",
    "        for i, sample_name in enumerate(order):\n",
    "            train_mask = xgb_label_train_dict[f'fold_{fold_idx}'] == i\n",
    "            val_mask = xgb_label_val_dict[f'fold_{fold_idx}'] == i\n",
    "            test_mask = xgb_label_test_dict[f'fold_{fold_idx}'] == i\n",
    "\n",
    "            train_np = (\n",
    "                data_df_dict[f'fold_{fold_idx}'].iloc[train_idxs_dict[f'fold_{fold_idx}']]\n",
    "            ).loc[train_mask, var_name].to_numpy()\n",
    "            val_np = (\n",
    "                data_df_dict[f'fold_{fold_idx}'].iloc[val_idxs_dict[f'fold_{fold_idx}']]\n",
    "            ).loc[val_mask, var_name].to_numpy()\n",
    "            test_np = data_test_df_dict[f'fold_{fold_idx}'].loc[test_mask, var_name].to_numpy()\n",
    "\n",
    "            train_hists[sample_name] = hist.Hist(VARIABLES[var_name]).fill(var=train_np)\n",
    "            val_hists[sample_name] = hist.Hist(VARIABLES[var_name]).fill(var=val_np)\n",
    "            test_hists[sample_name] = hist.Hist(VARIABLES[var_name]).fill(var=test_np)\n",
    "    \n",
    "            make_input_plot(\n",
    "                plot_dirpath_, var_name,\n",
    "                [train_hists[sample_name], val_hists[sample_name], test_hists[sample_name]], \n",
    "                fold_idx=fold_idx, labels=label_arr_fold[3*i : 3*(i+1)], plot_prefix=f'train_val_test_{sample_name}_'\n",
    "            )\n",
    "        for j, (plot_type, histdict) in enumerate([('train_', train_hists), ('val_', val_hists), ('test_', test_hists)]):\n",
    "            make_input_plot(\n",
    "                plot_dirpath_, var_name,\n",
    "                [histdict[sample_name] for sample_name in order], \n",
    "                fold_idx=fold_idx, labels=label_arr_fold[j::3], plot_prefix=plot_type\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"post_std\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\",\n",
    "    # MC_NAMES_PRETTY[\"single-H\"]+\" train\", MC_NAMES_PRETTY[\"single-H\"]+\" val\", MC_NAMES_PRETTY[\"single-H\"]+\" test\",\n",
    "    # MC_NAMES_PRETTY[\"non-res\"]+\" train\", MC_NAMES_PRETTY[\"non-res\"]+\" val\", MC_NAMES_PRETTY[\"non-res\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"VH\"]+\" train\", MC_NAMES_PRETTY[\"VH\"]+\" val\", MC_NAMES_PRETTY[\"VH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"non-res + ggFH + VBFH\"]+\" train\", MC_NAMES_PRETTY[\"non-res + ggFH + VBFH\"]+\" val\", MC_NAMES_PRETTY[\"non-res + ggFH + VBFH\"]+\" test\",\n",
    "]\n",
    "# Loop over and plot the per-fold variables\n",
    "for fold_idx in range(len(hlf_vars_columns_dict)):\n",
    "\n",
    "    for var_idx, var_name in enumerate(hlf_vars_columns_dict['fold_0']):\n",
    "        if var_name in {'puppiMET_eta'}:\n",
    "            continue\n",
    "\n",
    "        plot_dirpath_ = os.path.join(plot_dirpath, var_name)\n",
    "        if not os.path.exists(plot_dirpath_):\n",
    "            os.makedirs(plot_dirpath_)\n",
    "\n",
    "        train_hists, val_hists, test_hists = {}, {}, {}\n",
    "        for i, sample_name in enumerate(order):\n",
    "            train_mask = xgb_label_train_dict[f'fold_{fold_idx}'] == i\n",
    "            val_mask = xgb_label_val_dict[f'fold_{fold_idx}'] == i\n",
    "            test_mask = xgb_label_test_dict[f'fold_{fold_idx}'] == i\n",
    "\n",
    "            train_np = train_data_dict[f'fold_{fold_idx}'][train_mask, var_idx]\n",
    "            val_np = val_data_dict[f'fold_{fold_idx}'][val_mask, var_idx]\n",
    "            test_np = data_hlf_test_dict[f'fold_{fold_idx}'][test_mask, var_idx]\n",
    "\n",
    "            train_hists[sample_name] = hist.Hist(VARIABLES_STD[var_name]).fill(var=train_np)\n",
    "            val_hists[sample_name] = hist.Hist(VARIABLES_STD[var_name]).fill(var=val_np)\n",
    "            test_hists[sample_name] = hist.Hist(VARIABLES_STD[var_name]).fill(var=test_np)\n",
    "    \n",
    "            make_input_plot(\n",
    "                plot_dirpath_, var_name,\n",
    "                [train_hists[sample_name], val_hists[sample_name], test_hists[sample_name]], \n",
    "                fold_idx=fold_idx, labels=label_arr_fold[3*i : 3*(i+1)], plot_prefix=f'train_val_test_{sample_name}_'\n",
    "            )\n",
    "        for j, (plot_type, histdict) in enumerate([('train_', train_hists), ('val_', val_hists), ('test_', test_hists)]):\n",
    "            make_input_plot(\n",
    "                plot_dirpath_, var_name,\n",
    "                [histdict[sample_name] for sample_name in order], \n",
    "                fold_idx=fold_idx, labels=label_arr_fold[j::3], plot_prefix=plot_type\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save out new parquets for Yibo to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DATA_ON_ALL_FOLDS = True\n",
    "\n",
    "# load and pre-process the data\n",
    "DATA_FILEPATHS_DICT = {\n",
    "    'Data': [\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v5/Data_EraC/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v5/Data_EraD/nominal/*\",\n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v5/Data_EraE/nominal/*\",\n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v5/Data_EraF/nominal/*\",\n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v5/Data_EraG/nominal/*\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "(\n",
    "    NOTHING_IGNORE,\n",
    "    DATA_data_df_dict, DATA_data_test_df_dict, \n",
    "    DATA_data_hlf_dict, DATA_label_dict,\n",
    "    DATA_data_hlf_test_dict, DATA_label_test_dict, \n",
    "    DATA_hlf_vars_columns_dict,\n",
    "    DATA_data_aux_dict, DATA_data_test_aux_dict\n",
    ") = process_data(\n",
    "    DATA_FILEPATHS_DICT, OUTPUT_DIRPATH, order=['Data'], mod_vals=MOD_VALS, k_fold_test=True,\n",
    "    save=False, std_json_dirpath=OUTPUT_DIRPATH\n",
    ")\n",
    "\n",
    "BDT_DATA_preds = []\n",
    "\n",
    "if EVAL_DATA_ON_ALL_FOLDS:\n",
    "\n",
    "    bdt_train_data_dict = xgb.DMatrix(\n",
    "        data=DATA_data_hlf_dict[f\"fold_0\"], label=DATA_label_dict[f\"fold_0\"], \n",
    "        missing=-999.0, feature_names=list(DATA_hlf_vars_columns_dict[f\"fold_0\"])\n",
    "    )\n",
    "    bdt_test_data_dict = xgb.DMatrix(\n",
    "        data=DATA_data_hlf_test_dict[f\"fold_0\"], label=DATA_label_test_dict[f\"fold_0\"], \n",
    "        missing=-999.0, feature_names=list(DATA_hlf_vars_columns_dict[f\"fold_0\"])\n",
    "    )\n",
    "\n",
    "    for fold_idx in range(len(DATA_label_test_dict)):\n",
    "        booster = xgb.Booster(param)\n",
    "        booster.load_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "\n",
    "        BDT_train_preds = booster.predict(\n",
    "            bdt_train_data_dict, \n",
    "            iteration_range=(0, booster.best_iteration+1)\n",
    "        )\n",
    "        BDT_test_preds = booster.predict(\n",
    "            bdt_test_data_dict, \n",
    "            iteration_range=(0, booster.best_iteration+1)\n",
    "        )\n",
    "\n",
    "        BDT_all_preds = np.concatenate([BDT_train_preds, BDT_test_preds])\n",
    "        BDT_all_preds = BDT_all_preds[\n",
    "            np.argsort(\n",
    "                np.concatenate([DATA_data_aux_dict[f\"fold_0\"].loc[:, 'hash'].to_numpy(), DATA_data_test_aux_dict[f\"fold_0\"].loc[:, 'hash'].to_numpy()])\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if fold_idx == 0:\n",
    "            BDT_DATA_preds = copy.deepcopy(BDT_all_preds)\n",
    "        else:\n",
    "            BDT_DATA_preds += BDT_all_preds\n",
    "\n",
    "            if fold_idx == len(DATA_label_test_dict) - 1:\n",
    "                BDT_DATA_preds = BDT_DATA_preds / len(DATA_label_test_dict)\n",
    "else:\n",
    "\n",
    "    bdt_train_data_dict, bdt_test_data_dict = {}, {}\n",
    "    for fold_idx in range(len(DATA_label_test_dict)):\n",
    "        \n",
    "        bdt_train_data_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "            data=DATA_data_hlf_dict[f\"fold_{fold_idx}\"], label=DATA_label_dict[f\"fold_{fold_idx}\"], \n",
    "            missing=-999.0, feature_names=list(DATA_hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "        )\n",
    "        bdt_test_data_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "            data=DATA_data_hlf_test_dict[f\"fold_{fold_idx}\"], label=DATA_label_test_dict[f\"fold_{fold_idx}\"], \n",
    "            missing=-999.0, feature_names=list(DATA_hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "        )\n",
    "\n",
    "        booster = xgb.Booster(param)\n",
    "        booster.load_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "\n",
    "        BDT_DATA_preds.append(\n",
    "            booster.predict(\n",
    "                bdt_test_data_dict[f\"fold_{fold_idx}\"], \n",
    "                iteration_range=(0, booster.best_iteration+1)\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0QAAANUCAYAAACTz+21AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQhUlEQVR4nO3dfZSdVX0v8O+ZF8NLhCREB8igokB0xDcKhmCt2vLibeuVSgXhprUUyq13arvk1kXtsg1EvLS2tsu2p3qL9FIrNaJUEdoLSIsvrYgQ4AqMBcFiZAIHNANoiElm5rl/jDNkkpmTyeScOWfm+XzWmrVg9nOe+c3kIZzv7L1/u1IURREAAIAS6mh1AQAAAK0iEAEAAKUlEAEAAKUlEAEAAKUlEAEAAKUlEAEAAKUlEAEAAKXV1eoCGuXAAw/Mj3/843R2dub5z39+q8sBAABa5PHHH8/IyEj222+/bNmype61lYVyMGtnZ2dGR0dbXQYAANAmOjo6MjIyUveaBTNDNB6IOjo6cthhh7W0lqIosmnTphx++OGpVCotrWVcrVZLT09Pq8tIopapeGbqU8vu2u2ZaZefy7h2qqddavHM1NdO9bRLLZ6Z6bVTLUn71NNOz8yjjz6a0dHRdHZ27vHaBTND1Nvbm8HBwaxYsSKPPPJIS2t5+umnc/DBB+epp57KQQcd1NJaxvX19WVgYKDVZSRRy1Q8M/WpZXft9sy0y89lXDvV0y61eGbqa6d62qUWz8z02qmWpH3qaadnZm+ygaYKAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEJdHf39/qEiaoZX5op5+NWtpfu/1c2qmedqqlnbTbz6Wd6mmnWtpJO/1c2qmWpP3qmW+03W6Cdmo5yPzgmWFveWbYW54Z9pZnhr3VTs+MttsAAAAzIBABAAClJRABAAClJRABAAClJRABAACl1dXqAhqtVqulr69vyrH+/n5tCQEAYAGoVqupVqtTjtVqtRnfZ8EFop6engwMDLS6DAAAoInqTXaMt92eCUvmmmDRokVZu3ZtFi1a1OpSmCc8M+wtzwx7yzPD3vLMsLfm6zPjYFYAAGBBcTArAADADAhEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaXW1uoCFpiiKDI/u+bqujqRSqTS/IAAAYFoCUYMNjyafvH3zHq9bc8KydHfOQUEAAMC0BKImGxktcs+mrUmSVxy+fzo7zAoBAEC7EIia6B0/tTRdOwWg4dEi6zcMtbAiAABgZwJRE3V1VNLdaUYIAADalS5zAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaXW1uoCpjI6OZuvWrbt9vrOzM/vtt18LKgIAABaitpwhuuWWW7J48eLdPt7+9re3ujQAAGABacsZou985zs55JBDcu211076/NKlS1tUEQAAsBC1bSA66qij8rrXva7VpQAAAAtYQ5bMXXbZZalUKhkZGZn2mkcffTQXXHBBent7s//++2flypVZt25dtm/fvtu1Dz30UF784hcnSd17AgAA7It9DkRFUeTqq6+ue83GjRtz3HHH5fLLL8/g4GD222+/PPDAA1m7dm1OOeWU7NixY9L13/nOd7Jx48Yce+yxec5znpMXvvCFueSSS3a7DgAAYF/sUyAaGRnJunXrcvfdd9e97rzzzstjjz2WU089NRs3bszQ0FBuv/32rFixIl/5ylfyoQ99aNL1Dz30UO65556cf/75uf7663POOefkgx/8YN773vfuS7kAAACTzGoP0fXXX59rrrkmX/rSl/Lwww/Xvfauu+7KzTffnEMPPTTr16+faIxw/PHH5zOf+UxOOumkfOQjH8lFF12Urq6ujI6O5mMf+1he9apX5aUvfWmS5L/8l/+SRYsW5dJLL826dety0EEHzaZsAACASWY1Q3TNNdfkyiuv3GMYSpLrrrsuSXL66afv1iVu9erVWblyZZ544oncdtttYwV1dOSss86aCEPjfvEXfzEjIyMZGBiYTckAAAC7mVUguvTSS3PvvfdOfNRz6623JklOO+20KcfHPz9+3eDgYP7pn/5pt/1CHR1jpZodAgAAGmVWS+ZWrFiRFStWzOjaBx98MEly1FFHTTn+kpe8JMnYvqEk2bp1a37xF38x//AP/5Czzz574rrrrrsuS5cuzdFHHz2bkgEAAHbT9HOInnjiiSTJkiVLphxftmxZkqRWqyUZC05nnHFGfvM3fzPf+MY3snr16nz961/PX/7lX+ajH/1ouru76369oijy9NNPz7reRYsWZdGiRbN+PQAAsG+2bduWbdu2zfr1RVHM+NqmB6JnnnkmSXbbPzRu/PPj1yXJlVdemT/4gz/IZz/72Vx++eV5+ctfnvXr1+eMM87Y49fbtGlTDj744FnXu3bt2lx88cWzfj0AALBvLrvsslxyySVz8rWaHojGTZfSOjs7k0w+gHXx4sX58z//8/z5n//5Xn+dww8/PN/61rdmV2RidggAAFrsfe97Xy688MJZv/5lL3tZNm3aNKNrmx6IDjjggDz11FMZGhrK4sWLdxsfnxk68MADG/L1KpWKxgsAADCP7es2lkqlMuNr9+lg1plYvnx5kuTJJ5+ccvzxxx+fdB0AAMBcaXogGu8K98ADD0w5ft999026DgAAYK40PRCtXr06SXLTTTdNOX7jjTcmSU488cRmlwIAADBJ0wPRW97yliTJtddem82bN08a+9rXvpZvf/vbWb58eU466aRmlwIAADBJ0wPRa17zmpxyyimp1Wo555xz8sgjj6QoimzYsCFnnnlmkuTCCy/c4/lCAAAAjTYnbbc//vGPZ9WqVbnxxhtzxBFHZMmSJRNNFt70pjflve9971yUAQAAMEnTZ4iS5AUveEHuvPPOnH/++TnssMOydevWHHPMMVm3bl1uuOGGdHXN2XFIAAAAExqSRKY7dHVnhx12WC6//PJGfLm6arVa+vr6phzr7+9Pf39/02sAAACaq1qtplqtTjlWq9VmfJ8FNzXT09OTgYGBVpcBAAA0Ub3Jjt7e3gwODs7oPnOyZA4AAKAdCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpdbW6gEar1Wrp6+ubcqy/vz/9/f1zXBEAANBo1Wo11Wp1yrFarTbj+yy4QNTT05OBgYFWlwEAADRRvcmO3t7eDA4Ozug+lswBAAClJRABAAClJRABAAClJRABAAClJRABAAClJRABAAClJRABAAClJRABAAClJRABAAClJRABAAClJRABAACl1dXqAhqtVqulr69vyrH+/v709/fPcUUAAECjVavVVKvVKcdqtdqM77PgAlFPT08GBgZaXQYAANBE9SY7ent7Mzg4OKP7WDIHAACUlkAEAACUlkAEAACUlkAEAACUlkAEAACUlkAEAACUlkAEAACUlkAEAACUlkAEAACUlkAEAACUlkAEAACUlkAEAACUlkAEAACUVlerC2i0Wq2Wvr6+Kcf6+/vT398/xxUBAACNVq1WU61Wpxyr1Wozvs+CC0Q9PT0ZGBhodRkAAEAT1Zvs6O3tzeDg4IzuY8kcAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWgIRAABQWl2tLqDRarVa+vr6phzr7+9Pf3//HFcEAAA0WrVaTbVanXKsVqvN+D4LLhD19PRkYGCg1WUAAABNVG+yo7e3N4ODgzO6jyVzAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaXW1uoBGq9Vq6evrm3Ksv78//f39c1wRAADQaNVqNdVqdcqxWq024/ssuEDU09OTgYGBVpcBAAA0Ub3Jjt7e3gwODs7oPpbMAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApdXV6gLKani0mHasqyOpVCpzWA0AAJSTQNQi6zcMTTu25oRl6e6cw2IAAKCkBKIWGhktcs+mrUmSVxy+fzo7zAoBAMBcEojmUFfH2OzPVIZHi7qzRgAAQOMJRHOoUqlYCgcAAG1ElzkAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0Ftw5RLVaLX19fVOO9ff3p7+/f44rAgAAGq1araZarU45VqvVZnyfBReIenp6MjAw0OoyAACAJqo32dHb25vBwcEZ3ceSOQAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLQEIgAAoLS6Wl1Ao9VqtfT19U051t/fn/7+/jmuCAAAaLRqtZpqtTrlWK1Wm/F9Flwg6unpycDAQKvLAAAAmqjeZEdvb28GBwdndB9L5gAAgNISiAAAgNISiAAAgNISiAAAgNISiAAAgNISiAAAgNISiAAAgNISiAAAgNISiAAAgNISiAAAgNISiAAAgNISiAAAgNLqanUB7G54tKg73tWRVCqVOaoGAAAWLoGoDa3fMJSR0SL3bNqaJHnF4funs+PZALTmhGXp7mxVdQAAsHBYMgcAAJSWGaI20dUxNvMzneHRIus3DM1hRQAAsPAJRG2iUqlYBgcAAHPMkjkAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC0BCIAAKC05kUg+oVf+IWcffbZrS4DAABYYNo+EP3t3/5t/vmf/7nVZbSV4dEiO0am/iiKotXlAQDAvNHV6gLq+d73vpf3vOc9Oeigg1pdSlu56vbNuWfT1iTJKw7fP50dlYmxNScsS3dnqyoDAID5pSEzRJdddlkqlUpGRkamvebRRx/NBRdckN7e3uy///5ZuXJl1q1bl+3bt0/7mvPPPz9vectbctxxxzWiTAAAgEn2eYaoKIpcffXVda/ZuHFjVq1alcceeyxJsmTJkjzwwANZu3Zt/uVf/iU333xzuru7J73mb/7mb/LNb34z9913X84444x9LXPe6+oYm/2ZyvBokfUbhua4IgAAmP/2aYZoZGQk69aty9133133uvPOOy+PPfZYTj311GzcuDFDQ0O5/fbbs2LFinzlK1/Jhz70oUnXf/e7383v/u7v5mMf+1iWLZs6BJRNpVJJd+fUH107LZkDAABmblaB6Prrr8+5556bo446KhdffHHda++6667cfPPNOfTQQ7N+/focccQRSZLjjz8+n/nMZ5IkH/nIRzI8PJxkbMbp13/91/PWt741b33rW2dTHgAAwIzMKhBdc801ufLKK/Pwww/v8drrrrsuSXL66adn6dKlk8ZWr16dlStX5oknnshtt92WZKyr3De/+c188IMfzJYtW7Jly5aMjIxkx44d2bJly0RwAgAA2FezCkSXXnpp7r333omPem699dYkyWmnnTbl+Pjnx6+799578/3vfz8vfOELs3jx4ixevDhf/epXc80112Tx4sX59Kc/PZuSAQAAdjOrpgorVqzIihUrZnTtgw8+mCQ56qijphx/yUtekiR56KGHkiS//du/nV/+5V+edM273/3uLFu2LJdcckmOOeaYul+vKIo8/fTTM6ptKosWLcqiRYtm/XoAAGDfbNu2Ldu2bZv16/fmbM6mn0P0xBNPJBnrLDeV8aYJtVotSXLkkUfmyCOPnHTNwQcfnOc973l53etet8evt2nTphx88MGzrnft2rV73BcFAAA0z2WXXZZLLrlkTr5W0wPRM888kyS77R8aN/758ev21eGHH55vfetbs3692SEAAGit973vfbnwwgtn/fqXvexl2bRp04yubXogGjfdtFVnZ2eS1D3U9ZZbbpnx16lUKjnooIP2rjgAAKBt7Os2lkpl5sfS7NM5RDNxwAEHJEmGhqY+OHR8ZujAAw9sdikAAACTND0QLV++PEny5JNPTjn++OOPT7oOAABgrjQ9EB199NFJkgceeGDK8fvuu2/SdQAAAHOl6YFo9erVSZKbbrppyvEbb7wxSXLiiSc2uxQAAIBJmh6I3vKWtyRJrr322mzevHnS2Ne+9rV8+9vfzvLly3PSSSc1uxQAAIBJmh6IXvOa1+SUU05JrVbLOeeck0ceeSRFUWTDhg0588wzkyQXXnhhuru7m10KAADAJHPSdvvjH/94Vq1alRtvvDFHHHFElixZMtFk4U1velPe+973zkUZAAAAkzR9hihJXvCCF+TOO+/M+eefn8MOOyxbt27NMccck3Xr1uWGG25IV9ecHYcEAAAwoSFJZLpDV3d22GGH5fLLL2/El6urVqulr69vyrH+/v709/c3vQYAAKC5qtVqqtXqlGO1Wm3G91lwUzM9PT0ZGBhodRkAAEAT1Zvs6O3tzeDg4IzuMydL5gAAANqRQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJRWV6sLaLRarZa+vr4px+qdZgsAAMwf1Wo11Wp1yrFarTbj+yy4QNTT05OBgYFWlwEAADRRvcmO3t7eDA4Ozug+Cy4Qld3waDHtWFdHUqlU5rAaAABobwLRAnPV7Ztzz6atSZJXHL5/OjueDUBrTliW7s5WVQYAAO1HUwUAAKC0zBAtAF0dY7M/UxkeLbJ+w9AcVwQAAPODQLQAVCoVS+EAAGAWLJkDAABKSyACAABKSyACAABKSyACAABKSyACAABKSyACAABKSyACAABKa8GdQ1Sr1dLX1zflWH9/f/r7++e4IgAAoNGq1Wqq1eqUY7Vabcb3WXCBqKenJwMDA60uAwAAaKJ6kx29vb0ZHByc0X0smQMAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEqrq9UFNFqtVktfX9+UY/39/env75/jigAAgEarVqupVqtTjtVqtRnfZ8EFop6engwMDLS6DAAAoInqTXb09vZmcHBwRvexZA4AACitBTdDxPSGR4tpx7o6kkqlMofVAABA6wlEJXLV7Ztzz6atSZJXHL5/OjueDUBrTliW7s5WVQYAAK1hyRwAAFBaZogWuK6OsdmfqQyPFlm/YWiOKwIAgPYhEC1wlUrFUjgAAJiGJXMAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpCUQAAEBpdbW6gEar1Wrp6+ubcqy/vz/9/f1zXBEAANBo1Wo11Wp1yrFarTbj+yy4QNTT05OBgYFWlwEAADRRvcmO3t7eDA4Ozug+lswBAAClJRABAAClJRABAAClJRABAAClJRABAAClJRABAAClJRABAAClteDOIWJ2tu4YzSduG0qSnH38snR3VibGujqSSqUy3UsBAGDeEohIklx951Du2bQ1SVLcsTmdHc8GoDUnLEt3Z6sqAwCA5rFkDgAAKC0zRCXW1TE2+zOV4dEi6zcMzXFFAAAwtwSiEqtUKpbCAQBQapbMAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApbXgDmat1Wrp6+ubcqy/vz/9/f1zXBEAANBo1Wo11Wp1yrFarTbj+yy4QNTT05OBgYFWlwEAADRRvcmO3t7eDA4Ozug+lswBAAClJRABAAClteCWzNF4w6PFtGNdHUmlUpnDagAAoHEEIvboqts3555NW5Mkrzh8/3R2PBuA1pywLN2draoMAAD2jSVzAABAaZkhYkpdHWOzP0myY6TIp+7YnCQ5+/hlqVSS9RuGWlkeAAA0hEDElCqVysRSuO7OSs47afnE2I6R6fcUAQDAfGLJHAAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFoCEQAAUFpdrS6A+W3rjtF84rahJMnZxy9Ld2dlYqyrI6lUKtO9FAAAWk4gYp9cfedQ7tm0NUlS3LE5nR3PBqA1JyxLd2erKgMAgD1bcIGoVqulr69vyrH+/v709/fPcUUAAECjVavVVKvVKcdqtdqM77PgAlFPT08GBgZaXcaC1tUxNvuTJDtGinzqjs1JxpbMVSrJ+g1DrSwPAIASqDfZ0dvbm8HBwRndZ8EFIpqvUqlMLIXr7qzkvJOWT4ztGClaVBUAAOw9XeYAAIDSEogAAIDSEogAAIDSEogAAIDSEogAAIDSEogAAIDSEogAAIDSEogAAIDSEogAAIDSEogAAIDS6mp1ASxcW3eM5hO3DSVJzj5+Wbo7KxNjXR1JpVKZ7qUAADAnBCKa5uo7h3LPpq1JkuKOzenseDYArTlhWbo7W1UZAACMsWQOAAAoLTNENFRXx9jsT5LsGCnyqTs2JxlbMlepJOs3DLWyPAAAmEQgoqEqlcrEUrjuzkrOO2n5xNiOkaJFVQEAwNQsmQMAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEpLIAIAAEqrq9UFUE5bd4zmE7cNJUnOPn5ZujsrE2NdHUmlUpnupQAA0DACES1x9Z1DuWfT1iRJccfmdHY8G4DWnLAs3Z2tqgwAgDKxZA4AACgtM0TMma6OsdmfJNkxUuRTd2xOMrZkrlJJ1m8YamV5AACUkEDEnKlUKhNL4bo7KznvpOUTYztGihZVBQBAmVkyBwAAlJZABAAAlJZABAAAlJZABAAAlJZABAAAlJZABAAAlJZABAAAlJZABAAAlNaCO5i1Vqulr69vyrH+/v709/fPcUXsra07RvOJ24aSJGcfvyzdnZWJsa6OsQNeAQAot2q1mmq1OuVYrVab8X0WXCDq6enJwMBAq8tgH1x951Du2bQ1SVLcsTmdHc8GoDUnLEt3Z6sqAwCgXdSb7Ojt7c3g4OCM7mPJHAAAUFoLboaI+amrY2z2J0l2jBT51B2bk4wtmatUkvUbhlpZHgAAC5RARFuoVCqTlsJ1/GSZ3M77hwAAoNEEItpOd2cl5554yMS/7xgpWlgNAAALmT1EAABAaQlEAABAaQlEAABAadlDxLzi0FYAABpJIGJecWgrAACNZMkcAABQWmaIaHsObQUAoFkEItqeQ1sBAGgWgYh5xaGtAAA0kj1EAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaQlEAABAaXW1ugBopB0jRT55++YkyZoTlqW7s9LiigAAaGcCEQvG8GiRokhGR4skY+FoXFdHUqkIRwAATCYQsWCs3zCUkdEi92zamiQp7ticzo6xEDQ2W9TK6gAAaEf2EAEAAKVlhoh5ratjbPZn3I6RIp+6Y2wP0duPW5pr7n6yRZUBADAfCETMa5VKZdJSuO7OSs47aXmSyXuIAABgKpbMAQAApSUQAQAApSUQAQAApWUPEaUwPFpMarhw9vHPHtrqjCIAgPISiCgFZxQBADAVS+YAAIDSMkPEguWMIgAA9kQgYsHa9YyiJOnoGN83ZM8QAAACESXS3VnJuSceksShrQAAjLGHCAAAKC2BCAAAKC2BCAAAKC17iCBje4o+eftYB7qxc4k0XQAAKAOBiNIbHi1SFMno6FijhZ0bLnR1jHWrAwBgYRKIKL31G4YyMlrknk1bkyTFHZvT+ZO23GOzRa2sDgCAZrKHCAAAKC0zRJRSV8fY7M+4HSNFPnXH2B6itx+3NNfc/WSLKgMAYC4JRJRSpVLZbSlcx0+WyXV12DMEAFAWAhEk6e6s5NwTD0kyuakCAAALmz1EAABAaQlEAABAaQlEAABAaQlEAABAaWmqAHuwY6TIJ28fa8k9dlCrLnQAAAuFQAR1DI8WKYpkdHSs89zOHei6OsbadwMAMH8JRFDH+g1DGRktcs+mrUmS4o7N6fzJOUVjs0WtrA4AgH1lDxEAAFBabRmInn766fzWb/1WjjzyyCxevDjHHXdc1q9f3+qyKImujrHZn/GPc45fllcevn9eefj+OfO4pa0uDwCABmrLJXPnnXdebrnllrz//e/PEUcckeuvvz5nn312nvvc5+YXfuEXWl0eC1ylUtltKVzHT5bJdXXYMwQAsJC0XSD6wQ9+kM9+9rO54oor8uu//utJkjPOOCP33ntvPvnJTwpEzLnuzkrOPfGQJJObKgAAMP81ZMncZZddlkqlkpGRkWmvefTRR3PBBRekt7c3+++/f1auXJl169Zl+/btk67bvHlzTj755Lz+9a+f9PkXvOAF2bp1ayPKhYbZMVLk/3z9B/k/X/+BsAQAMA/t8wxRURS5+uqr616zcePGrFq1Ko899liSZMmSJXnggQeydu3a/Mu//EtuvvnmdHd3J0mOPvrofPGLX0ySbNu2Ld///vfz1a9+NTfddFP++q//el/LhYbRkhsAYP7bpxmikZGRrFu3LnfffXfd684777w89thjOfXUU7Nx48YMDQ3l9ttvz4oVK/KVr3wlH/rQh6Z83V/91V+lt7c3Z599ds4444ysWbNmX8qFhlq/YSj/cMfmfHPT1nxz09b8wx2b88nbxz6GR1tdHQAAMzGrQHT99dfn3HPPzVFHHZWLL7647rV33XVXbr755hx66KFZv359jjjiiCTJ8ccfn8985jNJko985CMZHh7e7bXveMc78sUvfjEf/OAH89nPfjbvfe97Z1MutITldAAA7W9WS+auueaaXHnllTO69rrrrkuSnH766Vm6dHLL4tWrV2flypW5//77c9ttt+V1r3vdpPEVK1ZkxYoVOfnkk1OpVPLnf/7n+ZM/+RNLkWiZ8Zbc43aMFPnUHZuTJG8/bmmuufvJJJbTAQDMF7OaIbr00ktz7733TnzUc+uttyZJTjvttCnHxz8/ft1VV12VV73qVbs1aDj66KPzxBNP5Mknn5xNydAQYy25J390dIx97NyS23I6AID5YVYzROMzNzPx4IMPJkmOOuqoKcdf8pKXJEkeeuihJElvb2+++c1v5vbbb8+JJ544cd2XvvSlHHbYYbvNMkErackNADC/Nf0coieeeCLJWGe5qSxbNrb8qFarJUle//rX57WvfW3OOuusXHTRRenp6cktt9ySj370o/noRz+6x69XFEWefvrpWde7aNGiLFq0aNavp7xmupwOAID6tm3blm3bts369UUx819UNz0QPfPMM0ky7czO+OfHr+vo6MgXvvCFXHTRRbnsssvy5JNP5qUvfWmuuuqqvOMd79jj19u0aVMOPvjgWde7du3aPTaKgKmMLaeb/LmOnyyj23k5HQAA9V122WW55JJL5uRrNT0QjZsupXV2jr2D3HnPUE9Pz4ybNuzq8MMPz7e+9a1ZvTaJ2SEaxnI6AIDZed/73pcLL7xw1q9/2ctelk2bNs3o2qYHogMOOCBPPfVUhoaGsnjx4t3Gx2eGDjzwwIZ8vUqlkoMOOqgh9wIAAObevm5j2ZuOvvt0MOtMLF++PEmm7Q73+OOPT7oOAABgrjQ9EB199NFJkgceeGDK8fvuu2/SdQAAAHOl6YFo9erVSZKbbrppyvEbb7wxSSa12AYAAJgLTQ9Eb3nLW5Ik1157bTZv3jxp7Gtf+1q+/e1vZ/ny5TnppJOaXQoAAMAkTQ9Er3nNa3LKKaekVqvlnHPOySOPPJKiKLJhw4aceeaZSZILL7ww3d3dzS4F2saOkSL/5+s/yP/5+g90oAMAaKE5abv98Y9/PKtWrcqNN96YI444IkuWLJlosvCmN70p733ve+eiDGgLw6NFiiIZHR0LQjsHoq6OveuKAgDAvpmTQPSCF7wgd955Z/7wD/8w//RP/5TNmzfnmGOOyZo1a3LRRRelq2vOjkOCllu/YSgjo0Xu2bQ1SVLcsTmdPzm4dc0Jy3Y73BUAgOZpSBKZ7tDVnR122GG5/PLLG/Hl6qrVaunr65tyrL+/P/39/U2vAQAAaK5qtZpqtTrlWK1Wm/F9FtzUTE9PTwYGBlpdBkzS1TE2+zNux0iRT90x1mTk7cctzTV3P9miygAA5qd6kx29vb0ZHByc0X0WXCCCdlSpVHZbCtfxk2VyXR2T9wztGCnyydvHwtLYEjp7igAAmqXpXeYAAADalRkiaIHuzkrOPfGQJJO7zOlABwAwtwQiaCM60AEAzC1L5gAAgNIyQwQtpgMdAEDrCETQYnvTgQ4AgMYSiKDNTNdwAQCAxrOHCAAAKC0zRDCPOLQVAKCxFlwgqtVq6evrm3Ksv78//f39c1wRNIYzigAAnlWtVlOtVqccq9VqM77PggtEPT09GRgYaHUZ0HDOKAIAeFa9yY7e3t4MDg7O6D72EAEAAKW14GaIYCFxRhEAQHMJRNDGnFEEANBcAhEsEDrQAQDsPYEI5pHZHtoqLAEATE0gggWgXkvu8X/XrhsAYHcCESwA9VpyJ9GuGwBgGtpuAwAApWWGCOapei25zz5+8j4h7boBAKYmEME8Va8ld3dnZbfGCdp1AwDszpI5AACgtBbcDFGtVktfX9+UY/39/env75/jimBu7NySu97Y3rTrBgBoV9VqNdVqdcqxWq024/ssuEDU09OTgYGBVpcBAAA0Ub3Jjt7e3gwODs7oPpbMAQAApbXgZoiAvbNjpMgnbx/rQDd2LpGmCwBAeZghAgAASssMEZTY8GiRokhGR8caLezacKEoilx1x1ASs0cAwMIkEEGJrd8wlJHRIvds2pokKe7YnM6dzik667ilrSoNAGBOCETAtIZHi2lnj7o6xg6HBQCYzwQiKJmujrHlb+N2jBT51B1jTRXOPn5ZKpWxmaMkufrOoWlnj8aW0M1h4QAATSAQQclUKpXdgkzHT4KOPUIAQNkIRMAkO88g1Zs9AgBYCAQiKLnuzkrOPfGQXT737D9PN3vk/CIAYCFwDhEAAFBaZoiAae06e7Rzpzkd6ACAhUAgAmZFBzoAYCFYcIGoVqulr69vyrH+/v709/fPcUUAAECjVavVVKvVKcdqtdqM77PgAlFPT08GBgZaXQYsSDrQAQDtot5kR29vbwYHB2d0nwUXiIDm2fUMI+cXAQDznS5zAABAaQlEAABAaVkyB8xKvZbcDm0FAOYLM0QAAEBpmSECGs6hrQDAfCEQAQ3n0FYAYL4QiIA5ZX8RANBOBCKgIRzaCgDMRwIR0BAzPbTV/iIAoJ0IRMCcsr8IAGgnAhHQcPXOKKrH/iIAYK4JREDT2V8EALQrgQhoOvuLAIB2JRABbaPe/qKzjluaT99pOR0A0FgCETCnZru/CACgGRZcIKrVaunr65tyrL+/P/39/XNcEVCP/UUAwGxUq9VUq9Upx2q12ozvs+ACUU9PTwYGBlpdBjBD9hcBALNRb7Kjt7c3g4ODM7rPggtEwMLk/CIAoBk6Wl0AAABAq5ghAtrGrg0XiqKwvwgAaCqBCGhbM91fBAAwWwIRsKDtGCnyydudXwQATE0gAuYF5xcBAM2gqQIAAFBaZoiAeW/XZXFdHcnw6LNjzi8CAKYjEAELzvBoJgLSyGjh/CIAYFqWzAEAAKVlhgiY94ZHJy+L23kV3JnHLc34vzq/CADYlUAEzHtX3zk07bK4ro6K84sAgGlZMgcAAJSWGSJgXurqGGuKkIwtk/vUHWNNFM4+fvLhq0Ux/XlFDm0FAAQiYF6qVCqTOsTtvCxucrBxoCsAMD2BCGAWzC4BwMIgEAHzXnfn5Fmgmdq1O93OHNoKAOUgEAGlVa87nUNbAaAcBCKAKey6JK6rIxkenTw+1eySmSUAmF8EIqBU6nWnq3do6/BoJgJSkoyMFlPOLplZAoD5ZcEFolqtlr6+vinH+vv709/fP8cVAe2kXnc6AGD+qFarqVarU47VarUZ32fBBaKenp4MDAy0ugxgntu14cLOq+De8VNLUxSZmF16+3FLc83dT06M60AHAM1Xb7Kjt7c3g4ODM7rPggtEAI1Qr+FC10/+eXx2qatD4AGA+UogAkpr13bdDm0FgPIRiAB+ol7DhZ2Xve3acW5nw6NFiiI60AHAPCEQAfxEvYYLu+4D6u7MxOzSzqFn/YYhHegAYB7paHUBAAAArWKGCGAf7bzULpm83G7XDnQAQHsRiACmsGvDhXp2XWqX6EAHAPOFQASwgDkTCQDqE4gA5pCAAgDtRVMFAACgtMwQATTYzvuP5vqw16IoJp2RtGOkcCYSANQhEAEsIMOjmViSl8SZSACwBwIRwBwZHi1SFJnVjI29RwDQHAIRwBxZv2GoKTM2O4els45bOvH5d/zU0hRFZnUmkgAGQFkIRABtotEhZPwMJGciAcD0BCKAJurqGAs343aMFLOasQEAmkMgAmiiSqWy21K4mczYFEWRHSPP/nu9bnEAwOwJRABtaHg0+fSde+4WN75PaDwsDY/Ors33XLTrti8JgHYkEAG0gV070M002OzWqCGZCEvJzM9E0q4bgLISiADm0HQBpV6w2ZducQBAfQIRQJvbtVvcfl2VaRs1nH38s0vRZru/aE8BzNI3ABYSgQigRep1oNs12Oy8v2esUcPkEDIelro7dx/b+7oa0657pvuSxr7G7PcmAdAcZfkFmEAE0CL1OtDtHmxm1yyhlWa6LymxNwmA1hGIAOaBnfcelU1ZfkMJQGsIRADzXDPC0nTNH3bthjfTlty77ks6+/hlqVTGmkkAQCsJRABtYj7MAu3WDW+GLbl33ZdklgegPe28/7Msez8FIgAAIMnk/Z9l2fu54AJRrVZLX1/flGP9/f3p7++f44oA5rd63fCadSZSs39DaV8SwPxXrVZTrVanHKvVajO+z4ILRD09PRkYGGh1GQALRr1ueHvTknvXJYG7hpudlfE3lADt5szjlmb8b9x23PtZb7Kjt7c3g4ODM7rPggtEAMw/u87YANB6XR2VUuz9FIgAaGvt/htKAOY3gQiAttao31CWsXMSQLMspL2YAhEAe2W6M4ranX1JAExFIAIAAHazN81w5jOBCICWGx6dvISt2SvW7EsCYJxABEDLXX3n0LRL2JrxG8qydE4CaJZdf5G1s/m2F1MgAiipnTfEnnXc0hZXA8B8Uu8XWfNtL6ZABEDD7E3I6up49syhHSNFPnXH2OvOPn5yt6KujiYVCwARiABokUqlMuk3iDsvYZvpMraF1PYVoN3V+0XWfN6LKRABUDpl6ZwE0Ej1fpE1nwlEACU0PFqkKDKxIXZ4VCAYZ9YJoFwEIoASWr9haPLhpMmkDbGzIWQBMB8JRAA0xL6ErF2XsM1UvbavRVHkqjvG1rOPr3kHgF0JRAAlsfNm2GT6zm7zqatbvbavWokD7Nmuy4TLSCACKIldN8Mms+vstrOFGLIA2LOF1JxGIAJg1poRsvak2W1fF9Lp6wDsmUAEwLzS7LavC+n0daA8dMicPYEIgAVp15medp3YadabGG+OAGZGIAIoqdl2dpsv6s307Gqhnr4OMJWiKDI8OvbPO0bmxy+PmkkgAqD0Furp641m1gnmp6k6yY3/+6TjEvbwy6OFSiACoGFaPetUb6Zn5zfvzeh6JywAzE8CEQDz1lQBbLqZHgEFWGjqLX3b9brpnHnc0oz/7TgXvzxqRwIRAMxQo2aBZvomZm/afO98z3r31TocFo7h0Zktfat3UHVXR6X0vzwSiABgjs30TczetPne+Z717rs39xSyYO41Y/ntfOm62SoCEQDsZCGdvj4bO78ZO+u4pfn0nc922GtEyAKaY9elbzt3yNybrptlJBABwCw1YulbvTcxs/WOn1qaoshEU4m3H7c019z95D7dk3LTNKT97br0jZkTiABYkOai410jlr41401M10/uN37frgb8NljIgtabqn32dPam6+ZCPpNuJgQiAJihMq/Db0bIAsY0olvcruqdr2YGaTKBCABmqN46/GYsfduTXff7tKt6y60sxYLGdItj9gQiAGiA+bp+v14nueHRcjWUAMpJIAKAOma6Dr/eUpZ66/731NVutjMoO9+3Xqe8uu26E92oSkrL9daZ7WzzXOybXKgEIgCoY6br8HeMzHVl0DzNONeKmak321zmfYzNJBABAEl27yS38yzY+EzATGadgOaY7XlCZo/qE4gAYJ4oimLSTFSj9/vs2klutt2o9ma51WxpxjB3tFxnoROIAKAB9rQXqBGGR5NP39n++31mutxq/I12u+xNEbKmNp9ari/EP8O9OU+I2RGIAGCGLDtprPUbhubN3pSF+Eab1tmbA1adJ9R8AhEANNlsN0IPjxaTZlB2Xha3p/0+7cJyK9pFM0JtM+7pFy9zb8EFolqtlr6+vinH+vv709/fP8cVAVB2s90IvdsMSp5dFteo/T6NMt0hsbvWuV9XZdJvxHdeAiQssZCYVWy+arWaarU65VitVpvxfRZcIOrp6cnAwECrywCAeanZv50eW/4z+Y3hfNibArSfepMdvb29GRwcnNF9FlwgAoB2MNuN0Du/rt5ruzoyqZPb3phuNqedNeug0J3vO909x68rs5ke9LurZs+S6GhIIwhEANAEs90Ivevr6r+2PG/Sm3VQ6M73ne6eydwHR2/CZ2ZvnotWqxe+Z5rh7S9qDoEIACiNvenutbNdG2PsrBUtwmmNemeB7WlWcabhm7knEAHAPNUuvy2u1w0vaXydzepcd+ZxSzP+tvTs45elUhlrbJHUb4zRbi3Cy6rec7HrMzrbZZZ1zwLb5bk449VLZjULxNwTiACAfVKvG96ezGZvSrMOCu3qqExansj8Uu+5aMWZV/VC9K7h2wGrrSUQAQBMoV5jjJ1nj+z3KaepzgLb+bmoZ9fw7ZlpLYEIAJpsX5aMNXq52Z6Wt83U3nTDm2uNCij1GmMwtWZ1A5yNes9ovWWW9b6Hnf97meossJl2lyx718J2IxABQInsy/K2ne1dNzxmYqYtwGcbJvYUFBvRjr1Z3QBnY7ZnXtX9HlL/v5eZdpfcuTEDrScQAQALWiPaHc+FmXYh08QBGksgAoAFrp2Xt83W3jRj0O64NZrVDXAuTbVPqBGHI9NeBCIAWOAsb5uZdmljntRvAT5fNKsbYL2lfY3+M5xqn1BZD0deyAQiAKA0ZtvueNc32jvPSjXj0FYtwOdWI/ZP7Y12Ct8IRADAAjLVAZw755FmtDue7aGtuzY5mK/mOkzM1mzOvNqbezJ/CUQAwD5rxhvD2bzRrncAZxnNtHPdruOzbcfOs4Sl+UMgAgDYSzM9tLXV9qahRCPascN8JBABAPPaTLvoJY07ENOhrbBwCEQAUDILbSnP3nTRm+sDMfd0GOpcqtdQImlOO/Z6+3bq/Wx2Hvtvxy+d1Jhirpf2LbT/XtidQAQAlMZ8eXNbr6vdbM2koUQ7tmMfHk0+fefmiX9v9NK+XRtx2D9VPgIRAMAcqdfkYKbduWc6szIXnesWQpjYrRFH7J8qG4EIAGg7C+GN9lT2psnBfDDXYeIdP7U0RZGGL+2j3AQiAKDt+K09U+n6yTOwr0v7ZtqIQ8gqB4EIAGAfzHa/T70mB+38RnwhhIm9acTBwicQAQBtoVFvtOdL44SZNDloR8IEC41ABAC0hYX4Rnt4dHaNE5phT0FxLoPkrnvEpmrJvdD2j9G+BCIAgCa5+s6hed84oRl22yO2y8/G/jHmkkAEADCP7DrrtLPZtvKGMhOIAAAaaOe9UNPtg0rGziSajXqzTu3cynume8TqjXV1jC2tnA97xJg/BCIAgAbadS/UdPugdozMdWWttbd7xOZy/9h8acRBcwhEAAAtsDdvwmc661RvrFltsIUJ5juBCABoS95oP2ums057GgN218ZHZgEAADSXQAQAAJSWQAQAAJSWPUQAAE1iH9T06v1s/NyYS2aIAACA0jJDBAAwj5hZgcYyQwQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQAQAAJSWQNQEO7Zvy+c//sfZtm1bq0thnti2bVsuvvhizwwz5plhb3lm2FueGfbWfH0PLBA1wfCO7fnCFX8y7x4GWmfbtm255JJLPDPMmGeGveWZYW95Zthb8/U9cFsGoqIoUq1Wc+yxx2bx4sU59thj8+EPfzgjIyOtLg0AAFhA2jIQ/dVf/VV++7d/O6eeemquvPLK/OIv/mJ+//d/PxdddFGrSwMAABaQrlYXMJU//dM/zTvf+c782Z/9WZLkl3/5l7Ns2bL8/u//ftauXZvnPve5La4QAABYCBoyQ3TZZZelUqnUXdL26KOP5oILLkhvb2/233//rFy5MuvWrcv27dsnXbd169Z873vfy8/93M9N+vzP/MzPZGRkJPfff38jSgYAANj3GaKiKHL11VfXvWbjxo1ZtWpVHnvssSTJkiVL8sADD2Tt2rX5l3/5l9x8883p7u5OknR2dubWW2/NypUrJ93jq1/9aiqVSg4//PB9LRkAACDJPs4QjYyMZN26dbn77rvrXnfeeeflsccey6mnnpqNGzdmaGgot99+e1asWJGvfOUr+dCHPjRx7XOe85ysWrUqS5Ysmfjc5z73ufzhH/5h3va2twlEAABAw8wqEF1//fU599xzc9RRR+Xiiy+ue+1dd92Vm2++OYceemjWr1+fI444Ikly/PHH5zOf+UyS5CMf+UiGh4d3e+1jjz2WX/3VX83b3va2/NRP/VSuuOKK2ZQLAAAwpVkFomuuuSZXXnllHn744T1ee9111yVJTj/99CxdunTS2OrVq7Ny5co88cQTue2223b7Gscee2w+//nP50/+5E/y5S9/OQcffPBsyiVJtVptdQkT1DI/tNPPRi3tr91+Lu1UTzvV0k7a7efSTvW0Uy3tpJ1+Lu1US9J+9cw3swpEl156ae69996Jj3puvfXWJMlpp5025fj458evS5Krr746b3/727Nq1ao88MAD+d3f/d10dnbOplR+op3+Q1HL/NBOPxu1tL92+7m0Uz3tVEs7abefSzvV0061tJN2+rm0Uy1J+9Uz38yqqcKKFSuyYsWKGV374IMPJkmOOuqoKcdf8pKXJEkeeuihJMmOHTvy7ne/O6eddlquu+66dHS05VFJAADAAtD0c4ieeOKJJJnUJGFny5YtS5LUarUkyde//vU8/vjjWbVqVb74xS/udv1rX/va3Zbe7awoijz99NOzrnfRokVZtGjRrF8PAADsm23btmXbtm2zfn1RFDO+tumB6JlnnkmSaUPM+OfHrxtvzX3JJZdMef2XvvSlvOENb5j2623atGmf9hqtXbt2j40iAACA5rnsssumzQON1vRANG66lDa+N2j8UNe3v/3te5XodnX44YfnW9/61qxfb3YIAABa633ve18uvPDCWb/+ZS97WTZt2jSja5seiA444IA89dRTGRoayuLFi3cbH58ZOvDAAxvy9SqVSg466KCG3AsAAJh7+7qNpVKpzPjapgei5cuX56mnnsqTTz45cQbRzh5//PGJ6/bF+H0effTR9Pb27tO99tWWbWOzXa94eV86Omb+h9FMtVqt5T+XcWrZ3fis6Mte9rK9+g+4mdrlZ5OoZSrt9sy0y89lXDvV0y61eGbqa6d62qUWz8z02qmWpH3qaaf3wI8++miSZzNCXUUDJCmSFMPDw7uNvfnNby6SFJ/97GenfO1v/dZvFUmKP/qjP9qnGjo6Oibq8OHDhw8fPnz48OHDh4+Ojo495oimzxCtXr06N9xwQ2666aacccYZu43feOONSZITTzxxn77Ofvvtlx//+Mfp7OzM85///H26FwAAMH89/vjjGRkZyX777bfHays/meHZJ+PTqMPDw7sdoHrXXXfluOOOS09PTwYGBibabCfJ1772tbzuda/L8uXLs2nTpnR3d+9rKQAAADPW9FNPX/Oa1+SUU05JrVbLOeeck0ceeSRFUWTDhg0588wzkyQXXnihMAQAAMy5ps8QJcnGjRuzatWqiTOGlixZkieffDJJ8qY3vSk33XRTurrmrAM4AABAkjmYIUqSF7zgBbnzzjtz/vnn57DDDsvWrVtzzDHHZN26dbnhhhuEIQAAoCUaMkMEAAAwH83JDBEAAEA7EogAAIDSEohm4NFHH80FF1yQ3t7e7L///lm5cmXWrVuX7du37/W9tm/fng984AN56Utfmv333z8rVqzIb/zGb2TTpk1NqJxWaeQzs2XLlvze7/1eVq9enSVLluTII4/ML/3SL+XLX/5yEyqnVRr5zOxqy5YtOfLII3PEEUc0oFLaRaOfmX/913/Nz//8z+d5z3teDjnkkJx88sn+nllgGvnMbNu2LZdccklOPPHEHHTQQXn5y1+e888/P48++mgTKqcdXHbZZalUKhkZGdnr17b9+989Ht1act/97neLQw89dOK02yVLlkz888/8zM8U27dvn/G9tm/fXrzhDW+Y8l6HHnpo8d3vfreJ3wlzpZHPzMMPP1y8+MUvnnj98uXLi+7u7iJJUalUive///1N/E6YK418Zqbynve8p0hS9Pb2NqhiWq3Rz8xHPvKRolKpFEmK/fffv1i8ePHE3zMf//jHm/RdMJca+cw8+eSTxctf/vKJ1z//+c8vOjs7iyTF0qVLi9tuu62J3wmtMDo6Wrz61a8ukhTDw8N79dr58P5XINqDk08+uUhSnHrqqcXGjRuLoiiK22+/vVixYkWRpLj00ktnfK9LL7104k3Jhg0biqIY+wvqlFNOKZIUJ598clO+B+ZWI5+ZNWvWFEmK1atXFw899FBRFEWxbdu24vLLLy8OPPDAIknxxS9+sSnfB3Onkc/Mrr7xjW9MvFERiBaORj4zt956a9HZ2Vl0d3cXf//3f18888wzxcjISPHRj360qFQqxeLFi4vvfe97zfpWmCONfGYuuOCCIknx0z/908XDDz9cFEVR/OhHPyre9a53FUmKY489dp9/kUP7GB4eLi6++OKJELO3gWg+vP8ViOq48847J9Lr5s2bJ4197WtfK5IUz3ve84odO3bs8V7bt28vli9fXiQpbr311kljmzdvnvitzd13393Q74G51chn5rvf/W7R0dFRdHd3F4888shu43/9139dJCle97rXNax+5l4jn5ldbd++vXjlK1858T8xgWhhaPQzc9pppxVJio997GO7jb3zne8skhQf/vCHG1I7rdHo9zPd3d3Fc57znN3+3zQyMlIce+yxRZLiy1/+ckO/B+beddddV/zar/1a8aIXvWji/yN7G4jmy/tfe4jquO6665Ikp59+epYuXTppbPXq1Vm5cmWeeOKJ3HbbbXu816233prvf//7eelLX5oTTzxx0tjSpUvz1re+NUly/fXXN6h6WqGRz8x//Md/ZHR0ND/7sz+bFStW7Db+q7/6q+no6Mjdd9+dQvf8eauRz8yuPvShD+Wb3/xmzj333IbUSnto5DPz+OOP56abbsqSJUvy67/+67uNX3DBBXnjG9+YzZs3N6Z4WqLR/2/asWNHVq5cudv/mzo6OvLGN74xSfLNb36zMcXTMtdcc02uvPLKPPzww7O+x3x5/ysQ1XHrrbcmSU477bQpx8c/P37dXN2L9tXIP+fxv4Be9KIXTTl+4IEH5qCDDsqWLVvy/e9/f++LpS006++G+++/Px/4wAfS19eX3/u939u3ImkrjXxmbr755hRFkbe85S3p7u7ebfykk07KLbfckksvvXQfKqbVGvnMbNmyJUmm3Vg/PDycJHnmmWf2uk7ay6WXXpp777134mM25sv7366WfvU29+CDDyZJjjrqqCnHX/KSlyRJHnrooTm9F+2rkX/OJ598cm644YYceeSR036tJ598Mvvtt1+WL18+y4pptWb83VAURX7jN34j27dvz+WXX55Fixbte6G0jUY+MwMDA0mSV77ylQ2qjnbUyGfmZS97WRYtWpT7778/999/f1auXDkxtm3bttx0001Jkle96lX7WjYttmLFiilXqOyN+fL+1wxRHU888USSZMmSJVOOL1u2LElSq9Xm9F60r0b+Ob/4xS/OaaedlmOOOWa3saIoctFFFyUZ++1KpVKZZcW0WjP+bvjf//t/56tf/Wre9a535aSTTtrnGmkvjXxmvvOd7yRJnve85+Ub3/hGzjnnnBx55JF5/vOfn9NOOy2f/vSnG1M0LdXIZ+bggw/O//yf/zMjIyM5/fTTc8stt+RHP/pR7rvvvpxxxhn5zne+k5/+6Z/OySef3LD6mb/my/tfM0R1jE/37rredtz452cyLdzIe9G+5uLPecuWLfnv//2/5x//8R/T1dWV973vfbO+F63X6GdmcHAwF110UVasWJHLLrusMUXSVhr5zDz99NNJMhGgt27dmmXLlmXr1q256aabctNNN+Wf//mf83d/93cNqp5WaPTfMx/4wAfyox/9KH/xF3+Rn/3Zn5009oY3vCGf+9zn0tnZuQ8Vs1DMl/e/ZohmYLoN6+P/sc/kgKrxezTiXrS/Zv05X3vttenr68tVV12VJPnIRz6SVatWza5I2kqjnpn+/v48/fTTqVarOeiggxpWH+2nEc/Mj3/84yTJFVdckde97nW5//7784Mf/CA//OEP84//+I9ZtmxZPvGJT5gpWiAa9ffMHXfckRtuuCFJUqlUcuihh07sQft//+//TTRxgPny/lcgquOAAw5IkgwNDU05Pp5mDzzwwD3ea/yaRtyL9tXIZ2ZnTz75ZM4888ycfvrp2bhxY5YuXZrPfe5z+R//43/sW8G0XCOfmc9+9rO59tprc8YZZ0x07mHhaeQzM/7b2Re/+MW59tprJ5bodnZ25pd+6Zfy4Q9/OMlYx0Lmr0Y+Mw888EBOPfXUPPjgg1m3bl2efvrpPProo3nmmWeyfv36dHZ25p3vfGfWr1/fuG+AeWu+vP8ViOoY36j+5JNPTjn++OOPT7puru5F+2rGn/Ptt9+eV7/61fnMZz6TJPmVX/mVDAwM5PTTT9+nWmkPjXpmtm/fnne/+905+OCD85d/+ZcNrZH20si/Zw499NAkyVlnnTXxpnlnZ555ZiqVSgYGBlr+G1xmr5HPzB//8R/nqaeeyu/8zu/kD/7gD7J48eIkSVdXV84666xcccUVSZL3v//9Daic+W6+vP8ViOo4+uijk4z9NmQq991336Tr5upetK9G/zk/+OCDefOb35zvfve7edGLXpSvfOUr+cQnPjHxJob5r1HPzNatW/PYY4/lqaeeyuGHH55KpTLxMd66/ZFHHpn43LXXXtu4b4I51ci/Z3p6epJk2k5SBxxwQJYsWZIf//jH076hof018pm54447kiRve9vbphz/hV/4hSxatCgPPfSQZ4Z58/5XIKpj9erVSTLRQnJXN954Y5LsdtBUs+9F+2rkn3NRFHnb296WzZs35/Wvf33uvvvuvP71r29csbSFRj0zHR0dOeqoo6b8eOELX5hkbBnU+OdavTyB2Wvk3zPjLZOne7Py1FNPZWhoKMuXL88hhxwym3JpA418Zsb3Ju6pu2lXV1f222+/vSmTBWjevP8tmNadd95ZJCl6enqKH/zgB5PG/v3f/71IUixfvrzYvn37Hu+1ffv2Yvny5UWS4t///d8njf3gBz8oDj300CJJcddddzXyW2CONfKZueWWW4okxeGHH1489dRTzSqZFmvkMzOdhx9+uEhS9Pb27mu5tIFGPjNDQ0PFc57znGL58uW73asoiuJP//RPiyTFm9/85obVz9xr5DPz7ne/u0hSvOc975ly/POf/3yRpHjVq17ViNJpI0mKJMXw8PCMXzNf3v8KRHtwyimnFEmK0047rfje975XjI6OFnfccUexYsWKIknxv/7X/5p0/eDgYPHSl760eOlLX1p84xvfmDT2wQ9+cOJNyYYNG4qiGHujcvLJJxdJilNPPXXOvi+ap1HPzG/+5m8WSYpLLrlkrr8F5lgj/56ZikC08DTymenv7y+SFK997WuL++67ryiKoti2bVvxN3/zN8X+++9fdHZ2Tvw/i/mrUc/MwMBAsf/++xcdHR3FpZdeWvzoRz8qiqIoduzYUXzqU58qDjnkkCJJ8Xd/93dz+v3RfPUC0Xx//ysQ7cF3v/vdifSapFiyZMnEP7/pTW8qduzYMen68TceSYovfelLk8a2b99evOENb5gYX7p06cQ/H3bYYcXGjRvn8lujSRr1zPzcz/3cxG/0jjrqqLofe/PbGtpPI/+emYpAtPA08pl5+umni1e96lUT44ccckjxnOc8p0hSdHV1FX/2Z382l98aTdLIZ+bv/u7vikWLFhVJikqlUhx22GFFd3f3xPXvete75vJbY47UC0Tz/f2vPUR78IIXvCB33nlnzj///Bx22GHZunVrjjnmmKxbty433HBDurpmfrZtd3d3brrpplxyySU5+uij88wzz+Swww7Lb/zGb+TOO+/MEUcc0cTvhLnSqGfm4YcfTjJ2evODDz5Y94P5rZF/z1AOjXxmnvvc5+ZrX/ta3v/+9+eYY47Jli1bcsQRR+Ttb397br311rznPe9p4nfCXGnkM/Orv/qrGRgYyLnnnptXvOIVeeqpp/LCF74wb33rW3PLLbfkr//6r5v4nTDfzIf3v5WimOakJAAAgAXODBEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAAFBaAhEAADBrv/u7v5tKpZLu7u4MDQ1Ne92NN96YSqWSSqWSa6+9duLzTzzxRC6++OK89rWvzYoVK7LffvvlhS98YX76p386f/mXf5kf/vCHU97vTW9608S9RkdH86d/+qd5yUteks7Oznz5y1+ecf1dM/9WAQAAJjvrrLPy4Q9/OMPDw7n++uvzK7/yK1Ned/XVVydJli9fnp//+Z9PkvzHf/xHVq1alaeffnrStRs3bszGjRvz7//+7/mrv/qr3HrrrVm2bNmU9x0dHc0555yTT3/607Oq3wwRAAAwayeccEJe/OIXJ0n+8R//ccprduzYkc997nNJknPOOSfd3d1JkjPPPDNPP/10DjzwwFx88cX56le/mnvvvTc333xz3vWudyVJHnjggaxdu3bar/9Hf/RH+fSnP52f+ZmfyRVXXJEvf/nLOeGEE2ZcvxkiAABgn5x55pn5oz/6o9x4443ZsmVLDjzwwEnjX/ziFyeW073zne9Mkjz++OO55557kiQf/ehHJ80svfzlL8/P/dzPZWRkJH/zN3+Tr3/969N+7W984xt573vfmz/+4z9OpVLZ69rNEAEAAPvkrLPOSpJs3bo1N9xww27j48vZjj322Bx33HFJkqGhoaxZsyZr1qzJGWecMeV9V61alST5/ve/P+3XXr58edauXTurMJQIRAAAwD569atfnZUrVybZfdnc9u3bJ5oo/Nqv/drE51euXJm///u/z9///d/ngAMO2O2eRVHk3/7t3/b4td/4xjfuNiO1NyyZAwAA9tlZZ52VdevW5Z/+6Z+yffv2POc5z0mS3HDDDXnqqafS2dmZ//bf/tuUr92yZUvuvvvu3H///fnP//zPfPvb385tt92Whx9+eI9f9/DDD9+nugUiAABgn40Hoqeeeir/+q//mje/+c1Jnu0ud9ppp+XQQw+d9Jr77rsvv//7v58bbrgh27dvnzS2bNmyvOY1r8ldd91V9+suXbp0n+q2ZA4AANhnfX19OfbYY5M8u2zuxz/+cb7whS8kebaZwri77rorq1evzhe+8IV0d3dnzZo1+Yu/+IvcfPPN+c///M98//vfz+/8zu/s8evOdu/QODNEAABAQ5x11lm599578/nPfz4f+9jH8n//7//ND3/4wyxZsiT/9b/+10nXvu9978sPf/jDrFy5Ml/96lfzvOc9b7f77dixo+k1myECAAAaYrzb3BNPPJF/+7d/m1gu9453vCP77bffpGtvvfXWJMmaNWumDEM7X9NMAhEAANAQRx999ERb7U9+8pO57rrrkuy+XC5Jnvvc5yZJHnnkkSnvddNNN+Wqq65KkgwPDzej3CQCEQAA0EDjs0RXXHFFtmzZkmOOOSYnnnjibteddNJJSZKPf/zjWbt2bW677bbcc889ufbaa7NmzZr8/M///EQQeuSRR3LFFVdMG572hUAEAAA0zJlnnpkkGR0dTTL17FCS/Nmf/VmWL1+ekZGRrFu3LieeeGJe+cpX5vTTT89VV12Vk08+Offdd99EZ7rzzz8/7373uxter0AEAAA0zIte9KKJGaGOjo78yq/8ypTX9fb25t57781v//Zv59hjj82BBx6YQw45JG984xvzt3/7t7nhhhuycuXK/O3f/m1e9KIX5eCDD87q1asbXm+lKIqi4XcFAACYB8wQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApSUQAQAApfX/ASgVy9O6N2JjAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot to show data labels look ok\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "hist_axis = hist.axis.Regular(100, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "\n",
    "data_hists, data_plot_labels = [], []\n",
    "if not EVAL_DATA_ON_ALL_FOLDS:\n",
    "    \n",
    "    for fold_idx in range(len(BDT_DATA_preds)):\n",
    "\n",
    "        data_hists.append(\n",
    "            hist.Hist(hist_axis, storage='weight').fill(\n",
    "                var=np.array(BDT_DATA_preds[fold_idx])[:, 0],\n",
    "            )\n",
    "        )\n",
    "        data_plot_labels.append(f\"fold {fold_idx}\")\n",
    "else:\n",
    "\n",
    "    data_hists.append(\n",
    "        hist.Hist(hist_axis, storage='weight').fill(\n",
    "            var=np.array(BDT_DATA_preds)[:, 0],\n",
    "        )\n",
    "    )\n",
    "    data_plot_labels.append(f\"sum over folds\")\n",
    "\n",
    "hep.histplot(\n",
    "    data_hists,\n",
    "    alpha=0.5, density=False, histtype='step',\n",
    "    label=data_plot_labels\n",
    ")\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022preEE_merged_v5_MultiBDT_output/GluGluToHH/nominal/GluGluToHH_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/GluGluToHH/nominal/GluGluToHH_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022preEE_merged_v5_MultiBDT_output/ttHToGG/nominal/ttHToGG_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/ttHToGG/nominal/ttHToGG_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022preEE_merged_v5_MultiBDT_output/VHToGG/nominal/VHToGG_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/VHToGG/nominal/VHToGG_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022preEE_merged_v5_MultiBDT_output/GGJets/nominal/GGJets_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/GGJets/nominal/GGJets_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022preEE_merged_v5_MultiBDT_output/GJetPt20To40/nominal/GJetPt20To40_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/GJetPt20To40/nominal/GJetPt20To40_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022preEE_merged_v5_MultiBDT_output/GJetPt40/nominal/GJetPt40_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/GJetPt40/nominal/GJetPt40_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022preEE_merged_v5_MultiBDT_output/GluGluHToGG/nominal/GluGluHToGG_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/GluGluHToGG/nominal/GluGluHToGG_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022preEE_merged_v5_MultiBDT_output/VBFHToGG/nominal/VBFHToGG_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/VBFHToGG/nominal/VBFHToGG_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022preEE_merged_v5_MultiBDT_output/Data_EraC/nominal/Data_EraC_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022preEE_merged_v5_MultiBDT_output/Data_EraD/nominal/Data_EraD_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/Data_EraE/nominal/Data_EraE_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/Data_EraF/nominal/Data_EraF_nominal_MultiBDT_output.parquet\n",
      "============================================================\n",
      "/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1/Run3_2022postEE_merged_v5_MultiBDT_output/Data_EraG/nominal/Data_EraG_nominal_MultiBDT_output.parquet\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# load and pre-process the data\n",
    "DATA_FILEPATHS_DICT = {\n",
    "    'Data': [\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v5/Data_EraC/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v5/Data_EraD/nominal/*\",\n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v5/Data_EraE/nominal/*\",\n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v5/Data_EraF/nominal/*\",\n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v5/Data_EraG/nominal/*\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Sorts the predictions to map the output to the correct event\n",
    "def sorted_preds(preds, data_aux, sample, sorted_preds=False):\n",
    "    if not sorted_preds:\n",
    "        flat_preds = np.concatenate([preds[fold_idx] for fold_idx in range(len(data_aux))])\n",
    "        preds_sort = np.argsort(\n",
    "            np.concatenate([data_aux[f\"fold_{fold_idx}\"].loc[:, 'hash'].to_numpy() for fold_idx in range(len(data_aux))])\n",
    "        )\n",
    "    else:\n",
    "        flat_preds = preds\n",
    "        preds_sort = np.arange(len(flat_preds))\n",
    "\n",
    "    sample_sort = np.argsort(np.argsort(\n",
    "        ak.to_numpy(sample['hash'], allow_missing=False)\n",
    "    ))\n",
    "\n",
    "    return flat_preds[preds_sort][sample_sort]\n",
    "\n",
    "## MC SAMPLES ##\n",
    "# Load parquet files #\n",
    "for i, sample_name in enumerate(order):\n",
    "    for dirpath in FILEPATHS_DICT[sample_name]:\n",
    "        parquet_filepath = glob.glob(dirpath)[0]\n",
    "        sample = ak.from_parquet(parquet_filepath)\n",
    "\n",
    "        (\n",
    "            NOTHING_IGNORE,\n",
    "            IGNORE_data_df_dict, SAMPLE_data_test_df_dict, \n",
    "            IGNORE_data_hlf_dict, IGNORE_label_dict,\n",
    "            SAMPLE_data_hlf_test_dict, SAMPLE_label_test_dict, \n",
    "            SAMPLE_hlf_vars_columns_dict,\n",
    "            IGNORE_data_aux_dict, SAMPLE_data_test_aux_dict\n",
    "        ) = process_data(\n",
    "            {\"sample\": [parquet_filepath]}, OUTPUT_DIRPATH, order=['sample'], mod_vals=MOD_VALS, k_fold_test=True,\n",
    "            save=False, std_json_dirpath=OUTPUT_DIRPATH\n",
    "        )\n",
    "\n",
    "        sample_preds = []\n",
    "        for fold_idx in range(len(SAMPLE_data_test_df_dict)):\n",
    "            booster = xgb.Booster(param)\n",
    "            booster.load_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "\n",
    "            bdt_test_sample_dict = xgb.DMatrix(\n",
    "                data=SAMPLE_data_hlf_test_dict[f\"fold_{fold_idx}\"], label=SAMPLE_label_test_dict[f\"fold_{fold_idx}\"], \n",
    "                missing=-999.0, feature_names=list(SAMPLE_hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "            )\n",
    "\n",
    "            sample_preds.append(\n",
    "                booster.predict(\n",
    "                    bdt_test_sample_dict, \n",
    "                    iteration_range=(0, booster.best_iteration+1)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        sample['MultiBDT_output'] = sorted_preds(\n",
    "            sample_preds, SAMPLE_data_test_aux_dict, sample\n",
    "        )\n",
    "\n",
    "        dest_filepath = parquet_filepath[:parquet_filepath.find('v5')+2] + '_MultiBDT_output' + parquet_filepath[parquet_filepath.find('v5')+2:parquet_filepath.rfind('.')] + '_MultiBDT_output' + parquet_filepath[parquet_filepath.rfind('.'):]\n",
    "        if not os.path.exists(dest_filepath[:dest_filepath.rfind('/')]):\n",
    "            os.makedirs(dest_filepath[:dest_filepath.rfind('/')])\n",
    "        print(dest_filepath)\n",
    "        print('='*60)\n",
    "        merged_parquet = ak.to_parquet(sample, dest_filepath)\n",
    "\n",
    "## DATA ##\n",
    "for dirpath in DATA_FILEPATHS_DICT['Data']:\n",
    "    parquet_filepath = glob.glob(dirpath)[0]\n",
    "    data_sample = ak.from_parquet(parquet_filepath)\n",
    "\n",
    "    (\n",
    "        NOTHING_IGNORE,\n",
    "        DATA_data_df_dict, DATA_data_test_df_dict, \n",
    "        DATA_data_hlf_dict, DATA_label_dict,\n",
    "        DATA_data_hlf_test_dict, DATA_label_test_dict, \n",
    "        DATA_hlf_vars_columns_dict,\n",
    "        DATA_data_aux_dict, DATA_data_test_aux_dict\n",
    "    ) = process_data(\n",
    "        {\"sample\": [parquet_filepath]}, OUTPUT_DIRPATH, order=['sample'], mod_vals=MOD_VALS, k_fold_test=True,\n",
    "        save=False, std_json_dirpath=OUTPUT_DIRPATH\n",
    "    )\n",
    "\n",
    "    bdt_train_data_dict = xgb.DMatrix(\n",
    "        data=DATA_data_hlf_dict[f\"fold_0\"], label=DATA_label_dict[f\"fold_0\"], \n",
    "        missing=-999.0, feature_names=list(DATA_hlf_vars_columns_dict[f\"fold_0\"])\n",
    "    )\n",
    "    bdt_test_data_dict = xgb.DMatrix(\n",
    "        data=DATA_data_hlf_test_dict[f\"fold_0\"], label=DATA_label_test_dict[f\"fold_0\"], \n",
    "        missing=-999.0, feature_names=list(DATA_hlf_vars_columns_dict[f\"fold_0\"])\n",
    "    )\n",
    "\n",
    "    for fold_idx in range(len(DATA_label_test_dict)):\n",
    "        booster = xgb.Booster(param)\n",
    "        booster.load_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "\n",
    "        BDT_train_preds = booster.predict(\n",
    "            bdt_train_data_dict, \n",
    "            iteration_range=(0, booster.best_iteration+1)\n",
    "        )\n",
    "        BDT_test_preds = booster.predict(\n",
    "            bdt_test_data_dict, \n",
    "            iteration_range=(0, booster.best_iteration+1)\n",
    "        )\n",
    "\n",
    "        BDT_all_preds = np.concatenate([BDT_train_preds, BDT_test_preds])\n",
    "        BDT_all_preds = BDT_all_preds[\n",
    "            np.argsort(\n",
    "                np.concatenate([DATA_data_aux_dict[f\"fold_0\"].loc[:, 'hash'].to_numpy(), DATA_data_test_aux_dict[f\"fold_0\"].loc[:, 'hash'].to_numpy()])\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if fold_idx == 0:\n",
    "            data_preds = copy.deepcopy(BDT_all_preds)\n",
    "        else:\n",
    "            data_preds += BDT_all_preds\n",
    "\n",
    "            if fold_idx == len(DATA_label_test_dict) - 1:\n",
    "                data_preds = data_preds / len(DATA_label_test_dict)\n",
    "\n",
    "    data_sample['MultiBDT_output'] = sorted_preds(\n",
    "        data_preds, DATA_data_test_aux_dict, data_sample,\n",
    "        sorted_preds=True\n",
    "    )\n",
    "\n",
    "    dest_filepath = parquet_filepath[:parquet_filepath.find('v5')+2] + '_MultiBDT_output' + parquet_filepath[parquet_filepath.find('v5')+2:parquet_filepath.rfind('.')] + '_MultiBDT_output' + parquet_filepath[parquet_filepath.rfind('.'):]\n",
    "    if not os.path.exists(dest_filepath[:dest_filepath.rfind('/')]):\n",
    "        os.makedirs(dest_filepath[:dest_filepath.rfind('/')])\n",
    "    print(dest_filepath)\n",
    "    print('='*60)\n",
    "    merged_parquet = ak.to_parquet(data_sample, dest_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
