{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu3.fnal.gov      Tue Nov 19 14:06:12 2024  555.42.06\n",
      "[0] Tesla P100-PCIE-12GB | 41Â°C,   0 % |     0 / 12288 MB |\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import h5py\n",
    "import hist\n",
    "import mplhep as hep\n",
    "import xgboost as xgb\n",
    "from cycler import cycler\n",
    "\n",
    "\n",
    "# ML packages\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from scipy.integrate import trapezoid\n",
    "\n",
    "# Module packages\n",
    "from data_processing_BDT import process_data\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "FILEPATHS_DICT = {\n",
    "    'ggF HH': [\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GluGluToHH/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GluGluToHH/nominal/*\"\n",
    "    ],\n",
    "    # 'VBF HH': [\n",
    "    #     lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", \n",
    "    #     lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\"\n",
    "    # ],\n",
    "    'ttH': [\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/ttHToGG/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/ttHToGG/nominal/*\"\n",
    "    ],\n",
    "    'non-res + single-H': [\n",
    "        # non-Resonant #\n",
    "        # GG + 3Jets\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GGJets/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GGJets/nominal/*\",\n",
    "        # GJet pT 20-40\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GJetPt20To40/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GJetPt20To40/nominal/*\",\n",
    "        # GJet pT 40-inf\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GJetPt40/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GJetPt40/nominal/*\",\n",
    "        # single-H #\n",
    "        # ggF H\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GluGluHToGG/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GluGluHToGG/nominal/*\",\n",
    "        # VBF H\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/VBFHToGG/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/VBFHToGG/nominal/*\",\n",
    "        # VH\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/VHToGG/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/VHToGG/nominal/*\",\n",
    "    ],\n",
    "    # 'VH': [\n",
    "    #     lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/VHToGG/nominal/*\", \n",
    "    #     lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/VHToGG/nominal/*\"\n",
    "    # ],\n",
    "}\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v1'\n",
    "MOD_VALS = (5, 5)\n",
    "VARS = 'nonres_and_ttH_vars'\n",
    "# CURRENT_TIME = '2024-11-08_13-13-20'\n",
    "# CURRENT_TIME = '2024-11-16_13-10-23'\n",
    "if 'CURRENT_TIME' in globals():\n",
    "    OUTPUT_DIRPATH = os.path.join(CURRENT_DIRPATH, f\"MultiClassBDT_model_outputs/{VERSION}/{VARS}\", CURRENT_TIME)\n",
    "else:\n",
    "    OUTPUT_DIRPATH = os.path.join(CURRENT_DIRPATH, f\"MultiClassBDT_model_outputs/{VERSION}/{VARS}\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_weights(event_weights, labels, order=None, weighttype='rescaled'):\n",
    "    if order is not None:\n",
    "        sig_idx = -1\n",
    "        for i, sample_name in enumerate(order):\n",
    "            if re.search('ggF HH', sample_name) is not None:\n",
    "                sig_idx = i\n",
    "                break\n",
    "    else:\n",
    "        sig_idx = 0\n",
    "    \n",
    "    sig_sum = np.sum(event_weights[labels[:, sig_idx] == 1])\n",
    "    bkg_sum = np.sum(event_weights[labels[:, sig_idx] == 0])\n",
    "    \n",
    "    sig_scale_factor = bkg_sum / sig_sum\n",
    "\n",
    "    scaled_weights = np.where(\n",
    "        labels[:, sig_idx] == 0, \n",
    "        event_weights,  # if bkg, do nothing\n",
    "        event_weights * sig_scale_factor  # if sig, rescale to equal sum of all bkgs\n",
    "    )\n",
    "\n",
    "    abs_weights = np.abs(scaled_weights)\n",
    "\n",
    "    if weighttype == 'abs':\n",
    "        return abs_weights\n",
    "    elif weighttype == 'rescaled':\n",
    "        mean_weights = np.mean(scaled_weights)\n",
    "        rescaled_weights = abs_weights / mean_weights\n",
    "        return rescaled_weights\n",
    "\n",
    "def xgb_labels(labels):\n",
    "    label_i = np.sum(\n",
    "        np.tile([i for i in range(np.shape(labels)[1])], (np.shape(labels)[0], 1)) * labels,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return label_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['ggF HH', 'ttH', 'non-res + single-H']\n",
    "\n",
    "(\n",
    "    data_df_dict, data_test_df_dict, \n",
    "    data_hlf_dict, label_dict, \n",
    "    data_hlf_test_dict, label_test_dict, \n",
    "    hlf_vars_columns_dict,\n",
    "    data_aux_dict, data_test_aux_dict\n",
    ") = process_data(\n",
    "    FILEPATHS_DICT, OUTPUT_DIRPATH, order=order, seed=SEED, mod_vals=MOD_VALS, k_fold_test=True,\n",
    "    save=False if 'CURRENT_TIME' in globals() else True\n",
    ")\n",
    "\n",
    "xgb_label_dict = {\n",
    "    f\"fold_{fold_idx}\": copy.deepcopy(xgb_labels(label_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "xgb_label_test_dict = {\n",
    "    f\"fold_{fold_idx}\": copy.deepcopy(xgb_labels(label_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "weight_train_dict = {\n",
    "    f\"fold_{fold_idx}\": copy.deepcopy(training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}'], order=order)) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Num train: 1250553 -> 109096 sig & 222178 ttH bkg & 919279 non-res + single-H bkg\n",
      "Num val: 312639 -> 27434 sig & 55027 ttH bkg & 230178 non-res + single-H bkg\n",
      "Num test: 391785 -> 34224 sig & 69297 ttH bkg & 288264 non-res + single-H bkg\n",
      "============================================================\n",
      "fold 1\n",
      "Num train: 1251336 -> 109298 sig & 221622 ttH bkg & 920416 non-res + single-H bkg\n",
      "Num val: 312835 -> 27168 sig & 55830 ttH bkg & 229837 non-res + single-H bkg\n",
      "Num test: 390806 -> 34288 sig & 69050 ttH bkg & 287468 non-res + single-H bkg\n",
      "============================================================\n",
      "fold 2\n",
      "Num train: 1250948 -> 109478 sig & 221207 ttH bkg & 920263 non-res + single-H bkg\n",
      "Num val: 312737 -> 27160 sig & 55420 ttH bkg & 230157 non-res + single-H bkg\n",
      "Num test: 391292 -> 34116 sig & 69875 ttH bkg & 287301 non-res + single-H bkg\n",
      "============================================================\n",
      "fold 3\n",
      "Num train: 1251535 -> 109266 sig & 221688 ttH bkg & 920581 non-res + single-H bkg\n",
      "Num val: 312884 -> 27405 sig & 55366 ttH bkg & 230113 non-res + single-H bkg\n",
      "Num test: 390558 -> 34083 sig & 69448 ttH bkg & 287027 non-res + single-H bkg\n",
      "============================================================\n",
      "fold 4\n",
      "Num train: 1251552 -> 109362 sig & 221957 ttH bkg & 920233 non-res + single-H bkg\n",
      "Num val: 312889 -> 27349 sig & 55713 ttH bkg & 229827 non-res + single-H bkg\n",
      "Num test: 390536 -> 34043 sig & 68832 ttH bkg & 287661 non-res + single-H bkg\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "bdt_train_dict, bdt_val_dict, bdt_test_dict = {}, {}, {}\n",
    "\n",
    "weights_plot_train, weights_plot_val = {}, {}\n",
    "for fold_idx in range(len(data_df_dict)):\n",
    "    if re.search('no_std', VARS) is not None:\n",
    "        print('no standardization')\n",
    "        train_val_data_dict = {key: value.to_numpy() for key, value in data_df_dict.items()}\n",
    "        test_data_dict = {key: value.to_numpy() for key, value in data_test_df_dict.items()}\n",
    "    else:\n",
    "        train_val_data_dict = data_hlf_dict\n",
    "        test_data_dict = data_hlf_test_dict\n",
    "    (\n",
    "        X_train, X_val, y_train, y_val, weight_train, weight_val\n",
    "    ) = train_test_split(\n",
    "        train_val_data_dict[f\"fold_{fold_idx}\"], xgb_label_dict[f\"fold_{fold_idx}\"], \n",
    "        weight_train_dict[f\"fold_{fold_idx}\"],\n",
    "        test_size=0.2, random_state=21\n",
    "    )\n",
    "    weights_plot_train[f\"fold_{fold_idx}\"] = copy.deepcopy(weight_train)\n",
    "    weights_plot_val[f\"fold_{fold_idx}\"] = copy.deepcopy(weight_val)\n",
    "\n",
    "    bdt_train_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "        data=X_train, label=y_train, \n",
    "        weight=weight_train,\n",
    "        missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "    )\n",
    "    bdt_val_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "        data=X_val, label=y_val, \n",
    "        weight=weight_val,\n",
    "        missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "    )\n",
    "    \n",
    "    bdt_test_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "        data=test_data_dict[f\"fold_{fold_idx}\"], label=xgb_label_test_dict[f\"fold_{fold_idx}\"], \n",
    "        weight=np.abs(weight_test_dict[f\"fold_{fold_idx}\"]),\n",
    "        missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "    )\n",
    "\n",
    "    print(f\"fold {fold_idx}\")\n",
    "    print(f\"Num train: {len(y_train)} -> {sum(y_train == 0)} sig & {sum(y_train == 1)} ttH bkg & {sum(y_train == 2)} non-res + single-H bkg\")\n",
    "    print(f\"Num val: {len(y_val)} -> {sum(y_val == 0)} sig & {sum(y_val == 1)} ttH bkg & {sum(y_val == 2)} non-res + single-H bkg\")\n",
    "    print(f\"Num test: {len(label_test_dict[f'fold_{fold_idx}'])} -> {sum(label_test_dict[f'fold_{fold_idx}'] == np.array([1, 0, 0]))[0]} sig & {sum(label_test_dict[f'fold_{fold_idx}'] == np.array([0, 1, 0]))[1]} ttH bkg & {sum(label_test_dict[f'fold_{fold_idx}'] == np.array([0, 0, 1]))[2]} non-res + single-H bkg\")\n",
    "    print('='*60)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57986259/multiclass-classification-with-xgboost-classifier\n",
    "# https://forecastegy.com/posts/xgboost-multiclass-classification-python/\n",
    "# https://indico.cern.ch/event/915265/contributions/3848138/attachments/2048174/3432202/kunlinRan_bbyy_20200531.pdf\n",
    "\n",
    "\n",
    "param = {}\n",
    "\n",
    "# Booster parameters\n",
    "param['eta']              = 0.05 # learning rate\n",
    "param['max_depth']        = 15  # maximum depth of a tree\n",
    "param['subsample']        = 0.3 # fraction of events to train tree on\n",
    "param['colsample_bytree'] = 0.8 # fraction of features to train tree on\n",
    "param['num_class']        = np.shape(label_dict['fold_0'])[1] # num classes for multi-class training\n",
    "\n",
    "# Learning task parameters\n",
    "param['objective']   = 'multi:softprob'   # objective function\n",
    "param['eval_metric'] = 'merror'           # evaluation metric for cross validation\n",
    "param = list(param.items()) + [('eval_metric', 'mlogloss')]\n",
    "\n",
    "num_trees = 1500  # number of trees to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "[0]\ttrain-merror:0.04957\ttrain-mlogloss:1.03359\ttest-merror:0.04574\ttest-mlogloss:1.03574\tval-merror:0.06404\tval-mlogloss:1.03565\n",
      "[25]\ttrain-merror:0.02536\ttrain-mlogloss:0.30901\ttest-merror:0.02982\ttest-mlogloss:0.32153\tval-merror:0.04487\tval-mlogloss:0.33788\n",
      "[50]\ttrain-merror:0.02174\ttrain-mlogloss:0.13135\ttest-merror:0.02888\ttest-mlogloss:0.15174\tval-merror:0.04361\tval-mlogloss:0.17805\n",
      "[75]\ttrain-merror:0.01813\ttrain-mlogloss:0.07426\ttest-merror:0.02851\ttest-mlogloss:0.10385\tval-merror:0.04327\tval-mlogloss:0.13488\n",
      "[100]\ttrain-merror:0.01508\ttrain-mlogloss:0.05271\ttest-merror:0.02833\ttest-mlogloss:0.08970\tval-merror:0.04270\tval-mlogloss:0.12213\n",
      "[125]\ttrain-merror:0.01182\ttrain-mlogloss:0.04165\ttest-merror:0.02702\ttest-mlogloss:0.08241\tval-merror:0.04233\tval-mlogloss:0.11847\n",
      "[150]\ttrain-merror:0.00810\ttrain-mlogloss:0.03348\ttest-merror:0.02477\ttest-mlogloss:0.07520\tval-merror:0.04214\tval-mlogloss:0.11790\n",
      "[152]\ttrain-merror:0.00781\ttrain-mlogloss:0.03291\ttest-merror:0.02466\ttest-mlogloss:0.07468\tval-merror:0.04213\tval-mlogloss:0.11793\n",
      "[143]\ttest-merror:0.024518\ttest-mlogloss:0.074521\n",
      "====================================================================================================\n",
      "fold 1\n",
      "[0]\ttrain-merror:0.05343\ttrain-mlogloss:1.03540\ttest-merror:0.05605\ttest-mlogloss:1.03881\tval-merror:0.06731\tval-mlogloss:1.03767\n",
      "[25]\ttrain-merror:0.02557\ttrain-mlogloss:0.30695\ttest-merror:0.03226\ttest-mlogloss:0.32335\tval-merror:0.04235\tval-mlogloss:0.33330\n",
      "[50]\ttrain-merror:0.02155\ttrain-mlogloss:0.13096\ttest-merror:0.03196\ttest-mlogloss:0.15743\tval-merror:0.04131\tval-mlogloss:0.17405\n",
      "[75]\ttrain-merror:0.01782\ttrain-mlogloss:0.07437\ttest-merror:0.03154\ttest-mlogloss:0.11088\tval-merror:0.04065\tval-mlogloss:0.12975\n",
      "[100]\ttrain-merror:0.01469\ttrain-mlogloss:0.05198\ttest-merror:0.03054\ttest-mlogloss:0.09634\tval-merror:0.04049\tval-mlogloss:0.11669\n",
      "[125]\ttrain-merror:0.01114\ttrain-mlogloss:0.04051\ttest-merror:0.02923\ttest-mlogloss:0.08849\tval-merror:0.03991\tval-mlogloss:0.11287\n",
      "[146]\ttrain-merror:0.00826\ttrain-mlogloss:0.03384\ttest-merror:0.02769\ttest-mlogloss:0.08332\tval-merror:0.03952\tval-mlogloss:0.11236\n",
      "[136]\ttest-merror:0.027689\ttest-mlogloss:0.083316\n",
      "====================================================================================================\n",
      "fold 2\n",
      "[0]\ttrain-merror:0.04652\ttrain-mlogloss:1.03185\ttest-merror:0.03894\ttest-mlogloss:1.03311\tval-merror:0.05934\tval-mlogloss:1.03367\n",
      "[25]\ttrain-merror:0.02552\ttrain-mlogloss:0.30791\ttest-merror:0.03139\ttest-mlogloss:0.32320\tval-merror:0.04430\tval-mlogloss:0.33513\n",
      "[50]\ttrain-merror:0.02173\ttrain-mlogloss:0.13085\ttest-merror:0.03041\ttest-mlogloss:0.15669\tval-merror:0.04313\tval-mlogloss:0.17524\n",
      "[75]\ttrain-merror:0.01833\ttrain-mlogloss:0.07360\ttest-merror:0.03005\ttest-mlogloss:0.10978\tval-merror:0.04225\tval-mlogloss:0.13108\n",
      "[100]\ttrain-merror:0.01497\ttrain-mlogloss:0.05219\ttest-merror:0.02928\ttest-mlogloss:0.09605\tval-merror:0.04200\tval-mlogloss:0.11833\n",
      "[125]\ttrain-merror:0.01161\ttrain-mlogloss:0.04099\ttest-merror:0.02807\ttest-mlogloss:0.08826\tval-merror:0.04149\tval-mlogloss:0.11489\n",
      "[150]\ttrain-merror:0.00780\ttrain-mlogloss:0.03285\ttest-merror:0.02645\ttest-mlogloss:0.08187\tval-merror:0.04081\tval-mlogloss:0.11378\n",
      "[156]\ttrain-merror:0.00712\ttrain-mlogloss:0.03130\ttest-merror:0.02590\ttest-mlogloss:0.08042\tval-merror:0.04099\tval-mlogloss:0.11408\n",
      "[146]\ttest-merror:0.025903\ttest-mlogloss:0.080422\n",
      "====================================================================================================\n",
      "fold 3\n",
      "[0]\ttrain-merror:0.04452\ttrain-mlogloss:1.03172\ttest-merror:0.03772\ttest-mlogloss:1.03276\tval-merror:0.05657\tval-mlogloss:1.03353\n",
      "[25]\ttrain-merror:0.02636\ttrain-mlogloss:0.30690\ttest-merror:0.03112\ttest-mlogloss:0.32060\tval-merror:0.04340\tval-mlogloss:0.33265\n",
      "[50]\ttrain-merror:0.02205\ttrain-mlogloss:0.13043\ttest-merror:0.03033\ttest-mlogloss:0.15484\tval-merror:0.04195\tval-mlogloss:0.17308\n",
      "[75]\ttrain-merror:0.01831\ttrain-mlogloss:0.07378\ttest-merror:0.03029\ttest-mlogloss:0.10869\tval-merror:0.04123\tval-mlogloss:0.12931\n",
      "[100]\ttrain-merror:0.01501\ttrain-mlogloss:0.05253\ttest-merror:0.02961\ttest-mlogloss:0.09494\tval-merror:0.04105\tval-mlogloss:0.11707\n",
      "[125]\ttrain-merror:0.01150\ttrain-mlogloss:0.04161\ttest-merror:0.02873\ttest-mlogloss:0.08827\tval-merror:0.04040\tval-mlogloss:0.11251\n",
      "[149]\ttrain-merror:0.00821\ttrain-mlogloss:0.03390\ttest-merror:0.02663\ttest-mlogloss:0.08161\tval-merror:0.04041\tval-mlogloss:0.11184\n",
      "[139]\ttest-merror:0.026631\ttest-mlogloss:0.081611\n",
      "====================================================================================================\n",
      "fold 4\n",
      "[0]\ttrain-merror:0.04473\ttrain-mlogloss:1.03153\ttest-merror:0.03754\ttest-mlogloss:1.03266\tval-merror:0.05591\tval-mlogloss:1.03317\n",
      "[25]\ttrain-merror:0.02582\ttrain-mlogloss:0.30584\ttest-merror:0.03095\ttest-mlogloss:0.31893\tval-merror:0.04219\tval-mlogloss:0.33040\n",
      "[50]\ttrain-merror:0.02119\ttrain-mlogloss:0.13079\ttest-merror:0.02966\ttest-mlogloss:0.15413\tval-merror:0.04165\tval-mlogloss:0.17242\n",
      "[75]\ttrain-merror:0.01811\ttrain-mlogloss:0.07356\ttest-merror:0.02928\ttest-mlogloss:0.10631\tval-merror:0.04064\tval-mlogloss:0.12806\n",
      "[100]\ttrain-merror:0.01473\ttrain-mlogloss:0.05183\ttest-merror:0.02863\ttest-mlogloss:0.09176\tval-merror:0.04028\tval-mlogloss:0.11523\n",
      "[125]\ttrain-merror:0.01152\ttrain-mlogloss:0.04147\ttest-merror:0.02768\ttest-mlogloss:0.08534\tval-merror:0.03977\tval-mlogloss:0.11186\n",
      "[147]\ttrain-merror:0.00854\ttrain-mlogloss:0.03452\ttest-merror:0.02674\ttest-mlogloss:0.07913\tval-merror:0.04012\tval-mlogloss:0.11170\n",
      "[138]\ttest-merror:0.026542\ttest-mlogloss:0.078901\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "if 'CURRENT_TIME' in globals():\n",
    "    OUTPUT_DIRPATH, OLD_TIME = os.path.split(OUTPUT_DIRPATH)\n",
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "OUTPUT_DIRPATH = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME)\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "evals_result_dict = {f\"fold_{fold_idx}\": dict() for fold_idx in range(len(bdt_train_dict))}\n",
    "for fold_idx in range(len(bdt_train_dict)):\n",
    "    print(f\"fold {fold_idx}\")\n",
    "    # Train bdt\n",
    "    evallist = [(bdt_train_dict[f\"fold_{fold_idx}\"], 'train'), (bdt_test_dict[f\"fold_{fold_idx}\"], 'test'), (bdt_val_dict[f\"fold_{fold_idx}\"], 'val')]\n",
    "    booster = xgb.train(\n",
    "        param, bdt_train_dict[f\"fold_{fold_idx}\"], num_boost_round=num_trees, \n",
    "        evals=evallist, early_stopping_rounds=10, verbose_eval=25, evals_result=evals_result_dict[f\"fold_{fold_idx}\"]\n",
    "    )\n",
    "\n",
    "    booster.save_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "    \n",
    "    # Print perf on test dataset\n",
    "    print(booster.eval(bdt_test_dict[f\"fold_{fold_idx}\"], name='test', iteration=booster.best_iteration))\n",
    "    print('='*100)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_eval_result.json'), 'w') as f:\n",
    "    json.dump(evals_result_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tpr = np.linspace(0, 1, 5000)  # copied from IN evaluate.py file\n",
    "roc_baseline = np.zeros(\n",
    "    (len(bdt_train_dict), len(base_tpr), len(order)), \n",
    "    dtype=float\n",
    ")\n",
    "area_baseline = np.zeros(\n",
    "    (len(bdt_train_dict), len(order)), \n",
    "    dtype=float\n",
    ")\n",
    "\n",
    "BDT_perf = {\n",
    "    sample_name: copy.deepcopy({\n",
    "        'base_tpr': base_tpr,\n",
    "        'class_order': copy.deepcopy(order),\n",
    "        # test data #\n",
    "        'preds': [],\n",
    "        'fprs_density': copy.deepcopy(roc_baseline), 'thresholds_density': copy.deepcopy(roc_baseline), 'areas_density': copy.deepcopy(area_baseline),\n",
    "        'fprs_weighted': copy.deepcopy(roc_baseline), 'thresholds_weighted': copy.deepcopy(roc_baseline), 'areas_weighted': copy.deepcopy(area_baseline),\n",
    "        'fprs_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'thresholds_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'areas_sum_density': copy.deepcopy(area_baseline[0, ...]),\n",
    "        'fprs_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'thresholds_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'areas_sum_weighted': copy.deepcopy(area_baseline[0, ...]),\n",
    "        # train data #\n",
    "        'train_preds': [], \n",
    "        'train_fprs_density': copy.deepcopy(roc_baseline), 'train_thresholds_density': copy.deepcopy(roc_baseline), 'train_areas_density': copy.deepcopy(area_baseline),\n",
    "        'train_fprs_weighted': copy.deepcopy(roc_baseline), 'train_thresholds_weighted': copy.deepcopy(roc_baseline), 'train_areas_weighted': copy.deepcopy(area_baseline),\n",
    "        'train_fprs_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'train_thresholds_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'train_areas_sum_density': copy.deepcopy(area_baseline[0, ...]),\n",
    "        'train_fprs_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'train_thresholds_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'train_areas_sum_weighted': copy.deepcopy(area_baseline[0, ...]),\n",
    "        # val data #\n",
    "        'val_preds': [],\n",
    "        'val_fprs_density': copy.deepcopy(roc_baseline), 'val_thresholds_density': copy.deepcopy(roc_baseline), 'val_areas_density': copy.deepcopy(area_baseline),\n",
    "        'val_fprs_weighted': copy.deepcopy(roc_baseline), 'val_thresholds_weighted': copy.deepcopy(roc_baseline), 'val_areas_weighted': copy.deepcopy(area_baseline),\n",
    "        'val_fprs_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'val_thresholds_sum_density': copy.deepcopy(roc_baseline[0, ...]), 'val_areas_sum_density': copy.deepcopy(area_baseline[0, ...]),\n",
    "        'val_fprs_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'val_thresholds_sum_weighted': copy.deepcopy(roc_baseline[0, ...]), 'val_areas_sum_weighted': copy.deepcopy(area_baseline[0, ...]),\n",
    "    }) for sample_name in order\n",
    "}\n",
    "\n",
    "for j, sample_name in enumerate(order):\n",
    "\n",
    "    for fold_idx in range(len(bdt_train_dict)):\n",
    "        booster = xgb.Booster(param)\n",
    "        booster.load_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "    \n",
    "        for pred_type, dataset in [\n",
    "            ('train_', bdt_train_dict[f\"fold_{fold_idx}\"]),\n",
    "            ('val_', bdt_val_dict[f\"fold_{fold_idx}\"]),\n",
    "            ('', bdt_test_dict[f\"fold_{fold_idx}\"])\n",
    "        ]:\n",
    "            BDT_perf[sample_name][pred_type + 'preds'].append(\n",
    "                booster.predict(\n",
    "                    dataset, \n",
    "                    iteration_range=(0, booster.best_iteration+1)\n",
    "                ).tolist()\n",
    "            )\n",
    "\n",
    "            for i, sample_name_ in enumerate(order):\n",
    "                \n",
    "                if sample_name_ == sample_name:\n",
    "                    event_mask = dataset.get_label() > -1\n",
    "                    pred_rescale = np.ones_like(event_mask)\n",
    "                else:\n",
    "                    event_mask = np.logical_or(dataset.get_label() == j, dataset.get_label() == i)\n",
    "                    pred_rescale = np.array(BDT_perf[sample_name][pred_type + 'preds'][-1])[:, j][event_mask] + np.array(BDT_perf[sample_name][pred_type + 'preds'][-1])[:, i][event_mask]\n",
    "                class_preds = np.array(BDT_perf[sample_name][pred_type + 'preds'][-1])[:, j][event_mask] / pred_rescale\n",
    "                class_truths = np.where(dataset.get_label() == j, 1, 0)[event_mask]\n",
    "                \n",
    "                for roc_type in ['density', 'weighted']:\n",
    "\n",
    "                    if roc_type == 'weighted':\n",
    "                        if re.search('train', pred_type) is not None:\n",
    "                            roc_weights = weights_plot_train[f\"fold_{fold_idx}\"][event_mask]\n",
    "                        elif re.search('val', pred_type) is not None:\n",
    "                            roc_weights = weights_plot_val[f\"fold_{fold_idx}\"][event_mask]\n",
    "                        else:\n",
    "                            roc_weights = weight_test_dict[f\"fold_{fold_idx}\"][event_mask]\n",
    "                    else:\n",
    "                        roc_weights = None\n",
    "\n",
    "                    fpr_bdt, tpr_bdt, threshold_bdt = roc_curve(class_truths, class_preds, sample_weight=roc_weights)\n",
    "                    fpr_bdt = np.interp(base_tpr, tpr_bdt, fpr_bdt)\n",
    "                    threshold_bdt = np.interp(base_tpr, tpr_bdt, threshold_bdt)\n",
    "\n",
    "                    BDT_perf[sample_name][pred_type + 'fprs_' + roc_type][fold_idx][:, i] = fpr_bdt\n",
    "                    BDT_perf[sample_name][pred_type + 'thresholds_' + roc_type][fold_idx][:, i] = threshold_bdt\n",
    "                    BDT_perf[sample_name][pred_type + 'areas_' + roc_type][fold_idx][i] = float(trapezoid(base_tpr, fpr_bdt))\n",
    "    \n",
    "    for pred_type, dataset_dict in [\n",
    "        ('train_', bdt_train_dict),\n",
    "        ('val_', bdt_val_dict),\n",
    "        ('', bdt_test_dict)\n",
    "    ]:\n",
    "\n",
    "        flat_preds = np.concatenate(BDT_perf[sample_name][f'{pred_type}preds'], axis=0)\n",
    "        flat_truths = np.concatenate([dataset_dict[f\"fold_{fold_idx}\"].get_label() for fold_idx in range(len(dataset_dict))], axis=0)\n",
    "\n",
    "        for i, sample_name_ in enumerate(order):\n",
    "            \n",
    "            if sample_name_ == sample_name:\n",
    "                event_mask = flat_truths > -1\n",
    "                pred_rescale = np.ones_like(event_mask)\n",
    "            else:\n",
    "                event_mask = np.logical_or(flat_truths == j, flat_truths == i)\n",
    "                pred_rescale = flat_preds[:, j][event_mask] + flat_preds[:, i][event_mask]\n",
    "            class_preds = flat_preds[:, j][event_mask] / pred_rescale\n",
    "            class_truths = np.where(flat_truths == j, 1, 0)[event_mask]\n",
    "            \n",
    "            for roc_type in ['density', 'weighted']:\n",
    "\n",
    "                if roc_type == 'weighted':\n",
    "                    if re.search('train', pred_type) is not None:\n",
    "                        roc_weights = np.concatenate([weights_plot_train[f\"fold_{fold_idx}\"] for fold_idx in range(len(weights_plot_train))], axis=0)[event_mask]\n",
    "                    elif re.search('val', pred_type) is not None:\n",
    "                        roc_weights = np.concatenate([weights_plot_val[f\"fold_{fold_idx}\"] for fold_idx in range(len(weights_plot_val))], axis=0)[event_mask]\n",
    "                    else:\n",
    "                        roc_weights = np.concatenate([weight_test_dict[f\"fold_{fold_idx}\"] for fold_idx in range(len(weight_test_dict))], axis=0)[event_mask]\n",
    "                else:\n",
    "                    roc_weights = None\n",
    "\n",
    "                fpr_bdt, tpr_bdt, threshold_bdt = roc_curve(class_truths, class_preds, sample_weight=roc_weights)\n",
    "                fpr_bdt = np.interp(base_tpr, tpr_bdt, fpr_bdt)\n",
    "                threshold_bdt = np.interp(base_tpr, tpr_bdt, threshold_bdt)\n",
    "\n",
    "                BDT_perf[sample_name][pred_type + 'fprs_sum_' + roc_type][:, i] = fpr_bdt\n",
    "                BDT_perf[sample_name][pred_type + 'thresholds_sum_' + roc_type][:, i] = threshold_bdt\n",
    "                BDT_perf[sample_name][pred_type + 'areas_sum_' + roc_type][i] = float(trapezoid(base_tpr, fpr_bdt))\n",
    "    \n",
    "    for key in BDT_perf[sample_name].keys():\n",
    "        if type(BDT_perf[sample_name][key]) is list:\n",
    "            continue\n",
    "        BDT_perf[sample_name][key] = BDT_perf[sample_name][key].tolist()\n",
    "\n",
    "    # with h5py.File(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+f\"_BDT_ROC_fold{fold_idx}.h5\"), \"w\") as out:\n",
    "    #     out['FPR'] = fpr_bdt\n",
    "    #     out['TPR'] = tpr_bdt\n",
    "    #     out['Thresholds'] = threshold_bdt\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'w') as f:\n",
    "    json.dump(BDT_perf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(list_of_lists):\n",
    "    max_length = np.max([len(list_i) for list_i in list_of_lists])\n",
    "    for list_i in list_of_lists:\n",
    "        while len(list_i) < max_length:\n",
    "            list_i.append(list_i[-1])\n",
    "\n",
    "    return list_of_lists\n",
    "\n",
    "def plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='png'):\n",
    "    plot_prefix = plot_prefix + ('_' if plot_prefix != '' else '')\n",
    "    plot_postfix = plot_postfix + ('_' if plot_postfix != '' else '')\n",
    "    plot_name = plot_prefix + plot_name + plot_postfix + f'.{format}'\n",
    "\n",
    "    plot_filepath = os.path.join(plot_dirpath, plot_name)\n",
    "    return plot_filepath\n",
    "\n",
    "def plot_train_val_losses(\n",
    "    losses_arrs, labels, plot_name, plot_dirpath, \n",
    "    plot_prefix='', plot_postfix='', linestyles=None,\n",
    "    losses_std_arrs=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    \n",
    "    if type(losses_arrs[0]) is float:\n",
    "        losses_arrs = [losses_arrs]\n",
    "    if linestyles is None:\n",
    "        linestyles = ['solid'] * len(losses_arrs)\n",
    "    if labels is None:\n",
    "        labels = [i for i in range(len(losses_arrs))]\n",
    "\n",
    "    if losses_std_arrs is not None:\n",
    "        for i in range(len(losses_std_arrs)):\n",
    "            plt.fill_between(\n",
    "                range(len(losses_std_arrs[i])), \n",
    "                losses_arrs[i]+losses_std_arrs[i], losses_arrs[i]-losses_std_arrs[i],\n",
    "                alpha=0.7\n",
    "            )\n",
    "\n",
    "    for i in range(len(losses_arrs)):\n",
    "        plt.plot(\n",
    "            range(len(losses_arrs[i])), \n",
    "            losses_arrs[i], \n",
    "            label=f\"{labels[i]} losses\", linestyle=linestyles[i],\n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "def plot_rocs(\n",
    "    fprs, tprs, labels, plot_name, plot_dirpath,\n",
    "    plot_prefix='', plot_postfix='', close=True, log=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    \n",
    "    for fpr, tpr, label in zip(fprs, tprs, labels):\n",
    "        linestyle = 'solid' if re.search('IN', label) is not None else ('dashed' if re.search('BDT', label) is not None else 'dotted')\n",
    "        plt.plot(fpr, tpr, label=label, linestyle=linestyle)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    if log is not None and re.search('x', log) is not None:\n",
    "        plt.xscale('log')\n",
    "    elif log is not None and re.search('y', log) is not None:\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    if close:\n",
    "        plt.close()\n",
    "\n",
    "def plot_output_scores(\n",
    "    sigs_and_bkgs, order, plot_name, plot_dirpath,\n",
    "    plot_prefix='', plot_postfix='', bins=50, weights=None, log=False\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "\n",
    "    hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "    hists, labels = [], []\n",
    "    for sample_name in order:\n",
    "        if sample_name not in sigs_and_bkgs:\n",
    "            continue\n",
    "        hists.append(\n",
    "            hist.Hist(hist_axis, storage='weight').fill(\n",
    "                var=sigs_and_bkgs[sample_name], \n",
    "                weight=weights[sample_name] if weights is not None else np.ones_like(sigs_and_bkgs[sample_name])\n",
    "            )\n",
    "        )\n",
    "        labels.append(sample_name)\n",
    "    hep.histplot(\n",
    "        hists,\n",
    "        yerr=(True if weights is not None else False),\n",
    "        alpha=0.7, density=(False if weights is not None else True), histtype='step',\n",
    "        label=labels\n",
    "    )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Output score')\n",
    "    if log:\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "def plot_s_over_root_b(\n",
    "    sig, bkg, label, plot_name, plot_dirpath,\n",
    "    plot_prefix='', plot_postfix='', bins=50, weights={'sig': None, 'bkg': None},\n",
    "    lines=None, lines_labels=None, line_colors=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "\n",
    "    hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "    sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig))\n",
    "    bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg, weight=weights['bkg'] if weights['bkg'] is not None else np.ones_like(bkg))\n",
    "    s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "    plt.plot(\n",
    "        np.arange(0., 1., 1/bins), s_over_root_b_points, \n",
    "        label=f'{label} - s/âb', alpha=0.8\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        for i in range(len(lines)):\n",
    "            plt.vlines(\n",
    "                lines[i], 0, np.max(s_over_root_b_points), \n",
    "                label='s/âb'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                alpha=0.5, colors=line_colors[i]\n",
    "            )\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Output score')\n",
    "    plt.ylabel('s/âb')\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(sigs, bkgs, weights, bins=50):\n",
    "    hist_list_fold = []\n",
    "    cut_boundaries_fold = []\n",
    "    cut_s_over_root_bs_fold = []\n",
    "    sig_weights_fold = []\n",
    "    bkg_weights_fold = []\n",
    "    if len(np.shape(sigs)) == 1:\n",
    "        sigs, bkgs = [sigs], [bkgs] \n",
    "    for sig, bkg in zip(sigs, bkgs):\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg, weight=weights['bkg'])\n",
    "        hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "        fold_idx_cuts_bins_inclusive = []\n",
    "        fold_idx_sig_weights = []\n",
    "        fold_idx_bkg_weights = []\n",
    "        fold_idx_prev_s_over_root_b = []\n",
    "        prev_s_over_root_b = 0\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                fold_idx_sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        fold_idx_sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_cuts_bins_inclusive.append(0)\n",
    "        fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "        fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "        cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "        cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "        sig_weights_fold.append(fold_idx_sig_weights)\n",
    "        bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "    return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"losses\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "if 'evals_result_dict' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_eval_result.json\"), 'r') as f:\n",
    "        evals_result_dict = json.load(f)\n",
    "\n",
    "# plot train/val/test losses\n",
    "all_train, all_val, all_test = [], [], []\n",
    "for fold_idx in range(len(evals_result_dict)):\n",
    "    all_train.append(evals_result_dict[f\"fold_{fold_idx}\"]['train']['mlogloss'])\n",
    "    all_val.append(evals_result_dict[f\"fold_{fold_idx}\"]['val']['mlogloss'])\n",
    "    all_test.append(evals_result_dict[f\"fold_{fold_idx}\"]['test']['mlogloss'])\n",
    "\n",
    "plot_train_val_losses(\n",
    "    all_train + all_val, [f'train fold {i}' for i in range(len(all_train))]+[f'val fold {i}' for i in range(len(all_val))],\n",
    "    'train_val_losses_vs_epoch', plot_dirpath, \n",
    "    linestyles=['solid']*len(all_train) + ['dashed']*len(all_val),\n",
    ")\n",
    "plot_train_val_losses(\n",
    "    all_train + all_test, [f'train fold {i}' for i in range(len(all_train))]+[f'test fold {i}' for i in range(len(all_test))],\n",
    "    'train_test_losses_vs_epoch', plot_dirpath,\n",
    "    linestyles=['solid']*len(all_train) + ['dotted']*len(all_test),\n",
    ")\n",
    "avg_train, avg_val, avg_test = np.mean(pad_list(all_train), axis=0), np.mean(pad_list(all_val), axis=0), np.mean(pad_list(all_test), axis=0)\n",
    "std_train, std_val, std_test = np.std(pad_list(all_train), axis=0), np.std(pad_list(all_val), axis=0), np.std(pad_list(all_test), axis=0)\n",
    "plot_train_val_losses(\n",
    "    [avg_train, avg_val, avg_test], ['train avg', 'val avg', 'test avg'],\n",
    "    'train_val_test_avg_vs_epoch', plot_dirpath,\n",
    "    losses_std_arrs=[std_train, std_val, std_test],\n",
    "    linestyles=['solid', 'dashed', 'dotted'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"ROCs\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "base_tpr = np.array(BDT_perf['ggF HH']['base_tpr'])\n",
    "\n",
    "# plot ROCs\n",
    "for j, sample_name in enumerate(order):\n",
    "\n",
    "    for fold_idx in range(len(bdt_train_dict)):\n",
    "\n",
    "        for roc_type in ['density', 'weighted']:\n",
    "\n",
    "            fprs = [np.array(BDT_perf[sample_name][f'fprs_{roc_type}'][fold_idx])[:, i] for i in range(len(order))]\n",
    "            tprs = [base_tpr for _ in range(len(order))]\n",
    "            labels = [\n",
    "                f\"{sample_name} vs. {'all' if i == j else sample_name_}, AUC = {BDT_perf[sample_name][f'areas_{roc_type}'][fold_idx][i]:.4f}\" \n",
    "                for i, sample_name_ in enumerate(order)\n",
    "            ]\n",
    "\n",
    "            plot_rocs(fprs, tprs, labels, f\"BDT_roc_{sample_name}_{roc_type}_testData_fold{fold_idx}\", plot_dirpath)\n",
    "\n",
    "    for roc_type in ['sum_density', 'sum_weighted']:\n",
    "\n",
    "        fprs = [np.array(BDT_perf[sample_name][f'fprs_{roc_type}'])[:, i] for i in range(len(order))]\n",
    "        tprs = [base_tpr for _ in range(len(order))]\n",
    "        labels = [\n",
    "            f\"{sample_name} vs. {'all' if i == j else sample_name_}, AUC = {BDT_perf[sample_name][f'areas_{roc_type}'][i]:.4f}\" \n",
    "            for i, sample_name_ in enumerate(order)\n",
    "        ]\n",
    "\n",
    "        plot_rocs(fprs, tprs, labels, f\"BDT_roc_{sample_name}_{roc_type}_testData_sum\", plot_dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"output_scores\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "# plot Output scores\n",
    "for j, sample_name in enumerate(order):\n",
    "\n",
    "    for i, sample_name_ in enumerate(order):\n",
    "\n",
    "        for fold_idx in range(len(bdt_train_dict)):\n",
    "            \n",
    "            sigs_and_bkgs = {\n",
    "                sample_name__: np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j][bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == k]\n",
    "                for k, sample_name__ in enumerate(order)\n",
    "            }\n",
    "            score_weights = {\n",
    "                sample_name__: weight_test_dict[f\"fold_{fold_idx}\"][bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == k]\n",
    "                for k, sample_name__ in enumerate(order)\n",
    "            }\n",
    "\n",
    "            if sample_name_ != sample_name:\n",
    "                event_j_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == j\n",
    "                pred_j_rescale = np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j][event_j_mask] + np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, i][event_j_mask]\n",
    "                event_i_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == i\n",
    "                pred_i_rescale = np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j][event_i_mask] + np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, i][event_i_mask]\n",
    "\n",
    "                for sample_name__ in order:\n",
    "                    if sample_name__ == sample_name:\n",
    "                        sigs_and_bkgs[sample_name__] = sigs_and_bkgs[sample_name__] / pred_j_rescale\n",
    "                    elif sample_name__ == sample_name_:\n",
    "                        sigs_and_bkgs[sample_name__] = sigs_and_bkgs[sample_name__] / pred_i_rescale\n",
    "                    else:\n",
    "                        del sigs_and_bkgs[sample_name__]\n",
    "                        del score_weights[sample_name__]\n",
    "\n",
    "            plot_output_scores(\n",
    "                sigs_and_bkgs, order, \n",
    "                f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_outputScoreWeighted_testData_fold{fold_idx}\", \n",
    "                plot_dirpath, weights=score_weights, log=True\n",
    "            )\n",
    "            plot_output_scores(\n",
    "                sigs_and_bkgs, order, \n",
    "                f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_outputScoreDensity_testData_fold{fold_idx}\", \n",
    "                plot_dirpath\n",
    "            )\n",
    "\n",
    "        flat_preds = np.concatenate([BDT_perf[sample_name]['preds'][fold_idx] for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "        flat_truths = np.concatenate([bdt_test_dict[f\"fold_{fold_idx}\"].get_label() for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "        flat_weights = np.concatenate([weight_test_dict[f\"fold_{fold_idx}\"] for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "\n",
    "        sigs_and_bkgs = {\n",
    "            sample_name__: flat_preds[:, j][flat_truths == k]\n",
    "            for k, sample_name__ in enumerate(order)\n",
    "        }\n",
    "        score_weights = {\n",
    "            sample_name__: flat_weights[flat_truths == k]\n",
    "            for k, sample_name__ in enumerate(order)\n",
    "        }\n",
    "        \n",
    "        if sample_name_ != sample_name:\n",
    "            event_j_mask = flat_truths == j\n",
    "            pred_j_rescale = (flat_preds[:, j] + flat_preds[:, i])[event_j_mask]\n",
    "            event_i_mask = flat_truths == i\n",
    "            pred_i_rescale = (flat_preds[:, j] + flat_preds[:, i])[event_i_mask]\n",
    "\n",
    "            for sample_name__ in order:\n",
    "                if sample_name__ == sample_name:\n",
    "                    sigs_and_bkgs[sample_name__] = sigs_and_bkgs[sample_name__] / pred_j_rescale\n",
    "                elif sample_name__ == sample_name_:\n",
    "                    sigs_and_bkgs[sample_name__] = sigs_and_bkgs[sample_name__] / pred_i_rescale\n",
    "                else:\n",
    "                    del sigs_and_bkgs[sample_name__]\n",
    "                    del score_weights[sample_name__]\n",
    "        \n",
    "        plot_output_scores(\n",
    "            sigs_and_bkgs, order, \n",
    "            f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_outputScoreWeighted_testData_sum\", \n",
    "            plot_dirpath, weights=score_weights, log=True\n",
    "        )\n",
    "        plot_output_scores(\n",
    "            sigs_and_bkgs, order, \n",
    "            f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_outputScoreDensity_testData_sum\", \n",
    "            plot_dirpath\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:24: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/4090294250.py:24: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/4090294250.py:24: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/4090294250.py:24: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/4090294250.py:24: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/4090294250.py:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  if prev_s_over_root_b < (s / sqrt_b):\n",
      "/tmp/ipykernel_880118/4090294250.py:28: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  prev_s_over_root_b = s / sqrt_b\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_880118/12448675.py:139: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    }
   ],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"s_over_rootb\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "if 'BDT_perf' not in globals():\n",
    "    with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "        BDT_perf = json.load(f)\n",
    "\n",
    "# plot s/âb curves\n",
    "for j, sample_name in enumerate(order):\n",
    "\n",
    "    for i, sample_name_ in enumerate(order):\n",
    "\n",
    "        for fold_idx in range(len(bdt_train_dict)):\n",
    "\n",
    "            if sample_name_ == sample_name:\n",
    "                sig_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == j\n",
    "                bkg_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() != j\n",
    "\n",
    "                sig_rescale = np.ones_like(sig_mask)\n",
    "                bkg_rescale = np.ones_like(bkg_mask)\n",
    "            else:\n",
    "                sig_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == j\n",
    "                bkg_mask = bdt_test_dict[f\"fold_{fold_idx}\"].get_label() == i\n",
    "\n",
    "                sig_rescale = (\n",
    "                    np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j] \n",
    "                    + np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, i]\n",
    "                )\n",
    "                bkg_rescale = (\n",
    "                    np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j] \n",
    "                    + np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, i]\n",
    "                )\n",
    "\n",
    "            sigs_and_bkgs = {\n",
    "                'sig': (np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j] / sig_rescale)[sig_mask],\n",
    "                'bkg': (np.array(BDT_perf[sample_name]['preds'][fold_idx])[:, j] / bkg_rescale)[bkg_mask]\n",
    "            }\n",
    "            score_weights = {\n",
    "                'sig': weight_test_dict[f\"fold_{fold_idx}\"][sig_mask],\n",
    "                'bkg': weight_test_dict[f\"fold_{fold_idx}\"][bkg_mask]\n",
    "            }\n",
    "\n",
    "            plot_s_over_root_b(\n",
    "                sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], f\"{sample_name} vs. {sample_name_ if sample_name_ != sample_name else 'all'}\", \n",
    "                f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_sOverRootb_testData_fold{fold_idx}\", \n",
    "                plot_dirpath, weights=score_weights\n",
    "            )\n",
    "\n",
    "            (\n",
    "                cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "            ) = optimize_cut_boundaries(\n",
    "                sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], score_weights\n",
    "            )\n",
    "\n",
    "            BDT_cut_labels = [\n",
    "                f\"s/âb={cut_s_over_root_bs_fold[0][cut_idx]:.4f}, s={sig_weights_fold[0][cut_idx]['value']:.4f}Â±{sig_weights_fold[0][cut_idx]['w2']:.4f}, b={bkg_weights_fold[0][cut_idx]['value']:.4f}Â±{bkg_weights_fold[0][cut_idx]['w2']:.4f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[0]))\n",
    "            ]\n",
    "            line_labels = BDT_cut_labels[:10]\n",
    "            lines = cut_boundaries_fold[0][:10]\n",
    "            line_colors = cmap_petroff10\n",
    "\n",
    "            plot_s_over_root_b(\n",
    "                sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], f\"{sample_name} vs. {sample_name_ if sample_name_ != sample_name else 'all'}\", \n",
    "                f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_sOverRootb_withCuts_testData_fold{fold_idx}_{sample_name}\", plot_dirpath, \n",
    "                weights=score_weights,\n",
    "                lines=lines, lines_labels=line_labels, line_colors=line_colors\n",
    "            )\n",
    "            \n",
    "\n",
    "        flat_preds = np.concatenate([BDT_perf[sample_name]['preds'][fold_idx] for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "        flat_truths = np.concatenate([bdt_test_dict[f\"fold_{fold_idx}\"].get_label() for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "        flat_weights = np.concatenate([weight_test_dict[f\"fold_{fold_idx}\"] for fold_idx in range(len(bdt_test_dict))], axis=0)\n",
    "\n",
    "        if sample_name_ == sample_name:\n",
    "            sig_mask = flat_truths == j\n",
    "            bkg_mask = flat_truths != j\n",
    "\n",
    "            sig_rescale = np.ones_like(sig_mask)\n",
    "            bkg_rescale = np.ones_like(bkg_mask)\n",
    "        else:\n",
    "            sig_mask = flat_truths == j\n",
    "            bkg_mask = flat_truths == i\n",
    "\n",
    "            sig_rescale = flat_preds[:, j] + flat_preds[:, i]\n",
    "            bkg_rescale = flat_preds[:, j] + flat_preds[:, i]\n",
    "\n",
    "        sigs_and_bkgs = {\n",
    "            'sig': (flat_preds[:, j] / sig_rescale)[sig_mask],\n",
    "            'bkg': (flat_preds[:, j] / bkg_rescale)[bkg_mask]\n",
    "        }\n",
    "        score_weights = {\n",
    "            'sig': flat_weights[sig_mask],\n",
    "            'bkg': flat_weights[bkg_mask]\n",
    "        }\n",
    "\n",
    "        plot_s_over_root_b(\n",
    "            sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], f\"{sample_name} vs. {sample_name_ if sample_name_ != sample_name else 'all'}\", \n",
    "            f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_sOverRootb_testData_sum\", \n",
    "            plot_dirpath, weights=score_weights\n",
    "        )\n",
    "\n",
    "        (\n",
    "            cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "        ) = optimize_cut_boundaries(\n",
    "            sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], score_weights\n",
    "        )\n",
    "\n",
    "        BDT_cut_labels = [\n",
    "            f\"s/âb={cut_s_over_root_bs_fold[0][cut_idx]:.4f}, s={sig_weights_fold[0][cut_idx]['value']:.4f}Â±{sig_weights_fold[0][cut_idx]['w2']:.4f}, b={bkg_weights_fold[0][cut_idx]['value']:.4f}Â±{bkg_weights_fold[0][cut_idx]['w2']:.4f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[0]))\n",
    "        ]\n",
    "        line_labels = BDT_cut_labels[:10]\n",
    "        lines = cut_boundaries_fold[0][:10]\n",
    "        line_colors = cmap_petroff10\n",
    "\n",
    "        plot_s_over_root_b(\n",
    "            sigs_and_bkgs['sig'], sigs_and_bkgs['bkg'], f\"{sample_name} vs. {sample_name_ if sample_name_ != sample_name else 'all'}\", \n",
    "            f\"BDT_{sample_name}_vs_{sample_name_ if sample_name_ != sample_name else 'all'}_sOverRootb_withCuts_testData_sum\", plot_dirpath, \n",
    "            weights=score_weights,\n",
    "            lines=lines, lines_labels=line_labels, line_colors=line_colors\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\", \"variable_importance\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "for fold_idx in range(len(bdt_train_dict)):\n",
    "    booster = xgb.Booster(param)\n",
    "    booster.load_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "    \n",
    "    xgb.plot_importance(booster)\n",
    "    plt.savefig(\n",
    "        plot_filepath(f'xgb_importance_fold{fold_idx}', plot_dirpath, '', ''),\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(f'xgb_importance_fold{fold_idx}', plot_dirpath, '', '', format='pdf'),\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_NAMES_PRETTY = {\n",
    "#     \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "#     \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "#     \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "#     \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "#     \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "#     \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "#     \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "#     \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "#     \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "#     # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "#     # Need to fill in pretty print for BSM samples #\n",
    "# }\n",
    "# LUMINOSITIES = {\n",
    "#     '2022preEE': 7.9804, \n",
    "#     '2022postEE': 26.6717,\n",
    "#     # Need to fill in lumis for other eras #\n",
    "# }\n",
    "# LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# # Dictionary of variables\n",
    "# VARIABLES = {\n",
    "#     # key: hist.axis axes for plotting #\n",
    "#     # MET variables\n",
    "#     'puppiMET_sumEt': hist.axis.Regular(40, 150., 2000, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-MET variables\n",
    "#     'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "#     'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-photon variables\n",
    "#     'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet variables\n",
    "#     # 'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     # 'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     'lead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     'lead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "#     'lead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     'sublead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     'sublead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "#     'sublead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "#     'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "#     'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "#     # lepton variables\n",
    "#     'n_leptons': hist.axis.Integer(0, 10, name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "#     'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "#     'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     # diphoton variables\n",
    "#     'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "#     'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     # angular (cos) variables\n",
    "#     'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "#     'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "#     'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "#     'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-lepton variables\n",
    "#     'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "#     # dijet variables (must be blinded on data)\n",
    "#     'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     # diphoton variables (must be blinded on data)\n",
    "#     'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "# }\n",
    "# # Dictionary of variables to do MC/Data comparison\n",
    "# VARIABLES_STD = {\n",
    "#     # key: hist.axis axes for plotting #\n",
    "#     # MET variables\n",
    "#     'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($\\Sigma E_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-MET variables\n",
    "#     'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "#     'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-photon variables\n",
    "#     'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet variables\n",
    "#     'lead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'sublead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'lead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "#     'sublead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "#     'lead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "#     'sublead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     'n_jets': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "#     'chi_t0': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t0}^2$)', growth=False, underflow=False, overflow=False), \n",
    "#     'chi_t1': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t1}^2$)', growth=False, underflow=False, overflow=False), \n",
    "#     # lepton variables\n",
    "#     'n_leptons': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "#     'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "#     'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     # diphoton variables\n",
    "#     'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma$ ln($p_{T}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "#     'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     # angular (cos) variables\n",
    "#     'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "#     'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "#     'CosThetaStar_CS': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "#     'CosThetaStar_jj': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-lepton variables\n",
    "#     'leadBjet_leadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'leadBjet_subleadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'subleadBjet_leadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'subleadBjet_subleadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "#     # dijet variables (must be blinded on data)\n",
    "#     'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'ln($M_{jj}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     # diphoton variables (must be blinded on data)\n",
    "#     'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "# }\n",
    "\n",
    "# def post_std_np_arrays(\n",
    "#         data, data_test, fold, var_name, train_index=None, val_index=None\n",
    "# ):\n",
    "#     sig_mask = label_dict[f'fold_{fold}'] == 1\n",
    "#     sig_test_mask = label_test_dict[f'fold_{fold}'] == 1\n",
    "#     bkg_mask = label_dict[f'fold_{fold}'] == 0\n",
    "#     bkg_test_mask = label_test_dict[f'fold_{fold}'] == 0\n",
    "#     if train_index is not None and val_index is not None:\n",
    "#         sig_train_mask = sig_mask & train_index \n",
    "#         sig_val_mask = sig_mask & val_index\n",
    "#         bkg_train_mask = bkg_mask & train_index\n",
    "#         bkg_val_mask = bkg_mask & val_index\n",
    "#         if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "#             sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             bkg_train_np = data[data_list_index_map(var_name, data, bkg_train_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#         else:\n",
    "#             index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "#             sig_train_np = data[sig_train_mask, index2]\n",
    "#             sig_val_np = data[sig_val_mask, index2]\n",
    "#             sig_test_np = data_test[sig_test_mask, index2]\n",
    "#             bkg_train_np = data[bkg_train_mask, index2]\n",
    "#             bkg_val_np = data[bkg_val_mask, index2]\n",
    "#             bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "#         return (\n",
    "#             sig_train_np, sig_val_np, sig_test_np, \n",
    "#             bkg_train_np, bkg_val_np, bkg_test_np\n",
    "#         )\n",
    "#     elif train_index is None and val_index is None:\n",
    "#         if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "#             # index2, index3 = index_map[var_name]\n",
    "#             sig_train_np = data[data_list_index_map(var_name, data, sig_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#         else:\n",
    "#             index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "#             sig_train_np = data[sig_mask, index2]\n",
    "#             sig_test_np = data_test[sig_test_mask, index2]\n",
    "#             bkg_train_np = data[bkg_mask, index2]\n",
    "#             bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "#         return (\n",
    "#             copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "#             copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "#         )\n",
    "#     else:\n",
    "#         raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "# def aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold):\n",
    "#     sig_train_mask = (label_dict[f'fold_{fold}'] == 1) & (\n",
    "#         np.exp(IN_full_eval_dict['train']['all_preds'][fold])[:, 1] > score_cut\n",
    "#     )\n",
    "#     sig_test_mask = (label_test_dict[f'fold_{fold}'] == 1) & (\n",
    "#         np.exp(IN_full_eval_dict['test']['all_preds'][fold])[:, 1] > score_cut\n",
    "#     )\n",
    "#     bkg_train_mask = (label_dict[f'fold_{fold}'] == 0) & (\n",
    "#         np.exp(IN_full_eval_dict['train']['all_preds'][fold])[:, 1] > score_cut\n",
    "#     )\n",
    "#     bkg_test_mask = (label_test_dict[f'fold_{fold}'] == 0) & (\n",
    "#         np.exp(IN_full_eval_dict['test']['all_preds'][fold])[:, 1] > score_cut\n",
    "#     )\n",
    "\n",
    "#     sig_train_np = data_aux_dict[f'fold_{fold}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "#     sig_test_np = data_test_aux_dict[f'fold_{fold}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "#     bkg_train_np = data_aux_dict[f'fold_{fold}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "#     bkg_test_np = data_test_aux_dict[f'fold_{fold}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "#     return (\n",
    "#         copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "#         copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "#     )\n",
    "\n",
    "# def make_input_plot(\n",
    "#     output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, \n",
    "#     plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True\n",
    "# ):\n",
    "#     fig, ax = plt.subplots()\n",
    "#     if linestyle:\n",
    "#         if fold_idx is not None:\n",
    "#             linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "#         else:\n",
    "#             linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "#         linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "#         linestyles = linestyles[:len(hist_list)]\n",
    "#     else:\n",
    "#         linestyles = None\n",
    "#     hep.histplot(\n",
    "#         hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "#         linestyle=linestyles, label=labels, alpha=alpha\n",
    "#     )\n",
    "#     # Plotting niceties #\n",
    "#     hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "#     hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "#     # Plot legend properly\n",
    "#     ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "#     # Make angular and chi^2 plots linear, otherwise log\n",
    "#     if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "#         ax.set_yscale('log')\n",
    "#     else:\n",
    "#         ax.set_yscale('linear')\n",
    "#     ax.set_yscale('linear')\n",
    "#     # Save out the plot\n",
    "#     if fold_idx is not None:\n",
    "#         output_dir_ = os.path.join(output_dir, \"fold\")\n",
    "#         if not os.path.exists(output_dir_):\n",
    "#             os.makedirs(output_dir_)\n",
    "#         plt.savefig(f'{output_dir_}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "#         plt.savefig(f'{output_dir_}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "#     else:\n",
    "#         plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "#         plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "# def plot_input_vars_after_score_cut(\n",
    "#     IN_info, score_cut, destdir, fold, plot_prefix, plot_postfix='', method='std', \n",
    "#     weights={'sig': None, 'bkg': None}, all_sig=False, all_bkg=False,\n",
    "#     mask=None\n",
    "# ):\n",
    "#     if method == 'std':\n",
    "#         if mask is None:\n",
    "#             mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "#         sig_mask = np.exp(\n",
    "#             IN_info['mean_pred']\n",
    "#         )[\n",
    "#             np.logical_and(\n",
    "#                 np.array(IN_info['mean_label']) == 1, mask\n",
    "#             ),1\n",
    "#         ] > score_cut\n",
    "#         bkg_mask = np.exp(\n",
    "#             IN_info['mean_pred']\n",
    "#         )[\n",
    "#             np.logical_and(\n",
    "#                 np.array(IN_info['mean_label']) == 0, mask\n",
    "#             ),1\n",
    "#         ] > score_cut\n",
    "\n",
    "#         for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "#             if var_name in {'event', 'puppiMET_eta'}:\n",
    "#                 continue\n",
    "#             sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "#             sig_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                 var=sig_var.loc[sig_mask], \n",
    "#                 weight=weights['sig'][sig_mask] if weights['sig'] is not None else np.ones(np.sum(sig_mask))\n",
    "#             )\n",
    "#             bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "#             bkg_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                 var=bkg_var.loc[bkg_mask], \n",
    "#                 weight=weights['bkg'][bkg_mask] if weights['bkg'] is not None else np.ones(np.sum(bkg_mask))\n",
    "#             )\n",
    "#             make_input_plot(\n",
    "#                 destdir, var_name, [sig_hist, bkg_hist], fold, plot_prefix=plot_prefix, \n",
    "#                 plot_postfix=plot_postfix+f'_ttHscore{score_cut}', labels=['HH signal', 'ttH background'], density=False if weights['sig'] is not None else True\n",
    "#             )\n",
    "#     elif method == 'round_robin':\n",
    "#         if mask is None:\n",
    "#             mask = np.ones(np.shape(IN_info['all_preds'][fold])[0], dtype=bool)\n",
    "        \n",
    "#         for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "#             if var_name in {'event', 'puppiMET_eta'}:\n",
    "#                 continue\n",
    "#             sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "#             bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "#             sig_masks, bkg_masks = [], []\n",
    "#             hists, labels = [], []\n",
    "#             for cut in score_cut if score_cut is list else [score_cut]:\n",
    "#                 sig_masks.append(np.exp(\n",
    "#                     IN_info['all_preds'][fold]\n",
    "#                 )[\n",
    "#                     np.logical_and(\n",
    "#                         np.array(IN_info['all_labels'][fold]) == 1, mask\n",
    "#                     ),1\n",
    "#                 ] > cut)\n",
    "#                 bkg_masks.append(np.exp(\n",
    "#                     IN_info['all_preds'][fold]\n",
    "#                 )[\n",
    "#                     np.logical_and(\n",
    "#                         np.array(IN_info['all_labels'][fold]) == 0, mask\n",
    "#                     ),1\n",
    "#                 ] > cut)\n",
    "                \n",
    "#                 hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                     var=sig_var.loc[sig_masks[-1]], \n",
    "#                     weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "#                 ))\n",
    "#                 hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                     var=bkg_var.loc[bkg_masks[-1]], \n",
    "#                     weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "#                 ))\n",
    "#                 labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "#             make_input_plot(\n",
    "#                 destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "#                 plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "#             )\n",
    "#     elif method == 'arr':\n",
    "#         if mask is None:\n",
    "#             mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "#         for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "#             if var_name in {'event', 'puppiMET_eta'}:\n",
    "#                 continue\n",
    "#             sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "#             bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "#             sig_masks, bkg_masks = [], []\n",
    "#             hists, labels = [], []\n",
    "#             for cut in score_cut:\n",
    "#                 sig_masks.append(np.exp(\n",
    "#                     IN_info['mean_pred']\n",
    "#                 )[\n",
    "#                     np.logical_and(\n",
    "#                         np.array(IN_info['mean_label']) == 1, mask\n",
    "#                     ),1\n",
    "#                 ] > cut)\n",
    "#                 bkg_masks.append(np.exp(\n",
    "#                     IN_info['mean_pred']\n",
    "#                 )[\n",
    "#                     np.logical_and(\n",
    "#                         np.array(IN_info['mean_label']) == 0, mask\n",
    "#                     ),1\n",
    "#                 ] > cut)\n",
    "                \n",
    "#                 hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                     var=sig_var.loc[sig_masks[-1]], \n",
    "#                     weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "#                 ))\n",
    "#                 hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                     var=bkg_var.loc[bkg_masks[-1]], \n",
    "#                     weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "#                 ))\n",
    "#                 labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "#             make_input_plot(\n",
    "#                 destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "#                 plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "#             )\n",
    "#     else:\n",
    "#         raise Exception(f\"Must used method 'std'. You used {method}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
