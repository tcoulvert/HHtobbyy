{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu3.fnal.gov      Sun Nov 17 12:34:40 2024  555.42.06\n",
      "[0] Tesla P100-PCIE-12GB | 55Â°C,  98 % |  4362 / 12288 MB | ckapsiak(4360M)\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import h5py\n",
    "import hist\n",
    "import mplhep as hep\n",
    "import xgboost as xgb\n",
    "from cycler import cycler\n",
    "\n",
    "\n",
    "# ML packages\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from scipy.integrate import trapezoid\n",
    "\n",
    "# Module packages\n",
    "from data_processing_BDT import process_data\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "FILEPATHS_DICT = {\n",
    "    'ggF HH': [\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GluGluToHH/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GluGluToHH/nominal/*\"\n",
    "    ],\n",
    "    # 'VBF HH': [\n",
    "    #     lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", \n",
    "    #     lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\"\n",
    "    # ],\n",
    "    'ttH': [\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/ttHToGG/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/ttHToGG/nominal/*\"\n",
    "    ],\n",
    "    'non-res + single-H': [\n",
    "        # non-Resonant #\n",
    "        # GG + 3Jets\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GGJets/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GGJets/nominal/*\",\n",
    "        # GJet pT 20-40\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GJetPt20To40/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GJetPt20To40/nominal/*\",\n",
    "        # GJet pT 40-inf\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GJetPt40/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GJetPt40/nominal/*\",\n",
    "        # single-H #\n",
    "        # ggF H\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/GluGluHToGG/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/GluGluHToGG/nominal/*\",\n",
    "        # VBF H\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/VBFHToGG/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/VBFHToGG/nominal/*\",\n",
    "        # VH\n",
    "        lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/VHToGG/nominal/*\", \n",
    "        lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/VHToGG/nominal/*\",\n",
    "    ],\n",
    "    # 'VH': [\n",
    "    #     lpc_fileprefix+f\"/Run3_2022preEE_merged_v3/VHToGG/nominal/*\", \n",
    "    #     lpc_fileprefix+f\"/Run3_2022postEE_merged_v3/VHToGG/nominal/*\"\n",
    "    # ],\n",
    "}\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v1'\n",
    "MOD_VALS = (5, 5)\n",
    "VARS = 'nonres_and_ttH_vars'\n",
    "# CURRENT_TIME = '2024-11-08_13-13-20'\n",
    "CURRENT_TIME = '2024-11-16_13-10-23'\n",
    "if 'CURRENT_TIME' in globals():\n",
    "    OUTPUT_DIRPATH = os.path.join(CURRENT_DIRPATH, f\"MultiClassBDT_model_outputs/{VERSION}/{VARS}\", CURRENT_TIME)\n",
    "else:\n",
    "    OUTPUT_DIRPATH = os.path.join(CURRENT_DIRPATH, f\"MultiClassBDT_model_outputs/{VERSION}/{VARS}\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_weights(event_weights, labels, order=None):\n",
    "    if order is not None:\n",
    "        sig_idx = -1\n",
    "        for i, sample_name in enumerate(order):\n",
    "            if re.search('ggF HH', sample_name) is not None:\n",
    "                sig_idx = i\n",
    "                break\n",
    "    else:\n",
    "        sig_idx = 0\n",
    "    \n",
    "    sig_sum = np.sum(event_weights[labels[:, sig_idx] == 1])\n",
    "    bkg_sum = np.sum(event_weights[labels[:, sig_idx] == 0])\n",
    "    \n",
    "    sig_scale_factor = bkg_sum / sig_sum\n",
    "\n",
    "    scaled_weights = np.where(\n",
    "        labels[:, sig_idx] == 0, \n",
    "        event_weights,  # if bkg, do nothing\n",
    "        event_weights * sig_scale_factor  # if sig, rescale to equal sum of all bkgs\n",
    "    )\n",
    "\n",
    "    abs_weights = np.abs(scaled_weights)\n",
    "\n",
    "    return abs_weights\n",
    "\n",
    "def xgb_labels(labels):\n",
    "    label_i = np.sum(\n",
    "        np.tile([i for i in range(np.shape(labels)[1])], (np.shape(labels)[0], 1)) * labels,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return label_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data HLF: (1563192, 48)\n",
      "num ggF HH = 136530\n",
      "num ttH = 277205\n",
      "num non-res + single-H = 1149457\n",
      "Data HLF test: (391785, 48)\n",
      "num ggF HH = 34224\n",
      "num ttH = 69297\n",
      "num non-res + single-H = 288264\n",
      "Data HLF: (1564171, 48)\n",
      "num ggF HH = 136466\n",
      "num ttH = 277452\n",
      "num non-res + single-H = 1150253\n",
      "Data HLF test: (390806, 48)\n",
      "num ggF HH = 34288\n",
      "num ttH = 69050\n",
      "num non-res + single-H = 287468\n",
      "Data HLF: (1563685, 48)\n",
      "num ggF HH = 136638\n",
      "num ttH = 276627\n",
      "num non-res + single-H = 1150420\n",
      "Data HLF test: (391292, 48)\n",
      "num ggF HH = 34116\n",
      "num ttH = 69875\n",
      "num non-res + single-H = 287301\n",
      "Data HLF: (1564419, 48)\n",
      "num ggF HH = 136671\n",
      "num ttH = 277054\n",
      "num non-res + single-H = 1150694\n",
      "Data HLF test: (390558, 48)\n",
      "num ggF HH = 34083\n",
      "num ttH = 69448\n",
      "num non-res + single-H = 287027\n",
      "Data HLF: (1564441, 48)\n",
      "num ggF HH = 136711\n",
      "num ttH = 277670\n",
      "num non-res + single-H = 1150060\n",
      "Data HLF test: (390536, 48)\n",
      "num ggF HH = 34043\n",
      "num ttH = 68832\n",
      "num non-res + single-H = 287661\n"
     ]
    }
   ],
   "source": [
    "order = ['ggF HH', 'ttH', 'non-res + single-H']\n",
    "\n",
    "(\n",
    "    data_df_dict, data_test_df_dict, \n",
    "    data_hlf_dict, label_dict, \n",
    "    data_hlf_test_dict, label_test_dict, \n",
    "    hlf_vars_columns_dict,\n",
    "    data_aux_dict, data_test_aux_dict\n",
    ") = process_data(\n",
    "    FILEPATHS_DICT, OUTPUT_DIRPATH, order=order, seed=SEED, mod_vals=MOD_VALS, k_fold_test=True,\n",
    "    save=False if 'CURRENT_TIME' in globals() else True\n",
    ")\n",
    "\n",
    "xgb_label_dict = {\n",
    "    f\"fold_{fold_idx}\": copy.deepcopy(xgb_labels(label_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "xgb_label_test_dict = {\n",
    "    f\"fold_{fold_idx}\": copy.deepcopy(xgb_labels(label_test_dict[f\"fold_{fold_idx}\"])) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "\n",
    "weight_train_dict = {\n",
    "    f\"fold_{fold_idx}\": copy.deepcopy(training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}'], order=order)) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy()) for fold_idx in range(len(data_test_aux_dict))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Num train: 1250553 -> 109096 sig & 222178 ttH bkg & 919279 non-res + single-H bkg\n",
      "Num val: 312639 -> 27434 sig & 55027 ttH bkg & 230178 non-res + single-H bkg\n",
      "Num test: 391785 -> 34224 sig & 69297 ttH bkg & 288264 non-res + single-H bkg\n",
      "============================================================\n",
      "fold 1\n",
      "Num train: 1251336 -> 109298 sig & 221622 ttH bkg & 920416 non-res + single-H bkg\n",
      "Num val: 312835 -> 27168 sig & 55830 ttH bkg & 229837 non-res + single-H bkg\n",
      "Num test: 390806 -> 34288 sig & 69050 ttH bkg & 287468 non-res + single-H bkg\n",
      "============================================================\n",
      "fold 2\n",
      "Num train: 1250948 -> 109478 sig & 221207 ttH bkg & 920263 non-res + single-H bkg\n",
      "Num val: 312737 -> 27160 sig & 55420 ttH bkg & 230157 non-res + single-H bkg\n",
      "Num test: 391292 -> 34116 sig & 69875 ttH bkg & 287301 non-res + single-H bkg\n",
      "============================================================\n",
      "fold 3\n",
      "Num train: 1251535 -> 109266 sig & 221688 ttH bkg & 920581 non-res + single-H bkg\n",
      "Num val: 312884 -> 27405 sig & 55366 ttH bkg & 230113 non-res + single-H bkg\n",
      "Num test: 390558 -> 34083 sig & 69448 ttH bkg & 287027 non-res + single-H bkg\n",
      "============================================================\n",
      "fold 4\n",
      "Num train: 1251552 -> 109362 sig & 221957 ttH bkg & 920233 non-res + single-H bkg\n",
      "Num val: 312889 -> 27349 sig & 55713 ttH bkg & 229827 non-res + single-H bkg\n",
      "Num test: 390536 -> 34043 sig & 68832 ttH bkg & 287661 non-res + single-H bkg\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "bdt_train_dict, bdt_val_dict, bdt_test_dict = {}, {}, {}\n",
    "\n",
    "weights_plot_train, weights_plot_val = {}, {}\n",
    "for fold_idx in range(len(data_df_dict)):\n",
    "    if re.search('no_std', VARS) is not None:\n",
    "        print('no standardization')\n",
    "        train_val_data_dict = {key: value.to_numpy() for key, value in data_df_dict.items()}\n",
    "        test_data_dict = {key: value.to_numpy() for key, value in data_test_df_dict.items()}\n",
    "    else:\n",
    "        train_val_data_dict = data_hlf_dict\n",
    "        test_data_dict = data_hlf_test_dict\n",
    "    (\n",
    "        X_train, X_val, y_train, y_val, weight_train, weight_val\n",
    "    ) = train_test_split(\n",
    "        train_val_data_dict[f\"fold_{fold_idx}\"], xgb_label_dict[f\"fold_{fold_idx}\"], \n",
    "        weight_train_dict[f\"fold_{fold_idx}\"],\n",
    "        test_size=0.2, random_state=21\n",
    "    )\n",
    "    weights_plot_train[f\"fold_{fold_idx}\"] = copy.deepcopy(weight_train)\n",
    "    weights_plot_val[f\"fold_{fold_idx}\"] = copy.deepcopy(weight_val)\n",
    "\n",
    "    bdt_train_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "        data=X_train, label=y_train, \n",
    "        weight=weight_train,\n",
    "        missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "    )\n",
    "    bdt_val_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "        data=X_val, label=y_val, \n",
    "        weight=weight_val,\n",
    "        missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "    )\n",
    "    \n",
    "    bdt_test_dict[f\"fold_{fold_idx}\"] = xgb.DMatrix(\n",
    "        data=test_data_dict[f\"fold_{fold_idx}\"], label=xgb_label_test_dict[f\"fold_{fold_idx}\"], \n",
    "        weight=np.abs(weight_test_dict[f\"fold_{fold_idx}\"]),\n",
    "        missing=-999.0, feature_names=list(hlf_vars_columns_dict[f\"fold_{fold_idx}\"])\n",
    "    )\n",
    "\n",
    "    print(f\"fold {fold_idx}\")\n",
    "    print(f\"Num train: {len(y_train)} -> {sum(y_train == 0)} sig & {sum(y_train == 1)} ttH bkg & {sum(y_train == 2)} non-res + single-H bkg\")\n",
    "    print(f\"Num val: {len(y_val)} -> {sum(y_val == 0)} sig & {sum(y_val == 1)} ttH bkg & {sum(y_val == 2)} non-res + single-H bkg\")\n",
    "    print(f\"Num test: {len(label_test_dict[f'fold_{fold_idx}'])} -> {sum(label_test_dict[f'fold_{fold_idx}'] == np.array([1, 0, 0]))[0]} sig & {sum(label_test_dict[f'fold_{fold_idx}'] == np.array([0, 1, 0]))[1]} ttH bkg & {sum(label_test_dict[f'fold_{fold_idx}'] == np.array([0, 0, 1]))[2]} non-res + single-H bkg\")\n",
    "    print('='*60)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/57986259/multiclass-classification-with-xgboost-classifier\n",
    "# https://forecastegy.com/posts/xgboost-multiclass-classification-python/\n",
    "# https://indico.cern.ch/event/915265/contributions/3848138/attachments/2048174/3432202/kunlinRan_bbyy_20200531.pdf\n",
    "\n",
    "\n",
    "param = {}\n",
    "\n",
    "# Booster parameters\n",
    "param['eta']              = 0.03 # learning rate\n",
    "param['max_depth']        = 5  # maximum depth of a tree\n",
    "param['subsample']        = 0.4 # fraction of events to train tree on\n",
    "param['colsample_bytree'] = 0.6 # fraction of features to train tree on\n",
    "param['num_class']        = np.shape(label_dict['fold_0'])[1] # num classes for ulti-class training\n",
    "\n",
    "# Learning task parameters\n",
    "param['objective']   = 'multi:softprob'   # objective function\n",
    "param['eval_metric'] = 'merror'           # evaluation metric for cross validation\n",
    "param = list(param.items()) + [('eval_metric', 'mlogloss')]\n",
    "\n",
    "num_trees = 1000  # number of trees to make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "[0]\ttrain-merror:0.05958\ttrain-mlogloss:1.06213\ttest-merror:0.07042\ttest-mlogloss:1.06329\tval-merror:0.06225\tval-mlogloss:1.06236\n",
      "[10]\ttrain-merror:0.05563\ttrain-mlogloss:0.79157\ttest-merror:0.06323\ttest-mlogloss:0.79604\tval-merror:0.05808\tval-mlogloss:0.79280\n",
      "[20]\ttrain-merror:0.05212\ttrain-mlogloss:0.60706\ttest-merror:0.05681\ttest-mlogloss:0.61321\tval-merror:0.05484\tval-mlogloss:0.60930\n",
      "[30]\ttrain-merror:0.05114\ttrain-mlogloss:0.48099\ttest-merror:0.05637\ttest-mlogloss:0.48708\tval-merror:0.05364\tval-mlogloss:0.48383\n",
      "[40]\ttrain-merror:0.04976\ttrain-mlogloss:0.38991\ttest-merror:0.05477\ttest-mlogloss:0.39568\tval-merror:0.05235\tval-mlogloss:0.39337\n",
      "[50]\ttrain-merror:0.04826\ttrain-mlogloss:0.32269\ttest-merror:0.05173\ttest-mlogloss:0.32767\tval-merror:0.05150\tval-mlogloss:0.32669\n",
      "[60]\ttrain-merror:0.04743\ttrain-mlogloss:0.27376\ttest-merror:0.04894\ttest-mlogloss:0.27790\tval-merror:0.05090\tval-mlogloss:0.27834\n",
      "[70]\ttrain-merror:0.04688\ttrain-mlogloss:0.23801\ttest-merror:0.04821\ttest-mlogloss:0.24169\tval-merror:0.05029\tval-mlogloss:0.24301\n",
      "[80]\ttrain-merror:0.04642\ttrain-mlogloss:0.21138\ttest-merror:0.04723\ttest-mlogloss:0.21479\tval-merror:0.04961\tval-mlogloss:0.21684\n",
      "[90]\ttrain-merror:0.04588\ttrain-mlogloss:0.19063\ttest-merror:0.04596\ttest-mlogloss:0.19370\tval-merror:0.04946\tval-mlogloss:0.19648\n",
      "[100]\ttrain-merror:0.04537\ttrain-mlogloss:0.17445\ttest-merror:0.04537\ttest-mlogloss:0.17704\tval-merror:0.04903\tval-mlogloss:0.18063\n",
      "[110]\ttrain-merror:0.04512\ttrain-mlogloss:0.16234\ttest-merror:0.04462\ttest-mlogloss:0.16469\tval-merror:0.04882\tval-mlogloss:0.16888\n",
      "[120]\ttrain-merror:0.04472\ttrain-mlogloss:0.15260\ttest-merror:0.04388\ttest-mlogloss:0.15458\tval-merror:0.04819\tval-mlogloss:0.15948\n",
      "[130]\ttrain-merror:0.04440\ttrain-mlogloss:0.14474\ttest-merror:0.04315\ttest-mlogloss:0.14629\tval-merror:0.04799\tval-mlogloss:0.15196\n",
      "[140]\ttrain-merror:0.04401\ttrain-mlogloss:0.13870\ttest-merror:0.04240\ttest-mlogloss:0.14018\tval-merror:0.04753\tval-mlogloss:0.14622\n",
      "[150]\ttrain-merror:0.04343\ttrain-mlogloss:0.13362\ttest-merror:0.04158\ttest-mlogloss:0.13506\tval-merror:0.04718\tval-mlogloss:0.14146\n",
      "[160]\ttrain-merror:0.04306\ttrain-mlogloss:0.12967\ttest-merror:0.04142\ttest-mlogloss:0.13123\tval-merror:0.04679\tval-mlogloss:0.13783\n",
      "[170]\ttrain-merror:0.04278\ttrain-mlogloss:0.12637\ttest-merror:0.04101\ttest-mlogloss:0.12795\tval-merror:0.04652\tval-mlogloss:0.13482\n",
      "[180]\ttrain-merror:0.04248\ttrain-mlogloss:0.12348\ttest-merror:0.04081\ttest-mlogloss:0.12528\tval-merror:0.04624\tval-mlogloss:0.13219\n",
      "[190]\ttrain-merror:0.04218\ttrain-mlogloss:0.12105\ttest-merror:0.04036\ttest-mlogloss:0.12302\tval-merror:0.04609\tval-mlogloss:0.13003\n",
      "[200]\ttrain-merror:0.04197\ttrain-mlogloss:0.11899\ttest-merror:0.04000\ttest-mlogloss:0.12107\tval-merror:0.04590\tval-mlogloss:0.12821\n",
      "[210]\ttrain-merror:0.04157\ttrain-mlogloss:0.11711\ttest-merror:0.03964\ttest-mlogloss:0.11933\tval-merror:0.04586\tval-mlogloss:0.12657\n",
      "[220]\ttrain-merror:0.04126\ttrain-mlogloss:0.11556\ttest-merror:0.03956\ttest-mlogloss:0.11790\tval-merror:0.04568\tval-mlogloss:0.12528\n",
      "[230]\ttrain-merror:0.04104\ttrain-mlogloss:0.11408\ttest-merror:0.03926\ttest-mlogloss:0.11653\tval-merror:0.04555\tval-mlogloss:0.12405\n",
      "[240]\ttrain-merror:0.04085\ttrain-mlogloss:0.11280\ttest-merror:0.03884\ttest-mlogloss:0.11543\tval-merror:0.04542\tval-mlogloss:0.12299\n",
      "[250]\ttrain-merror:0.04062\ttrain-mlogloss:0.11169\ttest-merror:0.03871\ttest-mlogloss:0.11458\tval-merror:0.04503\tval-mlogloss:0.12205\n",
      "[260]\ttrain-merror:0.04034\ttrain-mlogloss:0.11059\ttest-merror:0.03850\ttest-mlogloss:0.11372\tval-merror:0.04503\tval-mlogloss:0.12119\n",
      "[270]\ttrain-merror:0.04021\ttrain-mlogloss:0.10959\ttest-merror:0.03827\ttest-mlogloss:0.11292\tval-merror:0.04496\tval-mlogloss:0.12039\n",
      "[280]\ttrain-merror:0.03996\ttrain-mlogloss:0.10880\ttest-merror:0.03815\ttest-mlogloss:0.11234\tval-merror:0.04495\tval-mlogloss:0.11978\n",
      "[290]\ttrain-merror:0.03967\ttrain-mlogloss:0.10791\ttest-merror:0.03802\ttest-mlogloss:0.11168\tval-merror:0.04480\tval-mlogloss:0.11907\n",
      "[300]\ttrain-merror:0.03942\ttrain-mlogloss:0.10720\ttest-merror:0.03788\ttest-mlogloss:0.11115\tval-merror:0.04484\tval-mlogloss:0.11857\n",
      "[310]\ttrain-merror:0.03929\ttrain-mlogloss:0.10659\ttest-merror:0.03789\ttest-mlogloss:0.11076\tval-merror:0.04462\tval-mlogloss:0.11815\n",
      "[320]\ttrain-merror:0.03908\ttrain-mlogloss:0.10591\ttest-merror:0.03805\ttest-mlogloss:0.11022\tval-merror:0.04462\tval-mlogloss:0.11768\n",
      "[330]\ttrain-merror:0.03892\ttrain-mlogloss:0.10541\ttest-merror:0.03816\ttest-mlogloss:0.10990\tval-merror:0.04452\tval-mlogloss:0.11739\n",
      "[340]\ttrain-merror:0.03870\ttrain-mlogloss:0.10490\ttest-merror:0.03817\ttest-mlogloss:0.10956\tval-merror:0.04432\tval-mlogloss:0.11707\n",
      "[350]\ttrain-merror:0.03857\ttrain-mlogloss:0.10433\ttest-merror:0.03791\ttest-mlogloss:0.10922\tval-merror:0.04441\tval-mlogloss:0.11668\n",
      "[360]\ttrain-merror:0.03842\ttrain-mlogloss:0.10388\ttest-merror:0.03790\ttest-mlogloss:0.10898\tval-merror:0.04432\tval-mlogloss:0.11644\n",
      "[370]\ttrain-merror:0.03824\ttrain-mlogloss:0.10345\ttest-merror:0.03806\ttest-mlogloss:0.10867\tval-merror:0.04434\tval-mlogloss:0.11623\n",
      "[380]\ttrain-merror:0.03811\ttrain-mlogloss:0.10304\ttest-merror:0.03801\ttest-mlogloss:0.10850\tval-merror:0.04440\tval-mlogloss:0.11601\n",
      "[390]\ttrain-merror:0.03791\ttrain-mlogloss:0.10253\ttest-merror:0.03796\ttest-mlogloss:0.10823\tval-merror:0.04429\tval-mlogloss:0.11564\n",
      "[400]\ttrain-merror:0.03779\ttrain-mlogloss:0.10211\ttest-merror:0.03793\ttest-mlogloss:0.10798\tval-merror:0.04421\tval-mlogloss:0.11541\n",
      "[410]\ttrain-merror:0.03770\ttrain-mlogloss:0.10171\ttest-merror:0.03796\ttest-mlogloss:0.10774\tval-merror:0.04420\tval-mlogloss:0.11517\n",
      "[420]\ttrain-merror:0.03757\ttrain-mlogloss:0.10133\ttest-merror:0.03797\ttest-mlogloss:0.10772\tval-merror:0.04392\tval-mlogloss:0.11501\n",
      "[430]\ttrain-merror:0.03743\ttrain-mlogloss:0.10094\ttest-merror:0.03787\ttest-mlogloss:0.10761\tval-merror:0.04376\tval-mlogloss:0.11483\n",
      "[440]\ttrain-merror:0.03725\ttrain-mlogloss:0.10057\ttest-merror:0.03790\ttest-mlogloss:0.10738\tval-merror:0.04375\tval-mlogloss:0.11463\n",
      "[450]\ttrain-merror:0.03718\ttrain-mlogloss:0.10014\ttest-merror:0.03786\ttest-mlogloss:0.10714\tval-merror:0.04360\tval-mlogloss:0.11438\n",
      "[460]\ttrain-merror:0.03685\ttrain-mlogloss:0.09963\ttest-merror:0.03766\ttest-mlogloss:0.10686\tval-merror:0.04341\tval-mlogloss:0.11404\n",
      "[470]\ttrain-merror:0.03674\ttrain-mlogloss:0.09927\ttest-merror:0.03723\ttest-mlogloss:0.10663\tval-merror:0.04342\tval-mlogloss:0.11387\n",
      "[480]\ttrain-merror:0.03660\ttrain-mlogloss:0.09893\ttest-merror:0.03719\ttest-mlogloss:0.10644\tval-merror:0.04353\tval-mlogloss:0.11372\n",
      "[490]\ttrain-merror:0.03655\ttrain-mlogloss:0.09863\ttest-merror:0.03715\ttest-mlogloss:0.10638\tval-merror:0.04338\tval-mlogloss:0.11361\n",
      "[500]\ttrain-merror:0.03644\ttrain-mlogloss:0.09826\ttest-merror:0.03706\ttest-mlogloss:0.10622\tval-merror:0.04355\tval-mlogloss:0.11344\n",
      "[510]\ttrain-merror:0.03624\ttrain-mlogloss:0.09791\ttest-merror:0.03712\ttest-mlogloss:0.10611\tval-merror:0.04348\tval-mlogloss:0.11328\n",
      "[520]\ttrain-merror:0.03615\ttrain-mlogloss:0.09758\ttest-merror:0.03719\ttest-mlogloss:0.10600\tval-merror:0.04348\tval-mlogloss:0.11313\n",
      "[530]\ttrain-merror:0.03606\ttrain-mlogloss:0.09728\ttest-merror:0.03713\ttest-mlogloss:0.10594\tval-merror:0.04343\tval-mlogloss:0.11300\n",
      "[540]\ttrain-merror:0.03591\ttrain-mlogloss:0.09702\ttest-merror:0.03719\ttest-mlogloss:0.10600\tval-merror:0.04351\tval-mlogloss:0.11291\n",
      "[550]\ttrain-merror:0.03573\ttrain-mlogloss:0.09657\ttest-merror:0.03723\ttest-mlogloss:0.10573\tval-merror:0.04328\tval-mlogloss:0.11264\n",
      "[560]\ttrain-merror:0.03555\ttrain-mlogloss:0.09620\ttest-merror:0.03719\ttest-mlogloss:0.10540\tval-merror:0.04332\tval-mlogloss:0.11245\n",
      "[570]\ttrain-merror:0.03540\ttrain-mlogloss:0.09593\ttest-merror:0.03724\ttest-mlogloss:0.10533\tval-merror:0.04331\tval-mlogloss:0.11234\n",
      "[580]\ttrain-merror:0.03527\ttrain-mlogloss:0.09553\ttest-merror:0.03701\ttest-mlogloss:0.10514\tval-merror:0.04330\tval-mlogloss:0.11207\n",
      "[590]\ttrain-merror:0.03520\ttrain-mlogloss:0.09525\ttest-merror:0.03706\ttest-mlogloss:0.10496\tval-merror:0.04321\tval-mlogloss:0.11192\n",
      "[600]\ttrain-merror:0.03506\ttrain-mlogloss:0.09485\ttest-merror:0.03701\ttest-mlogloss:0.10472\tval-merror:0.04317\tval-mlogloss:0.11172\n",
      "[610]\ttrain-merror:0.03503\ttrain-mlogloss:0.09460\ttest-merror:0.03710\ttest-mlogloss:0.10454\tval-merror:0.04315\tval-mlogloss:0.11161\n",
      "[620]\ttrain-merror:0.03491\ttrain-mlogloss:0.09435\ttest-merror:0.03707\ttest-mlogloss:0.10449\tval-merror:0.04303\tval-mlogloss:0.11157\n",
      "[630]\ttrain-merror:0.03478\ttrain-mlogloss:0.09403\ttest-merror:0.03695\ttest-mlogloss:0.10443\tval-merror:0.04297\tval-mlogloss:0.11141\n",
      "[640]\ttrain-merror:0.03468\ttrain-mlogloss:0.09373\ttest-merror:0.03692\ttest-mlogloss:0.10423\tval-merror:0.04289\tval-mlogloss:0.11128\n",
      "[650]\ttrain-merror:0.03454\ttrain-mlogloss:0.09349\ttest-merror:0.03709\ttest-mlogloss:0.10411\tval-merror:0.04276\tval-mlogloss:0.11121\n",
      "[660]\ttrain-merror:0.03443\ttrain-mlogloss:0.09323\ttest-merror:0.03716\ttest-mlogloss:0.10405\tval-merror:0.04265\tval-mlogloss:0.11110\n",
      "[670]\ttrain-merror:0.03432\ttrain-mlogloss:0.09296\ttest-merror:0.03711\ttest-mlogloss:0.10401\tval-merror:0.04257\tval-mlogloss:0.11098\n",
      "[680]\ttrain-merror:0.03426\ttrain-mlogloss:0.09267\ttest-merror:0.03712\ttest-mlogloss:0.10390\tval-merror:0.04242\tval-mlogloss:0.11086\n",
      "[690]\ttrain-merror:0.03413\ttrain-mlogloss:0.09239\ttest-merror:0.03692\ttest-mlogloss:0.10372\tval-merror:0.04243\tval-mlogloss:0.11078\n",
      "[700]\ttrain-merror:0.03406\ttrain-mlogloss:0.09215\ttest-merror:0.03686\ttest-mlogloss:0.10373\tval-merror:0.04241\tval-mlogloss:0.11071\n",
      "[710]\ttrain-merror:0.03397\ttrain-mlogloss:0.09188\ttest-merror:0.03682\ttest-mlogloss:0.10376\tval-merror:0.04236\tval-mlogloss:0.11059\n",
      "[720]\ttrain-merror:0.03389\ttrain-mlogloss:0.09167\ttest-merror:0.03680\ttest-mlogloss:0.10368\tval-merror:0.04240\tval-mlogloss:0.11055\n",
      "[730]\ttrain-merror:0.03377\ttrain-mlogloss:0.09142\ttest-merror:0.03675\ttest-mlogloss:0.10363\tval-merror:0.04225\tval-mlogloss:0.11048\n",
      "[740]\ttrain-merror:0.03370\ttrain-mlogloss:0.09112\ttest-merror:0.03660\ttest-mlogloss:0.10349\tval-merror:0.04227\tval-mlogloss:0.11031\n",
      "[750]\ttrain-merror:0.03366\ttrain-mlogloss:0.09084\ttest-merror:0.03660\ttest-mlogloss:0.10332\tval-merror:0.04223\tval-mlogloss:0.11020\n",
      "[760]\ttrain-merror:0.03343\ttrain-mlogloss:0.09052\ttest-merror:0.03669\ttest-mlogloss:0.10310\tval-merror:0.04215\tval-mlogloss:0.11004\n",
      "[770]\ttrain-merror:0.03336\ttrain-mlogloss:0.09022\ttest-merror:0.03661\ttest-mlogloss:0.10294\tval-merror:0.04203\tval-mlogloss:0.10990\n",
      "[780]\ttrain-merror:0.03330\ttrain-mlogloss:0.08996\ttest-merror:0.03673\ttest-mlogloss:0.10286\tval-merror:0.04208\tval-mlogloss:0.10984\n",
      "[790]\ttrain-merror:0.03317\ttrain-mlogloss:0.08975\ttest-merror:0.03669\ttest-mlogloss:0.10276\tval-merror:0.04204\tval-mlogloss:0.10977\n",
      "[800]\ttrain-merror:0.03309\ttrain-mlogloss:0.08948\ttest-merror:0.03681\ttest-mlogloss:0.10267\tval-merror:0.04202\tval-mlogloss:0.10969\n",
      "[810]\ttrain-merror:0.03294\ttrain-mlogloss:0.08915\ttest-merror:0.03663\ttest-mlogloss:0.10252\tval-merror:0.04185\tval-mlogloss:0.10949\n",
      "[820]\ttrain-merror:0.03283\ttrain-mlogloss:0.08886\ttest-merror:0.03660\ttest-mlogloss:0.10247\tval-merror:0.04183\tval-mlogloss:0.10938\n",
      "[830]\ttrain-merror:0.03275\ttrain-mlogloss:0.08857\ttest-merror:0.03664\ttest-mlogloss:0.10238\tval-merror:0.04179\tval-mlogloss:0.10929\n",
      "[840]\ttrain-merror:0.03264\ttrain-mlogloss:0.08834\ttest-merror:0.03645\ttest-mlogloss:0.10223\tval-merror:0.04175\tval-mlogloss:0.10923\n",
      "[850]\ttrain-merror:0.03256\ttrain-mlogloss:0.08812\ttest-merror:0.03657\ttest-mlogloss:0.10217\tval-merror:0.04171\tval-mlogloss:0.10918\n",
      "[860]\ttrain-merror:0.03243\ttrain-mlogloss:0.08790\ttest-merror:0.03659\ttest-mlogloss:0.10208\tval-merror:0.04179\tval-mlogloss:0.10917\n",
      "[870]\ttrain-merror:0.03231\ttrain-mlogloss:0.08753\ttest-merror:0.03647\ttest-mlogloss:0.10187\tval-merror:0.04165\tval-mlogloss:0.10897\n",
      "[880]\ttrain-merror:0.03220\ttrain-mlogloss:0.08718\ttest-merror:0.03651\ttest-mlogloss:0.10171\tval-merror:0.04166\tval-mlogloss:0.10874\n",
      "[890]\ttrain-merror:0.03212\ttrain-mlogloss:0.08689\ttest-merror:0.03662\ttest-mlogloss:0.10158\tval-merror:0.04162\tval-mlogloss:0.10862\n",
      "[900]\ttrain-merror:0.03203\ttrain-mlogloss:0.08660\ttest-merror:0.03663\ttest-mlogloss:0.10143\tval-merror:0.04153\tval-mlogloss:0.10846\n",
      "[910]\ttrain-merror:0.03194\ttrain-mlogloss:0.08637\ttest-merror:0.03665\ttest-mlogloss:0.10136\tval-merror:0.04143\tval-mlogloss:0.10843\n",
      "[920]\ttrain-merror:0.03190\ttrain-mlogloss:0.08611\ttest-merror:0.03670\ttest-mlogloss:0.10136\tval-merror:0.04142\tval-mlogloss:0.10832\n",
      "[930]\ttrain-merror:0.03175\ttrain-mlogloss:0.08589\ttest-merror:0.03650\ttest-mlogloss:0.10127\tval-merror:0.04147\tval-mlogloss:0.10830\n",
      "[940]\ttrain-merror:0.03163\ttrain-mlogloss:0.08565\ttest-merror:0.03644\ttest-mlogloss:0.10114\tval-merror:0.04138\tval-mlogloss:0.10823\n",
      "[950]\ttrain-merror:0.03155\ttrain-mlogloss:0.08543\ttest-merror:0.03646\ttest-mlogloss:0.10110\tval-merror:0.04140\tval-mlogloss:0.10816\n",
      "[960]\ttrain-merror:0.03141\ttrain-mlogloss:0.08517\ttest-merror:0.03628\ttest-mlogloss:0.10096\tval-merror:0.04146\tval-mlogloss:0.10810\n",
      "[970]\ttrain-merror:0.03131\ttrain-mlogloss:0.08491\ttest-merror:0.03625\ttest-mlogloss:0.10092\tval-merror:0.04137\tval-mlogloss:0.10800\n",
      "[980]\ttrain-merror:0.03122\ttrain-mlogloss:0.08469\ttest-merror:0.03625\ttest-mlogloss:0.10084\tval-merror:0.04140\tval-mlogloss:0.10797\n",
      "[990]\ttrain-merror:0.03111\ttrain-mlogloss:0.08448\ttest-merror:0.03621\ttest-mlogloss:0.10080\tval-merror:0.04140\tval-mlogloss:0.10793\n",
      "[999]\ttrain-merror:0.03104\ttrain-mlogloss:0.08427\ttest-merror:0.03609\ttest-mlogloss:0.10074\tval-merror:0.04145\tval-mlogloss:0.10789\n",
      "[997]\ttest-merror:0.036087\ttest-mlogloss:0.100738\n",
      "====================================================================================================\n",
      "fold 1\n",
      "[0]\ttrain-merror:0.06668\ttrain-mlogloss:1.06396\ttest-merror:0.08564\ttest-mlogloss:1.06602\tval-merror:0.06842\tval-mlogloss:1.06409\n",
      "[10]\ttrain-merror:0.05315\ttrain-mlogloss:0.78345\ttest-merror:0.06268\ttest-mlogloss:0.79163\tval-merror:0.05392\tval-mlogloss:0.78439\n",
      "[20]\ttrain-merror:0.05174\ttrain-mlogloss:0.60279\ttest-merror:0.05924\ttest-mlogloss:0.61231\tval-merror:0.05279\tval-mlogloss:0.60446\n",
      "[30]\ttrain-merror:0.04961\ttrain-mlogloss:0.47509\ttest-merror:0.05665\ttest-mlogloss:0.48515\tval-merror:0.05100\tval-mlogloss:0.47715\n",
      "[40]\ttrain-merror:0.04937\ttrain-mlogloss:0.38637\ttest-merror:0.05721\ttest-mlogloss:0.39663\tval-merror:0.05069\tval-mlogloss:0.38892\n",
      "[50]\ttrain-merror:0.04810\ttrain-mlogloss:0.32022\ttest-merror:0.05399\ttest-mlogloss:0.33038\tval-merror:0.04944\tval-mlogloss:0.32315\n",
      "[60]\ttrain-merror:0.04706\ttrain-mlogloss:0.27092\ttest-merror:0.05184\ttest-mlogloss:0.28021\tval-merror:0.04820\tval-mlogloss:0.27408\n",
      "[70]\ttrain-merror:0.04650\ttrain-mlogloss:0.23614\ttest-merror:0.05100\ttest-mlogloss:0.24527\tval-merror:0.04764\tval-mlogloss:0.23950\n",
      "[80]\ttrain-merror:0.04602\ttrain-mlogloss:0.20989\ttest-merror:0.05031\ttest-mlogloss:0.21893\tval-merror:0.04707\tval-mlogloss:0.21362\n",
      "[90]\ttrain-merror:0.04541\ttrain-mlogloss:0.18821\ttest-merror:0.04920\ttest-mlogloss:0.19667\tval-merror:0.04667\tval-mlogloss:0.19216\n",
      "[100]\ttrain-merror:0.04505\ttrain-mlogloss:0.17226\ttest-merror:0.04839\ttest-mlogloss:0.18037\tval-merror:0.04636\tval-mlogloss:0.17642\n",
      "[110]\ttrain-merror:0.04486\ttrain-mlogloss:0.16014\ttest-merror:0.04772\ttest-mlogloss:0.16798\tval-merror:0.04619\tval-mlogloss:0.16454\n",
      "[120]\ttrain-merror:0.04438\ttrain-mlogloss:0.15055\ttest-merror:0.04704\ttest-mlogloss:0.15814\tval-merror:0.04589\tval-mlogloss:0.15519\n",
      "[130]\ttrain-merror:0.04397\ttrain-mlogloss:0.14337\ttest-merror:0.04680\ttest-mlogloss:0.15105\tval-merror:0.04547\tval-mlogloss:0.14822\n",
      "[140]\ttrain-merror:0.04369\ttrain-mlogloss:0.13731\ttest-merror:0.04563\ttest-mlogloss:0.14484\tval-merror:0.04500\tval-mlogloss:0.14236\n",
      "[150]\ttrain-merror:0.04327\ttrain-mlogloss:0.13241\ttest-merror:0.04527\ttest-mlogloss:0.14004\tval-merror:0.04483\tval-mlogloss:0.13770\n",
      "[160]\ttrain-merror:0.04291\ttrain-mlogloss:0.12868\ttest-merror:0.04556\ttest-mlogloss:0.13677\tval-merror:0.04457\tval-mlogloss:0.13420\n",
      "[170]\ttrain-merror:0.04262\ttrain-mlogloss:0.12537\ttest-merror:0.04474\ttest-mlogloss:0.13352\tval-merror:0.04417\tval-mlogloss:0.13108\n",
      "[180]\ttrain-merror:0.04221\ttrain-mlogloss:0.12256\ttest-merror:0.04404\ttest-mlogloss:0.13088\tval-merror:0.04382\tval-mlogloss:0.12848\n",
      "[190]\ttrain-merror:0.04197\ttrain-mlogloss:0.12032\ttest-merror:0.04392\ttest-mlogloss:0.12885\tval-merror:0.04356\tval-mlogloss:0.12648\n",
      "[200]\ttrain-merror:0.04183\ttrain-mlogloss:0.11826\ttest-merror:0.04409\ttest-mlogloss:0.12707\tval-merror:0.04330\tval-mlogloss:0.12466\n",
      "[210]\ttrain-merror:0.04160\ttrain-mlogloss:0.11655\ttest-merror:0.04362\ttest-mlogloss:0.12575\tval-merror:0.04315\tval-mlogloss:0.12319\n",
      "[220]\ttrain-merror:0.04135\ttrain-mlogloss:0.11504\ttest-merror:0.04340\ttest-mlogloss:0.12442\tval-merror:0.04295\tval-mlogloss:0.12190\n",
      "[230]\ttrain-merror:0.04119\ttrain-mlogloss:0.11367\ttest-merror:0.04312\ttest-mlogloss:0.12337\tval-merror:0.04278\tval-mlogloss:0.12077\n",
      "[240]\ttrain-merror:0.04105\ttrain-mlogloss:0.11256\ttest-merror:0.04310\ttest-mlogloss:0.12262\tval-merror:0.04237\tval-mlogloss:0.11983\n",
      "[250]\ttrain-merror:0.04085\ttrain-mlogloss:0.11152\ttest-merror:0.04299\ttest-mlogloss:0.12173\tval-merror:0.04215\tval-mlogloss:0.11897\n",
      "[260]\ttrain-merror:0.04065\ttrain-mlogloss:0.11056\ttest-merror:0.04301\ttest-mlogloss:0.12102\tval-merror:0.04181\tval-mlogloss:0.11822\n",
      "[270]\ttrain-merror:0.04048\ttrain-mlogloss:0.10971\ttest-merror:0.04287\ttest-mlogloss:0.12048\tval-merror:0.04180\tval-mlogloss:0.11761\n",
      "[280]\ttrain-merror:0.04027\ttrain-mlogloss:0.10887\ttest-merror:0.04270\ttest-mlogloss:0.11980\tval-merror:0.04152\tval-mlogloss:0.11698\n",
      "[290]\ttrain-merror:0.04014\ttrain-mlogloss:0.10805\ttest-merror:0.04262\ttest-mlogloss:0.11918\tval-merror:0.04145\tval-mlogloss:0.11639\n",
      "[300]\ttrain-merror:0.03994\ttrain-mlogloss:0.10737\ttest-merror:0.04264\ttest-mlogloss:0.11879\tval-merror:0.04147\tval-mlogloss:0.11594\n",
      "[310]\ttrain-merror:0.03973\ttrain-mlogloss:0.10665\ttest-merror:0.04206\ttest-mlogloss:0.11835\tval-merror:0.04138\tval-mlogloss:0.11537\n",
      "[320]\ttrain-merror:0.03947\ttrain-mlogloss:0.10601\ttest-merror:0.04186\ttest-mlogloss:0.11792\tval-merror:0.04131\tval-mlogloss:0.11495\n",
      "[330]\ttrain-merror:0.03927\ttrain-mlogloss:0.10541\ttest-merror:0.04166\ttest-mlogloss:0.11752\tval-merror:0.04120\tval-mlogloss:0.11458\n",
      "[340]\ttrain-merror:0.03914\ttrain-mlogloss:0.10481\ttest-merror:0.04132\ttest-mlogloss:0.11719\tval-merror:0.04103\tval-mlogloss:0.11419\n",
      "[350]\ttrain-merror:0.03908\ttrain-mlogloss:0.10437\ttest-merror:0.04121\ttest-mlogloss:0.11698\tval-merror:0.04091\tval-mlogloss:0.11397\n",
      "[360]\ttrain-merror:0.03898\ttrain-mlogloss:0.10390\ttest-merror:0.04109\ttest-mlogloss:0.11676\tval-merror:0.04091\tval-mlogloss:0.11369\n",
      "[370]\ttrain-merror:0.03874\ttrain-mlogloss:0.10347\ttest-merror:0.04112\ttest-mlogloss:0.11650\tval-merror:0.04086\tval-mlogloss:0.11345\n",
      "[380]\ttrain-merror:0.03864\ttrain-mlogloss:0.10298\ttest-merror:0.04106\ttest-mlogloss:0.11634\tval-merror:0.04079\tval-mlogloss:0.11317\n",
      "[390]\ttrain-merror:0.03846\ttrain-mlogloss:0.10245\ttest-merror:0.04128\ttest-mlogloss:0.11607\tval-merror:0.04075\tval-mlogloss:0.11284\n",
      "[400]\ttrain-merror:0.03828\ttrain-mlogloss:0.10205\ttest-merror:0.04156\ttest-mlogloss:0.11597\tval-merror:0.04079\tval-mlogloss:0.11265\n",
      "[410]\ttrain-merror:0.03813\ttrain-mlogloss:0.10162\ttest-merror:0.04145\ttest-mlogloss:0.11583\tval-merror:0.04066\tval-mlogloss:0.11242\n",
      "[420]\ttrain-merror:0.03794\ttrain-mlogloss:0.10122\ttest-merror:0.04138\ttest-mlogloss:0.11563\tval-merror:0.04058\tval-mlogloss:0.11222\n",
      "[430]\ttrain-merror:0.03784\ttrain-mlogloss:0.10083\ttest-merror:0.04136\ttest-mlogloss:0.11539\tval-merror:0.04063\tval-mlogloss:0.11205\n",
      "[440]\ttrain-merror:0.03777\ttrain-mlogloss:0.10052\ttest-merror:0.04135\ttest-mlogloss:0.11529\tval-merror:0.04046\tval-mlogloss:0.11191\n",
      "[450]\ttrain-merror:0.03752\ttrain-mlogloss:0.09995\ttest-merror:0.04098\ttest-mlogloss:0.11492\tval-merror:0.04038\tval-mlogloss:0.11149\n",
      "[460]\ttrain-merror:0.03737\ttrain-mlogloss:0.09965\ttest-merror:0.04094\ttest-mlogloss:0.11487\tval-merror:0.04054\tval-mlogloss:0.11136\n",
      "[470]\ttrain-merror:0.03732\ttrain-mlogloss:0.09928\ttest-merror:0.04095\ttest-mlogloss:0.11469\tval-merror:0.04035\tval-mlogloss:0.11119\n",
      "[480]\ttrain-merror:0.03715\ttrain-mlogloss:0.09897\ttest-merror:0.04101\ttest-mlogloss:0.11452\tval-merror:0.04030\tval-mlogloss:0.11104\n",
      "[490]\ttrain-merror:0.03703\ttrain-mlogloss:0.09861\ttest-merror:0.04098\ttest-mlogloss:0.11444\tval-merror:0.04024\tval-mlogloss:0.11091\n",
      "[500]\ttrain-merror:0.03684\ttrain-mlogloss:0.09828\ttest-merror:0.04084\ttest-mlogloss:0.11432\tval-merror:0.04022\tval-mlogloss:0.11081\n",
      "[510]\ttrain-merror:0.03664\ttrain-mlogloss:0.09795\ttest-merror:0.04085\ttest-mlogloss:0.11422\tval-merror:0.04020\tval-mlogloss:0.11067\n",
      "[520]\ttrain-merror:0.03654\ttrain-mlogloss:0.09764\ttest-merror:0.04065\ttest-mlogloss:0.11417\tval-merror:0.04017\tval-mlogloss:0.11058\n",
      "[530]\ttrain-merror:0.03638\ttrain-mlogloss:0.09737\ttest-merror:0.04056\ttest-mlogloss:0.11401\tval-merror:0.04010\tval-mlogloss:0.11050\n",
      "[540]\ttrain-merror:0.03630\ttrain-mlogloss:0.09708\ttest-merror:0.04057\ttest-mlogloss:0.11399\tval-merror:0.04008\tval-mlogloss:0.11038\n",
      "[550]\ttrain-merror:0.03622\ttrain-mlogloss:0.09679\ttest-merror:0.04053\ttest-mlogloss:0.11394\tval-merror:0.04006\tval-mlogloss:0.11025\n",
      "[560]\ttrain-merror:0.03608\ttrain-mlogloss:0.09637\ttest-merror:0.04054\ttest-mlogloss:0.11377\tval-merror:0.04001\tval-mlogloss:0.11000\n",
      "[570]\ttrain-merror:0.03600\ttrain-mlogloss:0.09606\ttest-merror:0.04066\ttest-mlogloss:0.11374\tval-merror:0.03985\tval-mlogloss:0.10985\n",
      "[580]\ttrain-merror:0.03585\ttrain-mlogloss:0.09577\ttest-merror:0.04079\ttest-mlogloss:0.11361\tval-merror:0.03996\tval-mlogloss:0.10974\n",
      "[590]\ttrain-merror:0.03573\ttrain-mlogloss:0.09549\ttest-merror:0.04074\ttest-mlogloss:0.11350\tval-merror:0.04004\tval-mlogloss:0.10962\n",
      "[600]\ttrain-merror:0.03558\ttrain-mlogloss:0.09520\ttest-merror:0.04071\ttest-mlogloss:0.11346\tval-merror:0.03999\tval-mlogloss:0.10951\n",
      "[610]\ttrain-merror:0.03548\ttrain-mlogloss:0.09487\ttest-merror:0.04056\ttest-mlogloss:0.11331\tval-merror:0.03990\tval-mlogloss:0.10933\n",
      "[620]\ttrain-merror:0.03532\ttrain-mlogloss:0.09462\ttest-merror:0.04073\ttest-mlogloss:0.11327\tval-merror:0.03983\tval-mlogloss:0.10926\n",
      "[630]\ttrain-merror:0.03529\ttrain-mlogloss:0.09433\ttest-merror:0.04048\ttest-mlogloss:0.11315\tval-merror:0.03979\tval-mlogloss:0.10911\n",
      "[640]\ttrain-merror:0.03522\ttrain-mlogloss:0.09410\ttest-merror:0.04034\ttest-mlogloss:0.11298\tval-merror:0.03975\tval-mlogloss:0.10906\n",
      "[650]\ttrain-merror:0.03509\ttrain-mlogloss:0.09378\ttest-merror:0.04065\ttest-mlogloss:0.11294\tval-merror:0.03967\tval-mlogloss:0.10893\n",
      "[660]\ttrain-merror:0.03491\ttrain-mlogloss:0.09351\ttest-merror:0.04041\ttest-mlogloss:0.11284\tval-merror:0.03963\tval-mlogloss:0.10887\n",
      "[670]\ttrain-merror:0.03481\ttrain-mlogloss:0.09319\ttest-merror:0.04045\ttest-mlogloss:0.11272\tval-merror:0.03962\tval-mlogloss:0.10870\n",
      "[680]\ttrain-merror:0.03470\ttrain-mlogloss:0.09294\ttest-merror:0.04050\ttest-mlogloss:0.11268\tval-merror:0.03953\tval-mlogloss:0.10862\n",
      "[690]\ttrain-merror:0.03461\ttrain-mlogloss:0.09264\ttest-merror:0.04048\ttest-mlogloss:0.11252\tval-merror:0.03954\tval-mlogloss:0.10850\n",
      "[700]\ttrain-merror:0.03451\ttrain-mlogloss:0.09236\ttest-merror:0.04047\ttest-mlogloss:0.11249\tval-merror:0.03946\tval-mlogloss:0.10840\n",
      "[710]\ttrain-merror:0.03438\ttrain-mlogloss:0.09206\ttest-merror:0.04067\ttest-mlogloss:0.11255\tval-merror:0.03939\tval-mlogloss:0.10827\n",
      "[720]\ttrain-merror:0.03427\ttrain-mlogloss:0.09181\ttest-merror:0.04054\ttest-mlogloss:0.11228\tval-merror:0.03944\tval-mlogloss:0.10821\n",
      "[730]\ttrain-merror:0.03414\ttrain-mlogloss:0.09152\ttest-merror:0.04032\ttest-mlogloss:0.11228\tval-merror:0.03939\tval-mlogloss:0.10809\n",
      "[740]\ttrain-merror:0.03412\ttrain-mlogloss:0.09128\ttest-merror:0.04052\ttest-mlogloss:0.11224\tval-merror:0.03943\tval-mlogloss:0.10806\n",
      "[750]\ttrain-merror:0.03400\ttrain-mlogloss:0.09101\ttest-merror:0.04050\ttest-mlogloss:0.11216\tval-merror:0.03931\tval-mlogloss:0.10796\n",
      "[760]\ttrain-merror:0.03396\ttrain-mlogloss:0.09076\ttest-merror:0.04045\ttest-mlogloss:0.11212\tval-merror:0.03920\tval-mlogloss:0.10787\n",
      "[770]\ttrain-merror:0.03385\ttrain-mlogloss:0.09049\ttest-merror:0.04031\ttest-mlogloss:0.11196\tval-merror:0.03918\tval-mlogloss:0.10775\n",
      "[780]\ttrain-merror:0.03379\ttrain-mlogloss:0.09027\ttest-merror:0.04013\ttest-mlogloss:0.11185\tval-merror:0.03912\tval-mlogloss:0.10769\n",
      "[790]\ttrain-merror:0.03358\ttrain-mlogloss:0.08991\ttest-merror:0.04004\ttest-mlogloss:0.11160\tval-merror:0.03917\tval-mlogloss:0.10747\n",
      "[800]\ttrain-merror:0.03353\ttrain-mlogloss:0.08962\ttest-merror:0.04016\ttest-mlogloss:0.11152\tval-merror:0.03906\tval-mlogloss:0.10736\n",
      "[810]\ttrain-merror:0.03340\ttrain-mlogloss:0.08939\ttest-merror:0.04016\ttest-mlogloss:0.11144\tval-merror:0.03901\tval-mlogloss:0.10729\n",
      "[820]\ttrain-merror:0.03331\ttrain-mlogloss:0.08911\ttest-merror:0.04017\ttest-mlogloss:0.11134\tval-merror:0.03908\tval-mlogloss:0.10721\n",
      "[830]\ttrain-merror:0.03322\ttrain-mlogloss:0.08888\ttest-merror:0.03992\ttest-mlogloss:0.11124\tval-merror:0.03887\tval-mlogloss:0.10714\n",
      "[840]\ttrain-merror:0.03306\ttrain-mlogloss:0.08860\ttest-merror:0.04009\ttest-mlogloss:0.11116\tval-merror:0.03890\tval-mlogloss:0.10700\n",
      "[850]\ttrain-merror:0.03298\ttrain-mlogloss:0.08833\ttest-merror:0.03987\ttest-mlogloss:0.11098\tval-merror:0.03885\tval-mlogloss:0.10687\n",
      "[860]\ttrain-merror:0.03288\ttrain-mlogloss:0.08808\ttest-merror:0.04014\ttest-mlogloss:0.11094\tval-merror:0.03885\tval-mlogloss:0.10678\n",
      "[870]\ttrain-merror:0.03273\ttrain-mlogloss:0.08777\ttest-merror:0.04006\ttest-mlogloss:0.11093\tval-merror:0.03878\tval-mlogloss:0.10665\n",
      "[880]\ttrain-merror:0.03265\ttrain-mlogloss:0.08754\ttest-merror:0.03999\ttest-mlogloss:0.11089\tval-merror:0.03883\tval-mlogloss:0.10658\n",
      "[890]\ttrain-merror:0.03255\ttrain-mlogloss:0.08730\ttest-merror:0.03988\ttest-mlogloss:0.11079\tval-merror:0.03877\tval-mlogloss:0.10651\n",
      "[900]\ttrain-merror:0.03246\ttrain-mlogloss:0.08704\ttest-merror:0.04007\ttest-mlogloss:0.11069\tval-merror:0.03881\tval-mlogloss:0.10642\n",
      "[910]\ttrain-merror:0.03235\ttrain-mlogloss:0.08679\ttest-merror:0.03989\ttest-mlogloss:0.11061\tval-merror:0.03891\tval-mlogloss:0.10635\n",
      "[920]\ttrain-merror:0.03220\ttrain-mlogloss:0.08649\ttest-merror:0.03988\ttest-mlogloss:0.11052\tval-merror:0.03887\tval-mlogloss:0.10620\n",
      "[930]\ttrain-merror:0.03209\ttrain-mlogloss:0.08626\ttest-merror:0.03966\ttest-mlogloss:0.11050\tval-merror:0.03883\tval-mlogloss:0.10612\n",
      "[940]\ttrain-merror:0.03201\ttrain-mlogloss:0.08595\ttest-merror:0.03957\ttest-mlogloss:0.11028\tval-merror:0.03874\tval-mlogloss:0.10595\n",
      "[950]\ttrain-merror:0.03186\ttrain-mlogloss:0.08567\ttest-merror:0.03950\ttest-mlogloss:0.11024\tval-merror:0.03876\tval-mlogloss:0.10582\n",
      "[960]\ttrain-merror:0.03177\ttrain-mlogloss:0.08543\ttest-merror:0.03961\ttest-mlogloss:0.11012\tval-merror:0.03873\tval-mlogloss:0.10570\n",
      "[970]\ttrain-merror:0.03161\ttrain-mlogloss:0.08520\ttest-merror:0.03963\ttest-mlogloss:0.11002\tval-merror:0.03877\tval-mlogloss:0.10565\n",
      "[980]\ttrain-merror:0.03154\ttrain-mlogloss:0.08493\ttest-merror:0.03954\ttest-mlogloss:0.10994\tval-merror:0.03877\tval-mlogloss:0.10554\n",
      "[990]\ttrain-merror:0.03133\ttrain-mlogloss:0.08472\ttest-merror:0.03952\ttest-mlogloss:0.10992\tval-merror:0.03870\tval-mlogloss:0.10549\n",
      "[999]\ttrain-merror:0.03128\ttrain-mlogloss:0.08450\ttest-merror:0.03955\ttest-mlogloss:0.10987\tval-merror:0.03877\tval-mlogloss:0.10543\n",
      "[999]\ttest-merror:0.039547\ttest-mlogloss:0.109873\n",
      "====================================================================================================\n",
      "fold 2\n",
      "[0]\ttrain-merror:0.06262\ttrain-mlogloss:1.06129\ttest-merror:0.05283\ttest-mlogloss:1.06255\tval-merror:0.06380\tval-mlogloss:1.06157\n",
      "[10]\ttrain-merror:0.05046\ttrain-mlogloss:0.78107\ttest-merror:0.05455\ttest-mlogloss:0.78587\tval-merror:0.05254\tval-mlogloss:0.78165\n",
      "[20]\ttrain-merror:0.04960\ttrain-mlogloss:0.59948\ttest-merror:0.05316\ttest-mlogloss:0.60569\tval-merror:0.05237\tval-mlogloss:0.60052\n",
      "[30]\ttrain-merror:0.04838\ttrain-mlogloss:0.47225\ttest-merror:0.05095\ttest-mlogloss:0.47954\tval-merror:0.05158\tval-mlogloss:0.47368\n",
      "[40]\ttrain-merror:0.04794\ttrain-mlogloss:0.38287\ttest-merror:0.05023\ttest-mlogloss:0.39092\tval-merror:0.05079\tval-mlogloss:0.38475\n",
      "[50]\ttrain-merror:0.04758\ttrain-mlogloss:0.31709\ttest-merror:0.05004\ttest-mlogloss:0.32546\tval-merror:0.05048\tval-mlogloss:0.31948\n",
      "[60]\ttrain-merror:0.04709\ttrain-mlogloss:0.26910\ttest-merror:0.04924\ttest-mlogloss:0.27727\tval-merror:0.04991\tval-mlogloss:0.27189\n",
      "[70]\ttrain-merror:0.04644\ttrain-mlogloss:0.23338\ttest-merror:0.04788\ttest-mlogloss:0.24141\tval-merror:0.04930\tval-mlogloss:0.23655\n",
      "[80]\ttrain-merror:0.04591\ttrain-mlogloss:0.20624\ttest-merror:0.04711\ttest-mlogloss:0.21378\tval-merror:0.04876\tval-mlogloss:0.20973\n",
      "[90]\ttrain-merror:0.04549\ttrain-mlogloss:0.18665\ttest-merror:0.04664\ttest-mlogloss:0.19420\tval-merror:0.04831\tval-mlogloss:0.19053\n",
      "[100]\ttrain-merror:0.04501\ttrain-mlogloss:0.17179\ttest-merror:0.04653\ttest-mlogloss:0.17942\tval-merror:0.04807\tval-mlogloss:0.17598\n",
      "[110]\ttrain-merror:0.04451\ttrain-mlogloss:0.15947\ttest-merror:0.04557\ttest-mlogloss:0.16672\tval-merror:0.04753\tval-mlogloss:0.16395\n",
      "[120]\ttrain-merror:0.04407\ttrain-mlogloss:0.14990\ttest-merror:0.04443\ttest-mlogloss:0.15701\tval-merror:0.04711\tval-mlogloss:0.15465\n",
      "[130]\ttrain-merror:0.04361\ttrain-mlogloss:0.14262\ttest-merror:0.04377\ttest-mlogloss:0.14970\tval-merror:0.04673\tval-mlogloss:0.14764\n",
      "[140]\ttrain-merror:0.04324\ttrain-mlogloss:0.13685\ttest-merror:0.04359\ttest-mlogloss:0.14407\tval-merror:0.04643\tval-mlogloss:0.14212\n",
      "[150]\ttrain-merror:0.04281\ttrain-mlogloss:0.13184\ttest-merror:0.04270\ttest-mlogloss:0.13906\tval-merror:0.04611\tval-mlogloss:0.13740\n",
      "[160]\ttrain-merror:0.04240\ttrain-mlogloss:0.12804\ttest-merror:0.04260\ttest-mlogloss:0.13548\tval-merror:0.04588\tval-mlogloss:0.13387\n",
      "[170]\ttrain-merror:0.04205\ttrain-mlogloss:0.12476\ttest-merror:0.04204\ttest-mlogloss:0.13251\tval-merror:0.04555\tval-mlogloss:0.13083\n",
      "[180]\ttrain-merror:0.04183\ttrain-mlogloss:0.12211\ttest-merror:0.04169\ttest-mlogloss:0.13007\tval-merror:0.04557\tval-mlogloss:0.12839\n",
      "[190]\ttrain-merror:0.04146\ttrain-mlogloss:0.11982\ttest-merror:0.04152\ttest-mlogloss:0.12814\tval-merror:0.04567\tval-mlogloss:0.12629\n",
      "[200]\ttrain-merror:0.04114\ttrain-mlogloss:0.11786\ttest-merror:0.04135\ttest-mlogloss:0.12651\tval-merror:0.04538\tval-mlogloss:0.12451\n",
      "[210]\ttrain-merror:0.04086\ttrain-mlogloss:0.11614\ttest-merror:0.04106\ttest-mlogloss:0.12502\tval-merror:0.04499\tval-mlogloss:0.12300\n",
      "[220]\ttrain-merror:0.04058\ttrain-mlogloss:0.11464\ttest-merror:0.04071\ttest-mlogloss:0.12376\tval-merror:0.04470\tval-mlogloss:0.12167\n",
      "[230]\ttrain-merror:0.04034\ttrain-mlogloss:0.11329\ttest-merror:0.04051\ttest-mlogloss:0.12258\tval-merror:0.04485\tval-mlogloss:0.12053\n",
      "[240]\ttrain-merror:0.04012\ttrain-mlogloss:0.11208\ttest-merror:0.04034\ttest-mlogloss:0.12156\tval-merror:0.04456\tval-mlogloss:0.11948\n",
      "[250]\ttrain-merror:0.03987\ttrain-mlogloss:0.11103\ttest-merror:0.04042\ttest-mlogloss:0.12082\tval-merror:0.04448\tval-mlogloss:0.11860\n",
      "[260]\ttrain-merror:0.03971\ttrain-mlogloss:0.10997\ttest-merror:0.04053\ttest-mlogloss:0.12003\tval-merror:0.04419\tval-mlogloss:0.11774\n",
      "[270]\ttrain-merror:0.03957\ttrain-mlogloss:0.10909\ttest-merror:0.04026\ttest-mlogloss:0.11939\tval-merror:0.04400\tval-mlogloss:0.11709\n",
      "[280]\ttrain-merror:0.03939\ttrain-mlogloss:0.10828\ttest-merror:0.04041\ttest-mlogloss:0.11890\tval-merror:0.04382\tval-mlogloss:0.11649\n",
      "[290]\ttrain-merror:0.03920\ttrain-mlogloss:0.10750\ttest-merror:0.04011\ttest-mlogloss:0.11840\tval-merror:0.04360\tval-mlogloss:0.11590\n",
      "[300]\ttrain-merror:0.03909\ttrain-mlogloss:0.10676\ttest-merror:0.04014\ttest-mlogloss:0.11788\tval-merror:0.04349\tval-mlogloss:0.11532\n",
      "[310]\ttrain-merror:0.03889\ttrain-mlogloss:0.10615\ttest-merror:0.04004\ttest-mlogloss:0.11750\tval-merror:0.04349\tval-mlogloss:0.11493\n",
      "[320]\ttrain-merror:0.03868\ttrain-mlogloss:0.10555\ttest-merror:0.03980\ttest-mlogloss:0.11707\tval-merror:0.04333\tval-mlogloss:0.11452\n",
      "[330]\ttrain-merror:0.03854\ttrain-mlogloss:0.10489\ttest-merror:0.03973\ttest-mlogloss:0.11658\tval-merror:0.04327\tval-mlogloss:0.11407\n",
      "[340]\ttrain-merror:0.03824\ttrain-mlogloss:0.10424\ttest-merror:0.03971\ttest-mlogloss:0.11614\tval-merror:0.04315\tval-mlogloss:0.11366\n",
      "[350]\ttrain-merror:0.03818\ttrain-mlogloss:0.10370\ttest-merror:0.03951\ttest-mlogloss:0.11590\tval-merror:0.04316\tval-mlogloss:0.11336\n",
      "[360]\ttrain-merror:0.03804\ttrain-mlogloss:0.10324\ttest-merror:0.03950\ttest-mlogloss:0.11559\tval-merror:0.04303\tval-mlogloss:0.11310\n",
      "[370]\ttrain-merror:0.03783\ttrain-mlogloss:0.10277\ttest-merror:0.03966\ttest-mlogloss:0.11540\tval-merror:0.04279\tval-mlogloss:0.11280\n",
      "[380]\ttrain-merror:0.03773\ttrain-mlogloss:0.10233\ttest-merror:0.03961\ttest-mlogloss:0.11528\tval-merror:0.04280\tval-mlogloss:0.11258\n",
      "[390]\ttrain-merror:0.03754\ttrain-mlogloss:0.10179\ttest-merror:0.03971\ttest-mlogloss:0.11484\tval-merror:0.04282\tval-mlogloss:0.11225\n",
      "[400]\ttrain-merror:0.03744\ttrain-mlogloss:0.10145\ttest-merror:0.03953\ttest-mlogloss:0.11472\tval-merror:0.04276\tval-mlogloss:0.11210\n",
      "[410]\ttrain-merror:0.03730\ttrain-mlogloss:0.10097\ttest-merror:0.03905\ttest-mlogloss:0.11434\tval-merror:0.04268\tval-mlogloss:0.11181\n",
      "[420]\ttrain-merror:0.03710\ttrain-mlogloss:0.10053\ttest-merror:0.03890\ttest-mlogloss:0.11425\tval-merror:0.04252\tval-mlogloss:0.11158\n",
      "[430]\ttrain-merror:0.03695\ttrain-mlogloss:0.10010\ttest-merror:0.03898\ttest-mlogloss:0.11402\tval-merror:0.04244\tval-mlogloss:0.11131\n",
      "[440]\ttrain-merror:0.03687\ttrain-mlogloss:0.09971\ttest-merror:0.03872\ttest-mlogloss:0.11376\tval-merror:0.04240\tval-mlogloss:0.11114\n",
      "[450]\ttrain-merror:0.03676\ttrain-mlogloss:0.09938\ttest-merror:0.03880\ttest-mlogloss:0.11361\tval-merror:0.04234\tval-mlogloss:0.11103\n",
      "[460]\ttrain-merror:0.03669\ttrain-mlogloss:0.09901\ttest-merror:0.03890\ttest-mlogloss:0.11343\tval-merror:0.04224\tval-mlogloss:0.11085\n",
      "[470]\ttrain-merror:0.03656\ttrain-mlogloss:0.09867\ttest-merror:0.03911\ttest-mlogloss:0.11331\tval-merror:0.04216\tval-mlogloss:0.11073\n",
      "[480]\ttrain-merror:0.03643\ttrain-mlogloss:0.09828\ttest-merror:0.03899\ttest-mlogloss:0.11305\tval-merror:0.04217\tval-mlogloss:0.11057\n",
      "[490]\ttrain-merror:0.03630\ttrain-mlogloss:0.09793\ttest-merror:0.03893\ttest-mlogloss:0.11289\tval-merror:0.04217\tval-mlogloss:0.11041\n",
      "[500]\ttrain-merror:0.03619\ttrain-mlogloss:0.09763\ttest-merror:0.03887\ttest-mlogloss:0.11280\tval-merror:0.04204\tval-mlogloss:0.11027\n",
      "[510]\ttrain-merror:0.03608\ttrain-mlogloss:0.09728\ttest-merror:0.03869\ttest-mlogloss:0.11267\tval-merror:0.04192\tval-mlogloss:0.11014\n",
      "[520]\ttrain-merror:0.03595\ttrain-mlogloss:0.09690\ttest-merror:0.03879\ttest-mlogloss:0.11260\tval-merror:0.04196\tval-mlogloss:0.10998\n",
      "[530]\ttrain-merror:0.03583\ttrain-mlogloss:0.09654\ttest-merror:0.03872\ttest-mlogloss:0.11259\tval-merror:0.04185\tval-mlogloss:0.10981\n",
      "[540]\ttrain-merror:0.03562\ttrain-mlogloss:0.09614\ttest-merror:0.03876\ttest-mlogloss:0.11242\tval-merror:0.04188\tval-mlogloss:0.10959\n",
      "[550]\ttrain-merror:0.03557\ttrain-mlogloss:0.09590\ttest-merror:0.03882\ttest-mlogloss:0.11240\tval-merror:0.04179\tval-mlogloss:0.10952\n",
      "[560]\ttrain-merror:0.03544\ttrain-mlogloss:0.09560\ttest-merror:0.03867\ttest-mlogloss:0.11217\tval-merror:0.04163\tval-mlogloss:0.10941\n",
      "[570]\ttrain-merror:0.03532\ttrain-mlogloss:0.09534\ttest-merror:0.03860\ttest-mlogloss:0.11200\tval-merror:0.04162\tval-mlogloss:0.10935\n",
      "[580]\ttrain-merror:0.03520\ttrain-mlogloss:0.09502\ttest-merror:0.03865\ttest-mlogloss:0.11190\tval-merror:0.04164\tval-mlogloss:0.10922\n",
      "[590]\ttrain-merror:0.03506\ttrain-mlogloss:0.09471\ttest-merror:0.03833\ttest-mlogloss:0.11175\tval-merror:0.04154\tval-mlogloss:0.10914\n",
      "[600]\ttrain-merror:0.03496\ttrain-mlogloss:0.09444\ttest-merror:0.03840\ttest-mlogloss:0.11161\tval-merror:0.04152\tval-mlogloss:0.10907\n",
      "[610]\ttrain-merror:0.03479\ttrain-mlogloss:0.09412\ttest-merror:0.03839\ttest-mlogloss:0.11156\tval-merror:0.04156\tval-mlogloss:0.10893\n",
      "[620]\ttrain-merror:0.03471\ttrain-mlogloss:0.09384\ttest-merror:0.03839\ttest-mlogloss:0.11140\tval-merror:0.04153\tval-mlogloss:0.10885\n",
      "[630]\ttrain-merror:0.03453\ttrain-mlogloss:0.09350\ttest-merror:0.03829\ttest-mlogloss:0.11117\tval-merror:0.04151\tval-mlogloss:0.10867\n",
      "[640]\ttrain-merror:0.03439\ttrain-mlogloss:0.09321\ttest-merror:0.03816\ttest-mlogloss:0.11108\tval-merror:0.04136\tval-mlogloss:0.10856\n",
      "[650]\ttrain-merror:0.03426\ttrain-mlogloss:0.09296\ttest-merror:0.03809\ttest-mlogloss:0.11102\tval-merror:0.04131\tval-mlogloss:0.10850\n",
      "[660]\ttrain-merror:0.03412\ttrain-mlogloss:0.09267\ttest-merror:0.03801\ttest-mlogloss:0.11082\tval-merror:0.04130\tval-mlogloss:0.10839\n",
      "[670]\ttrain-merror:0.03407\ttrain-mlogloss:0.09237\ttest-merror:0.03785\ttest-mlogloss:0.11075\tval-merror:0.04124\tval-mlogloss:0.10830\n",
      "[680]\ttrain-merror:0.03392\ttrain-mlogloss:0.09204\ttest-merror:0.03783\ttest-mlogloss:0.11059\tval-merror:0.04121\tval-mlogloss:0.10818\n",
      "[690]\ttrain-merror:0.03381\ttrain-mlogloss:0.09180\ttest-merror:0.03788\ttest-mlogloss:0.11055\tval-merror:0.04113\tval-mlogloss:0.10806\n",
      "[700]\ttrain-merror:0.03363\ttrain-mlogloss:0.09152\ttest-merror:0.03783\ttest-mlogloss:0.11043\tval-merror:0.04121\tval-mlogloss:0.10794\n",
      "[710]\ttrain-merror:0.03350\ttrain-mlogloss:0.09125\ttest-merror:0.03785\ttest-mlogloss:0.11033\tval-merror:0.04095\tval-mlogloss:0.10785\n",
      "[720]\ttrain-merror:0.03344\ttrain-mlogloss:0.09096\ttest-merror:0.03770\ttest-mlogloss:0.11027\tval-merror:0.04104\tval-mlogloss:0.10773\n",
      "[730]\ttrain-merror:0.03336\ttrain-mlogloss:0.09070\ttest-merror:0.03772\ttest-mlogloss:0.11015\tval-merror:0.04098\tval-mlogloss:0.10762\n",
      "[740]\ttrain-merror:0.03329\ttrain-mlogloss:0.09045\ttest-merror:0.03765\ttest-mlogloss:0.11009\tval-merror:0.04090\tval-mlogloss:0.10758\n",
      "[750]\ttrain-merror:0.03324\ttrain-mlogloss:0.09015\ttest-merror:0.03765\ttest-mlogloss:0.10990\tval-merror:0.04095\tval-mlogloss:0.10745\n",
      "[760]\ttrain-merror:0.03316\ttrain-mlogloss:0.08990\ttest-merror:0.03775\ttest-mlogloss:0.10985\tval-merror:0.04095\tval-mlogloss:0.10738\n",
      "[770]\ttrain-merror:0.03311\ttrain-mlogloss:0.08968\ttest-merror:0.03780\ttest-mlogloss:0.10976\tval-merror:0.04095\tval-mlogloss:0.10733\n",
      "[780]\ttrain-merror:0.03296\ttrain-mlogloss:0.08941\ttest-merror:0.03780\ttest-mlogloss:0.10986\tval-merror:0.04090\tval-mlogloss:0.10729\n",
      "[790]\ttrain-merror:0.03292\ttrain-mlogloss:0.08915\ttest-merror:0.03786\ttest-mlogloss:0.10974\tval-merror:0.04101\tval-mlogloss:0.10720\n",
      "[800]\ttrain-merror:0.03278\ttrain-mlogloss:0.08883\ttest-merror:0.03777\ttest-mlogloss:0.10962\tval-merror:0.04098\tval-mlogloss:0.10704\n",
      "[810]\ttrain-merror:0.03267\ttrain-mlogloss:0.08855\ttest-merror:0.03782\ttest-mlogloss:0.10950\tval-merror:0.04084\tval-mlogloss:0.10695\n",
      "[820]\ttrain-merror:0.03261\ttrain-mlogloss:0.08829\ttest-merror:0.03781\ttest-mlogloss:0.10937\tval-merror:0.04073\tval-mlogloss:0.10685\n",
      "[830]\ttrain-merror:0.03253\ttrain-mlogloss:0.08801\ttest-merror:0.03777\ttest-mlogloss:0.10929\tval-merror:0.04089\tval-mlogloss:0.10672\n",
      "[840]\ttrain-merror:0.03243\ttrain-mlogloss:0.08772\ttest-merror:0.03766\ttest-mlogloss:0.10916\tval-merror:0.04080\tval-mlogloss:0.10662\n",
      "[850]\ttrain-merror:0.03235\ttrain-mlogloss:0.08751\ttest-merror:0.03761\ttest-mlogloss:0.10908\tval-merror:0.04086\tval-mlogloss:0.10659\n",
      "[860]\ttrain-merror:0.03228\ttrain-mlogloss:0.08724\ttest-merror:0.03745\ttest-mlogloss:0.10890\tval-merror:0.04083\tval-mlogloss:0.10648\n",
      "[870]\ttrain-merror:0.03214\ttrain-mlogloss:0.08694\ttest-merror:0.03755\ttest-mlogloss:0.10879\tval-merror:0.04081\tval-mlogloss:0.10635\n",
      "[880]\ttrain-merror:0.03207\ttrain-mlogloss:0.08663\ttest-merror:0.03753\ttest-mlogloss:0.10868\tval-merror:0.04071\tval-mlogloss:0.10620\n",
      "[890]\ttrain-merror:0.03200\ttrain-mlogloss:0.08644\ttest-merror:0.03732\ttest-mlogloss:0.10861\tval-merror:0.04074\tval-mlogloss:0.10615\n",
      "[900]\ttrain-merror:0.03191\ttrain-mlogloss:0.08620\ttest-merror:0.03739\ttest-mlogloss:0.10855\tval-merror:0.04056\tval-mlogloss:0.10610\n",
      "[910]\ttrain-merror:0.03185\ttrain-mlogloss:0.08599\ttest-merror:0.03741\ttest-mlogloss:0.10855\tval-merror:0.04055\tval-mlogloss:0.10604\n",
      "[920]\ttrain-merror:0.03177\ttrain-mlogloss:0.08575\ttest-merror:0.03737\ttest-mlogloss:0.10848\tval-merror:0.04055\tval-mlogloss:0.10598\n",
      "[930]\ttrain-merror:0.03168\ttrain-mlogloss:0.08551\ttest-merror:0.03735\ttest-mlogloss:0.10842\tval-merror:0.04047\tval-mlogloss:0.10590\n",
      "[940]\ttrain-merror:0.03157\ttrain-mlogloss:0.08529\ttest-merror:0.03735\ttest-mlogloss:0.10836\tval-merror:0.04038\tval-mlogloss:0.10584\n",
      "[950]\ttrain-merror:0.03151\ttrain-mlogloss:0.08500\ttest-merror:0.03727\ttest-mlogloss:0.10819\tval-merror:0.04026\tval-mlogloss:0.10571\n",
      "[960]\ttrain-merror:0.03139\ttrain-mlogloss:0.08478\ttest-merror:0.03722\ttest-mlogloss:0.10819\tval-merror:0.04034\tval-mlogloss:0.10564\n",
      "[970]\ttrain-merror:0.03132\ttrain-mlogloss:0.08448\ttest-merror:0.03693\ttest-mlogloss:0.10789\tval-merror:0.04026\tval-mlogloss:0.10551\n",
      "[980]\ttrain-merror:0.03116\ttrain-mlogloss:0.08425\ttest-merror:0.03699\ttest-mlogloss:0.10791\tval-merror:0.04029\tval-mlogloss:0.10544\n",
      "[990]\ttrain-merror:0.03107\ttrain-mlogloss:0.08405\ttest-merror:0.03682\ttest-mlogloss:0.10787\tval-merror:0.04018\tval-mlogloss:0.10538\n",
      "[999]\ttrain-merror:0.03102\ttrain-mlogloss:0.08386\ttest-merror:0.03700\ttest-mlogloss:0.10783\tval-merror:0.04014\tval-mlogloss:0.10535\n",
      "[999]\ttest-merror:0.036996\ttest-mlogloss:0.107831\n",
      "====================================================================================================\n",
      "fold 3\n",
      "[0]\ttrain-merror:0.05780\ttrain-mlogloss:1.06075\ttest-merror:0.05623\ttest-mlogloss:1.06168\tval-merror:0.05857\tval-mlogloss:1.06097\n",
      "[10]\ttrain-merror:0.05146\ttrain-mlogloss:0.77807\ttest-merror:0.04943\ttest-mlogloss:0.78203\tval-merror:0.05253\tval-mlogloss:0.77833\n",
      "[20]\ttrain-merror:0.05023\ttrain-mlogloss:0.59981\ttest-merror:0.05271\ttest-mlogloss:0.60679\tval-merror:0.05121\tval-mlogloss:0.60054\n",
      "[30]\ttrain-merror:0.05071\ttrain-mlogloss:0.47871\ttest-merror:0.05713\ttest-mlogloss:0.48748\tval-merror:0.05186\tval-mlogloss:0.47973\n",
      "[40]\ttrain-merror:0.04958\ttrain-mlogloss:0.38955\ttest-merror:0.05443\ttest-mlogloss:0.39880\tval-merror:0.05090\tval-mlogloss:0.39092\n",
      "[50]\ttrain-merror:0.04881\ttrain-mlogloss:0.32264\ttest-merror:0.05266\ttest-mlogloss:0.33183\tval-merror:0.04997\tval-mlogloss:0.32427\n",
      "[60]\ttrain-merror:0.04806\ttrain-mlogloss:0.27498\ttest-merror:0.05240\ttest-mlogloss:0.28432\tval-merror:0.04960\tval-mlogloss:0.27688\n",
      "[70]\ttrain-merror:0.04743\ttrain-mlogloss:0.23911\ttest-merror:0.05084\ttest-mlogloss:0.24809\tval-merror:0.04874\tval-mlogloss:0.24123\n",
      "[80]\ttrain-merror:0.04694\ttrain-mlogloss:0.21216\ttest-merror:0.04954\ttest-mlogloss:0.22081\tval-merror:0.04794\tval-mlogloss:0.21455\n",
      "[90]\ttrain-merror:0.04647\ttrain-mlogloss:0.19090\ttest-merror:0.04755\ttest-mlogloss:0.19893\tval-merror:0.04748\tval-mlogloss:0.19357\n",
      "[100]\ttrain-merror:0.04601\ttrain-mlogloss:0.17453\ttest-merror:0.04621\ttest-mlogloss:0.18200\tval-merror:0.04691\tval-mlogloss:0.17747\n",
      "[110]\ttrain-merror:0.04542\ttrain-mlogloss:0.16197\ttest-merror:0.04567\ttest-mlogloss:0.16913\tval-merror:0.04652\tval-mlogloss:0.16513\n",
      "[120]\ttrain-merror:0.04518\ttrain-mlogloss:0.15233\ttest-merror:0.04478\ttest-mlogloss:0.15932\tval-merror:0.04641\tval-mlogloss:0.15570\n",
      "[130]\ttrain-merror:0.04468\ttrain-mlogloss:0.14455\ttest-merror:0.04427\ttest-mlogloss:0.15132\tval-merror:0.04588\tval-mlogloss:0.14813\n",
      "[140]\ttrain-merror:0.04417\ttrain-mlogloss:0.13838\ttest-merror:0.04353\ttest-mlogloss:0.14519\tval-merror:0.04564\tval-mlogloss:0.14220\n",
      "[150]\ttrain-merror:0.04374\ttrain-mlogloss:0.13355\ttest-merror:0.04303\ttest-mlogloss:0.14033\tval-merror:0.04541\tval-mlogloss:0.13759\n",
      "[160]\ttrain-merror:0.04326\ttrain-mlogloss:0.12958\ttest-merror:0.04262\ttest-mlogloss:0.13641\tval-merror:0.04506\tval-mlogloss:0.13388\n",
      "[170]\ttrain-merror:0.04297\ttrain-mlogloss:0.12627\ttest-merror:0.04211\ttest-mlogloss:0.13320\tval-merror:0.04490\tval-mlogloss:0.13082\n",
      "[180]\ttrain-merror:0.04269\ttrain-mlogloss:0.12340\ttest-merror:0.04207\ttest-mlogloss:0.13044\tval-merror:0.04460\tval-mlogloss:0.12820\n",
      "[190]\ttrain-merror:0.04236\ttrain-mlogloss:0.12116\ttest-merror:0.04163\ttest-mlogloss:0.12835\tval-merror:0.04444\tval-mlogloss:0.12621\n",
      "[200]\ttrain-merror:0.04202\ttrain-mlogloss:0.11905\ttest-merror:0.04129\ttest-mlogloss:0.12630\tval-merror:0.04436\tval-mlogloss:0.12437\n",
      "[210]\ttrain-merror:0.04179\ttrain-mlogloss:0.11726\ttest-merror:0.04122\ttest-mlogloss:0.12473\tval-merror:0.04424\tval-mlogloss:0.12284\n",
      "[220]\ttrain-merror:0.04152\ttrain-mlogloss:0.11570\ttest-merror:0.04087\ttest-mlogloss:0.12351\tval-merror:0.04397\tval-mlogloss:0.12153\n",
      "[230]\ttrain-merror:0.04133\ttrain-mlogloss:0.11426\ttest-merror:0.04062\ttest-mlogloss:0.12224\tval-merror:0.04371\tval-mlogloss:0.12037\n",
      "[240]\ttrain-merror:0.04110\ttrain-mlogloss:0.11310\ttest-merror:0.04069\ttest-mlogloss:0.12134\tval-merror:0.04358\tval-mlogloss:0.11942\n",
      "[250]\ttrain-merror:0.04089\ttrain-mlogloss:0.11200\ttest-merror:0.04093\ttest-mlogloss:0.12040\tval-merror:0.04354\tval-mlogloss:0.11850\n",
      "[260]\ttrain-merror:0.04057\ttrain-mlogloss:0.11098\ttest-merror:0.04077\ttest-mlogloss:0.11957\tval-merror:0.04334\tval-mlogloss:0.11773\n",
      "[270]\ttrain-merror:0.04041\ttrain-mlogloss:0.11019\ttest-merror:0.04054\ttest-mlogloss:0.11900\tval-merror:0.04318\tval-mlogloss:0.11712\n",
      "[280]\ttrain-merror:0.04010\ttrain-mlogloss:0.10931\ttest-merror:0.04057\ttest-mlogloss:0.11828\tval-merror:0.04303\tval-mlogloss:0.11638\n",
      "[290]\ttrain-merror:0.03992\ttrain-mlogloss:0.10845\ttest-merror:0.04043\ttest-mlogloss:0.11764\tval-merror:0.04282\tval-mlogloss:0.11576\n",
      "[300]\ttrain-merror:0.03984\ttrain-mlogloss:0.10778\ttest-merror:0.04021\ttest-mlogloss:0.11711\tval-merror:0.04270\tval-mlogloss:0.11530\n",
      "[310]\ttrain-merror:0.03967\ttrain-mlogloss:0.10712\ttest-merror:0.04018\ttest-mlogloss:0.11658\tval-merror:0.04274\tval-mlogloss:0.11485\n",
      "[320]\ttrain-merror:0.03948\ttrain-mlogloss:0.10651\ttest-merror:0.03991\ttest-mlogloss:0.11610\tval-merror:0.04260\tval-mlogloss:0.11442\n",
      "[330]\ttrain-merror:0.03929\ttrain-mlogloss:0.10588\ttest-merror:0.03963\ttest-mlogloss:0.11557\tval-merror:0.04246\tval-mlogloss:0.11397\n",
      "[340]\ttrain-merror:0.03909\ttrain-mlogloss:0.10532\ttest-merror:0.03948\ttest-mlogloss:0.11522\tval-merror:0.04237\tval-mlogloss:0.11363\n",
      "[350]\ttrain-merror:0.03904\ttrain-mlogloss:0.10483\ttest-merror:0.03967\ttest-mlogloss:0.11508\tval-merror:0.04212\tval-mlogloss:0.11335\n",
      "[360]\ttrain-merror:0.03894\ttrain-mlogloss:0.10436\ttest-merror:0.03980\ttest-mlogloss:0.11487\tval-merror:0.04194\tval-mlogloss:0.11305\n",
      "[370]\ttrain-merror:0.03880\ttrain-mlogloss:0.10388\ttest-merror:0.03966\ttest-mlogloss:0.11464\tval-merror:0.04196\tval-mlogloss:0.11282\n",
      "[380]\ttrain-merror:0.03859\ttrain-mlogloss:0.10348\ttest-merror:0.03964\ttest-mlogloss:0.11434\tval-merror:0.04217\tval-mlogloss:0.11262\n",
      "[390]\ttrain-merror:0.03850\ttrain-mlogloss:0.10304\ttest-merror:0.03966\ttest-mlogloss:0.11407\tval-merror:0.04195\tval-mlogloss:0.11240\n",
      "[400]\ttrain-merror:0.03836\ttrain-mlogloss:0.10254\ttest-merror:0.03956\ttest-mlogloss:0.11386\tval-merror:0.04172\tval-mlogloss:0.11207\n",
      "[410]\ttrain-merror:0.03818\ttrain-mlogloss:0.10212\ttest-merror:0.03962\ttest-mlogloss:0.11362\tval-merror:0.04173\tval-mlogloss:0.11185\n",
      "[420]\ttrain-merror:0.03805\ttrain-mlogloss:0.10165\ttest-merror:0.03952\ttest-mlogloss:0.11326\tval-merror:0.04151\tval-mlogloss:0.11162\n",
      "[430]\ttrain-merror:0.03792\ttrain-mlogloss:0.10132\ttest-merror:0.03938\ttest-mlogloss:0.11309\tval-merror:0.04163\tval-mlogloss:0.11150\n",
      "[440]\ttrain-merror:0.03778\ttrain-mlogloss:0.10102\ttest-merror:0.03936\ttest-mlogloss:0.11295\tval-merror:0.04158\tval-mlogloss:0.11138\n",
      "[450]\ttrain-merror:0.03772\ttrain-mlogloss:0.10061\ttest-merror:0.03929\ttest-mlogloss:0.11281\tval-merror:0.04163\tval-mlogloss:0.11117\n",
      "[460]\ttrain-merror:0.03756\ttrain-mlogloss:0.10020\ttest-merror:0.03931\ttest-mlogloss:0.11270\tval-merror:0.04144\tval-mlogloss:0.11096\n",
      "[470]\ttrain-merror:0.03741\ttrain-mlogloss:0.09987\ttest-merror:0.03917\ttest-mlogloss:0.11252\tval-merror:0.04138\tval-mlogloss:0.11078\n",
      "[480]\ttrain-merror:0.03727\ttrain-mlogloss:0.09956\ttest-merror:0.03916\ttest-mlogloss:0.11238\tval-merror:0.04129\tval-mlogloss:0.11065\n",
      "[490]\ttrain-merror:0.03706\ttrain-mlogloss:0.09924\ttest-merror:0.03902\ttest-mlogloss:0.11222\tval-merror:0.04139\tval-mlogloss:0.11050\n",
      "[500]\ttrain-merror:0.03702\ttrain-mlogloss:0.09889\ttest-merror:0.03907\ttest-mlogloss:0.11219\tval-merror:0.04138\tval-mlogloss:0.11031\n",
      "[510]\ttrain-merror:0.03678\ttrain-mlogloss:0.09850\ttest-merror:0.03895\ttest-mlogloss:0.11196\tval-merror:0.04128\tval-mlogloss:0.11012\n",
      "[520]\ttrain-merror:0.03670\ttrain-mlogloss:0.09813\ttest-merror:0.03880\ttest-mlogloss:0.11183\tval-merror:0.04125\tval-mlogloss:0.10993\n",
      "[530]\ttrain-merror:0.03661\ttrain-mlogloss:0.09780\ttest-merror:0.03867\ttest-mlogloss:0.11178\tval-merror:0.04106\tval-mlogloss:0.10982\n",
      "[540]\ttrain-merror:0.03645\ttrain-mlogloss:0.09746\ttest-merror:0.03879\ttest-mlogloss:0.11162\tval-merror:0.04099\tval-mlogloss:0.10969\n",
      "[550]\ttrain-merror:0.03634\ttrain-mlogloss:0.09714\ttest-merror:0.03873\ttest-mlogloss:0.11146\tval-merror:0.04095\tval-mlogloss:0.10955\n",
      "[560]\ttrain-merror:0.03623\ttrain-mlogloss:0.09685\ttest-merror:0.03869\ttest-mlogloss:0.11148\tval-merror:0.04085\tval-mlogloss:0.10942\n",
      "[570]\ttrain-merror:0.03611\ttrain-mlogloss:0.09652\ttest-merror:0.03858\ttest-mlogloss:0.11136\tval-merror:0.04084\tval-mlogloss:0.10929\n",
      "[580]\ttrain-merror:0.03601\ttrain-mlogloss:0.09621\ttest-merror:0.03862\ttest-mlogloss:0.11110\tval-merror:0.04076\tval-mlogloss:0.10910\n",
      "[590]\ttrain-merror:0.03589\ttrain-mlogloss:0.09592\ttest-merror:0.03855\ttest-mlogloss:0.11108\tval-merror:0.04076\tval-mlogloss:0.10900\n",
      "[600]\ttrain-merror:0.03575\ttrain-mlogloss:0.09558\ttest-merror:0.03852\ttest-mlogloss:0.11087\tval-merror:0.04078\tval-mlogloss:0.10878\n",
      "[610]\ttrain-merror:0.03564\ttrain-mlogloss:0.09521\ttest-merror:0.03843\ttest-mlogloss:0.11070\tval-merror:0.04086\tval-mlogloss:0.10859\n",
      "[620]\ttrain-merror:0.03552\ttrain-mlogloss:0.09488\ttest-merror:0.03843\ttest-mlogloss:0.11060\tval-merror:0.04083\tval-mlogloss:0.10846\n",
      "[630]\ttrain-merror:0.03538\ttrain-mlogloss:0.09461\ttest-merror:0.03833\ttest-mlogloss:0.11055\tval-merror:0.04075\tval-mlogloss:0.10838\n",
      "[640]\ttrain-merror:0.03523\ttrain-mlogloss:0.09426\ttest-merror:0.03814\ttest-mlogloss:0.11029\tval-merror:0.04068\tval-mlogloss:0.10817\n",
      "[650]\ttrain-merror:0.03505\ttrain-mlogloss:0.09400\ttest-merror:0.03795\ttest-mlogloss:0.11008\tval-merror:0.04067\tval-mlogloss:0.10811\n",
      "[660]\ttrain-merror:0.03485\ttrain-mlogloss:0.09378\ttest-merror:0.03784\ttest-mlogloss:0.10999\tval-merror:0.04059\tval-mlogloss:0.10805\n",
      "[670]\ttrain-merror:0.03476\ttrain-mlogloss:0.09351\ttest-merror:0.03774\ttest-mlogloss:0.10981\tval-merror:0.04048\tval-mlogloss:0.10799\n",
      "[680]\ttrain-merror:0.03464\ttrain-mlogloss:0.09322\ttest-merror:0.03787\ttest-mlogloss:0.10973\tval-merror:0.04051\tval-mlogloss:0.10787\n",
      "[690]\ttrain-merror:0.03460\ttrain-mlogloss:0.09293\ttest-merror:0.03774\ttest-mlogloss:0.10955\tval-merror:0.04033\tval-mlogloss:0.10772\n",
      "[700]\ttrain-merror:0.03454\ttrain-mlogloss:0.09265\ttest-merror:0.03764\ttest-mlogloss:0.10955\tval-merror:0.04025\tval-mlogloss:0.10763\n",
      "[710]\ttrain-merror:0.03442\ttrain-mlogloss:0.09237\ttest-merror:0.03773\ttest-mlogloss:0.10936\tval-merror:0.04030\tval-mlogloss:0.10753\n",
      "[720]\ttrain-merror:0.03421\ttrain-mlogloss:0.09210\ttest-merror:0.03771\ttest-mlogloss:0.10928\tval-merror:0.04037\tval-mlogloss:0.10742\n",
      "[730]\ttrain-merror:0.03412\ttrain-mlogloss:0.09183\ttest-merror:0.03782\ttest-mlogloss:0.10917\tval-merror:0.04045\tval-mlogloss:0.10733\n",
      "[740]\ttrain-merror:0.03399\ttrain-mlogloss:0.09157\ttest-merror:0.03778\ttest-mlogloss:0.10916\tval-merror:0.04031\tval-mlogloss:0.10725\n",
      "[750]\ttrain-merror:0.03393\ttrain-mlogloss:0.09136\ttest-merror:0.03815\ttest-mlogloss:0.10915\tval-merror:0.04028\tval-mlogloss:0.10720\n",
      "[760]\ttrain-merror:0.03382\ttrain-mlogloss:0.09109\ttest-merror:0.03799\ttest-mlogloss:0.10913\tval-merror:0.04015\tval-mlogloss:0.10712\n",
      "[770]\ttrain-merror:0.03375\ttrain-mlogloss:0.09083\ttest-merror:0.03795\ttest-mlogloss:0.10895\tval-merror:0.04011\tval-mlogloss:0.10705\n",
      "[780]\ttrain-merror:0.03370\ttrain-mlogloss:0.09056\ttest-merror:0.03777\ttest-mlogloss:0.10900\tval-merror:0.04005\tval-mlogloss:0.10694\n",
      "[790]\ttrain-merror:0.03360\ttrain-mlogloss:0.09028\ttest-merror:0.03802\ttest-mlogloss:0.10889\tval-merror:0.04006\tval-mlogloss:0.10684\n",
      "[800]\ttrain-merror:0.03342\ttrain-mlogloss:0.09000\ttest-merror:0.03804\ttest-mlogloss:0.10883\tval-merror:0.04004\tval-mlogloss:0.10671\n",
      "[810]\ttrain-merror:0.03336\ttrain-mlogloss:0.08978\ttest-merror:0.03803\ttest-mlogloss:0.10874\tval-merror:0.03993\tval-mlogloss:0.10665\n",
      "[820]\ttrain-merror:0.03320\ttrain-mlogloss:0.08944\ttest-merror:0.03798\ttest-mlogloss:0.10860\tval-merror:0.03978\tval-mlogloss:0.10645\n",
      "[830]\ttrain-merror:0.03311\ttrain-mlogloss:0.08917\ttest-merror:0.03789\ttest-mlogloss:0.10845\tval-merror:0.03976\tval-mlogloss:0.10633\n",
      "[840]\ttrain-merror:0.03304\ttrain-mlogloss:0.08892\ttest-merror:0.03763\ttest-mlogloss:0.10834\tval-merror:0.03979\tval-mlogloss:0.10625\n",
      "[850]\ttrain-merror:0.03295\ttrain-mlogloss:0.08871\ttest-merror:0.03783\ttest-mlogloss:0.10831\tval-merror:0.03980\tval-mlogloss:0.10623\n",
      "[860]\ttrain-merror:0.03292\ttrain-mlogloss:0.08848\ttest-merror:0.03790\ttest-mlogloss:0.10831\tval-merror:0.03974\tval-mlogloss:0.10617\n",
      "[870]\ttrain-merror:0.03273\ttrain-mlogloss:0.08825\ttest-merror:0.03802\ttest-mlogloss:0.10825\tval-merror:0.03978\tval-mlogloss:0.10609\n",
      "[880]\ttrain-merror:0.03263\ttrain-mlogloss:0.08803\ttest-merror:0.03795\ttest-mlogloss:0.10823\tval-merror:0.03985\tval-mlogloss:0.10604\n",
      "[890]\ttrain-merror:0.03251\ttrain-mlogloss:0.08782\ttest-merror:0.03788\ttest-mlogloss:0.10811\tval-merror:0.03978\tval-mlogloss:0.10598\n",
      "[900]\ttrain-merror:0.03241\ttrain-mlogloss:0.08756\ttest-merror:0.03770\ttest-mlogloss:0.10802\tval-merror:0.03981\tval-mlogloss:0.10589\n",
      "[910]\ttrain-merror:0.03223\ttrain-mlogloss:0.08731\ttest-merror:0.03761\ttest-mlogloss:0.10792\tval-merror:0.03988\tval-mlogloss:0.10582\n",
      "[920]\ttrain-merror:0.03223\ttrain-mlogloss:0.08709\ttest-merror:0.03779\ttest-mlogloss:0.10781\tval-merror:0.03978\tval-mlogloss:0.10575\n",
      "[930]\ttrain-merror:0.03209\ttrain-mlogloss:0.08676\ttest-merror:0.03778\ttest-mlogloss:0.10766\tval-merror:0.03971\tval-mlogloss:0.10557\n",
      "[940]\ttrain-merror:0.03197\ttrain-mlogloss:0.08654\ttest-merror:0.03775\ttest-mlogloss:0.10755\tval-merror:0.03957\tval-mlogloss:0.10552\n",
      "[950]\ttrain-merror:0.03189\ttrain-mlogloss:0.08628\ttest-merror:0.03774\ttest-mlogloss:0.10748\tval-merror:0.03961\tval-mlogloss:0.10539\n",
      "[960]\ttrain-merror:0.03173\ttrain-mlogloss:0.08609\ttest-merror:0.03775\ttest-mlogloss:0.10744\tval-merror:0.03961\tval-mlogloss:0.10536\n",
      "[970]\ttrain-merror:0.03160\ttrain-mlogloss:0.08578\ttest-merror:0.03777\ttest-mlogloss:0.10737\tval-merror:0.03960\tval-mlogloss:0.10522\n",
      "[980]\ttrain-merror:0.03146\ttrain-mlogloss:0.08554\ttest-merror:0.03758\ttest-mlogloss:0.10730\tval-merror:0.03949\tval-mlogloss:0.10512\n",
      "[990]\ttrain-merror:0.03135\ttrain-mlogloss:0.08532\ttest-merror:0.03755\ttest-mlogloss:0.10723\tval-merror:0.03950\tval-mlogloss:0.10503\n",
      "[999]\ttrain-merror:0.03127\ttrain-mlogloss:0.08508\ttest-merror:0.03760\ttest-mlogloss:0.10724\tval-merror:0.03953\tval-mlogloss:0.10494\n",
      "[999]\ttest-merror:0.037604\ttest-mlogloss:0.107245\n",
      "====================================================================================================\n",
      "fold 4\n",
      "[0]\ttrain-merror:0.06481\ttrain-mlogloss:1.06345\ttest-merror:0.06547\ttest-mlogloss:1.06530\tval-merror:0.06392\tval-mlogloss:1.06346\n",
      "[10]\ttrain-merror:0.05191\ttrain-mlogloss:0.78142\ttest-merror:0.05410\ttest-mlogloss:0.78597\tval-merror:0.05207\tval-mlogloss:0.78053\n",
      "[20]\ttrain-merror:0.05041\ttrain-mlogloss:0.59557\ttest-merror:0.05210\ttest-mlogloss:0.60172\tval-merror:0.05039\tval-mlogloss:0.59439\n",
      "[30]\ttrain-merror:0.05109\ttrain-mlogloss:0.47666\ttest-merror:0.05568\ttest-mlogloss:0.48389\tval-merror:0.05053\tval-mlogloss:0.47518\n",
      "[40]\ttrain-merror:0.05005\ttrain-mlogloss:0.38845\ttest-merror:0.05394\ttest-mlogloss:0.39518\tval-merror:0.05029\tval-mlogloss:0.38695\n",
      "[50]\ttrain-merror:0.04923\ttrain-mlogloss:0.32418\ttest-merror:0.05155\ttest-mlogloss:0.33033\tval-merror:0.04956\tval-mlogloss:0.32280\n",
      "[60]\ttrain-merror:0.04829\ttrain-mlogloss:0.27574\ttest-merror:0.04948\ttest-mlogloss:0.28115\tval-merror:0.04834\tval-mlogloss:0.27450\n",
      "[70]\ttrain-merror:0.04737\ttrain-mlogloss:0.23817\ttest-merror:0.04769\ttest-mlogloss:0.24230\tval-merror:0.04763\tval-mlogloss:0.23712\n",
      "[80]\ttrain-merror:0.04671\ttrain-mlogloss:0.21080\ttest-merror:0.04617\ttest-mlogloss:0.21411\tval-merror:0.04690\tval-mlogloss:0.20999\n",
      "[90]\ttrain-merror:0.04639\ttrain-mlogloss:0.19076\ttest-merror:0.04561\ttest-mlogloss:0.19376\tval-merror:0.04669\tval-mlogloss:0.19015\n",
      "[100]\ttrain-merror:0.04600\ttrain-mlogloss:0.17516\ttest-merror:0.04501\ttest-mlogloss:0.17764\tval-merror:0.04619\tval-mlogloss:0.17475\n",
      "[110]\ttrain-merror:0.04566\ttrain-mlogloss:0.16349\ttest-merror:0.04479\ttest-mlogloss:0.16578\tval-merror:0.04570\tval-mlogloss:0.16326\n",
      "[120]\ttrain-merror:0.04514\ttrain-mlogloss:0.15367\ttest-merror:0.04403\ttest-mlogloss:0.15557\tval-merror:0.04523\tval-mlogloss:0.15372\n",
      "[130]\ttrain-merror:0.04463\ttrain-mlogloss:0.14540\ttest-merror:0.04321\ttest-mlogloss:0.14713\tval-merror:0.04498\tval-mlogloss:0.14571\n",
      "[140]\ttrain-merror:0.04420\ttrain-mlogloss:0.13933\ttest-merror:0.04281\ttest-mlogloss:0.14091\tval-merror:0.04482\tval-mlogloss:0.13987\n",
      "[150]\ttrain-merror:0.04381\ttrain-mlogloss:0.13447\ttest-merror:0.04240\ttest-mlogloss:0.13604\tval-merror:0.04426\tval-mlogloss:0.13520\n",
      "[160]\ttrain-merror:0.04349\ttrain-mlogloss:0.13032\ttest-merror:0.04213\ttest-mlogloss:0.13181\tval-merror:0.04375\tval-mlogloss:0.13127\n",
      "[170]\ttrain-merror:0.04314\ttrain-mlogloss:0.12695\ttest-merror:0.04139\ttest-mlogloss:0.12835\tval-merror:0.04348\tval-mlogloss:0.12813\n",
      "[180]\ttrain-merror:0.04272\ttrain-mlogloss:0.12414\ttest-merror:0.04101\ttest-mlogloss:0.12582\tval-merror:0.04311\tval-mlogloss:0.12553\n",
      "[190]\ttrain-merror:0.04250\ttrain-mlogloss:0.12182\ttest-merror:0.04096\ttest-mlogloss:0.12362\tval-merror:0.04299\tval-mlogloss:0.12347\n",
      "[200]\ttrain-merror:0.04217\ttrain-mlogloss:0.11993\ttest-merror:0.04127\ttest-mlogloss:0.12204\tval-merror:0.04274\tval-mlogloss:0.12178\n",
      "[210]\ttrain-merror:0.04192\ttrain-mlogloss:0.11817\ttest-merror:0.04091\ttest-mlogloss:0.12062\tval-merror:0.04236\tval-mlogloss:0.12024\n",
      "[220]\ttrain-merror:0.04172\ttrain-mlogloss:0.11655\ttest-merror:0.04056\ttest-mlogloss:0.11921\tval-merror:0.04205\tval-mlogloss:0.11886\n",
      "[230]\ttrain-merror:0.04137\ttrain-mlogloss:0.11513\ttest-merror:0.04027\ttest-mlogloss:0.11811\tval-merror:0.04191\tval-mlogloss:0.11766\n",
      "[240]\ttrain-merror:0.04110\ttrain-mlogloss:0.11386\ttest-merror:0.03997\ttest-mlogloss:0.11702\tval-merror:0.04185\tval-mlogloss:0.11664\n",
      "[250]\ttrain-merror:0.04085\ttrain-mlogloss:0.11275\ttest-merror:0.04006\ttest-mlogloss:0.11608\tval-merror:0.04183\tval-mlogloss:0.11574\n",
      "[260]\ttrain-merror:0.04059\ttrain-mlogloss:0.11172\ttest-merror:0.03973\ttest-mlogloss:0.11535\tval-merror:0.04163\tval-mlogloss:0.11492\n",
      "[270]\ttrain-merror:0.04045\ttrain-mlogloss:0.11083\ttest-merror:0.03986\ttest-mlogloss:0.11467\tval-merror:0.04155\tval-mlogloss:0.11423\n",
      "[280]\ttrain-merror:0.04035\ttrain-mlogloss:0.11001\ttest-merror:0.03967\ttest-mlogloss:0.11404\tval-merror:0.04156\tval-mlogloss:0.11363\n",
      "[290]\ttrain-merror:0.04006\ttrain-mlogloss:0.10922\ttest-merror:0.03955\ttest-mlogloss:0.11358\tval-merror:0.04145\tval-mlogloss:0.11306\n",
      "[300]\ttrain-merror:0.03990\ttrain-mlogloss:0.10844\ttest-merror:0.03942\ttest-mlogloss:0.11310\tval-merror:0.04128\tval-mlogloss:0.11248\n",
      "[310]\ttrain-merror:0.03977\ttrain-mlogloss:0.10781\ttest-merror:0.03921\ttest-mlogloss:0.11272\tval-merror:0.04112\tval-mlogloss:0.11207\n",
      "[320]\ttrain-merror:0.03960\ttrain-mlogloss:0.10714\ttest-merror:0.03908\ttest-mlogloss:0.11228\tval-merror:0.04104\tval-mlogloss:0.11162\n",
      "[330]\ttrain-merror:0.03939\ttrain-mlogloss:0.10659\ttest-merror:0.03942\ttest-mlogloss:0.11200\tval-merror:0.04105\tval-mlogloss:0.11129\n",
      "[340]\ttrain-merror:0.03926\ttrain-mlogloss:0.10607\ttest-merror:0.03910\ttest-mlogloss:0.11165\tval-merror:0.04098\tval-mlogloss:0.11096\n",
      "[350]\ttrain-merror:0.03915\ttrain-mlogloss:0.10549\ttest-merror:0.03916\ttest-mlogloss:0.11137\tval-merror:0.04103\tval-mlogloss:0.11058\n",
      "[360]\ttrain-merror:0.03901\ttrain-mlogloss:0.10496\ttest-merror:0.03879\ttest-mlogloss:0.11104\tval-merror:0.04091\tval-mlogloss:0.11026\n",
      "[370]\ttrain-merror:0.03883\ttrain-mlogloss:0.10444\ttest-merror:0.03903\ttest-mlogloss:0.11084\tval-merror:0.04072\tval-mlogloss:0.10997\n",
      "[380]\ttrain-merror:0.03875\ttrain-mlogloss:0.10397\ttest-merror:0.03885\ttest-mlogloss:0.11064\tval-merror:0.04074\tval-mlogloss:0.10972\n",
      "[390]\ttrain-merror:0.03853\ttrain-mlogloss:0.10353\ttest-merror:0.03893\ttest-mlogloss:0.11042\tval-merror:0.04055\tval-mlogloss:0.10950\n",
      "[400]\ttrain-merror:0.03823\ttrain-mlogloss:0.10296\ttest-merror:0.03870\ttest-mlogloss:0.11009\tval-merror:0.04054\tval-mlogloss:0.10912\n",
      "[410]\ttrain-merror:0.03814\ttrain-mlogloss:0.10241\ttest-merror:0.03836\ttest-mlogloss:0.10974\tval-merror:0.04019\tval-mlogloss:0.10878\n",
      "[420]\ttrain-merror:0.03794\ttrain-mlogloss:0.10190\ttest-merror:0.03833\ttest-mlogloss:0.10937\tval-merror:0.04023\tval-mlogloss:0.10848\n",
      "[430]\ttrain-merror:0.03785\ttrain-mlogloss:0.10153\ttest-merror:0.03830\ttest-mlogloss:0.10923\tval-merror:0.04015\tval-mlogloss:0.10827\n",
      "[440]\ttrain-merror:0.03768\ttrain-mlogloss:0.10104\ttest-merror:0.03828\ttest-mlogloss:0.10905\tval-merror:0.04022\tval-mlogloss:0.10799\n",
      "[450]\ttrain-merror:0.03758\ttrain-mlogloss:0.10063\ttest-merror:0.03829\ttest-mlogloss:0.10883\tval-merror:0.04016\tval-mlogloss:0.10781\n",
      "[460]\ttrain-merror:0.03737\ttrain-mlogloss:0.10026\ttest-merror:0.03837\ttest-mlogloss:0.10863\tval-merror:0.03991\tval-mlogloss:0.10764\n",
      "[470]\ttrain-merror:0.03718\ttrain-mlogloss:0.09991\ttest-merror:0.03818\ttest-mlogloss:0.10844\tval-merror:0.04014\tval-mlogloss:0.10753\n",
      "[480]\ttrain-merror:0.03703\ttrain-mlogloss:0.09956\ttest-merror:0.03808\ttest-mlogloss:0.10831\tval-merror:0.03995\tval-mlogloss:0.10738\n",
      "[490]\ttrain-merror:0.03694\ttrain-mlogloss:0.09926\ttest-merror:0.03794\ttest-mlogloss:0.10822\tval-merror:0.03998\tval-mlogloss:0.10728\n",
      "[500]\ttrain-merror:0.03681\ttrain-mlogloss:0.09894\ttest-merror:0.03803\ttest-mlogloss:0.10816\tval-merror:0.04006\tval-mlogloss:0.10719\n",
      "[510]\ttrain-merror:0.03671\ttrain-mlogloss:0.09857\ttest-merror:0.03823\ttest-mlogloss:0.10800\tval-merror:0.04005\tval-mlogloss:0.10702\n",
      "[520]\ttrain-merror:0.03655\ttrain-mlogloss:0.09820\ttest-merror:0.03824\ttest-mlogloss:0.10787\tval-merror:0.04010\tval-mlogloss:0.10685\n",
      "[530]\ttrain-merror:0.03643\ttrain-mlogloss:0.09790\ttest-merror:0.03816\ttest-mlogloss:0.10777\tval-merror:0.04007\tval-mlogloss:0.10673\n",
      "[540]\ttrain-merror:0.03627\ttrain-mlogloss:0.09764\ttest-merror:0.03831\ttest-mlogloss:0.10772\tval-merror:0.04009\tval-mlogloss:0.10663\n",
      "[550]\ttrain-merror:0.03615\ttrain-mlogloss:0.09735\ttest-merror:0.03816\ttest-mlogloss:0.10762\tval-merror:0.03999\tval-mlogloss:0.10654\n",
      "[560]\ttrain-merror:0.03594\ttrain-mlogloss:0.09696\ttest-merror:0.03817\ttest-mlogloss:0.10756\tval-merror:0.04006\tval-mlogloss:0.10633\n",
      "[570]\ttrain-merror:0.03587\ttrain-mlogloss:0.09661\ttest-merror:0.03799\ttest-mlogloss:0.10728\tval-merror:0.03992\tval-mlogloss:0.10616\n",
      "[580]\ttrain-merror:0.03572\ttrain-mlogloss:0.09629\ttest-merror:0.03804\ttest-mlogloss:0.10721\tval-merror:0.03987\tval-mlogloss:0.10608\n",
      "[590]\ttrain-merror:0.03557\ttrain-mlogloss:0.09594\ttest-merror:0.03814\ttest-mlogloss:0.10720\tval-merror:0.03975\tval-mlogloss:0.10594\n",
      "[600]\ttrain-merror:0.03547\ttrain-mlogloss:0.09564\ttest-merror:0.03801\ttest-mlogloss:0.10713\tval-merror:0.03968\tval-mlogloss:0.10587\n",
      "[610]\ttrain-merror:0.03535\ttrain-mlogloss:0.09534\ttest-merror:0.03796\ttest-mlogloss:0.10698\tval-merror:0.03972\tval-mlogloss:0.10576\n",
      "[620]\ttrain-merror:0.03522\ttrain-mlogloss:0.09502\ttest-merror:0.03795\ttest-mlogloss:0.10686\tval-merror:0.03970\tval-mlogloss:0.10562\n",
      "[630]\ttrain-merror:0.03520\ttrain-mlogloss:0.09477\ttest-merror:0.03790\ttest-mlogloss:0.10679\tval-merror:0.03966\tval-mlogloss:0.10556\n",
      "[640]\ttrain-merror:0.03487\ttrain-mlogloss:0.09431\ttest-merror:0.03775\ttest-mlogloss:0.10664\tval-merror:0.03959\tval-mlogloss:0.10530\n",
      "[650]\ttrain-merror:0.03468\ttrain-mlogloss:0.09396\ttest-merror:0.03777\ttest-mlogloss:0.10652\tval-merror:0.03959\tval-mlogloss:0.10515\n",
      "[660]\ttrain-merror:0.03464\ttrain-mlogloss:0.09361\ttest-merror:0.03765\ttest-mlogloss:0.10636\tval-merror:0.03961\tval-mlogloss:0.10501\n",
      "[670]\ttrain-merror:0.03452\ttrain-mlogloss:0.09334\ttest-merror:0.03759\ttest-mlogloss:0.10629\tval-merror:0.03954\tval-mlogloss:0.10495\n",
      "[680]\ttrain-merror:0.03445\ttrain-mlogloss:0.09309\ttest-merror:0.03762\ttest-mlogloss:0.10632\tval-merror:0.03965\tval-mlogloss:0.10491\n",
      "[690]\ttrain-merror:0.03428\ttrain-mlogloss:0.09282\ttest-merror:0.03766\ttest-mlogloss:0.10626\tval-merror:0.03962\tval-mlogloss:0.10482\n",
      "[700]\ttrain-merror:0.03417\ttrain-mlogloss:0.09248\ttest-merror:0.03751\ttest-mlogloss:0.10613\tval-merror:0.03957\tval-mlogloss:0.10467\n",
      "[710]\ttrain-merror:0.03408\ttrain-mlogloss:0.09218\ttest-merror:0.03746\ttest-mlogloss:0.10605\tval-merror:0.03949\tval-mlogloss:0.10454\n",
      "[720]\ttrain-merror:0.03396\ttrain-mlogloss:0.09194\ttest-merror:0.03737\ttest-mlogloss:0.10602\tval-merror:0.03940\tval-mlogloss:0.10447\n",
      "[730]\ttrain-merror:0.03385\ttrain-mlogloss:0.09165\ttest-merror:0.03724\ttest-mlogloss:0.10589\tval-merror:0.03934\tval-mlogloss:0.10435\n",
      "[740]\ttrain-merror:0.03363\ttrain-mlogloss:0.09130\ttest-merror:0.03733\ttest-mlogloss:0.10570\tval-merror:0.03928\tval-mlogloss:0.10420\n",
      "[750]\ttrain-merror:0.03353\ttrain-mlogloss:0.09102\ttest-merror:0.03725\ttest-mlogloss:0.10568\tval-merror:0.03918\tval-mlogloss:0.10412\n",
      "[760]\ttrain-merror:0.03341\ttrain-mlogloss:0.09076\ttest-merror:0.03728\ttest-mlogloss:0.10562\tval-merror:0.03920\tval-mlogloss:0.10405\n",
      "[770]\ttrain-merror:0.03333\ttrain-mlogloss:0.09044\ttest-merror:0.03716\ttest-mlogloss:0.10539\tval-merror:0.03913\tval-mlogloss:0.10392\n",
      "[780]\ttrain-merror:0.03310\ttrain-mlogloss:0.09011\ttest-merror:0.03717\ttest-mlogloss:0.10523\tval-merror:0.03908\tval-mlogloss:0.10377\n",
      "[790]\ttrain-merror:0.03305\ttrain-mlogloss:0.08991\ttest-merror:0.03714\ttest-mlogloss:0.10526\tval-merror:0.03893\tval-mlogloss:0.10375\n",
      "[800]\ttrain-merror:0.03295\ttrain-mlogloss:0.08967\ttest-merror:0.03717\ttest-mlogloss:0.10525\tval-merror:0.03915\tval-mlogloss:0.10372\n",
      "[810]\ttrain-merror:0.03283\ttrain-mlogloss:0.08945\ttest-merror:0.03722\ttest-mlogloss:0.10521\tval-merror:0.03905\tval-mlogloss:0.10368\n",
      "[820]\ttrain-merror:0.03274\ttrain-mlogloss:0.08912\ttest-merror:0.03721\ttest-mlogloss:0.10515\tval-merror:0.03896\tval-mlogloss:0.10352\n",
      "[830]\ttrain-merror:0.03258\ttrain-mlogloss:0.08884\ttest-merror:0.03716\ttest-mlogloss:0.10510\tval-merror:0.03888\tval-mlogloss:0.10343\n",
      "[840]\ttrain-merror:0.03248\ttrain-mlogloss:0.08859\ttest-merror:0.03715\ttest-mlogloss:0.10492\tval-merror:0.03889\tval-mlogloss:0.10336\n",
      "[850]\ttrain-merror:0.03244\ttrain-mlogloss:0.08838\ttest-merror:0.03693\ttest-mlogloss:0.10492\tval-merror:0.03886\tval-mlogloss:0.10332\n",
      "[860]\ttrain-merror:0.03232\ttrain-mlogloss:0.08806\ttest-merror:0.03679\ttest-mlogloss:0.10470\tval-merror:0.03881\tval-mlogloss:0.10319\n",
      "[870]\ttrain-merror:0.03223\ttrain-mlogloss:0.08777\ttest-merror:0.03690\ttest-mlogloss:0.10457\tval-merror:0.03902\tval-mlogloss:0.10305\n",
      "[880]\ttrain-merror:0.03210\ttrain-mlogloss:0.08754\ttest-merror:0.03694\ttest-mlogloss:0.10438\tval-merror:0.03898\tval-mlogloss:0.10299\n",
      "[890]\ttrain-merror:0.03202\ttrain-mlogloss:0.08729\ttest-merror:0.03694\ttest-mlogloss:0.10429\tval-merror:0.03898\tval-mlogloss:0.10297\n",
      "[900]\ttrain-merror:0.03190\ttrain-mlogloss:0.08701\ttest-merror:0.03697\ttest-mlogloss:0.10418\tval-merror:0.03880\tval-mlogloss:0.10286\n",
      "[910]\ttrain-merror:0.03191\ttrain-mlogloss:0.08677\ttest-merror:0.03680\ttest-mlogloss:0.10409\tval-merror:0.03881\tval-mlogloss:0.10278\n",
      "[920]\ttrain-merror:0.03181\ttrain-mlogloss:0.08652\ttest-merror:0.03681\ttest-mlogloss:0.10400\tval-merror:0.03860\tval-mlogloss:0.10269\n",
      "[930]\ttrain-merror:0.03167\ttrain-mlogloss:0.08628\ttest-merror:0.03697\ttest-mlogloss:0.10398\tval-merror:0.03857\tval-mlogloss:0.10264\n",
      "[940]\ttrain-merror:0.03157\ttrain-mlogloss:0.08597\ttest-merror:0.03679\ttest-mlogloss:0.10386\tval-merror:0.03864\tval-mlogloss:0.10250\n",
      "[950]\ttrain-merror:0.03146\ttrain-mlogloss:0.08565\ttest-merror:0.03662\ttest-mlogloss:0.10373\tval-merror:0.03860\tval-mlogloss:0.10233\n",
      "[960]\ttrain-merror:0.03135\ttrain-mlogloss:0.08548\ttest-merror:0.03687\ttest-mlogloss:0.10376\tval-merror:0.03859\tval-mlogloss:0.10231\n",
      "[970]\ttrain-merror:0.03132\ttrain-mlogloss:0.08522\ttest-merror:0.03681\ttest-mlogloss:0.10360\tval-merror:0.03867\tval-mlogloss:0.10222\n",
      "[976]\ttrain-merror:0.03124\ttrain-mlogloss:0.08509\ttest-merror:0.03676\ttest-mlogloss:0.10362\tval-merror:0.03868\tval-mlogloss:0.10223\n",
      "[969]\ttest-merror:0.036764\ttest-mlogloss:0.103616\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "if 'CURRENT_TIME' in globals():\n",
    "    OUTPUT_DIRPATH, OLD_TIME = os.path.split(OUTPUT_DIRPATH)\n",
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "OUTPUT_DIRPATH = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME)\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "evals_result_dict = {f\"fold_{fold_idx}\": dict() for fold_idx in range(len(bdt_train_dict))}\n",
    "for fold_idx in range(len(bdt_train_dict)):\n",
    "    print(f\"fold {fold_idx}\")\n",
    "    # Train bdt\n",
    "    evallist = [(bdt_train_dict[f\"fold_{fold_idx}\"], 'train'), (bdt_test_dict[f\"fold_{fold_idx}\"], 'test'), (bdt_val_dict[f\"fold_{fold_idx}\"], 'val')]\n",
    "    booster = xgb.train(\n",
    "        param, bdt_train_dict[f\"fold_{fold_idx}\"], num_boost_round=num_trees, \n",
    "        evals=evallist, early_stopping_rounds=7, verbose_eval=10, evals_result=evals_result_dict[f\"fold_{fold_idx}\"]\n",
    "    )\n",
    "\n",
    "    booster.save_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "    \n",
    "    # Print perf on test dataset\n",
    "    print(booster.eval(bdt_test_dict[f\"fold_{fold_idx}\"], name='test', iteration=booster.best_iteration))\n",
    "    print('='*100)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_eval_result.json'), 'w') as f:\n",
    "    json.dump(evals_result_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_DIRPATH = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME)\n",
    "# if not os.path.exists(OUTPUT_DIRPATH):\n",
    "#     os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "base_tpr = np.linspace(0, 1, 5000)  # copied from IN evaluate.py file\n",
    "roc_baseline = np.zeros(\n",
    "    (len(bdt_train_dict), len(base_tpr), len(order)), \n",
    "    dtype=float\n",
    ")\n",
    "area_baseline = np.zeros(\n",
    "    (len(bdt_train_dict), len(order)), \n",
    "    dtype=float\n",
    ")\n",
    "\n",
    "BDT_perf = {\n",
    "    sample_name: copy.deepcopy({\n",
    "        'base_tpr': base_tpr,\n",
    "        'class_order': copy.deepcopy(order),\n",
    "        # test data #\n",
    "        'preds': [],\n",
    "        'fprs_density': copy.deepcopy(roc_baseline), 'thresholds_density': copy.deepcopy(roc_baseline), 'areas_density': copy.deepcopy(area_baseline),\n",
    "        'fprs_weighted': copy.deepcopy(roc_baseline), 'thresholds_weighted': copy.deepcopy(roc_baseline), 'areas_weighted': copy.deepcopy(area_baseline),\n",
    "        # 'fprs_sum': copy.deepcopy(roc_baseline[..., 0]), 'thresholds_sum': copy.deepcopy(roc_baseline[..., 0]), 'areas_sum': copy.deepcopy(area_baseline[..., 0]),\n",
    "        # train data #\n",
    "        'train_preds': [], \n",
    "        'train_fprs_density': copy.deepcopy(roc_baseline), 'train_thresholds_density': copy.deepcopy(roc_baseline), 'train_areas_density': copy.deepcopy(area_baseline),\n",
    "        'train_fprs_weighted': copy.deepcopy(roc_baseline), 'train_thresholds_weighted': copy.deepcopy(roc_baseline), 'train_areas_weighted': copy.deepcopy(area_baseline),\n",
    "        # 'train_fprs_sum': copy.deepcopy(roc_baseline[..., 0]), 'train_thresholds_sum': copy.deepcopy(roc_baseline[..., 0]), 'train_areas_sum': copy.deepcopy(area_baseline[..., 0]),\n",
    "        # val data #\n",
    "        'val_preds': [],\n",
    "        'val_fprs_density': copy.deepcopy(roc_baseline), 'val_thresholds_density': copy.deepcopy(roc_baseline), 'val_areas_density': copy.deepcopy(area_baseline),\n",
    "        'val_fprs_weighted': copy.deepcopy(roc_baseline), 'val_thresholds_weighted': copy.deepcopy(roc_baseline), 'val_areas_weighted': copy.deepcopy(area_baseline),\n",
    "        # 'val_fprs_sum': copy.deepcopy(roc_baseline[..., 0]), 'val_thresholds_sum': copy.deepcopy(roc_baseline[..., 0]), 'val_areas_sum': copy.deepcopy(area_baseline[..., 0]),\n",
    "    }) for sample_name in order\n",
    "}\n",
    "\n",
    "for j, sample_name in enumerate(order):\n",
    "\n",
    "    for fold_idx in range(len(bdt_train_dict)):\n",
    "        booster = xgb.Booster(param)\n",
    "        booster.load_model(os.path.join(OUTPUT_DIRPATH, f'{CURRENT_TIME}_BDT_fold{fold_idx}.model'))\n",
    "    \n",
    "        for pred_type, dataset in [\n",
    "            ('train_', bdt_train_dict[f\"fold_{fold_idx}\"]),\n",
    "            ('val_', bdt_val_dict[f\"fold_{fold_idx}\"]),\n",
    "            ('', bdt_test_dict[f\"fold_{fold_idx}\"])\n",
    "        ]:\n",
    "            BDT_perf[sample_name][pred_type + 'preds'].append(\n",
    "                booster.predict(dataset, iteration_range=(booster.best_iteration, booster.best_iteration+1)).tolist()\n",
    "            )\n",
    "\n",
    "            for i, sample_name_ in enumerate(order):\n",
    "                \n",
    "                if sample_name_ == sample_name:\n",
    "                    event_mask = dataset.get_label() > -1\n",
    "                    pred_rescale = np.ones_like(event_mask)\n",
    "                else:\n",
    "                    event_mask = np.logical_or(dataset.get_label() == j, dataset.get_label() == i)\n",
    "                    pred_rescale = np.array(BDT_perf[sample_name][pred_type + 'preds'][-1])[:, j][event_mask] + np.array(BDT_perf[sample_name][pred_type + 'preds'][-1])[:, i][event_mask]\n",
    "                class_preds = np.array(BDT_perf[sample_name][pred_type + 'preds'][-1])[:, j][event_mask] / pred_rescale\n",
    "                class_truths = np.where(dataset.get_label() == j, 1, 0)[event_mask]\n",
    "                \n",
    "                for roc_type in ['density', 'weighted']:\n",
    "\n",
    "                    if roc_type == 'weighted':\n",
    "                        if re.search('train', pred_type) is not None:\n",
    "                            roc_weights = weights_plot_train[f\"fold_{fold_idx}\"][event_mask]\n",
    "                        elif re.search('val', pred_type) is not None:\n",
    "                            roc_weights = weights_plot_val[f\"fold_{fold_idx}\"][event_mask]\n",
    "                        else:\n",
    "                            roc_weights = weight_test_dict[f\"fold_{fold_idx}\"][event_mask]\n",
    "                    else:\n",
    "                        roc_weights = None\n",
    "\n",
    "                    fpr_bdt, tpr_bdt, threshold_bdt = roc_curve(class_truths, class_preds, sample_weight=roc_weights)\n",
    "                    fpr_bdt = np.interp(base_tpr, tpr_bdt, fpr_bdt)\n",
    "                    threshold_bdt = np.interp(base_tpr, tpr_bdt, threshold_bdt)\n",
    "\n",
    "                    BDT_perf[sample_name][pred_type + 'fprs_' + roc_type][fold_idx][:, i] = fpr_bdt\n",
    "                    BDT_perf[sample_name][pred_type + 'thresholds_' + roc_type][fold_idx][:, i] = threshold_bdt\n",
    "                    BDT_perf[sample_name][pred_type + 'areas_' + roc_type][fold_idx][i] = float(trapezoid(base_tpr, fpr_bdt))\n",
    "    \n",
    "    for key in BDT_perf[sample_name].keys():\n",
    "        if type(BDT_perf[sample_name][key]) is list:\n",
    "            continue\n",
    "        BDT_perf[sample_name][key] = BDT_perf[sample_name][key].tolist()\n",
    "\n",
    "    # with h5py.File(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+f\"_BDT_ROC_fold{fold_idx}.h5\"), \"w\") as out:\n",
    "    #     out['FPR'] = fpr_bdt\n",
    "    #     out['TPR'] = tpr_bdt\n",
    "    #     out['Thresholds'] = threshold_bdt\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'w') as f:\n",
    "    json.dump(BDT_perf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_NAMES_PRETTY = {\n",
    "#     \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "#     \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "#     \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "#     \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "#     \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "#     \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "#     \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "#     \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "#     \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "#     # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "#     # Need to fill in pretty print for BSM samples #\n",
    "# }\n",
    "# LUMINOSITIES = {\n",
    "#     '2022preEE': 7.9804, \n",
    "#     '2022postEE': 26.6717,\n",
    "#     # Need to fill in lumis for other eras #\n",
    "# }\n",
    "# LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# # Dictionary of variables\n",
    "# VARIABLES = {\n",
    "#     # key: hist.axis axes for plotting #\n",
    "#     # MET variables\n",
    "#     'puppiMET_sumEt': hist.axis.Regular(40, 150., 2000, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-MET variables\n",
    "#     'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "#     'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-photon variables\n",
    "#     'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet variables\n",
    "#     # 'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     # 'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     'lead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     'lead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "#     'lead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     'sublead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     'sublead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "#     'sublead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "#     'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "#     'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "#     # lepton variables\n",
    "#     'n_leptons': hist.axis.Integer(0, 10, name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "#     'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "#     'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     # diphoton variables\n",
    "#     'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "#     'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     # angular (cos) variables\n",
    "#     'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "#     'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "#     'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "#     'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-lepton variables\n",
    "#     'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "#     # dijet variables (must be blinded on data)\n",
    "#     'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     # diphoton variables (must be blinded on data)\n",
    "#     'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "# }\n",
    "# # Dictionary of variables to do MC/Data comparison\n",
    "# VARIABLES_STD = {\n",
    "#     # key: hist.axis axes for plotting #\n",
    "#     # MET variables\n",
    "#     'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($\\Sigma E_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-MET variables\n",
    "#     'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "#     'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-photon variables\n",
    "#     'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet variables\n",
    "#     'lead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'sublead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'lead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "#     'sublead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "#     'lead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "#     'sublead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     'n_jets': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "#     'chi_t0': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t0}^2$)', growth=False, underflow=False, overflow=False), \n",
    "#     'chi_t1': hist.axis.Regular(40, -4., 10., name='var', label=r'ln($\\chi_{t1}^2$)', growth=False, underflow=False, overflow=False), \n",
    "#     # lepton variables\n",
    "#     'n_leptons': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "#     'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "#     'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "#     'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     # diphoton variables\n",
    "#     'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma$ ln($p_{T}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "#     'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "#     # angular (cos) variables\n",
    "#     'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "#     'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "#     'CosThetaStar_CS': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "#     'CosThetaStar_jj': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "#     # jet-lepton variables\n",
    "#     'leadBjet_leadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'leadBjet_subleadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'subleadBjet_leadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "#     'subleadBjet_subleadLepton': hist.axis.Regular(40, -4., 4. if re.search('vars_to_RNN', VARS) is not None else 10., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "#     # dijet variables (must be blinded on data)\n",
    "#     'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'ln($M_{jj}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "#     # diphoton variables (must be blinded on data)\n",
    "#     'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "# }\n",
    "\n",
    "# def post_std_np_arrays(\n",
    "#         data, data_test, fold, var_name, train_index=None, val_index=None\n",
    "# ):\n",
    "#     sig_mask = label_dict[f'fold_{fold}'] == 1\n",
    "#     sig_test_mask = label_test_dict[f'fold_{fold}'] == 1\n",
    "#     bkg_mask = label_dict[f'fold_{fold}'] == 0\n",
    "#     bkg_test_mask = label_test_dict[f'fold_{fold}'] == 0\n",
    "#     if train_index is not None and val_index is not None:\n",
    "#         sig_train_mask = sig_mask & train_index \n",
    "#         sig_val_mask = sig_mask & val_index\n",
    "#         bkg_train_mask = bkg_mask & train_index\n",
    "#         bkg_val_mask = bkg_mask & val_index\n",
    "#         if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "#             sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             bkg_train_np = data[data_list_index_map(var_name, data, bkg_train_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#         else:\n",
    "#             index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "#             sig_train_np = data[sig_train_mask, index2]\n",
    "#             sig_val_np = data[sig_val_mask, index2]\n",
    "#             sig_test_np = data_test[sig_test_mask, index2]\n",
    "#             bkg_train_np = data[bkg_train_mask, index2]\n",
    "#             bkg_val_np = data[bkg_val_mask, index2]\n",
    "#             bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "#         return (\n",
    "#             sig_train_np, sig_val_np, sig_test_np, \n",
    "#             bkg_train_np, bkg_val_np, bkg_test_np\n",
    "#         )\n",
    "#     elif train_index is None and val_index is None:\n",
    "#         if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "#             # index2, index3 = index_map[var_name]\n",
    "#             sig_train_np = data[data_list_index_map(var_name, data, sig_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#             bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask, n_pFields=N_PARTICLE_FIELDS)]\n",
    "#         else:\n",
    "#             index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "#             sig_train_np = data[sig_mask, index2]\n",
    "#             sig_test_np = data_test[sig_test_mask, index2]\n",
    "#             bkg_train_np = data[bkg_mask, index2]\n",
    "#             bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "#         return (\n",
    "#             copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "#             copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "#         )\n",
    "#     else:\n",
    "#         raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "# def aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold):\n",
    "#     sig_train_mask = (label_dict[f'fold_{fold}'] == 1) & (\n",
    "#         np.exp(IN_full_eval_dict['train']['all_preds'][fold])[:, 1] > score_cut\n",
    "#     )\n",
    "#     sig_test_mask = (label_test_dict[f'fold_{fold}'] == 1) & (\n",
    "#         np.exp(IN_full_eval_dict['test']['all_preds'][fold])[:, 1] > score_cut\n",
    "#     )\n",
    "#     bkg_train_mask = (label_dict[f'fold_{fold}'] == 0) & (\n",
    "#         np.exp(IN_full_eval_dict['train']['all_preds'][fold])[:, 1] > score_cut\n",
    "#     )\n",
    "#     bkg_test_mask = (label_test_dict[f'fold_{fold}'] == 0) & (\n",
    "#         np.exp(IN_full_eval_dict['test']['all_preds'][fold])[:, 1] > score_cut\n",
    "#     )\n",
    "\n",
    "#     sig_train_np = data_aux_dict[f'fold_{fold}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "#     sig_test_np = data_test_aux_dict[f'fold_{fold}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "#     bkg_train_np = data_aux_dict[f'fold_{fold}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "#     bkg_test_np = data_test_aux_dict[f'fold_{fold}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "#     return (\n",
    "#         copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "#         copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "#     )\n",
    "\n",
    "# def make_input_plot(\n",
    "#     output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, \n",
    "#     plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True\n",
    "# ):\n",
    "#     fig, ax = plt.subplots()\n",
    "#     if linestyle:\n",
    "#         if fold_idx is not None:\n",
    "#             linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "#         else:\n",
    "#             linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "#         linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "#         linestyles = linestyles[:len(hist_list)]\n",
    "#     else:\n",
    "#         linestyles = None\n",
    "#     hep.histplot(\n",
    "#         hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "#         linestyle=linestyles, label=labels, alpha=alpha\n",
    "#     )\n",
    "#     # Plotting niceties #\n",
    "#     hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "#     hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "#     # Plot legend properly\n",
    "#     ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "#     # Make angular and chi^2 plots linear, otherwise log\n",
    "#     if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "#         ax.set_yscale('log')\n",
    "#     else:\n",
    "#         ax.set_yscale('linear')\n",
    "#     ax.set_yscale('linear')\n",
    "#     # Save out the plot\n",
    "#     if fold_idx is not None:\n",
    "#         output_dir_ = os.path.join(output_dir, \"fold\")\n",
    "#         if not os.path.exists(output_dir_):\n",
    "#             os.makedirs(output_dir_)\n",
    "#         plt.savefig(f'{output_dir_}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "#         plt.savefig(f'{output_dir_}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "#     else:\n",
    "#         plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "#         plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "#     plt.close()\n",
    "\n",
    "# def plot_input_vars_after_score_cut(\n",
    "#     IN_info, score_cut, destdir, fold, plot_prefix, plot_postfix='', method='std', \n",
    "#     weights={'sig': None, 'bkg': None}, all_sig=False, all_bkg=False,\n",
    "#     mask=None\n",
    "# ):\n",
    "#     if method == 'std':\n",
    "#         if mask is None:\n",
    "#             mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "#         sig_mask = np.exp(\n",
    "#             IN_info['mean_pred']\n",
    "#         )[\n",
    "#             np.logical_and(\n",
    "#                 np.array(IN_info['mean_label']) == 1, mask\n",
    "#             ),1\n",
    "#         ] > score_cut\n",
    "#         bkg_mask = np.exp(\n",
    "#             IN_info['mean_pred']\n",
    "#         )[\n",
    "#             np.logical_and(\n",
    "#                 np.array(IN_info['mean_label']) == 0, mask\n",
    "#             ),1\n",
    "#         ] > score_cut\n",
    "\n",
    "#         for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "#             if var_name in {'event', 'puppiMET_eta'}:\n",
    "#                 continue\n",
    "#             sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "#             sig_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                 var=sig_var.loc[sig_mask], \n",
    "#                 weight=weights['sig'][sig_mask] if weights['sig'] is not None else np.ones(np.sum(sig_mask))\n",
    "#             )\n",
    "#             bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "#             bkg_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                 var=bkg_var.loc[bkg_mask], \n",
    "#                 weight=weights['bkg'][bkg_mask] if weights['bkg'] is not None else np.ones(np.sum(bkg_mask))\n",
    "#             )\n",
    "#             make_input_plot(\n",
    "#                 destdir, var_name, [sig_hist, bkg_hist], fold, plot_prefix=plot_prefix, \n",
    "#                 plot_postfix=plot_postfix+f'_ttHscore{score_cut}', labels=['HH signal', 'ttH background'], density=False if weights['sig'] is not None else True\n",
    "#             )\n",
    "#     elif method == 'round_robin':\n",
    "#         if mask is None:\n",
    "#             mask = np.ones(np.shape(IN_info['all_preds'][fold])[0], dtype=bool)\n",
    "        \n",
    "#         for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "#             if var_name in {'event', 'puppiMET_eta'}:\n",
    "#                 continue\n",
    "#             sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "#             bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "#             sig_masks, bkg_masks = [], []\n",
    "#             hists, labels = [], []\n",
    "#             for cut in score_cut if score_cut is list else [score_cut]:\n",
    "#                 sig_masks.append(np.exp(\n",
    "#                     IN_info['all_preds'][fold]\n",
    "#                 )[\n",
    "#                     np.logical_and(\n",
    "#                         np.array(IN_info['all_labels'][fold]) == 1, mask\n",
    "#                     ),1\n",
    "#                 ] > cut)\n",
    "#                 bkg_masks.append(np.exp(\n",
    "#                     IN_info['all_preds'][fold]\n",
    "#                 )[\n",
    "#                     np.logical_and(\n",
    "#                         np.array(IN_info['all_labels'][fold]) == 0, mask\n",
    "#                     ),1\n",
    "#                 ] > cut)\n",
    "                \n",
    "#                 hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                     var=sig_var.loc[sig_masks[-1]], \n",
    "#                     weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "#                 ))\n",
    "#                 hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                     var=bkg_var.loc[bkg_masks[-1]], \n",
    "#                     weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "#                 ))\n",
    "#                 labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "#             make_input_plot(\n",
    "#                 destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "#                 plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "#             )\n",
    "#     elif method == 'arr':\n",
    "#         if mask is None:\n",
    "#             mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "#         for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "#             if var_name in {'event', 'puppiMET_eta'}:\n",
    "#                 continue\n",
    "#             sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "#             bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "#             sig_masks, bkg_masks = [], []\n",
    "#             hists, labels = [], []\n",
    "#             for cut in score_cut:\n",
    "#                 sig_masks.append(np.exp(\n",
    "#                     IN_info['mean_pred']\n",
    "#                 )[\n",
    "#                     np.logical_and(\n",
    "#                         np.array(IN_info['mean_label']) == 1, mask\n",
    "#                     ),1\n",
    "#                 ] > cut)\n",
    "#                 bkg_masks.append(np.exp(\n",
    "#                     IN_info['mean_pred']\n",
    "#                 )[\n",
    "#                     np.logical_and(\n",
    "#                         np.array(IN_info['mean_label']) == 0, mask\n",
    "#                     ),1\n",
    "#                 ] > cut)\n",
    "                \n",
    "#                 hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                     var=sig_var.loc[sig_masks[-1]], \n",
    "#                     weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "#                 ))\n",
    "#                 hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "#                     var=bkg_var.loc[bkg_masks[-1]], \n",
    "#                     weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "#                 ))\n",
    "#                 labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "#             make_input_plot(\n",
    "#                 destdir, var_name, hists, fold, plot_prefix=plot_prefix, \n",
    "#                 plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "#             )\n",
    "#     else:\n",
    "#         raise Exception(f\"Must used method 'std'. You used {method}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='png'):\n",
    "    plot_prefix = plot_prefix + ('_' if plot_prefix != '' else '')\n",
    "    plot_postfix = plot_postfix + ('_' if plot_postfix != '' else '')\n",
    "    plot_name = plot_prefix + plot_name + plot_postfix + f'.{format}'\n",
    "\n",
    "    plot_filepath = os.path.join(plot_dirpath, plot_name)\n",
    "    return plot_filepath\n",
    "\n",
    "def plot_rocs(\n",
    "    fprs, tprs, labels, plot_name, plot_dirpath,\n",
    "    plot_prefix='', plot_postfix='', close=True, log=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    \n",
    "    for fpr, tpr, label in zip(fprs, tprs, labels):\n",
    "        linestyle = 'solid' if re.search('IN', label) is not None else ('dashed' if re.search('BDT', label) is not None else 'dotted')\n",
    "        plt.plot(fpr, tpr, label=label, linestyle=linestyle)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    if log is not None and re.search('x', log) is not None:\n",
    "        plt.xscale('log')\n",
    "    elif log is not None and re.search('y', log) is not None:\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    if close:\n",
    "        plt.close()\n",
    "\n",
    "def plot_output_scores(\n",
    "    sigs_and_bkgs, order, plot_name, plot_dirpath,\n",
    "    plot_prefix='', plot_postfix='', bins=50, weights=None, log=False\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "\n",
    "    hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "    hists = {}\n",
    "    for sample_name in order:\n",
    "        hists[sample_name] = hist.Hist(hist_axis, storage='weight').fill(\n",
    "            var=sigs_and_bkgs[sample_name], \n",
    "            weight=weights[sample_name] if weights is not None else np.ones_like(sigs_and_bkgs[sample_name])\n",
    "        )\n",
    "    hep.histplot(\n",
    "        [hists[sample_name] for sample_name in order],\n",
    "        yerr=(True if weights is not None else False),\n",
    "        alpha=0.7, density=(False if weights is not None else True), histtype='step',\n",
    "        label=[sample_name for sample_name in order]\n",
    "    )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Output score')\n",
    "    if log:\n",
    "        plt.yscale('log')\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "def plot_s_over_root_b(\n",
    "    sig, bkg, sample_name, plot_name, plot_dirpath,\n",
    "    plot_prefix='', plot_postfix='', bins=50, weights={'sig': None, 'bkg': None},\n",
    "    lines=None, lines_labels=None, line_colors=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "\n",
    "    hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "    sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig))\n",
    "    bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg, weight=weights['bkg'] if weights['bkg'] is not None else np.ones_like(bkg))\n",
    "    s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "    plt.plot(\n",
    "        np.arange(0., 1., 1/bins), s_over_root_b_points, \n",
    "        label=f'{sample_name} - s/âb', alpha=0.8\n",
    "    )\n",
    "\n",
    "    if lines is not None:\n",
    "        for i in range(len(lines)):\n",
    "            plt.vlines(\n",
    "                lines[i], 0, np.max(s_over_root_b_points), \n",
    "                label='s/âb'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                alpha=0.5, colors=line_colors[i]\n",
    "            )\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Output score')\n",
    "    plt.ylabel('s/âb')\n",
    "    \n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.savefig(\n",
    "        plot_filepath(plot_name, plot_dirpath, plot_prefix, plot_postfix, format='pdf'), \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(sigs, bkgs, weights, bins=50):\n",
    "    hist_list_fold = []\n",
    "    cut_boundaries_fold = []\n",
    "    cut_s_over_root_bs_fold = []\n",
    "    sig_weights_fold = []\n",
    "    bkg_weights_fold = []\n",
    "    if len(np.shape(sigs)) == 1:\n",
    "        sigs, bkgs = [sigs], [bkgs] \n",
    "    for sig, bkg in zip(sigs, bkgs):\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg, weight=weights['bkg'])\n",
    "        hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "        fold_idx_cuts_bins_inclusive = []\n",
    "        fold_idx_sig_weights = []\n",
    "        fold_idx_bkg_weights = []\n",
    "        fold_idx_prev_s_over_root_b = []\n",
    "        prev_s_over_root_b = 0\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                fold_idx_sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        fold_idx_sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_cuts_bins_inclusive.append(0)\n",
    "        fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "        fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "        cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "        cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "        sig_weights_fold.append(fold_idx_sig_weights)\n",
    "        bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "    return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(\n",
    "    losses_arrs, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None, n_folds=5\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    \n",
    "    if len(np.shape(losses_arrs)) > 1:\n",
    "        losses_arrs = [losses_arrs]\n",
    "    for i, loss_arr in enumerate(losses_arrs):\n",
    "        plt.plot(\n",
    "            range(len(loss_arr)), loss_arr, label=f\"{labels[i]} losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        \n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_DIRPATH = os.path.join(OUTPUT_DIRPATH, CURRENT_TIME)\n",
    "plot_dirpath = os.path.join(OUTPUT_DIRPATH, \"plots\")\n",
    "if not os.path.exists(plot_dirpath):\n",
    "    os.makedirs(plot_dirpath)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIRPATH, CURRENT_TIME+\"_BDT_perf.json\"), 'r') as f:\n",
    "    BDT_perf = json.load(f)\n",
    "base_tpr = np.array(BDT_perf['tprs'][0])[:, 0]\n",
    "\n",
    "# plot ROCs\n",
    "avg_fprs, avg_aucs = [], []\n",
    "for fold_idx in range(len(bdt_train_dict)):\n",
    "    fprs = [np.array(BDT_perf['fprs'][fold_idx])[:, i] for i in range(len(order))]\n",
    "    avg_fprs.append(copy.deepcopy(BDT_perf['fprs'][fold_idx]))\n",
    "\n",
    "    tprs = [base_tpr for i in range(len(order))]\n",
    "\n",
    "    labels = [f\"{sample_name}, AUC = {BDT_perf['areas'][fold_idx][i]:.4f}\" for i, sample_name in enumerate(order)]\n",
    "\n",
    "    avg_aucs.append(copy.deepcopy(BDT_perf['areas'][fold_idx]))\n",
    "\n",
    "    plot_rocs(fprs, tprs, labels, f\"BDT_rocDensity_testData_fold{fold_idx}\", plot_dirpath)\n",
    "# plot_rocs(\n",
    "#     np.mean(avg_fprs, axis=0).T, [base_tpr for i in range(len(order))], \n",
    "#     [f\"{sample_name}, AUC = {np.mean(avg_aucs, axis=0)[i]:.4f}\" for i, sample_name in enumerate(order)], \n",
    "#     f\"BDT_roc_testData_Avg\", plot_dirpath\n",
    "# )\n",
    "flat_preds = np.concatenate(BDT_perf['preds'], axis=0)\n",
    "flat_truths = np.concatenate([bdt_test_dict[f\"fold_{fold_idx}\"].get_label() for fold_idx in range(len(bdt_train_dict))], axis=0)\n",
    "fprs, aucs = [], []\n",
    "for i, sample_name in enumerate(order):\n",
    "    fpr, tpr, threshold = roc_curve(flat_truths == i, flat_preds[:, i])\n",
    "    fpr = np.interp(base_tpr, tpr, fpr)\n",
    "    fpr[0] = 0.0\n",
    "    fprs.append(copy.deepcopy(fpr))\n",
    "    aucs.append(float(auc(fpr, base_tpr)))\n",
    "plot_rocs(\n",
    "    fprs, [base_tpr for i in range(len(order))], \n",
    "    [f\"{sample_name}-vs-all, AUC = {aucs[i]:.4f}\" for i, sample_name in enumerate(order)], \n",
    "    f\"BDT_rocDensity_testData_sum\", plot_dirpath\n",
    ")\n",
    "\n",
    "# plot Output scores and s/âb curves\n",
    "weights_sum, sigs_and_bkgs_sum = {}, {}\n",
    "for fold_idx in range(len(bdt_train_dict)):\n",
    "    weights_plot = {\n",
    "        sample_name: {\n",
    "            sample_name_: weight_test_dict[f\"fold_{fold_idx}\"][xgb_label_test_dict[f\"fold_{fold_idx}\"] == i] for i, sample_name_ in enumerate(order)\n",
    "        } for sample_name in order\n",
    "    }\n",
    "\n",
    "    sigs_and_bkgs = {\n",
    "        sample_name: {\n",
    "            sample_name_: np.array(BDT_perf['preds'][fold_idx])[:, j][xgb_label_test_dict[f\"fold_{fold_idx}\"] == i] for i, sample_name_ in enumerate(order)\n",
    "        } for j, sample_name in enumerate(order)\n",
    "    }\n",
    "\n",
    "    if fold_idx == 0:\n",
    "        weights_sum = copy.deepcopy(weights_plot)\n",
    "        sigs_and_bkgs_sum = copy.deepcopy(sigs_and_bkgs)\n",
    "    else:\n",
    "        for sample_name in order:\n",
    "            for sample_name_ in order:\n",
    "                weights_sum[sample_name][sample_name_] = np.concatenate((weights_sum[sample_name][sample_name_], weights_plot[sample_name][sample_name_]))\n",
    "                sigs_and_bkgs_sum[sample_name][sample_name_] = np.concatenate((sigs_and_bkgs_sum[sample_name][sample_name_], sigs_and_bkgs[sample_name][sample_name_]))\n",
    "\n",
    "    for sample_name in order:\n",
    "        plot_output_scores(\n",
    "            sigs_and_bkgs[sample_name], order, f\"BDT_outputScoreWeighted_testData_fold{fold_idx}_{sample_name}\", \n",
    "            plot_dirpath, weights=weights_plot[sample_name], log=True\n",
    "        )\n",
    "        plot_output_scores(\n",
    "            sigs_and_bkgs[sample_name], order, f\"BDT_outputScoreDensity_testData_fold{fold_idx}_{sample_name}\", \n",
    "            plot_dirpath\n",
    "        )\n",
    "for sample_name in order:\n",
    "    plot_output_scores(\n",
    "        sigs_and_bkgs_sum[sample_name], order, f\"BDT_outputScoreWeighted_testData_sum_{sample_name}\", \n",
    "        plot_dirpath, weights=weights_sum[sample_name], log=True\n",
    "    )\n",
    "    plot_output_scores(\n",
    "        sigs_and_bkgs_sum[sample_name], order, f\"BDT_outputScoreDensity_testData_sum_{sample_name}\", \n",
    "        plot_dirpath\n",
    "    )\n",
    "\n",
    "# plot s/âb curves\n",
    "weights_sum_sob, sigs_and_bkgs_sum_sob = {}, {}\n",
    "for fold_idx in range(len(bdt_train_dict)):\n",
    "    weights_plot = {\n",
    "        sample_name: {\n",
    "            'sig': weight_test_dict[f\"fold_{fold_idx}\"][xgb_label_test_dict[f\"fold_{fold_idx}\"] == j],\n",
    "            'bkg': weight_test_dict[f\"fold_{fold_idx}\"][xgb_label_test_dict[f\"fold_{fold_idx}\"] != j],\n",
    "        } for j, sample_name in enumerate(order)\n",
    "    }\n",
    "\n",
    "    sigs_and_bkgs = {\n",
    "        sample_name: {\n",
    "            'sig': np.array(BDT_perf['preds'][fold_idx])[:, j][xgb_label_test_dict[f\"fold_{fold_idx}\"] == j],\n",
    "            'bkg': np.array(BDT_perf['preds'][fold_idx])[:, j][xgb_label_test_dict[f\"fold_{fold_idx}\"] != j],\n",
    "        } for j, sample_name in enumerate(order)\n",
    "    }\n",
    "\n",
    "    if fold_idx == 0:\n",
    "        weights_sum = copy.deepcopy(weights_plot)\n",
    "        sigs_and_bkgs_sum = copy.deepcopy(sigs_and_bkgs)\n",
    "    else:\n",
    "        for sample_name in order:\n",
    "            for sample_name_ in weights_plot[sample_name].keys():\n",
    "                weights_sum[sample_name][sample_name_] = np.concatenate((weights_sum[sample_name][sample_name_], weights_plot[sample_name][sample_name_]))\n",
    "                sigs_and_bkgs_sum[sample_name][sample_name_] = np.concatenate((sigs_and_bkgs_sum[sample_name][sample_name_], sigs_and_bkgs[sample_name][sample_name_]))\n",
    "\n",
    "    for sample_name in order:\n",
    "        plot_s_over_root_b(\n",
    "            sigs_and_bkgs[sample_name]['sig'], sigs_and_bkgs[sample_name]['bkg'], sample_name, f\"BDT_sOverRootb_testData_fold{fold_idx}_{sample_name}\", \n",
    "            plot_dirpath, weights=weights_plot[sample_name]\n",
    "        )\n",
    "\n",
    "        (\n",
    "            cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "        ) = optimize_cut_boundaries(\n",
    "            sigs_and_bkgs[sample_name]['sig'], sigs_and_bkgs[sample_name]['bkg'], \n",
    "            weights_plot[sample_name]\n",
    "        )\n",
    "\n",
    "        BDT_cut_labels = [\n",
    "            f\"s/âb={cut_s_over_root_bs_fold[0][cut_idx]:.4f}, s={sig_weights_fold[0][cut_idx]['value']:.4f}Â±{sig_weights_fold[0][cut_idx]['w2']:.4f}, b={bkg_weights_fold[0][cut_idx]['value']:.4f}Â±{bkg_weights_fold[0][cut_idx]['w2']:.4f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[0]))\n",
    "        ]\n",
    "        line_labels = BDT_cut_labels\n",
    "        lines = cut_boundaries_fold[0]\n",
    "        line_colors = cmap_petroff10\n",
    "\n",
    "        plot_s_over_root_b(\n",
    "            sigs_and_bkgs[sample_name]['sig'], sigs_and_bkgs[sample_name]['bkg'], sample_name, \n",
    "            f\"BDT_sOverRootb_withCuts_testData_fold{fold_idx}_{sample_name}\", plot_dirpath, \n",
    "            weights=weights_plot[sample_name],\n",
    "            lines=lines, lines_labels=line_labels, line_colors=line_colors\n",
    "        )\n",
    "for sample_name in order:\n",
    "    plot_s_over_root_b(\n",
    "        sigs_and_bkgs_sum[sample_name]['sig'], sigs_and_bkgs_sum[sample_name]['bkg'], sample_name, f\"BDT_sOverRootb_testData_sum_{sample_name}\", \n",
    "        plot_dirpath, weights=weights_sum[sample_name]\n",
    "    )\n",
    "\n",
    "    (\n",
    "        cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "    ) = optimize_cut_boundaries(\n",
    "        sigs_and_bkgs_sum[sample_name]['sig'], sigs_and_bkgs_sum[sample_name]['bkg'], \n",
    "        weights_sum[sample_name]\n",
    "    )\n",
    "\n",
    "    BDT_cut_labels = [\n",
    "        f\"s/âb={cut_s_over_root_bs_fold[0][cut_idx]:.4f}, s={sig_weights_fold[0][cut_idx]['value']:.4f}Â±{sig_weights_fold[0][cut_idx]['w2']:.4f}, b={bkg_weights_fold[0][cut_idx]['value']:.4f}Â±{bkg_weights_fold[0][cut_idx]['w2']:.4f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[0]))\n",
    "    ]\n",
    "    line_labels = BDT_cut_labels\n",
    "    lines = cut_boundaries_fold[0]\n",
    "    line_colors = cmap_petroff10\n",
    "\n",
    "    plot_s_over_root_b(\n",
    "        sigs_and_bkgs_sum[sample_name]['sig'], sigs_and_bkgs_sum[sample_name]['bkg'], sample_name, \n",
    "        f\"BDT_sOverRootb_withCuts_testData_sum_{sample_name}\", plot_dirpath, \n",
    "        weights=weights_sum[sample_name],\n",
    "        lines=lines, lines_labels=line_labels, line_colors=line_colors\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(booster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "xgb.to_graphviz(booster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
