{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu3.fnal.gov      Fri Oct 11 13:16:33 2024  555.42.06\n",
      "[0] Tesla P100-PCIE-12GB | 60Â°C,  94 % |  1116 / 12288 MB | ckapsiak(380M) ckapsiak(734M)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "V2_MERGED = True\n",
    "\n",
    "SIGNAL_FILEPATHS = [\n",
    "    # Test sig files #\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE/GluGluToHH/nominal/*',\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE_merged_v2/GluGluToHH/nominal/*',\n",
    "    # ggF HH # \n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\",\n",
    "    # VBF HH #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [\n",
    "    # ttH (i.e. the main bkg to reduce) #\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    # # Other single H samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\",\n",
    "    # # Prompt-Prompt samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\",\n",
    "    # # Prompt-Fake samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\",\n",
    "    # Fake-Fake samples #\n",
    "    # ADD HERE #\n",
    "]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v8'\n",
    "CRITERION = \"NLLLoss\"\n",
    "N_PARTICLES, N_PARTICLE_FIELDS = 6, 7\n",
    "MOD_VALS = (5, 5)\n",
    "# VARS = 'base_vars'\n",
    "VARS = 'extra_vars+'\n",
    "# CURRENT_TIME = '2024-08-30_14-35-01'\n",
    "CURRENT_TIME = '2024-10-09_20-47-24'\n",
    "# VARS = 'extra_vars_no_dijet_mass'\n",
    "# VARS = 'no_bad_vars'\n",
    "# VARS = 'simplified_bad_vars'\n",
    "# VARS = 'extra_vars_and_bools'\n",
    "# VARS = 'extra_vars_in_RNN'\n",
    "# VARS = f'extra_vars_mod{MOD_VALS[0]}-{MOD_VALS[1]}'\n",
    "# VARS = 'extra_vars_lead_lep_only'\n",
    "# CURRENT_TIME = '2024-10-02_18-03-26'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 3, 6\n",
    "# VARS = 'extra_vars_no_lep'\n",
    "# CURRENT_TIME = '2024-10-04_12-49-31'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 2, 6\n",
    "OUTPUT_DIRPATH = CURRENT_DIRPATH + f\"/model_outputs/{VERSION}/{VARS}/\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413735, 6, 7)\n",
      "Data HLF: (413735, 15)\n",
      "n signal = 136530, n bkg = 277205\n",
      "Data list test: (103521, 6, 7)\n",
      "Data HLF test: (103521, 15)\n",
      "n signal = 34224, n bkg = 69297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413918, 6, 7)\n",
      "Data HLF: (413918, 15)\n",
      "n signal = 136466, n bkg = 277452\n",
      "Data list test: (103338, 6, 7)\n",
      "Data HLF test: (103338, 15)\n",
      "n signal = 34288, n bkg = 69050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413265, 6, 7)\n",
      "Data HLF: (413265, 15)\n",
      "n signal = 136638, n bkg = 276627\n",
      "Data list test: (103991, 6, 7)\n",
      "Data HLF test: (103991, 15)\n",
      "n signal = 34116, n bkg = 69875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (413725, 6, 7)\n",
      "Data HLF: (413725, 15)\n",
      "n signal = 136671, n bkg = 277054\n",
      "Data list test: (103531, 6, 7)\n",
      "Data HLF test: (103531, 15)\n",
      "n signal = 34083, n bkg = 69448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (414381, 6, 7)\n",
      "Data HLF: (414381, 15)\n",
      "n signal = 136711, n bkg = 277670\n",
      "Data list test: (102875, 6, 7)\n",
      "Data HLF test: (102875, 15)\n",
      "n signal = 34043, n bkg = 68832\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    data_df_dict, data_test_df_dict, \n",
    "    data_list_dict, data_hlf_dict, label_dict, \n",
    "    data_list_test_dict, data_hlf_test_dict, label_test_dict, \n",
    "    high_level_fields_dict, input_hlf_vars_dict, hlf_vars_columns_dict,\n",
    "    data_aux_dict, data_test_aux_dict\n",
    ") = process_data(\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED, mod_vals=MOD_VALS, k_fold_test=True\n",
    ")\n",
    "# skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_weights(event_weights, labels):\n",
    "    sum_of_bkg = np.sum(event_weights[labels==0])\n",
    "    sum_of_sig = np.sum(event_weights[labels==1])\n",
    "\n",
    "    sig_scale_factor = sum_of_bkg / sum_of_sig\n",
    "\n",
    "    weights = np.where(labels==0, event_weights, event_weights*sig_scale_factor)\n",
    "    mean_weight = np.mean(weights)\n",
    "    abs_weights = np.abs(weights)\n",
    "    scaled_weights = abs_weights / mean_weight\n",
    "\n",
    "    return scaled_weights\n",
    "\n",
    "def plot_train_val_losses(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None, n_folds=5\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dotted']\n",
    "        linestyles = linestyles * ((2*len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:2*len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx if fold_idx%2 == 0 else fold_idx+1]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx+1 if fold_idx%2 == 0 else fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='std', \n",
    "    labels=None, yscale='linear', run2=True, sort=None, run3=None,\n",
    "    mask=None, n_folds=5\n",
    "):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_fprs'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            plt.plot(\n",
    "                np.array(IN_info['fprs'][fold_idx])[mask], np.array(IN_info['base_tpr'])[mask],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            np.array(IN_info['mean_fprs'])[mask], np.array(IN_info['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for i in index_arr:\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_fprs'])[0], dtype=bool)\n",
    "            plt.plot(\n",
    "                np.array(IN_info[i]['mean_fprs'])[mask], np.array(IN_info[i]['base_tpr'])[mask], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i], alpha=0.5\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if run3 is not None:\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(run3['mean_fprs'])[0], dtype=bool)\n",
    "        plt.plot(\n",
    "            np.array(run3['mean_fprs'])[mask], np.array(run3['base_tpr'])[mask], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (run3['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, n_bins=50, all_sig=False, all_bkg=False,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        # for cut in np.linspace(0, 1, 10, endpoint=False):\n",
    "        #     print(f\"output score > {cut:.2f}\")\n",
    "        #     print('='*60)\n",
    "        #     print(f\"num sig > {cut:.2f} = {len(sig_np[sig_np > cut])}\")\n",
    "        #     print(f\"num bkg > {cut:.2f} = {len(bkg_np[bkg_np > cut])}\")\n",
    "        #     print('-'*60)\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[f\"fold_{fold_idx}\"]['sig'] if weights[f\"fold_{fold_idx}\"]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[f\"fold_{fold_idx}\"]['bkg'] if weights[f\"fold_{fold_idx}\"]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[f\"fold_{fold_idx}\"]['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights[f\"fold_{fold_idx}\"]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=1, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=[\n",
    "                f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal', \n",
    "                f'{\"ttH\" if not all_bkg else \"all\"} background'\n",
    "            ]\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'] if weights[i]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'] if weights[i]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[i]['sig'] is not None else False),\n",
    "                alpha=0.7, density=(False if weights[i]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[i] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[i], linestyles[i]]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def s_over_root_b(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, lines_fold=None, lines=None, lines_labels=None, \n",
    "        lines_colors=None, only_fold=None, no_fold=False, n_bins=50,\n",
    "        mask=None, n_folds=5\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/âb', alpha=0.8 \n",
    "        )\n",
    "    elif method == 'round_robin':\n",
    "        if mask is None:\n",
    "            mask = [np.ones(np.shape(IN_info['all_preds'][i])[0], dtype=bool) for i in range(n_folds)]\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[f\"fold_{fold_idx}\"]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[f\"fold_{fold_idx}\"]['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "    elif method == 'arr':\n",
    "        if mask is None or np.all(mask):\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(n_folds):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "        if only_fold is None:\n",
    "            sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb - avg. over folds', \n",
    "                alpha=0.5, \n",
    "            )\n",
    "            if lines is not None:\n",
    "                for i in range(len(lines)):\n",
    "                    plt.vlines(\n",
    "                        lines[i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb - avg.'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[i]\n",
    "                    )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            if mask is None or np.all(mask):\n",
    "                mask = np.ones(np.shape(IN_info[i]['mean_pred'])[0], dtype=bool)\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_info[i]['mean_label']) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'])\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "                label='s/âb'+(' - '+labels[i] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[i], \n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    # 'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # 'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead bjet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_eta': hist.axis.Regular(20, -5., 5., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'sublead_bjet_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Integer(0, 10, name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($\\Sigma E_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    # 'jet1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # 'jet2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead bjet $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'sublead_bjet_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead bjet $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -10., 4., name='var', label=r'ln($\\chi_{t0}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -10., 4., name='var', label=r'ln($\\chi_{t1}^2$)', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'n_leptons': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{leptons}$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton ln($p_T$) [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma$ ln($p_{T}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'ln($M_{jj}$) [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, fold, var_name, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label_dict[f'fold_{fold}'] == 1\n",
    "    sig_test_mask = label_test_dict[f'fold_{fold}'] == 1\n",
    "    bkg_mask = label_dict[f'fold_{fold}'] == 0\n",
    "    bkg_test_mask = label_test_dict[f'fold_{fold}'] == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields_dict[f'fold_{fold}'] - set(input_hlf_vars_dict[f'fold_{fold}'])):\n",
    "            # index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns_dict[f'fold_{fold}'][var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict, fold):\n",
    "    sig_train_mask = (label_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test_dict[f'fold_{fold}'] == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test_dict[f'fold_{fold}'] == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux_dict[f'fold_{fold}'].loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux_dict[f'fold_{fold}'].loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux_dict[f'fold_{fold}'].loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux_dict[f'fold_{fold}'].loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(\n",
    "    output_dir, var_name, hist_list, fold, fold_idx=None, labels=None, density=True, \n",
    "    plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True\n",
    "):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir = output_dir + \"fold/\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_input_vars_after_score_cut(\n",
    "    IN_info, score_cut, destdir, fold, plot_prefix, plot_postfix='', method='std', \n",
    "    weights={'sig': None, 'bkg': None}, all_sig=False, all_bkg=False,\n",
    "    mask=None\n",
    "):\n",
    "    if method == 'std':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        sig_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "        bkg_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_info['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ] > score_cut\n",
    "\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            sig_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=sig_var.loc[sig_mask], \n",
    "                weight=weights['sig'][sig_mask] if weights['sig'] is not None else np.ones(np.sum(sig_mask))\n",
    "            )\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            bkg_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=bkg_var.loc[bkg_mask], \n",
    "                weight=weights['bkg'][bkg_mask] if weights['bkg'] is not None else np.ones(np.sum(bkg_mask))\n",
    "            )\n",
    "            make_input_plot(\n",
    "                destdir, var_name, [sig_hist, bkg_hist], plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore{score_cut}', labels=['HH signal', 'ttH background'], density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        if mask is None:\n",
    "            mask = np.ones(np.shape(IN_info['mean_pred'])[0], dtype=bool)\n",
    "        for var_name in high_level_fields_dict[f'fold_{fold}']:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==1) & mask, var_name]\n",
    "            bkg_var = data_test_df_dict[f'fold_{fold}'].loc[(label_test_dict[f'fold_{fold}']==0) & mask, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 1, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.logical_and(\n",
    "                        np.array(IN_info['mean_label']) == 0, mask\n",
    "                    ),1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used method 'std'. You used {method}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha) (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706619781071/work/torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0002956877 Acc: 81.7938995361\n",
      "validation Loss: 0.0002458565 Acc: 87.3385086060\n",
      "training Loss: 0.0002357461 Acc: 86.6040420532\n",
      "validation Loss: 0.0002354310 Acc: 86.1698913574\n",
      "training Loss: 0.0002289164 Acc: 86.9212799072\n",
      "validation Loss: 0.0002239506 Acc: 86.9022445679\n",
      "training Loss: 0.0002228869 Acc: 87.2965164185\n",
      "validation Loss: 0.0002206906 Acc: 87.4436492920\n",
      "training Loss: 0.0002208050 Acc: 87.4321670532\n",
      "validation Loss: 0.0002168882 Acc: 87.9065093994\n",
      "training Loss: 0.0002182432 Acc: 87.6246261597\n",
      "validation Loss: 0.0002152072 Acc: 88.3041076660\n",
      "training Loss: 0.0002160266 Acc: 87.8778076172\n",
      "validation Loss: 0.0002132008 Acc: 87.7288589478\n",
      "training Loss: 0.0002161729 Acc: 87.8007659912\n",
      "validation Loss: 0.0002224483 Acc: 86.4611358643\n",
      "training Loss: 0.0002143337 Acc: 87.9074172974\n",
      "validation Loss: 0.0002127789 Acc: 87.0351791382\n",
      "training Loss: 0.0002127769 Acc: 88.0261535645\n",
      "validation Loss: 0.0002105076 Acc: 87.7723617554\n",
      "training Loss: 0.0002117502 Acc: 88.1059112549\n",
      "validation Loss: 0.0002095666 Acc: 88.7863006592\n",
      "training Loss: 0.0002103270 Acc: 88.2409591675\n",
      "validation Loss: 0.0002090770 Acc: 88.5953521729\n",
      "training Loss: 0.0002104527 Acc: 88.1457901001\n",
      "validation Loss: 0.0002090198 Acc: 89.2286071777\n",
      "training Loss: 0.0002103618 Acc: 88.1832580566\n",
      "validation Loss: 0.0002084151 Acc: 88.2485122681\n",
      "training Loss: 0.0002085075 Acc: 88.2886962891\n",
      "validation Loss: 0.0002074767 Acc: 89.0086593628\n",
      "training Loss: 0.0002096546 Acc: 88.2751007080\n",
      "validation Loss: 0.0002090802 Acc: 87.9826431274\n",
      "training Loss: 0.0002088412 Acc: 88.3158874512\n",
      "validation Loss: 0.0002071002 Acc: 87.8702545166\n",
      "training Loss: 0.0002072646 Acc: 88.3587875366\n",
      "validation Loss: 0.0002075221 Acc: 88.8443069458\n",
      "training Loss: 0.0002079483 Acc: 88.2536468506\n",
      "validation Loss: 0.0002042393 Acc: 88.2726821899\n",
      "training Loss: 0.0002073440 Acc: 88.3678512573\n",
      "validation Loss: 0.0002058180 Acc: 89.0666732788\n",
      "training Loss: 0.0002066207 Acc: 88.3757095337\n",
      "validation Loss: 0.0002047079 Acc: 88.2714767456\n",
      "training Loss: 0.0002065355 Acc: 88.3835678101\n",
      "validation Loss: 0.0002053399 Acc: 88.6799468994\n",
      "training Loss: 0.0002065731 Acc: 88.4038085938\n",
      "validation Loss: 0.0002037480 Acc: 89.0690917969\n",
      "training Loss: 0.0002059689 Acc: 88.4590988159\n",
      "validation Loss: 0.0002040218 Acc: 88.8588104248\n",
      "training Loss: 0.0002065379 Acc: 88.3760147095\n",
      "validation Loss: 0.0002043717 Acc: 88.3210220337\n",
      "training Loss: 0.0002061433 Acc: 88.4234466553\n",
      "validation Loss: 0.0002045893 Acc: 88.9349441528\n",
      "training Loss: 0.0002063460 Acc: 88.3929290771\n",
      "validation Loss: 0.0002030317 Acc: 88.4007873535\n",
      "training Loss: 0.0002061249 Acc: 88.4799423218\n",
      "validation Loss: 0.0002027630 Acc: 88.5627288818\n",
      "training Loss: 0.0002049846 Acc: 88.4391555786\n",
      "validation Loss: 0.0002033497 Acc: 88.9337387085\n",
      "training Loss: 0.0002052634 Acc: 88.5763244629\n",
      "validation Loss: 0.0002025111 Acc: 88.3149795532\n",
      "training Loss: 0.0002054068 Acc: 88.4013900757\n",
      "validation Loss: 0.0002022906 Acc: 89.2527770996\n",
      "training Loss: 0.0002058338 Acc: 88.4364395142\n",
      "validation Loss: 0.0002044515 Acc: 87.7433624268\n",
      "training Loss: 0.0002059334 Acc: 88.3935317993\n",
      "validation Loss: 0.0002049797 Acc: 89.3675918579\n",
      "training Loss: 0.0002046727 Acc: 88.5908203125\n",
      "validation Loss: 0.0002008268 Acc: 88.5095520020\n",
      "training Loss: 0.0002049591 Acc: 88.5736007690\n",
      "validation Loss: 0.0002023104 Acc: 88.2013854980\n",
      "training Loss: 0.0002053802 Acc: 88.4213333130\n",
      "validation Loss: 0.0002019130 Acc: 88.5022964478\n",
      "training Loss: 0.0002050645 Acc: 88.5491333008\n",
      "validation Loss: 0.0002052815 Acc: 87.4521102905\n",
      "training Loss: 0.0002049201 Acc: 88.5180130005\n",
      "validation Loss: 0.0002024912 Acc: 88.7017059326\n",
      "training Loss: 0.0002012559 Acc: 88.7116699219\n",
      "validation Loss: 0.0001987852 Acc: 88.6714935303\n",
      "training Loss: 0.0001996499 Acc: 88.7606201172\n",
      "validation Loss: 0.0001974234 Acc: 88.6171112061\n",
      "training Loss: 0.0002001574 Acc: 88.7464141846\n",
      "validation Loss: 0.0001973881 Acc: 89.0908432007\n",
      "training Loss: 0.0001992245 Acc: 88.8074493408\n",
      "validation Loss: 0.0001980541 Acc: 88.6992874146\n",
      "training Loss: 0.0001992453 Acc: 88.8358459473\n",
      "validation Loss: 0.0001985203 Acc: 88.8092575073\n",
      "training Loss: 0.0001994728 Acc: 88.7512512207\n",
      "validation Loss: 0.0001989245 Acc: 89.1681823730\n",
      "training Loss: 0.0001990455 Acc: 88.8310165405\n",
      "validation Loss: 0.0001973399 Acc: 88.6485290527\n",
      "training Loss: 0.0001988108 Acc: 88.7856903076\n",
      "validation Loss: 0.0001971167 Acc: 88.5603103638\n",
      "training Loss: 0.0001990185 Acc: 88.7696838379\n",
      "validation Loss: 0.0001995709 Acc: 88.8201370239\n",
      "training Loss: 0.0001989287 Acc: 88.7932510376\n",
      "validation Loss: 0.0001987547 Acc: 89.4618530273\n",
      "training Loss: 0.0001986976 Acc: 88.8201370239\n",
      "validation Loss: 0.0001995936 Acc: 89.5186538696\n",
      "training Loss: 0.0001984497 Acc: 88.8343353271\n",
      "validation Loss: 0.0001985079 Acc: 88.9119796753\n",
      "training Loss: 0.0001966537 Acc: 88.9506530762\n",
      "validation Loss: 0.0001949001 Acc: 89.1246795654\n",
      "training Loss: 0.0001960421 Acc: 88.9817733765\n",
      "validation Loss: 0.0001952268 Acc: 88.7572937012\n",
      "training Loss: 0.0001957346 Acc: 88.9500503540\n",
      "validation Loss: 0.0001954839 Acc: 89.3542938232\n",
      "training Loss: 0.0001959227 Acc: 88.9560928345\n",
      "validation Loss: 0.0001946173 Acc: 88.9458236694\n",
      "training Loss: 0.0001952277 Acc: 88.9908370972\n",
      "validation Loss: 0.0001959484 Acc: 88.9530715942\n",
      "training Loss: 0.0001955379 Acc: 89.0125885010\n",
      "validation Loss: 0.0001947132 Acc: 88.8503494263\n",
      "training Loss: 0.0001952726 Acc: 88.9618301392\n",
      "validation Loss: 0.0001948850 Acc: 89.4739379883\n",
      "training Loss: 0.0001955957 Acc: 89.0032272339\n",
      "validation Loss: 0.0001949470 Acc: 89.1307220459\n",
      "training Loss: 0.0001941153 Acc: 89.0361557007\n",
      "validation Loss: 0.0001938697 Acc: 88.9542770386\n",
      "training Loss: 0.0001931576 Acc: 89.0999069214\n",
      "validation Loss: 0.0001934339 Acc: 89.2757415771\n",
      "training Loss: 0.0001932225 Acc: 89.1150131226\n",
      "validation Loss: 0.0001929666 Acc: 89.2153167725\n",
      "training Loss: 0.0001932708 Acc: 89.0706024170\n",
      "validation Loss: 0.0001938595 Acc: 89.1029281616\n",
      "training Loss: 0.0001928639 Acc: 89.1240768433\n",
      "validation Loss: 0.0001932845 Acc: 89.2406921387\n",
      "training Loss: 0.0001930543 Acc: 89.1261901855\n",
      "validation Loss: 0.0001932342 Acc: 89.1391830444\n",
      "training Loss: 0.0001929367 Acc: 89.1603317261\n",
      "validation Loss: 0.0001933142 Acc: 89.4026336670\n",
      "training Loss: 0.0001920090 Acc: 89.1775512695\n",
      "validation Loss: 0.0001924478 Acc: 89.0823822021\n",
      "training Loss: 0.0001920423 Acc: 89.1551971436\n",
      "validation Loss: 0.0001923469 Acc: 88.9011077881\n",
      "training Loss: 0.0001922115 Acc: 89.1832885742\n",
      "validation Loss: 0.0001930434 Acc: 89.2092742920\n",
      "training Loss: 0.0001919398 Acc: 89.1506652832\n",
      "validation Loss: 0.0001930122 Acc: 89.3325424194\n",
      "training Loss: 0.0001914283 Acc: 89.2171325684\n",
      "validation Loss: 0.0001923402 Acc: 89.1730194092\n",
      "training Loss: 0.0001913485 Acc: 89.1633529663\n",
      "validation Loss: 0.0001924901 Acc: 89.2938690186\n",
      "training Loss: 0.0001911304 Acc: 89.2198486328\n",
      "validation Loss: 0.0001926842 Acc: 89.3579177856\n",
      "training Loss: 0.0001910238 Acc: 89.2340469360\n",
      "validation Loss: 0.0001919252 Acc: 89.1343460083\n",
      "training Loss: 0.0001908806 Acc: 89.2446212769\n",
      "validation Loss: 0.0001922147 Acc: 89.2781600952\n",
      "training Loss: 0.0001908313 Acc: 89.2286071777\n",
      "validation Loss: 0.0001922038 Acc: 89.2225646973\n",
      "training Loss: 0.0001906120 Acc: 89.2337493896\n",
      "validation Loss: 0.0001921238 Acc: 89.1343460083\n",
      "training Loss: 0.0001907365 Acc: 89.2225646973\n",
      "validation Loss: 0.0001921429 Acc: 89.2358627319\n",
      "training Loss: 0.0001906653 Acc: 89.2337493896\n",
      "validation Loss: 0.0001917097 Acc: 89.1983947754\n",
      "training Loss: 0.0001907117 Acc: 89.2536849976\n",
      "validation Loss: 0.0001920007 Acc: 89.0666732788\n",
      "training Loss: 0.0001902894 Acc: 89.2440185547\n",
      "validation Loss: 0.0001920643 Acc: 89.3132019043\n",
      "training Loss: 0.0001902993 Acc: 89.2509689331\n",
      "validation Loss: 0.0001921628 Acc: 89.1476440430\n",
      "training Loss: 0.0001903348 Acc: 89.2307281494\n",
      "validation Loss: 0.0001921082 Acc: 89.2551956177\n",
      "training Loss: 0.0001902855 Acc: 89.2406921387\n",
      "validation Loss: 0.0001924751 Acc: 89.1947708130\n",
      "training Loss: 0.0001905166 Acc: 89.2464370728\n",
      "validation Loss: 0.0001920823 Acc: 89.2648620605\n",
      "training Loss: 0.0001903231 Acc: 89.2845001221\n",
      "validation Loss: 0.0001923238 Acc: 89.2660751343\n",
      "Early stopped.\n",
      "Best val acc: 89.518654\n",
      "----------\n",
      "training Loss: 0.0003006695 Acc: 81.3486404419\n",
      "validation Loss: 0.0002437256 Acc: 86.5118865967\n",
      "training Loss: 0.0002386676 Acc: 86.6120681763\n",
      "validation Loss: 0.0002267985 Acc: 87.8913803101\n",
      "training Loss: 0.0002303429 Acc: 87.0913314819\n",
      "validation Loss: 0.0002233819 Acc: 88.2984619141\n",
      "training Loss: 0.0002258665 Acc: 87.3447036743\n",
      "validation Loss: 0.0002185629 Acc: 87.5181198120\n",
      "training Loss: 0.0002230266 Acc: 87.5787429810\n",
      "validation Loss: 0.0002138095 Acc: 88.1353836060\n",
      "training Loss: 0.0002204749 Acc: 87.6841430664\n",
      "validation Loss: 0.0002151354 Acc: 87.7524642944\n",
      "training Loss: 0.0002188984 Acc: 87.8919143677\n",
      "validation Loss: 0.0002168020 Acc: 89.0884704590\n",
      "training Loss: 0.0002177741 Acc: 87.8583908081\n",
      "validation Loss: 0.0002106463 Acc: 88.4627456665\n",
      "training Loss: 0.0002167676 Acc: 87.9728469849\n",
      "validation Loss: 0.0002100977 Acc: 88.3636932373\n",
      "training Loss: 0.0002163887 Acc: 87.9091262817\n",
      "validation Loss: 0.0002119067 Acc: 87.5543518066\n",
      "training Loss: 0.0002151415 Acc: 87.9779815674\n",
      "validation Loss: 0.0002104714 Acc: 87.9348602295\n",
      "training Loss: 0.0002144798 Acc: 88.0812606812\n",
      "validation Loss: 0.0002057821 Acc: 88.4796524048\n",
      "training Loss: 0.0002147686 Acc: 88.0625381470\n",
      "validation Loss: 0.0002097440 Acc: 88.2912139893\n",
      "training Loss: 0.0002129199 Acc: 88.1102523804\n",
      "validation Loss: 0.0002050634 Acc: 88.7466125488\n",
      "training Loss: 0.0002126937 Acc: 88.1365280151\n",
      "validation Loss: 0.0002069116 Acc: 89.0171966553\n",
      "training Loss: 0.0002116842 Acc: 88.2225952148\n",
      "validation Loss: 0.0002082743 Acc: 88.9749221802\n",
      "training Loss: 0.0002118474 Acc: 88.1845474243\n",
      "validation Loss: 0.0002068697 Acc: 87.9904327393\n",
      "training Loss: 0.0002113422 Acc: 88.2099075317\n",
      "validation Loss: 0.0002021310 Acc: 88.4458312988\n",
      "training Loss: 0.0002111639 Acc: 88.1896743774\n",
      "validation Loss: 0.0002045873 Acc: 87.9155349731\n",
      "training Loss: 0.0002101324 Acc: 88.2597427368\n",
      "validation Loss: 0.0002029915 Acc: 88.7913055420\n",
      "training Loss: 0.0002103924 Acc: 88.2304458618\n",
      "validation Loss: 0.0002032669 Acc: 88.9229736328\n",
      "training Loss: 0.0002096840 Acc: 88.2479629517\n",
      "validation Loss: 0.0002040325 Acc: 89.2950286865\n",
      "training Loss: 0.0002051283 Acc: 88.6037063599\n",
      "validation Loss: 0.0001999845 Acc: 89.5233383179\n",
      "training Loss: 0.0002049192 Acc: 88.5251922607\n",
      "validation Loss: 0.0001978386 Acc: 88.8625793457\n",
      "training Loss: 0.0002033462 Acc: 88.6375350952\n",
      "validation Loss: 0.0001986098 Acc: 89.3795852661\n",
      "training Loss: 0.0002035552 Acc: 88.6468963623\n",
      "validation Loss: 0.0001994297 Acc: 88.6330642700\n",
      "training Loss: 0.0002038203 Acc: 88.5889129639\n",
      "validation Loss: 0.0001978365 Acc: 88.6306533813\n",
      "training Loss: 0.0002035317 Acc: 88.5735092163\n",
      "validation Loss: 0.0001989299 Acc: 89.4303207397\n",
      "training Loss: 0.0002011961 Acc: 88.7577285767\n",
      "validation Loss: 0.0001955723 Acc: 88.8263397217\n",
      "training Loss: 0.0002003157 Acc: 88.7885284424\n",
      "validation Loss: 0.0001958909 Acc: 88.8275451660\n",
      "training Loss: 0.0002004074 Acc: 88.8054428101\n",
      "validation Loss: 0.0001956928 Acc: 89.1911468506\n",
      "training Loss: 0.0002008489 Acc: 88.7136383057\n",
      "validation Loss: 0.0001969616 Acc: 89.4375686646\n",
      "training Loss: 0.0001998841 Acc: 88.8371505737\n",
      "validation Loss: 0.0001969478 Acc: 89.1524887085\n",
      "training Loss: 0.0001983231 Acc: 88.8794250488\n",
      "validation Loss: 0.0001941974 Acc: 89.1996002197\n",
      "training Loss: 0.0001983006 Acc: 88.9226150513\n",
      "validation Loss: 0.0001948979 Acc: 89.0884704590\n",
      "training Loss: 0.0001978344 Acc: 88.8836593628\n",
      "validation Loss: 0.0001942389 Acc: 88.8891525269\n",
      "training Loss: 0.0001975420 Acc: 88.9129486084\n",
      "validation Loss: 0.0001939439 Acc: 89.0389404297\n",
      "training Loss: 0.0001972976 Acc: 88.8933181763\n",
      "validation Loss: 0.0001940330 Acc: 89.1609497070\n",
      "training Loss: 0.0001973258 Acc: 88.8836593628\n",
      "validation Loss: 0.0001937407 Acc: 89.0739746094\n",
      "training Loss: 0.0001974150 Acc: 88.9491882324\n",
      "validation Loss: 0.0001938281 Acc: 89.1947708130\n",
      "training Loss: 0.0001969674 Acc: 88.9165725708\n",
      "validation Loss: 0.0001950819 Acc: 89.2479171753\n",
      "training Loss: 0.0001970548 Acc: 88.9419403076\n",
      "validation Loss: 0.0001939367 Acc: 88.9749221802\n",
      "training Loss: 0.0001969647 Acc: 88.9600601196\n",
      "validation Loss: 0.0001939303 Acc: 89.0522308350\n",
      "training Loss: 0.0001960338 Acc: 88.9664001465\n",
      "validation Loss: 0.0001934457 Acc: 89.4967575073\n",
      "training Loss: 0.0001956891 Acc: 89.0370712280\n",
      "validation Loss: 0.0001931675 Acc: 89.0582656860\n",
      "training Loss: 0.0001957267 Acc: 89.0482406616\n",
      "validation Loss: 0.0001929127 Acc: 89.0763854980\n",
      "training Loss: 0.0001956082 Acc: 88.9733505249\n",
      "validation Loss: 0.0001933866 Acc: 89.2430877686\n",
      "training Loss: 0.0001956926 Acc: 89.0364685059\n",
      "validation Loss: 0.0001933516 Acc: 89.0800094604\n",
      "training Loss: 0.0001953408 Acc: 89.0488433838\n",
      "validation Loss: 0.0001930320 Acc: 89.2261734009\n",
      "training Loss: 0.0001954991 Acc: 89.0252914429\n",
      "validation Loss: 0.0001934661 Acc: 89.4037475586\n",
      "training Loss: 0.0001948653 Acc: 89.0823669434\n",
      "validation Loss: 0.0001930293 Acc: 89.2068481445\n",
      "training Loss: 0.0001950231 Acc: 89.0573043823\n",
      "validation Loss: 0.0001924659 Acc: 89.3312683105\n",
      "training Loss: 0.0001947285 Acc: 89.0633392334\n",
      "validation Loss: 0.0001934374 Acc: 89.1126251221\n",
      "training Loss: 0.0001944938 Acc: 89.0699844360\n",
      "validation Loss: 0.0001924973 Acc: 89.1718139648\n",
      "training Loss: 0.0001944521 Acc: 89.0654525757\n",
      "validation Loss: 0.0001922284 Acc: 89.3107299805\n",
      "training Loss: 0.0001945018 Acc: 89.1346130371\n",
      "validation Loss: 0.0001925381 Acc: 89.1210861206\n",
      "training Loss: 0.0001944979 Acc: 89.0859909058\n",
      "validation Loss: 0.0001926299 Acc: 89.1585311890\n",
      "training Loss: 0.0001943436 Acc: 89.0506591797\n",
      "validation Loss: 0.0001926621 Acc: 89.4097900391\n",
      "training Loss: 0.0001944603 Acc: 89.0684738159\n",
      "validation Loss: 0.0001927501 Acc: 89.1802749634\n",
      "training Loss: 0.0001943032 Acc: 89.0950546265\n",
      "validation Loss: 0.0001926157 Acc: 89.2612075806\n",
      "training Loss: 0.0001941257 Acc: 89.0772323608\n",
      "validation Loss: 0.0001928003 Acc: 89.3131484985\n",
      "training Loss: 0.0001938598 Acc: 89.1001815796\n",
      "validation Loss: 0.0001925480 Acc: 89.1730270386\n",
      "Early stopped.\n",
      "Best val acc: 89.523338\n",
      "----------\n",
      "training Loss: 0.0002979177 Acc: 81.9843215942\n",
      "validation Loss: 0.0002489287 Acc: 83.8783798218\n",
      "training Loss: 0.0002338404 Acc: 86.6928634644\n",
      "validation Loss: 0.0002255804 Acc: 86.8087081909\n",
      "training Loss: 0.0002275283 Acc: 87.0149917603\n",
      "validation Loss: 0.0002211286 Acc: 86.9514694214\n",
      "training Loss: 0.0002223933 Acc: 87.3443756104\n",
      "validation Loss: 0.0002160172 Acc: 87.2200622559\n",
      "training Loss: 0.0002181367 Acc: 87.5790939331\n",
      "validation Loss: 0.0002158628 Acc: 87.3289489746\n",
      "training Loss: 0.0002166514 Acc: 87.7130889893\n",
      "validation Loss: 0.0002116275 Acc: 87.8673477173\n",
      "training Loss: 0.0002145279 Acc: 87.9027404785\n",
      "validation Loss: 0.0002128639 Acc: 87.0555191040\n",
      "training Loss: 0.0002135563 Acc: 87.9453887939\n",
      "validation Loss: 0.0002111575 Acc: 87.5491485596\n",
      "training Loss: 0.0002119812 Acc: 87.9925689697\n",
      "validation Loss: 0.0002126342 Acc: 86.6296463013\n",
      "training Loss: 0.0002124587 Acc: 88.0058822632\n",
      "validation Loss: 0.0002108185 Acc: 87.0676193237\n",
      "training Loss: 0.0002125437 Acc: 88.0092086792\n",
      "validation Loss: 0.0002113710 Acc: 88.5352020264\n",
      "training Loss: 0.0002109645 Acc: 88.0557861328\n",
      "validation Loss: 0.0002080331 Acc: 88.9163131714\n",
      "training Loss: 0.0002098461 Acc: 88.0887527466\n",
      "validation Loss: 0.0002085681 Acc: 87.1341629028\n",
      "training Loss: 0.0002094639 Acc: 88.0793762207\n",
      "validation Loss: 0.0002057981 Acc: 88.3670272827\n",
      "training Loss: 0.0002091636 Acc: 88.1150741577\n",
      "validation Loss: 0.0002091524 Acc: 88.9598693848\n",
      "training Loss: 0.0002085937 Acc: 88.2354507446\n",
      "validation Loss: 0.0002050993 Acc: 88.2750778198\n",
      "training Loss: 0.0002085843 Acc: 88.2148818970\n",
      "validation Loss: 0.0002060280 Acc: 87.9520416260\n",
      "training Loss: 0.0002089094 Acc: 88.2148818970\n",
      "validation Loss: 0.0002042323 Acc: 88.2230529785\n",
      "training Loss: 0.0002073521 Acc: 88.2596511841\n",
      "validation Loss: 0.0002057075 Acc: 87.9459915161\n",
      "training Loss: 0.0002071419 Acc: 88.2281951904\n",
      "validation Loss: 0.0002037685 Acc: 89.0469818115\n",
      "training Loss: 0.0002065512 Acc: 88.2995758057\n",
      "validation Loss: 0.0002030107 Acc: 88.7566070557\n",
      "training Loss: 0.0002077302 Acc: 88.1843338013\n",
      "validation Loss: 0.0002113523 Acc: 89.2248306274\n",
      "training Loss: 0.0002075171 Acc: 88.2112579346\n",
      "validation Loss: 0.0002088075 Acc: 88.2000656128\n",
      "training Loss: 0.0002063072 Acc: 88.3637008667\n",
      "validation Loss: 0.0002061624 Acc: 87.4450988770\n",
      "training Loss: 0.0002075570 Acc: 88.3443450928\n",
      "validation Loss: 0.0002060983 Acc: 86.9659881592\n",
      "training Loss: 0.0002023396 Acc: 88.5058593750\n",
      "validation Loss: 0.0002005623 Acc: 88.2484588623\n",
      "training Loss: 0.0002010005 Acc: 88.6362228394\n",
      "validation Loss: 0.0002004839 Acc: 88.8570251465\n",
      "training Loss: 0.0002007310 Acc: 88.6180801392\n",
      "validation Loss: 0.0001997405 Acc: 88.5569763184\n",
      "training Loss: 0.0002005961 Acc: 88.5424575806\n",
      "validation Loss: 0.0001992982 Acc: 89.0251998901\n",
      "training Loss: 0.0002003128 Acc: 88.6256408691\n",
      "validation Loss: 0.0002009074 Acc: 89.1171493530\n",
      "training Loss: 0.0002004643 Acc: 88.6220092773\n",
      "validation Loss: 0.0001992532 Acc: 88.3319396973\n",
      "training Loss: 0.0002005033 Acc: 88.6244277954\n",
      "validation Loss: 0.0001998011 Acc: 88.5932769775\n",
      "training Loss: 0.0001991858 Acc: 88.6970214844\n",
      "validation Loss: 0.0001991994 Acc: 89.2502365112\n",
      "training Loss: 0.0002001461 Acc: 88.6177749634\n",
      "validation Loss: 0.0001998915 Acc: 88.1831283569\n",
      "training Loss: 0.0002000840 Acc: 88.5887374878\n",
      "validation Loss: 0.0001997896 Acc: 89.2780685425\n",
      "training Loss: 0.0002004192 Acc: 88.6504440308\n",
      "validation Loss: 0.0001988888 Acc: 88.9526062012\n",
      "training Loss: 0.0001999304 Acc: 88.6646575928\n",
      "validation Loss: 0.0001998039 Acc: 88.9695434570\n",
      "training Loss: 0.0001995079 Acc: 88.6071853638\n",
      "validation Loss: 0.0001997696 Acc: 88.1371536255\n",
      "training Loss: 0.0002000032 Acc: 88.6332015991\n",
      "validation Loss: 0.0001989133 Acc: 88.5097961426\n",
      "training Loss: 0.0001992108 Acc: 88.6894607544\n",
      "validation Loss: 0.0001981391 Acc: 88.5714950562\n",
      "training Loss: 0.0001994192 Acc: 88.6549758911\n",
      "validation Loss: 0.0002003249 Acc: 87.5491485596\n",
      "training Loss: 0.0001998985 Acc: 88.6301727295\n",
      "validation Loss: 0.0001988960 Acc: 88.4359893799\n",
      "training Loss: 0.0001996509 Acc: 88.6038589478\n",
      "validation Loss: 0.0001991116 Acc: 88.5702896118\n",
      "training Loss: 0.0001988110 Acc: 88.6462097168\n",
      "validation Loss: 0.0001998181 Acc: 87.8818664551\n",
      "training Loss: 0.0001965817 Acc: 88.8080291748\n",
      "validation Loss: 0.0001964667 Acc: 88.3222656250\n",
      "training Loss: 0.0001961725 Acc: 88.8549118042\n",
      "validation Loss: 0.0001959076 Acc: 88.8400878906\n",
      "training Loss: 0.0001956596 Acc: 88.7720336914\n",
      "validation Loss: 0.0001964718 Acc: 89.2405624390\n",
      "training Loss: 0.0001957960 Acc: 88.8506774902\n",
      "validation Loss: 0.0001966626 Acc: 88.8981628418\n",
      "training Loss: 0.0001956313 Acc: 88.8146820068\n",
      "validation Loss: 0.0001965122 Acc: 89.4438171387\n",
      "training Loss: 0.0001953572 Acc: 88.8657989502\n",
      "validation Loss: 0.0001960803 Acc: 88.7505569458\n",
      "training Loss: 0.0001937952 Acc: 89.0016098022\n",
      "validation Loss: 0.0001954792 Acc: 88.8739624023\n",
      "training Loss: 0.0001935758 Acc: 88.9471664429\n",
      "validation Loss: 0.0001953418 Acc: 89.1086807251\n",
      "training Loss: 0.0001936212 Acc: 88.9580535889\n",
      "validation Loss: 0.0001949043 Acc: 88.4650268555\n",
      "training Loss: 0.0001929744 Acc: 88.9719696045\n",
      "validation Loss: 0.0001953412 Acc: 88.7263641357\n",
      "training Loss: 0.0001933362 Acc: 88.9777145386\n",
      "validation Loss: 0.0001948812 Acc: 88.9889068604\n",
      "training Loss: 0.0001928761 Acc: 89.0034255981\n",
      "validation Loss: 0.0001943397 Acc: 88.9768066406\n",
      "training Loss: 0.0001931734 Acc: 89.0031204224\n",
      "validation Loss: 0.0001953381 Acc: 89.1074752808\n",
      "training Loss: 0.0001929446 Acc: 89.0203628540\n",
      "validation Loss: 0.0001943158 Acc: 88.9707565308\n",
      "training Loss: 0.0001930955 Acc: 88.9949569702\n",
      "validation Loss: 0.0001947680 Acc: 89.1038436890\n",
      "training Loss: 0.0001933700 Acc: 89.0206604004\n",
      "validation Loss: 0.0001951384 Acc: 88.7433013916\n",
      "training Loss: 0.0001927716 Acc: 88.9719696045\n",
      "validation Loss: 0.0001954162 Acc: 88.5218887329\n",
      "training Loss: 0.0001934131 Acc: 88.9450454712\n",
      "validation Loss: 0.0001952080 Acc: 89.1110992432\n",
      "training Loss: 0.0001919892 Acc: 89.0427474976\n",
      "validation Loss: 0.0001940401 Acc: 88.8739624023\n",
      "training Loss: 0.0001919322 Acc: 88.9943466187\n",
      "validation Loss: 0.0001944430 Acc: 89.1159439087\n",
      "training Loss: 0.0001914148 Acc: 89.0412292480\n",
      "validation Loss: 0.0001943312 Acc: 89.1885375977\n",
      "training Loss: 0.0001913982 Acc: 89.0276184082\n",
      "validation Loss: 0.0001941927 Acc: 89.2671737671\n",
      "training Loss: 0.0001913731 Acc: 89.0457687378\n",
      "validation Loss: 0.0001940521 Acc: 89.1365127563\n",
      "training Loss: 0.0001911395 Acc: 89.1216888428\n",
      "validation Loss: 0.0001934939 Acc: 89.1304626465\n",
      "training Loss: 0.0001908030 Acc: 89.1035385132\n",
      "validation Loss: 0.0001938493 Acc: 89.2224121094\n",
      "training Loss: 0.0001907172 Acc: 89.1032409668\n",
      "validation Loss: 0.0001937650 Acc: 89.1365127563\n",
      "training Loss: 0.0001904127 Acc: 89.1821823120\n",
      "validation Loss: 0.0001936882 Acc: 89.0118942261\n",
      "training Loss: 0.0001906088 Acc: 89.0856933594\n",
      "validation Loss: 0.0001933084 Acc: 89.0469818115\n",
      "training Loss: 0.0001904446 Acc: 89.0835800171\n",
      "validation Loss: 0.0001939015 Acc: 89.1728057861\n",
      "training Loss: 0.0001906635 Acc: 89.1764373779\n",
      "validation Loss: 0.0001933683 Acc: 89.0748062134\n",
      "training Loss: 0.0001906339 Acc: 89.0959777832\n",
      "validation Loss: 0.0001936021 Acc: 89.0675430298\n",
      "training Loss: 0.0001902844 Acc: 89.1180572510\n",
      "validation Loss: 0.0001935239 Acc: 89.1510314941\n",
      "training Loss: 0.0001903440 Acc: 89.1352996826\n",
      "validation Loss: 0.0001933109 Acc: 89.2175750732\n",
      "training Loss: 0.0001899512 Acc: 89.1265258789\n",
      "validation Loss: 0.0001935817 Acc: 89.1377182007\n",
      "training Loss: 0.0001898871 Acc: 89.1722030640\n",
      "validation Loss: 0.0001933079 Acc: 89.1280441284\n",
      "training Loss: 0.0001900987 Acc: 89.0968856812\n",
      "validation Loss: 0.0001932947 Acc: 89.0820617676\n",
      "training Loss: 0.0001900239 Acc: 89.1588897705\n",
      "validation Loss: 0.0001930472 Acc: 89.0542373657\n",
      "training Loss: 0.0001898936 Acc: 89.1434631348\n",
      "validation Loss: 0.0001927781 Acc: 89.0844879150\n",
      "training Loss: 0.0001897987 Acc: 89.1316680908\n",
      "validation Loss: 0.0001935048 Acc: 89.2066802979\n",
      "training Loss: 0.0001900821 Acc: 89.1277389526\n",
      "validation Loss: 0.0001930519 Acc: 89.1147308350\n",
      "training Loss: 0.0001898700 Acc: 89.1740188599\n",
      "validation Loss: 0.0001935233 Acc: 89.1909561157\n",
      "training Loss: 0.0001899845 Acc: 89.1670608521\n",
      "validation Loss: 0.0001933502 Acc: 89.2006301880\n",
      "training Loss: 0.0001897568 Acc: 89.1900482178\n",
      "validation Loss: 0.0001934836 Acc: 89.2224121094\n",
      "training Loss: 0.0001894791 Acc: 89.1973037720\n",
      "validation Loss: 0.0001931043 Acc: 89.1050491333\n",
      "training Loss: 0.0001900659 Acc: 89.1486053467\n",
      "validation Loss: 0.0001934432 Acc: 89.1377182007\n",
      "Early stopped.\n",
      "Best val acc: 89.443817\n",
      "----------\n",
      "training Loss: 0.0002888779 Acc: 82.2088928223\n",
      "validation Loss: 0.0002343331 Acc: 85.6341705322\n",
      "training Loss: 0.0002355790 Acc: 86.7496490479\n",
      "validation Loss: 0.0002259776 Acc: 85.4939880371\n",
      "training Loss: 0.0002272645 Acc: 87.1980133057\n",
      "validation Loss: 0.0002182389 Acc: 87.5545349121\n",
      "training Loss: 0.0002221660 Acc: 87.5648040771\n",
      "validation Loss: 0.0002144893 Acc: 88.3026123047\n",
      "training Loss: 0.0002203778 Acc: 87.5989456177\n",
      "validation Loss: 0.0002125251 Acc: 86.8475418091\n",
      "training Loss: 0.0002191882 Acc: 87.6481933594\n",
      "validation Loss: 0.0002115076 Acc: 88.5866165161\n",
      "training Loss: 0.0002171916 Acc: 87.7947311401\n",
      "validation Loss: 0.0002130074 Acc: 88.8065719604\n",
      "training Loss: 0.0002159868 Acc: 87.9312896729\n",
      "validation Loss: 0.0002087355 Acc: 87.6753845215\n",
      "training Loss: 0.0002144385 Acc: 87.9403533936\n",
      "validation Loss: 0.0002072838 Acc: 87.9243392944\n",
      "training Loss: 0.0002142588 Acc: 87.9911117554\n",
      "validation Loss: 0.0002052361 Acc: 88.1757202148\n",
      "training Loss: 0.0002123996 Acc: 88.0509338379\n",
      "validation Loss: 0.0002116595 Acc: 89.6211242676\n",
      "training Loss: 0.0002117044 Acc: 88.1183090210\n",
      "validation Loss: 0.0002032014 Acc: 88.5008163452\n",
      "training Loss: 0.0002102005 Acc: 88.2294998169\n",
      "validation Loss: 0.0002020346 Acc: 87.8590850830\n",
      "training Loss: 0.0002095012 Acc: 88.2171096802\n",
      "validation Loss: 0.0002006317 Acc: 88.7763595581\n",
      "training Loss: 0.0002097090 Acc: 88.2010955811\n",
      "validation Loss: 0.0002026644 Acc: 88.1225433350\n",
      "training Loss: 0.0002095143 Acc: 88.1757202148\n",
      "validation Loss: 0.0002021972 Acc: 88.3461227417\n",
      "training Loss: 0.0002086735 Acc: 88.2730026245\n",
      "validation Loss: 0.0002010513 Acc: 88.6675872803\n",
      "training Loss: 0.0002086992 Acc: 88.2847900391\n",
      "validation Loss: 0.0001983083 Acc: 88.7110977173\n",
      "training Loss: 0.0002079349 Acc: 88.3177185059\n",
      "validation Loss: 0.0001998344 Acc: 88.2446060181\n",
      "training Loss: 0.0002081973 Acc: 88.2340316772\n",
      "validation Loss: 0.0001987930 Acc: 88.2180175781\n",
      "training Loss: 0.0002086321 Acc: 88.3328247070\n",
      "validation Loss: 0.0001985568 Acc: 88.3074493408\n",
      "training Loss: 0.0002076724 Acc: 88.2733078003\n",
      "validation Loss: 0.0002006975 Acc: 89.0990371704\n",
      "training Loss: 0.0002036827 Acc: 88.5829925537\n",
      "validation Loss: 0.0001956553 Acc: 88.4705963135\n",
      "training Loss: 0.0002033241 Acc: 88.5189437866\n",
      "validation Loss: 0.0001949444 Acc: 88.8682098389\n",
      "training Loss: 0.0002019958 Acc: 88.5996093750\n",
      "validation Loss: 0.0001952546 Acc: 88.7678985596\n",
      "training Loss: 0.0002025931 Acc: 88.6132049561\n",
      "validation Loss: 0.0001970847 Acc: 88.2470245361\n",
      "training Loss: 0.0002020328 Acc: 88.6636657715\n",
      "validation Loss: 0.0001946562 Acc: 89.0059814453\n",
      "training Loss: 0.0002016491 Acc: 88.6546020508\n",
      "validation Loss: 0.0001969506 Acc: 88.3171157837\n",
      "training Loss: 0.0002017760 Acc: 88.5920562744\n",
      "validation Loss: 0.0001973517 Acc: 89.3504104614\n",
      "training Loss: 0.0002014268 Acc: 88.7032394409\n",
      "validation Loss: 0.0001955800 Acc: 88.2893218994\n",
      "training Loss: 0.0002015662 Acc: 88.6168289185\n",
      "validation Loss: 0.0001961621 Acc: 89.4313812256\n",
      "training Loss: 0.0001987099 Acc: 88.7682037354\n",
      "validation Loss: 0.0001940505 Acc: 89.3371200562\n",
      "training Loss: 0.0001987996 Acc: 88.7902526855\n",
      "validation Loss: 0.0001937082 Acc: 88.6639633179\n",
      "training Loss: 0.0001980294 Acc: 88.8386001587\n",
      "validation Loss: 0.0001924126 Acc: 89.0422363281\n",
      "training Loss: 0.0001983397 Acc: 88.8283233643\n",
      "validation Loss: 0.0001932487 Acc: 89.3322830200\n",
      "training Loss: 0.0001978071 Acc: 88.8017349243\n",
      "validation Loss: 0.0001937216 Acc: 88.9213790894\n",
      "training Loss: 0.0001981304 Acc: 88.8035507202\n",
      "validation Loss: 0.0001924825 Acc: 88.8923797607\n",
      "training Loss: 0.0001980284 Acc: 88.8253021240\n",
      "validation Loss: 0.0001933984 Acc: 89.1328735352\n",
      "training Loss: 0.0001961972 Acc: 88.9162445068\n",
      "validation Loss: 0.0001914838 Acc: 89.1582565308\n",
      "training Loss: 0.0001959472 Acc: 88.9718399048\n",
      "validation Loss: 0.0001920638 Acc: 89.0422363281\n",
      "training Loss: 0.0001959966 Acc: 88.9225921631\n",
      "validation Loss: 0.0001911045 Acc: 89.1957168579\n",
      "training Loss: 0.0001955721 Acc: 88.9319534302\n",
      "validation Loss: 0.0001923009 Acc: 89.3008575439\n",
      "training Loss: 0.0001953801 Acc: 88.9476699829\n",
      "validation Loss: 0.0001914113 Acc: 89.4108352661\n",
      "training Loss: 0.0001954527 Acc: 88.9105072021\n",
      "validation Loss: 0.0001914022 Acc: 89.0652008057\n",
      "training Loss: 0.0001956119 Acc: 88.9114151001\n",
      "validation Loss: 0.0001905770 Acc: 89.1739654541\n",
      "training Loss: 0.0001950734 Acc: 88.9367904663\n",
      "validation Loss: 0.0001917410 Acc: 88.8573303223\n",
      "training Loss: 0.0001952285 Acc: 88.9171524048\n",
      "validation Loss: 0.0001910855 Acc: 89.3371200562\n",
      "training Loss: 0.0001948925 Acc: 88.9570312500\n",
      "validation Loss: 0.0001908436 Acc: 89.4156723022\n",
      "training Loss: 0.0001946649 Acc: 88.9899673462\n",
      "validation Loss: 0.0001908315 Acc: 89.1751708984\n",
      "training Loss: 0.0001941968 Acc: 88.9591522217\n",
      "validation Loss: 0.0001915735 Acc: 89.4483032227\n",
      "training Loss: 0.0001938067 Acc: 89.0099029541\n",
      "validation Loss: 0.0001903773 Acc: 89.3528289795\n",
      "training Loss: 0.0001939308 Acc: 89.0086975098\n",
      "validation Loss: 0.0001903678 Acc: 89.1328735352\n",
      "training Loss: 0.0001938278 Acc: 89.0117187500\n",
      "validation Loss: 0.0001903320 Acc: 89.2150573730\n",
      "training Loss: 0.0001936009 Acc: 89.0419311523\n",
      "validation Loss: 0.0001905390 Acc: 89.2924041748\n",
      "training Loss: 0.0001936767 Acc: 89.0730514526\n",
      "validation Loss: 0.0001901542 Acc: 89.4072113037\n",
      "training Loss: 0.0001936924 Acc: 89.0071868896\n",
      "validation Loss: 0.0001902368 Acc: 89.1993408203\n",
      "training Loss: 0.0001933972 Acc: 89.0201797485\n",
      "validation Loss: 0.0001903073 Acc: 89.0458602905\n",
      "training Loss: 0.0001930427 Acc: 89.0984344482\n",
      "validation Loss: 0.0001900381 Acc: 89.1945114136\n",
      "training Loss: 0.0001931974 Acc: 89.0652008057\n",
      "validation Loss: 0.0001902744 Acc: 89.4567642212\n",
      "training Loss: 0.0001930161 Acc: 89.0685195923\n",
      "validation Loss: 0.0001903303 Acc: 89.3540344238\n",
      "training Loss: 0.0001931420 Acc: 89.0757751465\n",
      "validation Loss: 0.0001904239 Acc: 89.6319961548\n",
      "training Loss: 0.0001929671 Acc: 89.1280441284\n",
      "validation Loss: 0.0001896921 Acc: 89.0845336914\n",
      "training Loss: 0.0001930901 Acc: 89.0694274902\n",
      "validation Loss: 0.0001893968 Acc: 89.1884689331\n",
      "training Loss: 0.0001931161 Acc: 89.0670089722\n",
      "validation Loss: 0.0001901385 Acc: 89.0857391357\n",
      "training Loss: 0.0001930263 Acc: 89.0618743896\n",
      "validation Loss: 0.0001899865 Acc: 89.4615936279\n",
      "training Loss: 0.0001926007 Acc: 89.0972213745\n",
      "validation Loss: 0.0001909799 Acc: 89.6477127075\n",
      "training Loss: 0.0001929814 Acc: 89.0935974121\n",
      "validation Loss: 0.0001900949 Acc: 89.3129425049\n",
      "training Loss: 0.0001926209 Acc: 89.0981292725\n",
      "validation Loss: 0.0001893923 Acc: 89.3479919434\n",
      "training Loss: 0.0001923519 Acc: 89.0857391357\n",
      "validation Loss: 0.0001897643 Acc: 89.3371200562\n",
      "training Loss: 0.0001920483 Acc: 89.1105194092\n",
      "validation Loss: 0.0001897681 Acc: 89.2911911011\n",
      "training Loss: 0.0001920772 Acc: 89.1011505127\n",
      "validation Loss: 0.0001893311 Acc: 89.3334884644\n",
      "training Loss: 0.0001919313 Acc: 89.1186752319\n",
      "validation Loss: 0.0001895239 Acc: 89.3818359375\n",
      "training Loss: 0.0001918321 Acc: 89.1449584961\n",
      "validation Loss: 0.0001894173 Acc: 89.1473770142\n",
      "training Loss: 0.0001920028 Acc: 89.1494903564\n",
      "validation Loss: 0.0001895582 Acc: 89.1062850952\n",
      "training Loss: 0.0001917066 Acc: 89.0413284302\n",
      "validation Loss: 0.0001895120 Acc: 89.5522384644\n",
      "training Loss: 0.0001916393 Acc: 89.1014556885\n",
      "validation Loss: 0.0001893769 Acc: 89.3733749390\n",
      "training Loss: 0.0001912546 Acc: 89.1624832153\n",
      "validation Loss: 0.0001893478 Acc: 89.3274459839\n",
      "training Loss: 0.0001914155 Acc: 89.1398239136\n",
      "validation Loss: 0.0001892931 Acc: 89.3782043457\n",
      "training Loss: 0.0001914618 Acc: 89.1292495728\n",
      "validation Loss: 0.0001891983 Acc: 89.3890838623\n",
      "training Loss: 0.0001916862 Acc: 89.1791000366\n",
      "validation Loss: 0.0001891458 Acc: 89.0966186523\n",
      "training Loss: 0.0001911741 Acc: 89.1141433716\n",
      "validation Loss: 0.0001894054 Acc: 89.3915023804\n",
      "training Loss: 0.0001918138 Acc: 89.0905761719\n",
      "validation Loss: 0.0001892846 Acc: 89.2198867798\n",
      "training Loss: 0.0001915390 Acc: 89.1772918701\n",
      "validation Loss: 0.0001892851 Acc: 89.3492050171\n",
      "training Loss: 0.0001914632 Acc: 89.1618804932\n",
      "validation Loss: 0.0001893503 Acc: 89.2041778564\n",
      "training Loss: 0.0001913148 Acc: 89.1500930786\n",
      "validation Loss: 0.0001891727 Acc: 89.2573547363\n",
      "training Loss: 0.0001913043 Acc: 89.1564407349\n",
      "validation Loss: 0.0001885522 Acc: 89.2887725830\n",
      "training Loss: 0.0001909165 Acc: 89.1748733521\n",
      "validation Loss: 0.0001893948 Acc: 89.3056945801\n",
      "training Loss: 0.0001907857 Acc: 89.2177734375\n",
      "validation Loss: 0.0001891471 Acc: 89.2102203369\n",
      "training Loss: 0.0001910676 Acc: 89.1824264526\n",
      "validation Loss: 0.0001891354 Acc: 89.3383255005\n",
      "training Loss: 0.0001910327 Acc: 89.1902770996\n",
      "validation Loss: 0.0001895601 Acc: 89.3830413818\n",
      "training Loss: 0.0001909943 Acc: 89.1881637573\n",
      "validation Loss: 0.0001896447 Acc: 89.3141555786\n",
      "training Loss: 0.0001910800 Acc: 89.1821212769\n",
      "validation Loss: 0.0001888005 Acc: 89.3189926147\n",
      "training Loss: 0.0001910853 Acc: 89.1516036987\n",
      "validation Loss: 0.0001889303 Acc: 89.3020706177\n",
      "Early stopped.\n",
      "Best val acc: 89.647713\n",
      "----------\n",
      "training Loss: 0.0002962056 Acc: 81.8683929443\n",
      "validation Loss: 0.0002374248 Acc: 86.9988021851\n",
      "training Loss: 0.0002322130 Acc: 86.8583831787\n",
      "validation Loss: 0.0002278842 Acc: 87.6214141846\n",
      "training Loss: 0.0002251488 Acc: 87.3072433472\n",
      "validation Loss: 0.0002222827 Acc: 88.6337585449\n",
      "training Loss: 0.0002210889 Acc: 87.4993972778\n",
      "validation Loss: 0.0002176300 Acc: 88.5698089600\n",
      "training Loss: 0.0002184325 Acc: 87.6855239868\n",
      "validation Loss: 0.0002138883 Acc: 87.2811508179\n",
      "training Loss: 0.0002156427 Acc: 87.8218688965\n",
      "validation Loss: 0.0002109487 Acc: 87.8205032349\n",
      "training Loss: 0.0002134543 Acc: 87.9416275024\n",
      "validation Loss: 0.0002126792 Acc: 88.9462661743\n",
      "training Loss: 0.0002136652 Acc: 87.9177932739\n",
      "validation Loss: 0.0002086156 Acc: 88.6844329834\n",
      "training Loss: 0.0002119094 Acc: 87.9941177368\n",
      "validation Loss: 0.0002082433 Acc: 88.6675415039\n",
      "training Loss: 0.0002117486 Acc: 87.9754104614\n",
      "validation Loss: 0.0002144687 Acc: 86.6066513062\n",
      "training Loss: 0.0002108994 Acc: 88.0384597778\n",
      "validation Loss: 0.0002059874 Acc: 88.4177703857\n",
      "training Loss: 0.0002098111 Acc: 88.1832504272\n",
      "validation Loss: 0.0002094444 Acc: 88.4877548218\n",
      "training Loss: 0.0002108028 Acc: 88.1126632690\n",
      "validation Loss: 0.0002045876 Acc: 88.6301345825\n",
      "training Loss: 0.0002081568 Acc: 88.1696777344\n",
      "validation Loss: 0.0002083171 Acc: 88.0413131714\n",
      "training Loss: 0.0002082527 Acc: 88.1886825562\n",
      "validation Loss: 0.0002053413 Acc: 88.6047973633\n",
      "training Loss: 0.0002080839 Acc: 88.2628860474\n",
      "validation Loss: 0.0002027128 Acc: 88.8811111450\n",
      "training Loss: 0.0002073406 Acc: 88.3027114868\n",
      "validation Loss: 0.0002057890 Acc: 87.8615264893\n",
      "training Loss: 0.0002081800 Acc: 88.2607803345\n",
      "validation Loss: 0.0002019568 Acc: 88.5770416260\n",
      "training Loss: 0.0002078511 Acc: 88.3135681152\n",
      "validation Loss: 0.0002070501 Acc: 89.6762619019\n",
      "training Loss: 0.0002078986 Acc: 88.2915496826\n",
      "validation Loss: 0.0002041876 Acc: 88.9269638062\n",
      "training Loss: 0.0002066902 Acc: 88.3570098877\n",
      "validation Loss: 0.0002046572 Acc: 87.2871780396\n",
      "training Loss: 0.0002061556 Acc: 88.3627395630\n",
      "validation Loss: 0.0002030730 Acc: 88.2029953003\n",
      "training Loss: 0.0002025291 Acc: 88.5799331665\n",
      "validation Loss: 0.0001981401 Acc: 88.8992080688\n",
      "training Loss: 0.0002007131 Acc: 88.6438827515\n",
      "validation Loss: 0.0001980140 Acc: 88.8400878906\n",
      "training Loss: 0.0002010844 Acc: 88.6266860962\n",
      "validation Loss: 0.0001978747 Acc: 88.6989135742\n",
      "training Loss: 0.0002008002 Acc: 88.5935058594\n",
      "validation Loss: 0.0001982623 Acc: 89.0814056396\n",
      "training Loss: 0.0002002302 Acc: 88.6861114502\n",
      "validation Loss: 0.0001984095 Acc: 88.2295455933\n",
      "training Loss: 0.0002005841 Acc: 88.6598663330\n",
      "validation Loss: 0.0001976537 Acc: 88.9788436890\n",
      "training Loss: 0.0002000409 Acc: 88.6930541992\n",
      "validation Loss: 0.0001988186 Acc: 88.7049484253\n",
      "training Loss: 0.0001999917 Acc: 88.6797790527\n",
      "validation Loss: 0.0001967641 Acc: 88.9667816162\n",
      "training Loss: 0.0002004915 Acc: 88.6312103271\n",
      "validation Loss: 0.0001972581 Acc: 88.9209289551\n",
      "training Loss: 0.0001994621 Acc: 88.6634902954\n",
      "validation Loss: 0.0001973412 Acc: 88.9486770630\n",
      "training Loss: 0.0002000793 Acc: 88.7189941406\n",
      "validation Loss: 0.0001981032 Acc: 89.4675216675\n",
      "training Loss: 0.0001998740 Acc: 88.6465988159\n",
      "validation Loss: 0.0001965873 Acc: 88.7628631592\n",
      "training Loss: 0.0002000516 Acc: 88.6954650879\n",
      "validation Loss: 0.0001981207 Acc: 88.5046463013\n",
      "training Loss: 0.0001990629 Acc: 88.6963653564\n",
      "validation Loss: 0.0001982240 Acc: 89.3649597168\n",
      "training Loss: 0.0001991752 Acc: 88.6921463013\n",
      "validation Loss: 0.0001976602 Acc: 88.5734252930\n",
      "training Loss: 0.0001987716 Acc: 88.6945571899\n",
      "validation Loss: 0.0001970671 Acc: 89.3359985352\n",
      "training Loss: 0.0001969362 Acc: 88.8511199951\n",
      "validation Loss: 0.0001952255 Acc: 89.5326766968\n",
      "training Loss: 0.0001964189 Acc: 88.8984756470\n",
      "validation Loss: 0.0001951208 Acc: 89.3975372314\n",
      "training Loss: 0.0001963679 Acc: 88.8936538696\n",
      "validation Loss: 0.0001950346 Acc: 89.4988937378\n",
      "training Loss: 0.0001955467 Acc: 88.8951568604\n",
      "validation Loss: 0.0001948276 Acc: 89.4071884155\n",
      "training Loss: 0.0001955891 Acc: 88.8773651123\n",
      "validation Loss: 0.0001942606 Acc: 89.1900024414\n",
      "training Loss: 0.0001952723 Acc: 88.9419174194\n",
      "validation Loss: 0.0001940237 Acc: 88.5517044067\n",
      "training Loss: 0.0001952542 Acc: 88.8864135742\n",
      "validation Loss: 0.0001944966 Acc: 89.1924133301\n",
      "training Loss: 0.0001950998 Acc: 88.9036026001\n",
      "validation Loss: 0.0001946609 Acc: 89.1139831543\n",
      "training Loss: 0.0001942640 Acc: 88.9467391968\n",
      "validation Loss: 0.0001936828 Acc: 89.3299713135\n",
      "training Loss: 0.0001946925 Acc: 88.9428176880\n",
      "validation Loss: 0.0001942409 Acc: 88.7109756470\n",
      "training Loss: 0.0001950367 Acc: 88.9283447266\n",
      "validation Loss: 0.0001940794 Acc: 89.1284637451\n",
      "training Loss: 0.0001946824 Acc: 88.9256286621\n",
      "validation Loss: 0.0001939745 Acc: 88.7049484253\n",
      "training Loss: 0.0001950610 Acc: 88.9367904663\n",
      "validation Loss: 0.0001954562 Acc: 88.4901657104\n",
      "training Loss: 0.0001932623 Acc: 88.9950103760\n",
      "validation Loss: 0.0001923502 Acc: 88.9860839844\n",
      "training Loss: 0.0001928703 Acc: 89.0655975342\n",
      "validation Loss: 0.0001920969 Acc: 89.0126266479\n",
      "training Loss: 0.0001923784 Acc: 89.0885238647\n",
      "validation Loss: 0.0001926031 Acc: 89.4216690063\n",
      "training Loss: 0.0001924253 Acc: 89.0761566162\n",
      "validation Loss: 0.0001928352 Acc: 88.9957351685\n",
      "training Loss: 0.0001923887 Acc: 89.0710220337\n",
      "validation Loss: 0.0001922631 Acc: 89.1875915527\n",
      "training Loss: 0.0001922045 Acc: 89.0420684814\n",
      "validation Loss: 0.0001921810 Acc: 89.1091613770\n",
      "training Loss: 0.0001913757 Acc: 89.0583572388\n",
      "validation Loss: 0.0001915766 Acc: 89.3577194214\n",
      "training Loss: 0.0001911093 Acc: 89.1114501953\n",
      "validation Loss: 0.0001921521 Acc: 89.1767272949\n",
      "training Loss: 0.0001906860 Acc: 89.1959075928\n",
      "validation Loss: 0.0001915938 Acc: 89.3601303101\n",
      "training Loss: 0.0001912097 Acc: 89.1153717041\n",
      "validation Loss: 0.0001915016 Acc: 89.1224288940\n",
      "training Loss: 0.0001908739 Acc: 89.1075286865\n",
      "validation Loss: 0.0001915760 Acc: 89.2093048096\n",
      "training Loss: 0.0001905833 Acc: 89.1793212891\n",
      "validation Loss: 0.0001916368 Acc: 89.2889404297\n",
      "training Loss: 0.0001907322 Acc: 89.1238174438\n",
      "validation Loss: 0.0001911736 Acc: 89.2768783569\n",
      "training Loss: 0.0001904097 Acc: 89.1485519409\n",
      "validation Loss: 0.0001917183 Acc: 89.3625488281\n",
      "training Loss: 0.0001906665 Acc: 89.1572952271\n",
      "validation Loss: 0.0001912628 Acc: 89.2901535034\n",
      "training Loss: 0.0001905329 Acc: 89.1084289551\n",
      "validation Loss: 0.0001914417 Acc: 89.4216690063\n",
      "training Loss: 0.0001904130 Acc: 89.1663513184\n",
      "validation Loss: 0.0001911556 Acc: 89.3010101318\n",
      "training Loss: 0.0001895635 Acc: 89.2164230347\n",
      "validation Loss: 0.0001912486 Acc: 89.3359985352\n",
      "training Loss: 0.0001900362 Acc: 89.2179336548\n",
      "validation Loss: 0.0001910739 Acc: 89.3480682373\n",
      "training Loss: 0.0001898878 Acc: 89.1401062012\n",
      "validation Loss: 0.0001913296 Acc: 89.3215179443\n",
      "training Loss: 0.0001897763 Acc: 89.1473464966\n",
      "validation Loss: 0.0001914680 Acc: 89.4530410767\n",
      "training Loss: 0.0001900304 Acc: 89.2571487427\n",
      "validation Loss: 0.0001908232 Acc: 89.2865295410\n",
      "training Loss: 0.0001898285 Acc: 89.1835403442\n",
      "validation Loss: 0.0001908232 Acc: 89.2467117310\n",
      "training Loss: 0.0001896774 Acc: 89.2121963501\n",
      "validation Loss: 0.0001909054 Acc: 89.3227310181\n",
      "training Loss: 0.0001895182 Acc: 89.2121963501\n",
      "validation Loss: 0.0001909386 Acc: 89.2358551025\n",
      "training Loss: 0.0001895100 Acc: 89.2260742188\n",
      "validation Loss: 0.0001914622 Acc: 89.2346420288\n",
      "training Loss: 0.0001890378 Acc: 89.2592544556\n",
      "validation Loss: 0.0001909905 Acc: 89.1743164062\n",
      "training Loss: 0.0001889543 Acc: 89.1871643066\n",
      "validation Loss: 0.0001909576 Acc: 89.0476226807\n",
      "training Loss: 0.0001891958 Acc: 89.2115936279\n",
      "validation Loss: 0.0001907646 Acc: 89.2636032104\n",
      "training Loss: 0.0001891652 Acc: 89.2592544556\n",
      "validation Loss: 0.0001905796 Acc: 89.1972427368\n",
      "training Loss: 0.0001890388 Acc: 89.2164230347\n",
      "validation Loss: 0.0001908081 Acc: 89.0681304932\n",
      "training Loss: 0.0001888463 Acc: 89.2456817627\n",
      "validation Loss: 0.0001907198 Acc: 89.1912078857\n",
      "training Loss: 0.0001891424 Acc: 89.1850509644\n",
      "validation Loss: 0.0001911531 Acc: 89.2768783569\n",
      "training Loss: 0.0001889796 Acc: 89.2601623535\n",
      "validation Loss: 0.0001908048 Acc: 89.0922698975\n",
      "training Loss: 0.0001890158 Acc: 89.2619705200\n",
      "validation Loss: 0.0001909042 Acc: 89.1550064087\n",
      "training Loss: 0.0001888873 Acc: 89.2580490112\n",
      "validation Loss: 0.0001907430 Acc: 89.3299713135\n",
      "training Loss: 0.0001888797 Acc: 89.2360305786\n",
      "validation Loss: 0.0001905402 Acc: 89.2044830322\n",
      "training Loss: 0.0001890686 Acc: 89.2526245117\n",
      "validation Loss: 0.0001908203 Acc: 89.2213745117\n",
      "training Loss: 0.0001891289 Acc: 89.1551895142\n",
      "validation Loss: 0.0001907593 Acc: 89.3022155762\n",
      "training Loss: 0.0001887016 Acc: 89.2842941284\n",
      "validation Loss: 0.0001908825 Acc: 89.3782348633\n",
      "training Loss: 0.0001890433 Acc: 89.2354278564\n",
      "validation Loss: 0.0001906391 Acc: 89.2599868774\n",
      "training Loss: 0.0001887727 Acc: 89.2830886841\n",
      "validation Loss: 0.0001908052 Acc: 89.2286148071\n",
      "training Loss: 0.0001890640 Acc: 89.2218551636\n",
      "validation Loss: 0.0001911474 Acc: 89.2829132080\n",
      "training Loss: 0.0001890013 Acc: 89.2734375000\n",
      "validation Loss: 0.0001910357 Acc: 89.1863784790\n",
      "Early stopped.\n",
      "Best val acc: 89.676262\n",
      "----------\n",
      "Average best_acc across k-fold: 89.56195831298828\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json'\n",
    "    best_conf = optimize_hyperparams( # NEED TO FIX THIS FUNC STILL !!!\n",
    "        data_list_dict, data_hlf_dict, label_dict, {fold: training_weights(data_aux_dict[f'fold_{fold}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold}']) for fold in len(data_list_dict)},\n",
    "        config_file, epochs=10,\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "    # with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "        best_conf = json.load(f)\n",
    "        print(best_conf)\n",
    "\n",
    "fom = []\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx in range(len(data_hlf_dict)):\n",
    "    weight = training_weights(data_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_dict[f'fold_{fold_idx}'])\n",
    "    (\n",
    "        train_data_list, val_data_list,\n",
    "        train_data_hlf, val_data_hlf,\n",
    "        train_label, val_label,\n",
    "        train_weight, val_weight\n",
    "    ) = train_test_split(data_list_dict[f'fold_{fold_idx}'], data_hlf_dict[f'fold_{fold_idx}'], label_dict[f'fold_{fold_idx}'], weight, test_size=0.2)\n",
    "    model_file = OUTPUT_DIRPATH + CURRENT_TIME +'_ReallyTopclassStyle_'+ f'{fold_idx}.torch'\n",
    "    state_file = OUTPUT_DIRPATH + CURRENT_TIME +'_BestPerfReallyTopclass_'+ f'{fold_idx}.torch'\n",
    "    \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], \n",
    "        dnn_input=np.shape(data_hlf_dict[f'fold_{fold_idx}'])[-1], rnn_input=np.shape(data_list_dict[f'fold_{fold_idx}'])[-1],\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(train_data_list, train_data_hlf, train_label, train_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(val_data_list, val_data_hlf, val_label, val_weight), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        NUM_EPOCHS, model, optimizer, scheduler, \n",
    "        state_file, model_file, data_loader=data_loader, \n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(OUTPUT_DIRPATH+'/train+val_losses.json', 'w') as f:\n",
    "#     json.dump({'train_losses': train_losses_arr, 'val_losses': val_losses_arr}, f)\n",
    "\n",
    "with open(OUTPUT_DIRPATH+'/train+val_losses.json', 'r') as f:\n",
    "    train_val_losses_dict = json.load(f)\n",
    "train_losses_arr = train_val_losses_dict['train_losses']\n",
    "val_losses_arr = train_val_losses_dict['val_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/numpy/core/_methods.py:164: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "weight_test_dict = {\n",
    "    f'fold_{fold_idx}': copy.deepcopy(training_weights(data_test_aux_dict[f'fold_{fold_idx}'].loc[:, \"eventWeight\"].to_numpy(), label_test_dict[f'fold_{fold_idx}'])) for fold_idx in range(len(data_test_aux_dict))\n",
    "}\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, \n",
    "        save=True, dict_lists=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test_dict, data_hlf_test_dict, label_test_dict, weight_test_dict,\n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, None, best_conf,\n",
    "        save=True, dict_lists=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OUTPUT_DIRPATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43mOUTPUT_DIRPATH\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCURRENT_TIME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_IN_perf.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     IN_perf \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m TPR_thresholds \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.9704\u001b[39m, \u001b[38;5;241m0.9498\u001b[39m, \u001b[38;5;241m0.9196\u001b[39m, \u001b[38;5;241m0.7536\u001b[39m, \u001b[38;5;241m0.5777\u001b[39m, \u001b[38;5;241m0.3837\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OUTPUT_DIRPATH' is not defined"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "    NNtable.add_row(\n",
    "        [\n",
    "            IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "            \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "        ]\n",
    "    )\n",
    "print(NNtable)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = OUTPUT_DIRPATH + f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}/' + CURRENT_TIME\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [(data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "\n",
    "    plot_train_val_losses(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    "    )\n",
    "    plot_roc(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], run2=False,\n",
    "        mask=None\n",
    "    )\n",
    "    # print(f\"num bkg: {np.sum(label_test==0)}\")\n",
    "    # print(f\"num sig: {np.sum(label_test==1)}\")\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted', method='round_robin',\n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weight_test_dict, n_bins=25, \n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    plot_output_score(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], n_bins=25, \n",
    "        # all_bkg=True,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='round_robin', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weight_test_dict, n_bins=25,\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    # for score_cut in [0.2, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    #     plot_input_vars_after_score_cut(\n",
    "    #         IN_perf, score_cut, plot_destdir, plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #         mask=mask_arr\n",
    "    #     )\n",
    "    # plot_input_vars_after_score_cut(\n",
    "    #     IN_perf, [0.2, 0.6, 0.9], plot_destdir, method='arr', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    #     mask=mask_arr\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimized cut-boundaries for ttH score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(IN_perf, weights, method='arr', bins=50, mask=None, n_folds=5):\n",
    "    if method == 'round_robin':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask[fold_idx]\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[f\"fold_{fold_idx}\"]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[f\"fold_{fold_idx}\"]['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold\n",
    "    elif method == 'arr':\n",
    "        hist_list_fold = []\n",
    "        cut_boundaries_fold = []\n",
    "        cut_s_over_root_bs_fold = []\n",
    "        sig_weights_fold = []\n",
    "        bkg_weights_fold = []\n",
    "        for fold_idx in range(n_folds):\n",
    "            sig_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 1, mask\n",
    "                ),1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_perf['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.logical_and(\n",
    "                    np.array(IN_perf['all_labels'][fold_idx]) == 0, mask\n",
    "                ),1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "            fold_idx_cuts_bins_inclusive = []\n",
    "            fold_idx_sig_weights = []\n",
    "            fold_idx_bkg_weights = []\n",
    "            fold_idx_prev_s_over_root_b = []\n",
    "            prev_s_over_root_b = 0\n",
    "            for i in range(bins):\n",
    "                s = np.sum(sig_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])\n",
    "                sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                    (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]))\n",
    "                if prev_s_over_root_b < (s / sqrt_b):\n",
    "                    prev_s_over_root_b = s / sqrt_b\n",
    "                    continue\n",
    "                else:\n",
    "                    fold_idx_sig_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(sig_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_bkg_weights.append(\n",
    "                        {\n",
    "                            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ]),\n",
    "                            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                                (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                            ])),\n",
    "                        }\n",
    "                    )\n",
    "                    fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                    fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                    prev_s_over_root_b = 0\n",
    "            fold_idx_sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(sig_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sum(bkg_hist.values().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ]),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            fold_idx_cuts_bins_inclusive.append(0)\n",
    "            fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "            fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "            cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "            cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "            sig_weights_fold.append(fold_idx_sig_weights)\n",
    "            bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "                    \n",
    "        sig_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 1, mask\n",
    "            ),1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_perf['mean_pred']\n",
    "        )[\n",
    "            np.logical_and(\n",
    "                np.array(IN_perf['mean_label']) == 0, mask\n",
    "            ),1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "\n",
    "        cut_boundaries = []\n",
    "        cut_s_over_root_bs = []\n",
    "        prev_s_over_root_b = 0\n",
    "        sig_weights = []\n",
    "        bkg_weights = []\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                cut_boundaries.append(bins - i)\n",
    "                cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        cut_boundaries.append(0)\n",
    "        cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "        cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "        return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold, cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "==============================0_lepton==============================\n",
      "==============================1_lepton==============================\n",
      "==============================2+_lepton==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_338963/637662937.py:36: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
      "/tmp/ipykernel_338963/1299132964.py:314: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_338963/1299132964.py:314: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_338963/1299132964.py:314: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_338963/1299132964.py:314: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_338963/1299132964.py:314: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_338963/1299132964.py:314: RuntimeWarning: divide by zero encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
      "/tmp/ipykernel_338963/1299132964.py:345: RuntimeWarning: invalid value encountered in sqrt\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    }
   ],
   "source": [
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "for plot_type in ['', '0_lepton', '1_lepton', '2+_lepton']:\n",
    "    print(\"=\"*30 + plot_type + \"=\"*30)\n",
    "    # plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "    plot_destdir = OUTPUT_DIRPATH + f'plots{(\"_\" if plot_type != \"\" else \"\") + plot_type}/' + CURRENT_TIME\n",
    "    if not os.path.exists(plot_destdir):\n",
    "        os.makedirs(plot_destdir)\n",
    "\n",
    "    if plot_type == '':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt'] > -1000 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '0_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton1_pt']  == -999 for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '1_lepton':\n",
    "        mask_arr = [(data_test_aux_dict[f'fold_{i}']['lepton1_pt'] != -999) & (data_test_aux_dict[f'fold_{i}']['lepton2_pt'] == -999) for i in range(len(data_test_aux_dict))]\n",
    "    elif plot_type == '2+_lepton':\n",
    "        mask_arr = [data_test_aux_dict[f'fold_{i}']['lepton2_pt']  != -999 for i in range(len(data_test_aux_dict))]\n",
    "\n",
    "    (\n",
    "        cut_boundaries_fold, cut_s_over_root_bs_fold, \n",
    "        sig_weights_fold, bkg_weights_fold, \n",
    "        cut_boundaries, cut_s_over_root_bs, \n",
    "        sig_weights, bkg_weights \n",
    "    ) = optimize_cut_boundaries(\n",
    "        IN_perf, weight_test_dict, method='round_robin',\n",
    "        mask=mask_arr\n",
    "    )\n",
    "\n",
    "    fold_labels = [\n",
    "        [\n",
    "            f\"s/âb={cut_s_over_root_bs_fold[fold_idx][cut_idx]:.04f}, s={sig_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{sig_weights_fold[fold_idx][cut_idx]['w2']:.04f}, b={bkg_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{bkg_weights_fold[fold_idx][cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[fold_idx]))\n",
    "        ] for fold_idx in range(len(weight_test_dict))\n",
    "    ]\n",
    "    fold_colors = [copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries_fold[i]) // len(cmap_petroff10)) + 1)) for i in range(len(weight_test_dict))]\n",
    "    for fold_idx in range(len(weight_test_dict)):\n",
    "        s_over_root_b(\n",
    "            IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_{plot_type}_fold{fold_idx}', \n",
    "            labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weight_test_dict, method='round_robin',\n",
    "            lines_fold=cut_boundaries_fold, lines_labels=fold_labels, only_fold=fold_idx, lines_colors=fold_colors,\n",
    "            mask=mask_arr\n",
    "        )\n",
    "    # labels = [\n",
    "    #     f\"s/âb={cut_s_over_root_bs[cut_idx]:.04f}, s={sig_weights[cut_idx]['value']:.04f}Â±{sig_weights[cut_idx]['w2']:.04f}, b={bkg_weights[cut_idx]['value']:.04f}Â±{bkg_weights[cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs))\n",
    "    # ]\n",
    "    # colors = copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries) // len(cmap_petroff10)) + 1))\n",
    "    # s_over_root_b(\n",
    "    #     IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_{plot_type}_foldAvg', \n",
    "    #     labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights=weight_test_dict, method='round_robin', \n",
    "    #     lines=cut_boundaries, lines_labels=labels, no_fold=True, lines_colors=colors,\n",
    "    #     mask=mask_arr\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            data_list[train_index], data_hlf[train_index], label[train_index], weight[train_index],\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            data_list[val_index], data_hlf[val_index], label[val_index], weight[val_index],\n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'r') as f:\n",
    "    IN_perf_dict = json.load(f)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold ']\n",
    "val_weights_arr = []\n",
    "for fold_idx, (train_IN_dict, val_IN_dict, (train_index, val_index)) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'], skf.split(data_hlf, label))):\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )\n",
    "    rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "    rectified_train_index[val_index] = False\n",
    "    sig_train_mask = rectified_train_index & (label == 1)\n",
    "    sig_val_mask = np.logical_not(rectified_train_index) & (label == 1)\n",
    "    bkg_train_mask = rectified_train_index & (label == 0)\n",
    "    bkg_val_mask = np.logical_not(rectified_train_index) & (label == 0)\n",
    "    weights = [\n",
    "        {'sig': data_aux.loc[sig_train_mask, \"eventWeight\"], 'bkg': data_aux.loc[bkg_train_mask, \"eventWeight\"]},\n",
    "        {'sig': data_aux.loc[sig_val_mask, \"eventWeight\"], 'bkg': data_aux.loc[bkg_val_mask, \"eventWeight\"]}\n",
    "    ]\n",
    "    val_weights_arr.append(copy.deepcopy(weights[1]))\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_weighted_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_density_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=[{'sig': None, 'bkg': None}]*len(labels_arr)\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights\n",
    "    )\n",
    "labels_arr = ['val - fold 0', 'val - fold 1', 'val - fold 2', 'val - fold 3', 'val - fold 4']\n",
    "s_over_root_b(\n",
    "    IN_perf_dict['val'], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_val_comparison', \n",
    "    method='IN_arr', labels=labels_arr, weights=val_weights_arr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Vars Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    sig_mask = (label == 1)\n",
    "    sig_test_mask = (label_test == 1)\n",
    "    bkg_mask = (label == 0)\n",
    "    bkg_test_mask = (label_test == 0)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        \n",
    "        sig_train_mask = rectified_train_index & sig_mask\n",
    "        sig_val_mask = np.logical_not(rectified_train_index) & sig_mask\n",
    "        bkg_train_mask = rectified_train_index & bkg_mask\n",
    "        bkg_val_mask = np.logical_not(rectified_train_index) & bkg_mask\n",
    "\n",
    "        sig_train_np = data_df.loc[sig_train_mask, var_name].to_numpy()\n",
    "        sig_val_np = data_df.loc[sig_val_mask, var_name].to_numpy()\n",
    "        sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "        bkg_train_np = data_df.loc[bkg_train_mask, var_name].to_numpy()\n",
    "        bkg_val_np = data_df.loc[bkg_val_mask, var_name].to_numpy()\n",
    "        bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np = data_df.loc[sig_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_df.loc[bkg_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "    sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    pre_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_pre_std, var_name, pre_std_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### post-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 44\u001b[0m\n\u001b[1;32m     37\u001b[0m     bkg_test_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39mbkg_test_np[bkg_test_np \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     39\u001b[0m     make_input_plot(\n\u001b[1;32m     40\u001b[0m         output_dir_post_std, var_name, \n\u001b[1;32m     41\u001b[0m         [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n\u001b[1;32m     42\u001b[0m         fold_idx\u001b[38;5;241m=\u001b[39mfold_idx, labels\u001b[38;5;241m=\u001b[39mlabel_arr_fold\n\u001b[1;32m     43\u001b[0m     )\n\u001b[0;32m---> 44\u001b[0m sig_train_np, sig_test_np, bkg_train_np, bkg_test_np \u001b[38;5;241m=\u001b[39m \u001b[43mpost_std_np_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m sig_train_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39msig_train_np[sig_train_np \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     47\u001b[0m sig_test_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(VARIABLES_STD[var_name])\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39msig_test_np[sig_test_np \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[18], line 159\u001b[0m, in \u001b[0;36mpost_std_np_arrays\u001b[0;34m(data, data_test, var_name, train_index, val_index)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m train_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m val_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m (high_level_fields \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(input_hlf_vars)):\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;66;03m# index2, index3 = index_map[var_name]\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m         sig_train_np \u001b[38;5;241m=\u001b[39m data[\u001b[43mdata_list_index_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig_mask\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    160\u001b[0m         sig_test_np \u001b[38;5;241m=\u001b[39m data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n\u001b[1;32m    161\u001b[0m         bkg_train_np \u001b[38;5;241m=\u001b[39m data[data_list_index_map(var_name, data, bkg_mask)]\n",
      "File \u001b[0;32m/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/data_processing.py:30\u001b[0m, in \u001b[0;36mdata_list_index_map\u001b[0;34m(variable_name, data_list, event_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data_list)):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m event_mask[i]:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     lepton1_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(np\u001b[38;5;241m.\u001b[39mwhere(data_list[i, :, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m))[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m     mask_arr[i, lepton1_idx, index3] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = data_list, data_list_test\n",
    "    else:\n",
    "        data, data_test = data_hlf, data_hlf_test\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name,\n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    post_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_post_std, var_name, post_std_hists[var_name], labels=label_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian smearing on test set (for feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    mask_arr = data_list_index_map(var_name, particle_list_to_smear, np.ones(len(particle_list_to_smear), dtype=bool))\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear[mask_arr] *= rng.normal(size=len(particle_list_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear[mask_arr] += rng.normal(size=len(particle_list_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns[var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate smeared variable test-data on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "    gauss_data_list, gauss_data_hlf = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        gauss_data_list = smear_particle_list(var_name, copy.deepcopy(data_list_test))\n",
    "        gauss_data_hlf = data_hlf_test\n",
    "    else:\n",
    "        gauss_data_list = data_list_test\n",
    "        gauss_data_hlf = smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test))\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list, gauss_data_hlf, label_test, OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "IN_perf_smear_list = []\n",
    "label_arr = []\n",
    "for var_name, IN_perf_smear in IN_perf_smear_dict.items():\n",
    "    IN_perf_smear_list.append(IN_perf_smear)\n",
    "    label_arr.append(var_name)\n",
    "sort = np.argsort([IN_perf_smear['mean_area'] for IN_perf_smear in IN_perf_smear_list])\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_all', \n",
    "    method='IN_arr', labels=label_arr, yscale='log', run2=False, sort=sort\n",
    ")\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5', \n",
    "    method='IN_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False\n",
    ")\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5_and_orig', \n",
    "    method='IN_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False, run3=IN_perf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = smear_particle_list(var_name, data_list), smear_particle_list(var_name, data_list_test)\n",
    "    else:\n",
    "        data, data_test = smear_particle_hlf(var_name, data_hlf), smear_particle_hlf(var_name, data_hlf_test)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name, \n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_gauss_smear, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    gauss_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_gauss_smear, var_name, gauss_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y in [('train', data_list, data_hlf, label), ('test', data_list_test, data_hlf_test, label_test)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'r') as f:\n",
    "    IN_full_eval_dict = json.load(f)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {'mass': [], 'dijet_mass': []}\n",
    "for var_name in hist_dict.keys():\n",
    "    for i, score_cut in enumerate(score_cuts):\n",
    "        sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict)\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "        hist_dict[var_name].extend(\n",
    "            [\n",
    "                copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "            ]\n",
    "        )\n",
    "    for mod_factor, label_mod in enumerate(['sig_train', 'sig_test', 'bkg_train', 'bkg_test']):\n",
    "        plot_list = []\n",
    "        label_list = []\n",
    "        for i in range(len(hist_dict[var_name])):\n",
    "            if (i - mod_factor) % 4 == 0:\n",
    "                plot_list.append(hist_dict[var_name][i])\n",
    "                label_list.append(label_arr[i])\n",
    "        make_input_plot(\n",
    "            plot_destdir, var_name, plot_list, labels=label_list, density=True, \n",
    "            plot_prefix=CURRENT_TIME+'_', plot_postfix='_'+label_mod, alpha=0.5,\n",
    "            linestyle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset size plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_dirs = glob.glob(OUTPUT_DIRPATH[:-1]+'_mod*') + [OUTPUT_DIRPATH[:-1]]\n",
    "final_train_losses_arr, final_val_losses_arr = [], []\n",
    "mod_values_arr = []\n",
    "\n",
    "for train_size_dir in train_size_dirs:\n",
    "    if len(glob.glob(train_size_dir + '/*IN_perf.json')) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        mod_values_arr.append([\n",
    "            float(\n",
    "                train_size_dir[\n",
    "                    train_size_dir.find('_mod')+4 : train_size_dir.find('-')\n",
    "                ]\n",
    "            ),\n",
    "            float(\n",
    "                train_size_dir[train_size_dir.find('-')+1:]\n",
    "            )\n",
    "        ])\n",
    "    except:\n",
    "         mod_values_arr.append([2, 2])\n",
    "    IN_perf_path = glob.glob(f'{train_size_dir}/*IN_perf.json')[0]\n",
    "    with open(IN_perf_path, 'r') as f:\n",
    "        IN_perf = json.load(f)\n",
    "    final_train_losses_arr.append([train_losses[-7 if len(train_losses) < NUM_EPOCHS else -1] for train_losses in IN_perf['train_losses_arr']])\n",
    "    final_val_losses_arr.append([val_losses[-7 if len(val_losses) < NUM_EPOCHS else -1] for val_losses in IN_perf['val_losses_arr']])\n",
    "\n",
    "final_train_losses_arr = np.array(final_train_losses_arr)\n",
    "final_val_losses_arr = np.array(final_val_losses_arr)\n",
    "mod_values_arr = np.array(mod_values_arr)\n",
    "dataset_sizes = (len(label) + len(label_test)) / mod_values_arr\n",
    "sorted_indices = np.argsort(dataset_sizes[:, 0])\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_train_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_train_losses_arr, axis=1)-np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_train_losses_arr, axis=1)+np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[0], alpha=0.5, label='Train data'\n",
    ")\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_val_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_val_losses_arr, axis=1)-np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_val_losses_arr, axis=1)+np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[1], alpha=0.5, label='Val data'\n",
    ")\n",
    "plt.xlabel('Size of train dataset')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.pdf')\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna-hhbbgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
