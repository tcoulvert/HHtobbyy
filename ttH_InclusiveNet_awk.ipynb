{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu3.fnal.gov      Tue Oct  1 21:09:45 2024  555.42.06\n",
      "[0] Tesla P100-PCIE-12GB | 43°C,   0 % |   748 / 12288 MB | tsievert(746M)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "V2_MERGED = True\n",
    "\n",
    "SIGNAL_FILEPATHS = [\n",
    "    # Test sig files #\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE/GluGluToHH/nominal/*',\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE_merged_v2/GluGluToHH/nominal/*',\n",
    "    # ggF HH # \n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\",\n",
    "    # VBF HH #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [\n",
    "    # ttH (i.e. the main bkg to reduce) #\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    # # Other single H samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\",\n",
    "    # # Prompt-Prompt samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\",\n",
    "    # # Prompt-Fake samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\",\n",
    "    # Fake-Fake samples #\n",
    "    # ADD HERE #\n",
    "]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v4'\n",
    "CRITERION = \"NLLLoss\"\n",
    "N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "MOD_VALS = (2, 2)\n",
    "# VARS = 'base_vars'\n",
    "VARS = 'extra_vars'\n",
    "CURRENT_TIME = '2024-08-30_14-35-01'\n",
    "# VARS = 'extra_vars_no_dijet_mass'\n",
    "# VARS = 'no_bad_vars'\n",
    "# VARS = 'simplified_bad_vars'\n",
    "# VARS = 'extra_vars_and_bools'\n",
    "# VARS = 'extra_vars_in_RNN'\n",
    "# VARS = f'extra_vars_mod{MOD_VALS[0]}-{MOD_VALS[1]}'\n",
    "# VARS = 'extra_vars_lead_lep_only'\n",
    "# CURRENT_TIME = '2024-09-24_20-01-09'\n",
    "# N_PARTICLES, N_PARTICLE_FIELDS = 3, 6\n",
    "OUTPUT_DIRPATH = CURRENT_DIRPATH + f\"/model_outputs/{VERSION}/{VARS}/\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False\n",
    "NUM_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (258332, 4, 6)\n",
      "Data HLF: (258332, 14)\n",
      "n signal = 85041, n bkg = 173291\n",
      "Data list test: (258924, 4, 6)\n",
      "Data HLF test: (258924, 14)\n",
      "n signal = 85713, n bkg = 173211\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    data_df, data_test_df, \n",
    "    data_list, data_hlf, label, \n",
    "    data_list_test, data_hlf_test, label_test, \n",
    "    high_level_fields, input_hlf_vars, hlf_vars_columns,\n",
    "    data_aux, data_test_aux\n",
    ") = process_data(\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED, return_pre_std=True, mod_vals=MOD_VALS\n",
    ")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dotted']\n",
    "        linestyles = linestyles * ((2*len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:2*len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx if fold_idx%2 == 0 else fold_idx+1]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx+1 if fold_idx%2 == 0 else fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='std', \n",
    "    labels=None, yscale='linear', run2=True, sort=None, run3=None\n",
    "):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            IN_info['mean_fprs'], IN_info['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.plot(\n",
    "                IN_info['fprs'][fold_idx], IN_info['base_tpr'],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            IN_info['mean_fprs'], IN_info['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for i in index_arr:\n",
    "            plt.plot(\n",
    "                IN_info[i]['mean_fprs'], IN_info[i]['base_tpr'], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i], alpha=0.5\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if run3 is not None:\n",
    "        plt.plot(\n",
    "            run3['mean_fprs'], run3['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (run3['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, n_bins=50, all_sig=False, all_bkg=False\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        for cut in np.linspace(0, 1, 10, endpoint=False):\n",
    "            print(f\"output score > {cut:.2f}\")\n",
    "            print('='*60)\n",
    "            print(f\"num sig > {cut:.2f} = {len(sig_np[sig_np > cut])}\")\n",
    "            print(f\"num bkg > {cut:.2f} = {len(bkg_np[bkg_np > cut])}\")\n",
    "            print('-'*60)\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights['sig'] is not None else False),\n",
    "                alpha=0.5, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    f'{\"ttH\" if not all_bkg else \"all\"} background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        for cut in np.linspace(0, 1, 10, endpoint=False):\n",
    "            print(f\"output score > {cut:.2f}\")\n",
    "            print('='*60)\n",
    "            print(f\"num sig > {cut:.2f} = {len(sig_np[sig_np > cut])}\")\n",
    "            print(f\"num bkg > {cut:.2f} = {len(bkg_np[bkg_np > cut])}\")\n",
    "            print('-'*60)\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist],\n",
    "            yerr=(True if weights['sig'] is not None else False),\n",
    "            alpha=1, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=[\n",
    "                f'{\"ggF HH\" if not all_sig else \"ggF HH + VBF HH\"} signal', \n",
    "                f'{\"ttH\" if not all_bkg else \"all\"} background'\n",
    "            ]\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info[i]['mean_label']) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info[i]['mean_label']) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'] if weights[i]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'] if weights[i]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist],\n",
    "                yerr=(True if weights[i]['sig'] is not None else False),\n",
    "                alpha=0.7, density=(False if weights[i]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[i] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[i], linestyles[i]]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def s_over_root_b(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, lines_fold=None, lines=None, lines_labels=None, \n",
    "        lines_colors=None, only_fold=None, no_fold=False, n_bins=50\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/√b', alpha=0.8 \n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/√b'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/√b'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "        if only_fold is None:\n",
    "            sig_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info['mean_label']) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info['mean_label']) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/√b - avg. over folds', \n",
    "                alpha=0.5, \n",
    "            )\n",
    "            if lines is not None:\n",
    "                for i in range(len(lines)):\n",
    "                    plt.vlines(\n",
    "                        lines[i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/√b - avg.'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[i]\n",
    "                    )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['all_preds'][0]\n",
    "            )[\n",
    "                np.array(IN_info[i]['all_labels'][0]) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['all_preds'][0]\n",
    "            )[\n",
    "                np.array(IN_info[i]['all_labels'][0]) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'])\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "                label='s/√b'+(' - '+labels[i] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[i], \n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'jet1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'jet2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(12, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(40, -1., 1., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, var_name, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label == 1\n",
    "    sig_test_mask = label_test == 1\n",
    "    bkg_mask = label == 0\n",
    "    bkg_test_mask = label_test == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns[var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "            # index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns[var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict):\n",
    "    sig_train_mask = (label == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux.loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux.loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(\n",
    "    output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, \n",
    "    plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True\n",
    "):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir = output_dir + \"fold/\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_input_vars_after_score_cut(\n",
    "    IN_info, score_cut, destdir, plot_prefix, plot_postfix='', method='std', \n",
    "    weights={'sig': None, 'bkg': None}, all_sig=False, all_bkg=False\n",
    "):\n",
    "    if method == 'std':\n",
    "        sig_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ] > score_cut\n",
    "        bkg_mask = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ] > score_cut\n",
    "\n",
    "        for var_name in high_level_fields:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df.loc[label_test==1, var_name]\n",
    "            sig_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=sig_var.loc[sig_mask], \n",
    "                weight=weights['sig'][sig_mask] if weights['sig'] is not None else np.ones(np.sum(sig_mask))\n",
    "            )\n",
    "            bkg_var = data_test_df.loc[label_test==0, var_name]\n",
    "            bkg_hist = hist.Hist(VARIABLES[var_name]).fill(\n",
    "                var=bkg_var.loc[bkg_mask], \n",
    "                weight=weights['bkg'][bkg_mask] if weights['bkg'] is not None else np.ones(np.sum(bkg_mask))\n",
    "            )\n",
    "            make_input_plot(\n",
    "                destdir, var_name, [sig_hist, bkg_hist], plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore{score_cut}', labels=['HH signal', 'ttH background'], density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    elif method == 'arr':\n",
    "        for var_name in high_level_fields:\n",
    "            if var_name in {'event', 'puppiMET_eta'}:\n",
    "                continue\n",
    "            sig_var = data_test_df.loc[label_test==1, var_name]\n",
    "            bkg_var = data_test_df.loc[label_test==0, var_name]\n",
    "            sig_masks, bkg_masks = [], []\n",
    "            hists, labels = [], []\n",
    "            for cut in score_cut:\n",
    "                sig_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.array(IN_info['mean_label']) == 1,1\n",
    "                ] > cut)\n",
    "                bkg_masks.append(np.exp(\n",
    "                    IN_info['mean_pred']\n",
    "                )[\n",
    "                    np.array(IN_info['mean_label']) == 0,1\n",
    "                ] > cut)\n",
    "                \n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=sig_var.loc[sig_masks[-1]], \n",
    "                    weight=weights['sig'][sig_masks[-1]] if weights['sig'] is not None else np.ones(np.sum(sig_masks[-1]))\n",
    "                ))\n",
    "                hists.append(hist.Hist(VARIABLES[var_name]).fill(\n",
    "                    var=bkg_var.loc[bkg_masks[-1]], \n",
    "                    weight=weights['bkg'][bkg_masks[-1]] if weights['bkg'] is not None else np.ones(np.sum(bkg_masks[-1]))\n",
    "                ))\n",
    "                labels.extend([f'HH signal, ttH-score > {cut}', f'ttH background, ttH-score > {cut}'])\n",
    "            make_input_plot(\n",
    "                destdir, var_name, hists, plot_prefix=plot_prefix, \n",
    "                plot_postfix=plot_postfix+f'_ttHscore_scan', labels=labels, density=False if weights['sig'] is not None else True\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used method 'std'. You used {method}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha) (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706619781071/work/torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopped.\n",
      "Best val acc: 77.397942\n",
      "----------\n",
      "Early stopped.\n",
      "Best val acc: 80.067062\n",
      "----------\n",
      "Early stopped.\n",
      "Best val acc: 75.281052\n",
      "----------\n",
      "Early stopped.\n",
      "Best val acc: 76.687927\n",
      "----------\n",
      "Early stopped.\n",
      "Best val acc: 77.194138\n",
      "----------\n",
      "Average best_acc across k-fold: 77.32562255859375\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "if CRITERION == \"NLLLoss\":\n",
    "    train_weights = torch.FloatTensor(\n",
    "        [1.0, np.sum(data_aux.loc[label==0,'eventWeight']) / np.sum(data_aux.loc[label==1,'eventWeight'])]\n",
    "    ).cuda()\n",
    "    criterion = nn.NLLLoss(weight=train_weights)\n",
    "elif CRITERION == \"BCELoss\":\n",
    "    train_weights = torch.FloatTensor(data_aux.loc[:, \"eventWeight\"]).cuda()\n",
    "    criterion = nn.BCELoss(weight=train_weights)\n",
    "else:\n",
    "    raise Exception(f\"CRITERION must be either 'NLLLoss' or 'BCELoss'. You provided {CRITERION}.\")\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json'\n",
    "    best_conf = optimize_hyperparams(\n",
    "        skf, data_list, data_hlf, label, \n",
    "        config_file, epochs=10,\n",
    "        criterion=criterion\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "    # with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "        best_conf = json.load(f)\n",
    "        print(best_conf)\n",
    "\n",
    "fom = []\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    model_file = OUTPUT_DIRPATH + CURRENT_TIME +'_ReallyTopclassStyle_'+ f'{fold_idx}.torch'\n",
    "    state_file = OUTPUT_DIRPATH + CURRENT_TIME +'_BestPerfReallyTopclass_'+ f'{fold_idx}.torch'\n",
    "\n",
    "    if CRITERION == \"NLLLoss\":\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        sig_train_mask = rectified_train_index & (label == 1)\n",
    "        bkg_train_mask = rectified_train_index & (label == 0)\n",
    "        train_weights = torch.FloatTensor(\n",
    "            [1.0, np.sum(data_aux.loc[bkg_train_mask,'eventWeight']) / np.sum(data_aux.loc[sig_train_mask,'eventWeight'])]\n",
    "        ).cuda()\n",
    "        criterion = nn.NLLLoss(weight=train_weights)\n",
    "    elif CRITERION == \"BCELoss\":\n",
    "        train_weights = torch.FloatTensor((data_aux.iloc[train_index]).loc[:, \"eventWeight\"]).cuda()\n",
    "        criterion = nn.BCELoss(weight=train_weights)\n",
    "        \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], \n",
    "        dnn_input=np.shape(data_hlf)[-1], rnn_input=np.shape(data_list)[-1]\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(data_list[val_index], data_hlf[val_index], label[val_index]), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        NUM_EPOCHS, model, criterion, optimizer, scheduler, \n",
    "        state_file, model_file, data_loader=data_loader\n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        -999.0\n",
      "1        -999.0\n",
      "2        -999.0\n",
      "3        -999.0\n",
      "4        -999.0\n",
      "          ...  \n",
      "164363   -999.0\n",
      "164364   -999.0\n",
      "164365   -999.0\n",
      "164366   -999.0\n",
      "164367   -999.0\n",
      "Name: lepton1_pt, Length: 164368, dtype: float64\n",
      "[[[ 0.84494881 -1.16565029 -0.68858366  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.48266669  0.          2.02001953  0.          0.\n",
      "    1.        ]\n",
      "  [ 0.          0.          0.          1.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.11602272 -0.31216671 -3.09989393  0.          1.\n",
      "    0.        ]\n",
      "  [-0.06000286  0.         -2.47167969  0.          0.\n",
      "    1.        ]\n",
      "  [ 0.          0.          0.          1.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[-0.42883682  0.          0.73364258  0.          0.\n",
      "    1.        ]\n",
      "  [-0.73420357 -0.64723517  0.72110644  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.          1.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.91901461  0.13413224 -1.67067568  0.          1.\n",
      "    0.        ]\n",
      "  [-1.70953849  0.          2.24023438  0.          0.\n",
      "    1.        ]\n",
      "  [ 0.          0.          0.          1.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 1.14614884 -1.18062482  2.7229632   0.          1.\n",
      "    0.        ]\n",
      "  [-1.12277728  0.         -0.43127441  0.          0.\n",
      "    1.        ]\n",
      "  [ 0.          0.          0.          1.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.88133699  0.         -1.02514648  0.          0.\n",
      "    1.        ]\n",
      "  [-0.30801795  1.351733    0.61084733  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.          1.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.83133991  0.22068825  0.98841362  0.          1.\n",
      "    0.        ]\n",
      "  [-1.82017311  0.         -1.44946289  0.          0.\n",
      "    1.        ]\n",
      "  [ 0.          0.          0.          1.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[-0.15354807  0.          0.46417236  0.          0.\n",
      "    1.        ]\n",
      "  [-0.55471501 -0.35314096 -2.16128492  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.          1.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[ 0.15421382 -0.62020044  0.85681451  0.          1.\n",
      "    0.        ]\n",
      "  [-0.43420799  0.         -2.60986328  0.          0.\n",
      "    1.        ]\n",
      "  [ 0.          0.          0.          1.          0.\n",
      "    0.        ]]\n",
      "\n",
      " [[-0.00703994  0.         -0.61437988  0.          0.\n",
      "    1.        ]\n",
      "  [-1.38743046 -1.74468688 -2.43662823  0.          1.\n",
      "    0.        ]\n",
      "  [ 0.          0.          0.          1.          0.\n",
      "    0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(data_test_df['lepton1_pt'])\n",
    "print(data_list_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "[1. 1. 0. ... 1. 1. 0.]\n",
      "[0.9970413  0.50865428 0.58346946 ... 0.89538741 0.96870785 0.43477409]\n",
      "------------------------------------------------------------\n",
      "[1. 1. 0. ... 1. 1. 0.]\n",
      "[0.99695516 0.57272134 0.72251761 ... 0.87728638 0.96046223 0.45102384]\n",
      "------------------------------------------------------------\n",
      "[1. 1. 0. ... 1. 1. 0.]\n",
      "[0.99768235 0.66128387 0.7471156  ... 0.91467033 0.97058501 0.38047227]\n",
      "------------------------------------------------------------\n",
      "[1. 1. 0. ... 1. 1. 0.]\n",
      "[0.99807004 0.66378814 0.66590999 ... 0.93583847 0.97769712 0.37436685]\n",
      "------------------------------------------------------------\n",
      "[1. 1. 0. ... 1. 1. 0.]\n",
      "[0.99754331 0.59889301 0.86107738 ... 0.89176161 0.97538335 0.55379372]\n",
      "[[           inf 9.99950224e-01 9.99942526e-01 ... 3.85228293e-02\n",
      "  2.20779765e-02 7.98942379e-05]\n",
      " [           inf 9.99950079e-01 9.99939614e-01 ... 3.59063739e-02\n",
      "  2.21875446e-02 2.09104226e-05]\n",
      " [           inf 9.99955246e-01 9.99948323e-01 ... 3.95432171e-02\n",
      "  2.40019637e-02 9.12603587e-05]\n",
      " [           inf 9.99964841e-01 9.99960466e-01 ... 2.26190191e-02\n",
      "  1.33562218e-02 9.37109263e-06]\n",
      " [           inf 9.99957142e-01 9.99944950e-01 ... 2.75132871e-02\n",
      "  1.78648113e-02 3.24313268e-05]]\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, \n",
    "        # save=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, \n",
    "        # save=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 258924, 2)\n",
      "(258924,)\n",
      "0 leptons avg. predictions: [0.99745835 0.59817264 0.71013745 ... 0.66452211 0.90276211 0.97054857]\n",
      "0 leptons mean avg. prediction: 0.6799998078845455\n",
      "------------------------------------------------------------\n",
      "1 leptons avg. predictions: [0.08739618 0.56522411 0.00898111 ... 0.16889955 0.16554569 0.4343965 ]\n",
      "1 leptons mean avg. prediction: 0.37033628781155176\n",
      "------------------------------------------------------------\n",
      "2+ leptons avg. predictions: [0.0070591  0.0065739  0.01877095 ... 0.05114208 0.00990615 0.00498561]\n",
      "2+ leptons mean avg. prediction: 0.16121522429499868\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(IN_perf['all_preds']))\n",
    "print(np.shape(data_test_df['lepton1_pt'] == -999))\n",
    "\n",
    "slice_arr_0leps = np.array(data_test_df['lepton1_pt'] == -999, dtype=int)\n",
    "preds_0leps = np.where(slice_arr_0leps == 1, np.exp(IN_perf['mean_pred'])[:,1], -999)\n",
    "preds_0leps = preds_0leps[preds_0leps != -999]\n",
    "print(f\"0 leptons avg. predictions: {preds_0leps}\")\n",
    "print(f\"0 leptons mean avg. prediction: {np.mean(preds_0leps)}\")\n",
    "print('-'*60)\n",
    "\n",
    "slice_arr_1leps = np.logical_and(\n",
    "    np.array(data_test_df['lepton1_pt'] != -999, dtype=int),\n",
    "    np.array(data_test_df['lepton2_pt'] == -999, dtype=int)\n",
    ")\n",
    "preds_1leps = np.where(slice_arr_1leps == 1, np.exp(IN_perf['mean_pred'])[:,1], -999)\n",
    "preds_1leps = preds_1leps[preds_1leps != -999]\n",
    "preds_0leps = preds_0leps[preds_0leps != -999]\n",
    "print(f\"1 leptons avg. predictions: {preds_1leps}\")\n",
    "print(f\"1 leptons mean avg. prediction: {np.mean(preds_1leps)}\")\n",
    "print('-'*60)\n",
    "\n",
    "slice_arr_2plusleps = np.logical_and(\n",
    "    np.array(data_test_df['lepton1_pt'] != -999, dtype=int),\n",
    "    np.array(data_test_df['lepton2_pt'] != -999, dtype=int)\n",
    ")\n",
    "preds_2plusleps = np.where(slice_arr_2plusleps == 1, np.exp(IN_perf['mean_pred'])[:,1], -999)\n",
    "preds_2plusleps = preds_2plusleps[preds_2plusleps != -999]\n",
    "print(f\"2+ leptons avg. predictions: {preds_2plusleps}\")\n",
    "print(f\"2+ leptons mean avg. prediction: {np.mean(preds_2plusleps)}\")\n",
    "print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|    inf    |       0.9706      |    0.8698 +/- 0.1732     |\n",
      "|    inf    |       0.9500      |    0.8375 +/- 0.1971     |\n",
      "|    inf    |       0.9198      |    0.7901 +/- 0.2322     |\n",
      "|    inf    |       0.7538      |    0.6366 +/- 0.2121     |\n",
      "|    inf    |       0.5777      |    0.4879 +/- 0.1625     |\n",
      "|    inf    |       0.3839      |    0.3242 +/- 0.1080     |\n",
      "+-----------+-------------------+--------------------------+\n",
      "num bkg: 93800\n",
      "num sig: 70568\n",
      "output score > 0.00\n",
      "============================================================\n",
      "num sig > 0.00 = 70568\n",
      "num bkg > 0.00 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.10\n",
      "============================================================\n",
      "num sig > 0.10 = 70568\n",
      "num bkg > 0.10 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.20\n",
      "============================================================\n",
      "num sig > 0.20 = 70568\n",
      "num bkg > 0.20 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.30\n",
      "============================================================\n",
      "num sig > 0.30 = 70568\n",
      "num bkg > 0.30 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.40\n",
      "============================================================\n",
      "num sig > 0.40 = 70568\n",
      "num bkg > 0.40 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.50\n",
      "============================================================\n",
      "num sig > 0.50 = 70568\n",
      "num bkg > 0.50 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.60\n",
      "============================================================\n",
      "num sig > 0.60 = 70568\n",
      "num bkg > 0.60 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.70\n",
      "============================================================\n",
      "num sig > 0.70 = 70568\n",
      "num bkg > 0.70 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.80\n",
      "============================================================\n",
      "num sig > 0.80 = 70568\n",
      "num bkg > 0.80 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.90\n",
      "============================================================\n",
      "num sig > 0.90 = 70568\n",
      "num bkg > 0.90 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.00\n",
      "============================================================\n",
      "num sig > 0.00 = 70568\n",
      "num bkg > 0.00 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.10\n",
      "============================================================\n",
      "num sig > 0.10 = 70568\n",
      "num bkg > 0.10 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.20\n",
      "============================================================\n",
      "num sig > 0.20 = 70568\n",
      "num bkg > 0.20 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.30\n",
      "============================================================\n",
      "num sig > 0.30 = 70568\n",
      "num bkg > 0.30 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.40\n",
      "============================================================\n",
      "num sig > 0.40 = 70568\n",
      "num bkg > 0.40 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.50\n",
      "============================================================\n",
      "num sig > 0.50 = 70568\n",
      "num bkg > 0.50 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.60\n",
      "============================================================\n",
      "num sig > 0.60 = 70568\n",
      "num bkg > 0.60 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.70\n",
      "============================================================\n",
      "num sig > 0.70 = 70568\n",
      "num bkg > 0.70 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.80\n",
      "============================================================\n",
      "num sig > 0.80 = 70568\n",
      "num bkg > 0.80 = 93800\n",
      "------------------------------------------------------------\n",
      "output score > 0.90\n",
      "============================================================\n",
      "num sig > 0.90 = 70568\n",
      "num bkg > 0.90 = 93800\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_629083/2758394570.py:271: RuntimeWarning: invalid value encountered in divide\n",
      "  s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m\n\u001b[1;32m     33\u001b[0m plot_output_score(\n\u001b[1;32m     34\u001b[0m     IN_perf, plot_destdir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mCURRENT_TIME, plot_postfix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_test_data_weighted\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     35\u001b[0m     labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(IN_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_preds\u001b[39m\u001b[38;5;124m'\u001b[39m]))], weights\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# all_bkg=True\u001b[39;00m\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     40\u001b[0m plot_output_score(\n\u001b[1;32m     41\u001b[0m     IN_perf, plot_destdir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mCURRENT_TIME, plot_postfix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_test_data_density\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     42\u001b[0m     labels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(IN_perf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_preds\u001b[39m\u001b[38;5;124m'\u001b[39m]))], n_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, \n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# all_bkg=True\u001b[39;00m\n\u001b[1;32m     44\u001b[0m )\n\u001b[0;32m---> 45\u001b[0m \u001b[43ms_over_root_b\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mIN_perf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_destdir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mCURRENT_TIME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_postfix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_test_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIN_perf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall_preds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbkg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test_aux\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_test\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meventWeight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test_aux\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_test\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meventWeight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\n\u001b[1;32m     50\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m score_cut \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.8\u001b[39m]:\n\u001b[1;32m     53\u001b[0m     plot_input_vars_after_score_cut(\n\u001b[1;32m     54\u001b[0m         IN_perf, score_cut, plot_destdir, plot_prefix\u001b[38;5;241m=\u001b[39mCURRENT_TIME, plot_postfix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_test_data\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;66;03m# weights={\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m#     'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[4], line 258\u001b[0m, in \u001b[0;36ms_over_root_b\u001b[0;34m(IN_info, plot_prefix, plot_postfix, method, labels, weights, lines_fold, lines, lines_labels, lines_colors, only_fold, no_fold, n_bins)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (only_fold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m fold_idx \u001b[38;5;241m!=\u001b[39m only_fold) \u001b[38;5;129;01mor\u001b[39;00m no_fold \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m sig_np \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mIN_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall_preds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    261\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(IN_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_labels\u001b[39m\u001b[38;5;124m'\u001b[39m][fold_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    262\u001b[0m ]\n\u001b[1;32m    263\u001b[0m bkg_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\n\u001b[1;32m    264\u001b[0m     IN_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_preds\u001b[39m\u001b[38;5;124m'\u001b[39m][fold_idx]\n\u001b[1;32m    265\u001b[0m )[\n\u001b[1;32m    266\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(IN_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_labels\u001b[39m\u001b[38;5;124m'\u001b[39m][fold_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    267\u001b[0m ]\n\u001b[1;32m    268\u001b[0m hist_axis \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39maxis\u001b[38;5;241m.\u001b[39mRegular(n_bins, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1.\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar\u001b[39m\u001b[38;5;124m'\u001b[39m, growth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, underflow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overflow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "    NNtable.add_row(\n",
    "        [\n",
    "            IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "            \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "        ]\n",
    "    )\n",
    "print(NNtable)\n",
    "\n",
    "# plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples/' + CURRENT_TIME\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots_0lepton/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plot_train_val_losses(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data',\n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    ")\n",
    "plot_roc(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='arr',\n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], run2=False\n",
    ")\n",
    "print(f\"num bkg: {np.sum(label_test==0)}\")\n",
    "print(f\"num sig: {np.sum(label_test==1)}\")\n",
    "plot_output_score(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }, n_bins=25, \n",
    "    # all_bkg=True\n",
    ")\n",
    "plot_output_score(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], n_bins=25, \n",
    "    # all_bkg=True\n",
    ")\n",
    "s_over_root_b(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }, n_bins=25\n",
    ")\n",
    "\n",
    "for score_cut in [0.2, 0.5, 0.6, 0.7, 0.8]:\n",
    "    plot_input_vars_after_score_cut(\n",
    "        IN_perf, score_cut, plot_destdir, plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "        # weights={\n",
    "        #     'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "        # }\n",
    "    )\n",
    "plot_input_vars_after_score_cut(\n",
    "    IN_perf, [0.2, 0.5, 0.6, 0.7, 0.8], plot_destdir, method='arr', plot_prefix=CURRENT_TIME, plot_postfix='_test_data',\n",
    "    # weights={\n",
    "    #     'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimized cut-boundaries for ttH score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(IN_perf, weights, bins=50):\n",
    "    hist_list_fold = []\n",
    "    cut_boundaries_fold = []\n",
    "    cut_s_over_root_bs_fold = []\n",
    "    sig_weights_fold = []\n",
    "    bkg_weights_fold = []\n",
    "    for fold_idx in range(skf.get_n_splits()):\n",
    "        sig_np = np.exp(\n",
    "            IN_perf['all_preds'][fold_idx]\n",
    "        )[\n",
    "            np.array(IN_perf['all_labels'][fold_idx]) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_perf['all_preds'][fold_idx]\n",
    "        )[\n",
    "            np.array(IN_perf['all_labels'][fold_idx]) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "        fold_idx_cuts_bins_inclusive = []\n",
    "        fold_idx_sig_weights = []\n",
    "        fold_idx_bkg_weights = []\n",
    "        fold_idx_prev_s_over_root_b = []\n",
    "        prev_s_over_root_b = 0\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                fold_idx_sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        fold_idx_sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_cuts_bins_inclusive.append(0)\n",
    "        fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "        fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "        cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "        cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "        sig_weights_fold.append(fold_idx_sig_weights)\n",
    "        bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "                \n",
    "    sig_np = np.exp(\n",
    "        IN_perf['mean_pred']\n",
    "    )[\n",
    "        np.array(IN_perf['mean_label']) == 1,1\n",
    "    ]\n",
    "    bkg_np = np.exp(\n",
    "        IN_perf['mean_pred']\n",
    "    )[\n",
    "        np.array(IN_perf['mean_label']) == 0,1\n",
    "    ]\n",
    "    hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "    sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "    bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "\n",
    "    cut_boundaries = []\n",
    "    cut_s_over_root_bs = []\n",
    "    prev_s_over_root_b = 0\n",
    "    sig_weights = []\n",
    "    bkg_weights = []\n",
    "    for i in range(bins):\n",
    "        s = np.sum(sig_hist.values().flatten()[\n",
    "            (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "        ])\n",
    "        sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "            (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "        ]))\n",
    "        if prev_s_over_root_b < (s / sqrt_b):\n",
    "            prev_s_over_root_b = s / sqrt_b\n",
    "            continue\n",
    "        else:\n",
    "            sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            cut_boundaries.append(bins - i)\n",
    "            cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "            prev_s_over_root_b = 0\n",
    "    sig_weights.append(\n",
    "        {\n",
    "            'value': np.sum(sig_hist.values().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]),\n",
    "            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])),\n",
    "        }\n",
    "    )\n",
    "    bkg_weights.append(\n",
    "        {\n",
    "            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]),\n",
    "            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])),\n",
    "        }\n",
    "    )\n",
    "    cut_boundaries.append(0)\n",
    "    cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "    cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "    return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold, cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "(\n",
    "    cut_boundaries_fold, cut_s_over_root_bs_fold, \n",
    "    sig_weights_fold, bkg_weights_fold, \n",
    "    cut_boundaries, cut_s_over_root_bs, \n",
    "    sig_weights, bkg_weights \n",
    ") = optimize_cut_boundaries(\n",
    "    IN_perf, {\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "fold_labels = [\n",
    "    [\n",
    "        f\"s/√b={cut_s_over_root_bs_fold[fold_idx][cut_idx]:.04f}, s={sig_weights_fold[fold_idx][cut_idx]['value']:.04f}±{sig_weights_fold[fold_idx][cut_idx]['w2']:.04f}, b={bkg_weights_fold[fold_idx][cut_idx]['value']:.04f}±{bkg_weights_fold[fold_idx][cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[fold_idx]))\n",
    "    ] for fold_idx in range(skf.get_n_splits())\n",
    "]\n",
    "fold_colors = [copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries_fold[i]) // len(cmap_petroff10)) + 1)) for i in range(skf.get_n_splits())]\n",
    "for fold_idx in range(skf.get_n_splits()):\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_fold{fold_idx}', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "            'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "        }, lines_fold=cut_boundaries_fold, lines_labels=fold_labels, only_fold=fold_idx, lines_colors=fold_colors\n",
    "    )\n",
    "labels = [\n",
    "    f\"s/√b={cut_s_over_root_bs[cut_idx]:.04f}, s={sig_weights[cut_idx]['value']:.04f}±{sig_weights[cut_idx]['w2']:.04f}, b={bkg_weights[cut_idx]['value']:.04f}±{bkg_weights[cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs))\n",
    "]\n",
    "colors = copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries) // len(cmap_petroff10)) + 1))\n",
    "s_over_root_b(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_foldAvg', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }, lines=cut_boundaries, lines_labels=labels, no_fold=True, lines_colors=colors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Val Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            data_list[train_index], data_hlf[train_index], label[train_index], \n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            data_list[val_index], data_hlf[val_index], label[val_index], \n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'r') as f:\n",
    "    IN_perf_dict = json.load(f)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold ']\n",
    "val_weights_arr = []\n",
    "for fold_idx, (train_IN_dict, val_IN_dict, (train_index, val_index)) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'], skf.split(data_hlf, label))):\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )\n",
    "    rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "    rectified_train_index[val_index] = False\n",
    "    sig_train_mask = rectified_train_index & (label == 1)\n",
    "    sig_val_mask = np.logical_not(rectified_train_index) & (label == 1)\n",
    "    bkg_train_mask = rectified_train_index & (label == 0)\n",
    "    bkg_val_mask = np.logical_not(rectified_train_index) & (label == 0)\n",
    "    weights = [\n",
    "        {'sig': data_aux.loc[sig_train_mask, \"eventWeight\"], 'bkg': data_aux.loc[bkg_train_mask, \"eventWeight\"]},\n",
    "        {'sig': data_aux.loc[sig_val_mask, \"eventWeight\"], 'bkg': data_aux.loc[bkg_val_mask, \"eventWeight\"]}\n",
    "    ]\n",
    "    val_weights_arr.append(copy.deepcopy(weights[1]))\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_weighted_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_density_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=[{'sig': None, 'bkg': None}]*len(labels_arr)\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights\n",
    "    )\n",
    "labels_arr = ['val - fold 0', 'val - fold 1', 'val - fold 2', 'val - fold 3', 'val - fold 4']\n",
    "s_over_root_b(\n",
    "    IN_perf_dict['val'], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_val_comparison', \n",
    "    method='IN_arr', labels=labels_arr, weights=val_weights_arr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Vars Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pre-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    sig_mask = (label == 1)\n",
    "    sig_test_mask = (label_test == 1)\n",
    "    bkg_mask = (label == 0)\n",
    "    bkg_test_mask = (label_test == 0)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        \n",
    "        sig_train_mask = rectified_train_index & sig_mask\n",
    "        sig_val_mask = np.logical_not(rectified_train_index) & sig_mask\n",
    "        bkg_train_mask = rectified_train_index & bkg_mask\n",
    "        bkg_val_mask = np.logical_not(rectified_train_index) & bkg_mask\n",
    "\n",
    "        sig_train_np = data_df.loc[sig_train_mask, var_name].to_numpy()\n",
    "        sig_val_np = data_df.loc[sig_val_mask, var_name].to_numpy()\n",
    "        sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "        bkg_train_np = data_df.loc[bkg_train_mask, var_name].to_numpy()\n",
    "        bkg_val_np = data_df.loc[bkg_val_mask, var_name].to_numpy()\n",
    "        bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np = data_df.loc[sig_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_df.loc[bkg_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "    sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    pre_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_pre_std, var_name, pre_std_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### post-standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = data_list, data_list_test\n",
    "    else:\n",
    "        data, data_test = data_hlf, data_hlf_test\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name,\n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    post_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_post_std, var_name, post_std_hists[var_name], labels=label_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian smearing on test set (for feature importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    mask_arr = data_list_index_map(var_name, particle_list_to_smear, np.ones(len(particle_list_to_smear), dtype=bool))\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear[mask_arr] *= rng.normal(size=len(particle_list_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear[mask_arr] += rng.normal(size=len(particle_list_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns[var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate smeared variable test-data on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "    gauss_data_list, gauss_data_hlf = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        gauss_data_list = smear_particle_list(var_name, copy.deepcopy(data_list_test))\n",
    "        gauss_data_hlf = data_hlf_test\n",
    "    else:\n",
    "        gauss_data_list = data_list_test\n",
    "        gauss_data_hlf = smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test))\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list, gauss_data_hlf, label_test, OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "IN_perf_smear_list = []\n",
    "label_arr = []\n",
    "for var_name, IN_perf_smear in IN_perf_smear_dict.items():\n",
    "    IN_perf_smear_list.append(IN_perf_smear)\n",
    "    label_arr.append(var_name)\n",
    "sort = np.argsort([IN_perf_smear['mean_area'] for IN_perf_smear in IN_perf_smear_list])\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_all', \n",
    "    method='IN_arr', labels=label_arr, yscale='log', run2=False, sort=sort\n",
    ")\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5', \n",
    "    method='IN_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False\n",
    ")\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "plot_roc(\n",
    "    [list(IN_perf_smear_dict.values())[i] for i in sort[:5]], plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear_top5_and_orig', \n",
    "    method='IN_arr', labels=[label_arr[i] for i in sort[:5]], yscale='log', run2=False, run3=IN_perf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = smear_particle_list(var_name, data_list), smear_particle_list(var_name, data_list_test)\n",
    "    else:\n",
    "        data, data_test = smear_particle_hlf(var_name, data_hlf), smear_particle_hlf(var_name, data_hlf_test)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name, \n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_gauss_smear, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    gauss_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_gauss_smear, var_name, gauss_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y in [('train', data_list, data_hlf, label), ('test', data_list_test, data_hlf_test, label_test)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'r') as f:\n",
    "    IN_full_eval_dict = json.load(f)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {'mass': [], 'dijet_mass': []}\n",
    "for var_name in hist_dict.keys():\n",
    "    for i, score_cut in enumerate(score_cuts):\n",
    "        sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict)\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "        hist_dict[var_name].extend(\n",
    "            [\n",
    "                copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "            ]\n",
    "        )\n",
    "    for mod_factor, label_mod in enumerate(['sig_train', 'sig_test', 'bkg_train', 'bkg_test']):\n",
    "        plot_list = []\n",
    "        label_list = []\n",
    "        for i in range(len(hist_dict[var_name])):\n",
    "            if (i - mod_factor) % 4 == 0:\n",
    "                plot_list.append(hist_dict[var_name][i])\n",
    "                label_list.append(label_arr[i])\n",
    "        make_input_plot(\n",
    "            plot_destdir, var_name, plot_list, labels=label_list, density=True, \n",
    "            plot_prefix=CURRENT_TIME+'_', plot_postfix='_'+label_mod, alpha=0.5,\n",
    "            linestyle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train dataset size plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_dirs = glob.glob(OUTPUT_DIRPATH[:-1]+'_mod*') + [OUTPUT_DIRPATH[:-1]]\n",
    "final_train_losses_arr, final_val_losses_arr = [], []\n",
    "mod_values_arr = []\n",
    "\n",
    "for train_size_dir in train_size_dirs:\n",
    "    if len(glob.glob(train_size_dir + '/*IN_perf.json')) == 0:\n",
    "        continue\n",
    "    try:\n",
    "        mod_values_arr.append([\n",
    "            float(\n",
    "                train_size_dir[\n",
    "                    train_size_dir.find('_mod')+4 : train_size_dir.find('-')\n",
    "                ]\n",
    "            ),\n",
    "            float(\n",
    "                train_size_dir[train_size_dir.find('-')+1:]\n",
    "            )\n",
    "        ])\n",
    "    except:\n",
    "         mod_values_arr.append([2, 2])\n",
    "    IN_perf_path = glob.glob(f'{train_size_dir}/*IN_perf.json')[0]\n",
    "    with open(IN_perf_path, 'r') as f:\n",
    "        IN_perf = json.load(f)\n",
    "    final_train_losses_arr.append([train_losses[-7 if len(train_losses) < NUM_EPOCHS else -1] for train_losses in IN_perf['train_losses_arr']])\n",
    "    final_val_losses_arr.append([val_losses[-7 if len(val_losses) < NUM_EPOCHS else -1] for val_losses in IN_perf['val_losses_arr']])\n",
    "\n",
    "final_train_losses_arr = np.array(final_train_losses_arr)\n",
    "final_val_losses_arr = np.array(final_val_losses_arr)\n",
    "mod_values_arr = np.array(mod_values_arr)\n",
    "dataset_sizes = (len(label) + len(label_test)) / mod_values_arr\n",
    "sorted_indices = np.argsort(dataset_sizes[:, 0])\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots/' + CURRENT_TIME\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_train_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_train_losses_arr, axis=1)-np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_train_losses_arr, axis=1)+np.std(final_train_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[0], alpha=0.5, label='Train data'\n",
    ")\n",
    "plt.plot(\n",
    "    dataset_sizes[:, 0][sorted_indices], np.mean(final_val_losses_arr, axis=1)[sorted_indices], color='k'\n",
    ")\n",
    "plt.fill_between(\n",
    "    dataset_sizes[:, 0][sorted_indices], (np.mean(final_val_losses_arr, axis=1)-np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    (np.mean(final_val_losses_arr, axis=1)+np.std(final_val_losses_arr, axis=1))[sorted_indices], \n",
    "    color=cmap_petroff10[1], alpha=0.5, label='Val data'\n",
    ")\n",
    "plt.xlabel('Size of train dataset')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.pdf')\n",
    "plt.savefig(plot_destdir + '/train_val_comparison_varying_trainset_size.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna-hhbbgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
