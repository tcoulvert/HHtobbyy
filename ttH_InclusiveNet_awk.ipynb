{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu1.fnal.gov      Fri Jul 26 09:42:14 2024  555.42.02\n",
      "[0] Tesla P100-PCIE-12GB | 45Â°C,  12 % | 11762 / 12288 MB | lbrennan(256M) lbrennan(324M) lbrennan(256M) lbrennan(324M) lbrennan(324M) lbrennan(324M) lbrennan(324M) lbrennan(324M) lbrennan(324M) cwfair(8978M)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# import ROOT as rt\n",
    "# from root_numpy import root2array, tree2array\n",
    "\n",
    "import awkward as ak\n",
    "import h5py\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions as nlr\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import json\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import gpustat\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "SIGNAL_FILEPATHS = [lpc_fileprefix+\"/Run3_2022preEE_merged/GluGluToHH/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/GluGluToHH/nominal/*\"]\n",
    "SIGNAL_FILEPATHS = [\n",
    "    lpc_fileprefix+\"/Run3_2022preEE_merged/GluGluToHH/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/GluGluToHH/nominal/*\",\n",
    "    lpc_fileprefix+\"/Run3_2022preEE_merged/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", \n",
    "    # lpc_fileprefix+\"/Run3_2022preEE_merged/ZHH_HHto2B2G_CV-1p0_C2V-1p0_C3-1p0_TuneCP5_13p6TeV/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/ZHH_HHto2B2G_CV-1p0_C2V-1p0_C3-1p0_TuneCP5_13p6TeV/nominal/*\",\n",
    "    # lpc_fileprefix+\"/Run3_2022preEE_merged/WHH_HHto2B2G_CV-1p0_C2V-1p0_C3-1p0_TuneCP5_13p6TeV/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/WHH_HHto2B2G_CV-1p0_C2V-1p0_C3-1p0_TuneCP5_13p6TeV/nominal/*\"\n",
    "]\n",
    "BKG_FILEPATHS = [lpc_fileprefix+\"/Run3_2022preEE_merged/ttHToGG/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/ttHToGG/nominal/*\"]\n",
    "\n",
    "\n",
    "# SIGNAL_FILEPATHS = [\"/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_parquet/v1/GluGluToHH/nominal/*\", \"/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_parquet/v1_preEE/GluGluToHH/nominal/*\"]\n",
    "# BKG_FILEPATHS = [\"/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_parquet/v1/ttHToGG/nominal/*\", \"/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_parquet/v1_preEE/ttHToGG/nominal/*\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMSGrad(optim.Optimizer):\n",
    "    \"\"\"Implements AMSGrad algorithm.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(AMSGrad, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AMSGrad, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', True)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "destdir = 'v1_merged_plots'\n",
    "sig_samples_list = [ak.from_parquet(glob.glob(dir_path)) for dir_path in SIGNAL_FILEPATHS]\n",
    "sig_samples_pq = ak.concatenate(sig_samples_list)\n",
    "bkg_samples_list = [ak.from_parquet(glob.glob(dir_path)) for dir_path in BKG_FILEPATHS]\n",
    "bkg_samples_pq = ak.concatenate(bkg_samples_list)\n",
    "samples = {\n",
    "    'sig': sig_samples_pq,\n",
    "    'bkg': bkg_samples_pq,\n",
    "}\n",
    "\n",
    "# print(sig_samples_pq.fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_samples = {}\n",
    "high_level_fields = {\n",
    "    'puppiMET_sumEt', 'puppiMET_pt', 'puppiMET_eta', 'puppiMET_phi', # MET variables\n",
    "    'DeltaPhi_j1MET', 'DeltaPhi_j2MET', # jet-MET variables\n",
    "    'DeltaR_jg_min', 'n_jets', 'chi_t0', 'chi_t1', # jet variables\n",
    "    'lepton1_pt' ,'lepton2_pt', 'pt', # lepton and diphoton pt\n",
    "    'lepton1_eta', 'lepton2_eta', 'eta', # lepton and diphoton eta\n",
    "    'lepton1_phi', 'lepton2_phi', 'phi', # lepton and diphoton phi\n",
    "    'abs_CosThetaStar_CS', 'abs_CosThetaStar_jj', # angular variables\n",
    "    'dijet_mass', # mass of b-dijet (resonance for H->bb)\n",
    "    'leadBjet_leadLepton', 'leadBjet_subleadLepton', # deltaR btwn bjets and leptons (b/c b often decays to muons)\n",
    "    'subleadBjet_leadLepton', 'subleadBjet_subleadLepton'\n",
    "}\n",
    "\n",
    "pandas_aux_samples = {}\n",
    "high_level_aux_fields = {\n",
    "    'mass', 'dijet_mass' # diphoton and bb-dijet mass\n",
    "} # https://stackoverflow.com/questions/67003141/how-to-remove-a-field-from-a-collection-of-records-created-by-awkward-zip\n",
    "\n",
    "for sample_name, sample in samples.items():\n",
    "    pandas_samples[sample_name] = {\n",
    "        field: ak.to_numpy(sample[field], allow_missing=False) for field in high_level_fields\n",
    "    }\n",
    "    pandas_aux_samples[sample_name] = {\n",
    "        field: ak.to_numpy(sample[field], allow_missing=False) for field in high_level_aux_fields\n",
    "    }\n",
    "\n",
    "# del samples\n",
    "\n",
    "# sig_frame = ak.to_dataframe(pandas_samples['sig'])\n",
    "# sig_aux_frame = ak.to_dataframe(pandas_aux_samples['sig'])\n",
    "# bkg_frame = ak.to_dataframe(pandas_samples['bkg'])\n",
    "# bkg_aux_frame = ak.to_dataframe(pandas_aux_samples['bkg'])\n",
    "sig_frame = pd.DataFrame(pandas_samples['sig'])\n",
    "sig_aux_frame = pd.DataFrame(pandas_aux_samples['sig'])\n",
    "bkg_frame = pd.DataFrame(pandas_samples['bkg'])\n",
    "bkg_aux_frame = pd.DataFrame(pandas_aux_samples['bkg'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sig_train_frame, sig_test_frame, bkg_train_frame, bkg_test_frame \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(\n\u001b[1;32m      2\u001b[0m     sig_frame, bkg_frame, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "sig_train_frame, sig_test_frame, bkg_train_frame, bkg_test_frame = train_test_split(\n",
    "    sig_frame, bkg_frame, test_size=0.25, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (742869, 4, 6)\n",
      "background number: (346502, 4, 6)\n",
      "Data list: (1089371, 4, 6)\n",
      "Data HLF: (1089371, 14)\n",
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "# Because of zero-padding, standardization needs special treatment\n",
    "# Masked out zero\n",
    "# zero_entries = bkg_frame == 0 \n",
    "zero_entries = (bkg_train_frame == -999) # now pad with -999 instead of 0\n",
    "masked_x_sample = np.ma.array(bkg_train_frame, mask=zero_entries)\n",
    "x_mean = masked_x_sample.mean(axis=0)\n",
    "x_std = masked_x_sample.std(axis=0)\n",
    "print(\"Mean and std calculated.\")\n",
    "\n",
    "# Standardize background\n",
    "normed_bkg_train = (masked_x_sample - x_mean)/x_std\n",
    "normed_bkg_test = (np.ma.array(bkg_test_frame, mask=zero_entries) - x_mean)/x_std\n",
    "\n",
    "# Standardize signal\n",
    "# zero_entries = sig_frame == 0 \n",
    "zero_entries = (sig_frame == -999) # now pad with -999 instead of 0\n",
    "masked_x_sample = np.ma.array(sig_train_frame, mask=zero_entries)\n",
    "normed_sig_train = (masked_x_sample - x_mean)/x_std\n",
    "normed_sig_test = (np.ma.array(sig_test_frame, mask=zero_entries) - x_mean)/x_std\n",
    "\n",
    "normed_bkg_train_frame = pd.DataFrame(normed_bkg_train.filled(0), columns=list(bkg_train_frame))\n",
    "normed_bkg_train_frame.head()\n",
    "normed_bkg_test_frame = pd.DataFrame(normed_bkg_test.filled(0), columns=list(bkg_test_frame))\n",
    "normed_bkg_test_frame.head()\n",
    "\n",
    "normed_sig_train_frame = pd.DataFrame(normed_sig_train.filled(0), columns=list(sig_train_frame))\n",
    "normed_sig_train_frame.head()\n",
    "normed_sig_test_frame = pd.DataFrame(normed_sig_test.filled(0), columns=list(sig_test_frame))\n",
    "normed_sig_train_frame.head()\n",
    "\n",
    "\n",
    "\n",
    "def to_p_list(data_frame):\n",
    "    # Inputs: Pandas data frame\n",
    "    # Outputs: Numpy array of dimension (Event, Particle, Attributes)\n",
    "    \n",
    "    particle_list_sig = np.zeros(shape=(len(data_frame),4,6))\n",
    "    sorted_particle_list = np.zeros(shape=(len(data_frame),4,6))\n",
    "    # 4: max particles: l1, l2, dipho, MET\n",
    "    # 6: pt, eta, phi, isLep, isDipho, isMET\n",
    "   \n",
    "    for i in range(len(data_frame)): # loop through the list of events\n",
    "        ptl1 = data_frame['lepton1_pt'][i]\n",
    "        ptl2 = data_frame['lepton2_pt'][i]\n",
    "        ptdipho = data_frame['pt'][i]\n",
    "        ptMET = data_frame['puppiMET_pt'][i]\n",
    "\n",
    "        etal1 = data_frame['lepton1_eta'][i]\n",
    "        etal2 = data_frame['lepton2_eta'][i]\n",
    "        etadipho = data_frame['eta'][i]\n",
    "        etaMET = data_frame['puppiMET_eta'][i]\n",
    "\n",
    "        phil1 = data_frame['lepton1_phi'][i]\n",
    "        phil2 = data_frame['lepton2_phi'][i]\n",
    "        phidipho = data_frame['phi'][i]\n",
    "        phiMET = data_frame['puppiMET_phi'][i]\n",
    "\n",
    "        # list through list of particles: l1, l2, diphoton, MET\n",
    "        # 0: leading lep\n",
    "        particle_list_sig[i,0, 0] = ptl1\n",
    "        particle_list_sig[i,0, 1] = etal1\n",
    "        particle_list_sig[i,0, 2] = phil1\n",
    "        particle_list_sig[i,0, 3] = 1 if ptl1 != -999 else 0 # isLep\n",
    "        particle_list_sig[i,0, 4] = 0 # isDiPho\n",
    "        particle_list_sig[i,0, 5] = 0 # isMET\n",
    "\n",
    "        # 1: subleading lep\n",
    "        particle_list_sig[i,1, 0] = ptl2\n",
    "        particle_list_sig[i,1, 1] = etal2\n",
    "        particle_list_sig[i,1, 2] = phil2\n",
    "        particle_list_sig[i,1, 3] = 1 if ptl2 != -999 else 0 # isLep\n",
    "        particle_list_sig[i,1, 4] = 0 # isDiPho\n",
    "        particle_list_sig[i,1, 5] = 0 # isMET\n",
    "\n",
    "        # 2: dipho\n",
    "        particle_list_sig[i,2, 0] = ptdipho\n",
    "        particle_list_sig[i,2, 1] = etadipho\n",
    "        particle_list_sig[i,2, 2] = phidipho\n",
    "        particle_list_sig[i,2, 3] = 0 # isLep\n",
    "        particle_list_sig[i,2, 4] = 1 if ptdipho != -999 else 0 # isDiPho\n",
    "        particle_list_sig[i,2, 5] = 0 # isMET\n",
    "\n",
    "        # 3: MET\n",
    "        particle_list_sig[i,3, 0] = ptMET\n",
    "        particle_list_sig[i,3, 1] = etaMET\n",
    "        particle_list_sig[i,3, 2] = phiMET\n",
    "        particle_list_sig[i,3, 3] = 0 #isLep\n",
    "        particle_list_sig[i,3, 4] = 0 # isDiPho\n",
    "        particle_list_sig[i,3, 5] = 1 if ptMET != -999 else 0 # isMET\n",
    "    \n",
    "        # Sort by descending pT. \n",
    "        # This was implemented when standardization was done before sorting. Thus zero entry needs to be excluded\n",
    "        # Redesigned the code with standardization done after sorting. Same code still works.\n",
    "        nonzero_indices = np.nonzero(particle_list_sig[i,:,0])[0]\n",
    "        sorted_indices = particle_list_sig[i,nonzero_indices,0].argsort()[::-1] # sort by first column, which is the pT\n",
    "        global_sorted_indices = nonzero_indices[sorted_indices]\n",
    "        sorted_particle_list[i,:len(nonzero_indices),:] = particle_list_sig[i,global_sorted_indices,:]\n",
    "        \n",
    "    return sorted_particle_list\n",
    "\n",
    "sig_train_list = to_p_list(sig_train_frame)\n",
    "sig_test_list = to_p_list(sig_test_frame)\n",
    "bkg_train_list = to_p_list(bkg_train_frame)\n",
    "bkg_test_list = to_p_list(bkg_test_frame)\n",
    "\n",
    "# Standardize the particle list\n",
    "x_sample = bkg_train_list[:,:,:3] # don't standardize boolean flags\n",
    "# Flatten out\n",
    "x_flat = x_sample.reshape((x_sample.shape[0]*x_sample.shape[1], x_sample.shape[2]))\n",
    "# Masked out zero\n",
    "zero_entries = x_flat == 0 \n",
    "masked_x_sample = np.ma.array(x_flat, mask=zero_entries)\n",
    "x_list_mean = masked_x_sample.mean(axis=0)\n",
    "x_list_std = masked_x_sample.std(axis=0)\n",
    "print(\"Mean and std calculated for particle list.\")\n",
    "del x_sample, x_flat, zero_entries, masked_x_sample # release the memory\n",
    "\n",
    "def standardize_p_list(inputs):\n",
    "    global x_list_mean, x_list_std\n",
    "    to_norm = inputs[:,:,:3]\n",
    "    zero_entries = to_norm == 0\n",
    "    masked_to_norm = np.ma.array(to_norm, mask=zero_entries)\n",
    "    normed_x = (masked_to_norm - x_list_mean)/x_list_std\n",
    "    return np.concatenate((normed_x.filled(0), inputs[:,:,3:]), axis=2)\n",
    "\n",
    "##### CONTINUE HERE #####\n",
    "    \n",
    "normed_sig_list = standardize_p_list(sig_list)\n",
    "normed_bkg_list = standardize_p_list(bkg_list)\n",
    "\n",
    "input_hlf_vars = ['puppiMET_sumEt','DeltaPhi_j1MET','DeltaPhi_j2MET','DeltaR_jg_min','n_jets','chi_t0',\n",
    "                                   'chi_t1','abs_CosThetaStar_CS','abs_CosThetaStar_jj']\n",
    "input_hlf_vars = ['puppiMET_sumEt','DeltaPhi_j1MET','DeltaPhi_j2MET','DeltaR_jg_min','n_jets','chi_t0',\n",
    "                  'chi_t1','abs_CosThetaStar_CS','abs_CosThetaStar_jj','dijet_mass', 'leadBjet_leadLepton', \n",
    "                  'leadBjet_subleadLepton', 'subleadBjet_leadLepton', 'subleadBjet_subleadLepton']\n",
    "\n",
    "normed_sig_hlf = normed_sig_frame[input_hlf_vars].values\n",
    "\n",
    "normed_bkg_hlf = normed_bkg_frame[input_hlf_vars].values\n",
    "    \n",
    "# Shuffle before splitting into train-val\n",
    "randix = np.arange(len(normed_bkg_list))\n",
    "np.random.shuffle(randix)\n",
    "\n",
    "background_list = normed_bkg_list[randix]\n",
    "background_list = background_list[:len(normed_sig_list)] # downsampling\n",
    "print(f'signal number: {normed_sig_list.shape}')\n",
    "print(f'background number: {background_list.shape}')\n",
    "\n",
    "background_hlf = normed_bkg_hlf[randix]\n",
    "background_hlf = background_hlf[:len(normed_sig_hlf)]\n",
    "\n",
    "sig_label = np.ones(len(normed_sig_hlf))\n",
    "bkg_label = np.zeros(len(background_hlf))\n",
    "\n",
    "data_list_full = np.concatenate((normed_sig_list,background_list))\n",
    "data_hlf_full = np.concatenate((normed_sig_hlf,background_hlf))\n",
    "label_full = np.concatenate((sig_label,bkg_label))\n",
    "\n",
    "print(\"Data list: {}\".format(data_list_full.shape))\n",
    "print(\"Data HLF: {}\".format(data_hlf_full.shape))\n",
    "\n",
    "data_list, data_list_test, data_hlf, data_hlf_test, label, label_test = train_test_split(\n",
    "    data_list_full, data_hlf_full, label_full, test_size=0.25, random_state=None)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# skf.get_n_splits(data_hlf, label)\n",
    "print(skf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # inspo:\n",
    "# # #    https://github.com/Sara-mibo/LRP_EncoderDecoder_GRU/blob/main/LRP/utils_lrp.py\n",
    "# # #    https://github.com/pytorch/captum/blob/master/captum/attr/_utils/lrp_rules.py\n",
    "\n",
    "# print('='*50)\n",
    "# print('data list')\n",
    "# print('-'*50)\n",
    "# print('train/val')\n",
    "# print(data_list.shape)\n",
    "# print('test')\n",
    "# print(data_list_test.shape)\n",
    "# print('='*50)\n",
    "# print('-'*50)\n",
    "# print('data hlf')\n",
    "# print('train/val')\n",
    "# print(data_hlf.shape)\n",
    "# print('test')\n",
    "# print(data_hlf_test.shape)\n",
    "# print('='*50)\n",
    "# print('-'*50)\n",
    "# print('label')\n",
    "# print('-'*50)\n",
    "# print('train/val')\n",
    "# print(label.shape)\n",
    "# print(f'sig number: {label[np.where(label == 1)].shape}')\n",
    "# print(f'bkg number: {label[np.where(label == 0)].shape}')\n",
    "# print('test')\n",
    "# print(label_test.shape)\n",
    "# print(f'sig number: {label_test[np.where(label_test == 1)].shape}')\n",
    "# print(f'bkg number: {label_test[np.where(label_test == 0)].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=5\n",
    "model_file = 'ReallyTopclassStyle.torch'\n",
    "config_file = 'BestConfigReallyTopclass.json'\n",
    "retrain=True\n",
    "\n",
    "class ParticleHLF(Dataset):\n",
    "    def __init__(self, data_particles, data_hlf, data_y):\n",
    "        self.len = data_y.shape[0]\n",
    "        self.data_particles = torch.from_numpy(data_particles).float()\n",
    "        self.data_hlf = torch.from_numpy(data_hlf).float()\n",
    "        self.data_y = torch.from_numpy(data_y).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data_particles[idx], self.data_hlf[idx], self.data_y[idx])\n",
    "\n",
    "class InclusiveNetwork(nn.Module):\n",
    "    def __init__(self, num_hiddens=2, initial_node=500, dropout=0.5, gru_layers=2, gru_size=50, dropout_g=0.1, rnn_input=6, dnn_input=len(input_hlf_vars)):\n",
    "        super(InclusiveNetwork, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.dropout_g = dropout_g\n",
    "        self.hiddens = nn.ModuleList()\n",
    "        nodes = [initial_node]\n",
    "        for i in range(num_hiddens):\n",
    "            nodes.append(int(nodes[i]/2))\n",
    "            self.hiddens.append(nn.Linear(nodes[i],nodes[i+1]))\n",
    "        # self.gru = nn.GRU(input_size=7, hidden_size=gru_size, num_layers=gru_layers, batch_first=True, dropout=self.dropout_g)\n",
    "        self.gru = nn.GRU(input_size=rnn_input, hidden_size=gru_size, num_layers=gru_layers, batch_first=True, dropout=self.dropout_g)\n",
    "        self.merge = nn.Linear(dnn_input+gru_size,initial_node)\n",
    "        self.out = nn.Linear(nodes[-1],2)\n",
    "\n",
    "    def forward(self, particles, hlf):\n",
    "        _, hgru = self.gru(particles)\n",
    "        hgru = hgru[-1] # Get the last hidden layer\n",
    "        x = torch.cat((hlf,hgru), dim=1)\n",
    "        x = F.dropout(self.merge(x), training=self.training, p=self.dropout)\n",
    "        for i in range(len(self.hiddens)):\n",
    "            x = F.relu(self.hiddens[i](x))\n",
    "            x = F.dropout(x, training=self.training, p=self.dropout)\n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early Stopping to terminate training early under certain conditions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 monitor='val_loss',\n",
    "                 min_delta=0,\n",
    "                 patience=10):\n",
    "        self.monitor = monitor\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.stopped_epoch = 0\n",
    "        self.stop_training= False\n",
    "        #print(\"This is my patience {}\".format(patience))\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.wait = 0\n",
    "        self.best_loss = 1e15\n",
    "    \n",
    "    def on_epoch_end(self, epoch, current_loss):\n",
    "        if current_loss is None:\n",
    "            pass\n",
    "        else:\n",
    "            if (current_loss - self.best_loss) < -self.min_delta:\n",
    "                self.best_loss = current_loss\n",
    "                self.wait = 1\n",
    "            else:\n",
    "                if self.wait >= self.patience:\n",
    "                    self.stopped_epoch = epoch + 1\n",
    "                    self.stop_training = True\n",
    "                self.wait += 1\n",
    "            return  self.stop_training\n",
    "        \n",
    "    def on_train_end(self):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print('\\nTerminated training for early stopping at epoch %04i' % \n",
    "                (self.stopped_epoch))\n",
    "\n",
    "def train(num_epochs, model, criterion, optimizer,scheduler,volatile=False, data_loader=None):\n",
    "    best_model = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    train_losses ,val_losses = [],[]\n",
    "    callback = EarlyStopping(patience=10)\n",
    "    callback.on_train_begin()\n",
    "    breakdown = False\n",
    "    for epoch in range(num_epochs):\n",
    "        if breakdown:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['training', 'validation']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            if phase == 'training':\n",
    "                model.train() # Set model to training mode\n",
    "                volatile=False\n",
    "            else:\n",
    "                model.eval() # Set model to evaluate mode\n",
    "                volatile=True\n",
    "            \n",
    "\n",
    "            # Iterate over data.\n",
    "            for batch_idx, (particles_data, hlf_data, y_data) in enumerate(data_loader[phase]):\n",
    "                particles_data = particles_data.numpy()\n",
    "                arr = np.sum(particles_data!=0, axis=1)[:,0] # the number of particles in each batch\n",
    "                arr = [1 if x==0 else x for x in arr]\n",
    "                arr = np.array(arr)\n",
    "                sorted_indices_la = np.argsort(-arr)\n",
    "                particles_data = torch.from_numpy(particles_data[sorted_indices_la]).float()\n",
    "                hlf_data = hlf_data[sorted_indices_la]\n",
    "                y_data = y_data[sorted_indices_la]\n",
    "                particles_data = Variable(particles_data, requires_grad=not volatile).cuda() \n",
    "                # particles_data = Variable(particles_data, requires_grad=not volatile)\n",
    "                \n",
    "                hlf_data = Variable(hlf_data, requires_grad=not volatile).cuda()\n",
    "                # hlf_data = Variable(hlf_data, requires_grad=not volatile)\n",
    "                y_data = Variable(y_data, requires_grad=False).cuda()\n",
    "                # y_data = Variable(y_data, requires_grad=not volatile)\n",
    "                t_seq_length = [arr[i] for i in sorted_indices_la]\n",
    "                particles_data = torch.nn.utils.rnn.pack_padded_sequence(particles_data, t_seq_length, batch_first=True)\n",
    "                \n",
    "                if phase == 'training':\n",
    "                    optimizer.zero_grad()\n",
    "                # forward pass\n",
    "                outputs = model(particles_data, hlf_data)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, y_data)\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'training':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                # running_loss += loss.data[0]\n",
    "                running_loss += loss.data.item()\n",
    "                running_corrects += torch.sum(preds == y_data.data)\n",
    "                #print(\"I finished %d batch\" % batch_idx)\n",
    "            \n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = 100. * running_corrects / len(data_loader[phase].dataset)\n",
    "            if phase == 'training':\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                scheduler.step(epoch_loss)\n",
    "                val_losses.append(epoch_loss)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'validation' and epoch_acc > best_acc:\n",
    "                print('Saving..')\n",
    "                state = {\n",
    "                        'net': model, #.module if use_cuda else net,\n",
    "                        'epoch': epoch,\n",
    "                        'best_acc':epoch_acc,\n",
    "                        'train_loss':train_losses,\n",
    "                        'val_loss':val_losses,\n",
    "                        }\n",
    "                torch.save(state, model_file)\n",
    "                best_acc = epoch_acc\n",
    "                best_model = model.state_dict()\n",
    "            if phase == 'validation':\n",
    "                # breakdown = callback.on_epoch_end(epoch, -epoch_acc)\n",
    "                breakdown = callback.on_epoch_end(epoch, epoch_loss)\n",
    "                \n",
    "         \n",
    "    print('Best val acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    print('-' * 10)\n",
    "    return best_acc, train_losses, val_losses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.01, 'gru_layers': 2, 'gru_size': 442, 'dropout_g': 0.01, 'learning_rate': 0.003297552560160522, 'batch_size': 4000, 'L2_reg': 4.783663281646104e-05}\n",
      "Loaded best configuration from BestConfigReallyTopclass.json\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/99\n",
      "training Loss: 0.0001 Acc: 85.9313\n",
      "validation Loss: 0.0001 Acc: 87.2630\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0001 Acc: 86.7766\n",
      "validation Loss: 0.0001 Acc: 87.6522\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0001 Acc: 88.2532\n",
      "validation Loss: 0.0001 Acc: 88.5671\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0001 Acc: 88.7706\n",
      "validation Loss: 0.0001 Acc: 88.9753\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0001 Acc: 88.9901\n",
      "validation Loss: 0.0001 Acc: 89.0910\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0001 Acc: 89.1160\n",
      "validation Loss: 0.0001 Acc: 89.0904\n",
      "Epoch 6/99\n",
      "training Loss: 0.0001 Acc: 89.2049\n",
      "validation Loss: 0.0001 Acc: 89.2379\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0001 Acc: 89.2511\n",
      "validation Loss: 0.0001 Acc: 89.2801\n",
      "Saving..\n",
      "Epoch 8/99\n",
      "training Loss: 0.0001 Acc: 89.2856\n",
      "validation Loss: 0.0001 Acc: 89.1057\n",
      "Epoch 9/99\n",
      "training Loss: 0.0001 Acc: 89.3856\n",
      "validation Loss: 0.0001 Acc: 89.1907\n",
      "Epoch 10/99\n",
      "training Loss: 0.0001 Acc: 89.4072\n",
      "validation Loss: 0.0001 Acc: 89.3070\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0001 Acc: 89.4243\n",
      "validation Loss: 0.0001 Acc: 89.4129\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0001 Acc: 89.4610\n",
      "validation Loss: 0.0001 Acc: 89.4294\n",
      "Saving..\n",
      "Epoch 13/99\n",
      "training Loss: 0.0001 Acc: 89.4639\n",
      "validation Loss: 0.0001 Acc: 89.3278\n",
      "Epoch 14/99\n",
      "training Loss: 0.0001 Acc: 89.5495\n",
      "validation Loss: 0.0001 Acc: 89.4986\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0001 Acc: 89.5111\n",
      "validation Loss: 0.0001 Acc: 89.5646\n",
      "Saving..\n",
      "Epoch 16/99\n",
      "training Loss: 0.0001 Acc: 89.5614\n",
      "validation Loss: 0.0001 Acc: 89.5071\n",
      "Epoch 17/99\n",
      "training Loss: 0.0001 Acc: 89.5414\n",
      "validation Loss: 0.0001 Acc: 89.4643\n",
      "Epoch 18/99\n",
      "training Loss: 0.0001 Acc: 89.5313\n",
      "validation Loss: 0.0001 Acc: 89.4949\n",
      "Epoch 19/99\n",
      "training Loss: 0.0001 Acc: 89.5666\n",
      "validation Loss: 0.0001 Acc: 89.4594\n",
      "Epoch 20/99\n",
      "training Loss: 0.0001 Acc: 89.5469\n",
      "validation Loss: 0.0001 Acc: 89.5604\n",
      "Epoch 21/99\n",
      "training Loss: 0.0001 Acc: 89.5762\n",
      "validation Loss: 0.0001 Acc: 89.5047\n",
      "Epoch 22/99\n",
      "training Loss: 0.0001 Acc: 89.6139\n",
      "validation Loss: 0.0001 Acc: 89.6644\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0001 Acc: 89.5658\n",
      "validation Loss: 0.0001 Acc: 89.5518\n",
      "Epoch 24/99\n",
      "training Loss: 0.0001 Acc: 89.6839\n",
      "validation Loss: 0.0001 Acc: 89.5542\n",
      "Epoch 25/99\n",
      "training Loss: 0.0001 Acc: 89.6335\n",
      "validation Loss: 0.0001 Acc: 89.5640\n",
      "Epoch 26/99\n",
      "training Loss: 0.0001 Acc: 89.6306\n",
      "validation Loss: 0.0001 Acc: 89.6179\n",
      "Epoch 27/99\n",
      "training Loss: 0.0001 Acc: 89.7591\n",
      "validation Loss: 0.0001 Acc: 89.7176\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0001 Acc: 89.7750\n",
      "validation Loss: 0.0001 Acc: 89.7556\n",
      "Saving..\n",
      "Epoch 29/99\n",
      "training Loss: 0.0001 Acc: 89.7586\n",
      "validation Loss: 0.0001 Acc: 89.7654\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0001 Acc: 89.7793\n",
      "validation Loss: 0.0001 Acc: 89.7231\n",
      "Epoch 31/99\n",
      "training Loss: 0.0001 Acc: 89.8244\n",
      "validation Loss: 0.0001 Acc: 89.7960\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0001 Acc: 89.8099\n",
      "validation Loss: 0.0001 Acc: 89.7562\n",
      "Epoch 33/99\n",
      "training Loss: 0.0001 Acc: 89.8109\n",
      "validation Loss: 0.0001 Acc: 89.7856\n",
      "Epoch 34/99\n",
      "training Loss: 0.0001 Acc: 89.8123\n",
      "validation Loss: 0.0001 Acc: 89.7097\n",
      "Epoch 35/99\n",
      "training Loss: 0.0001 Acc: 89.8128\n",
      "validation Loss: 0.0001 Acc: 89.7868\n",
      "Epoch 36/99\n",
      "training Loss: 0.0001 Acc: 89.8258\n",
      "validation Loss: 0.0001 Acc: 89.7586\n",
      "Epoch 37/99\n",
      "training Loss: 0.0001 Acc: 89.8241\n",
      "validation Loss: 0.0001 Acc: 89.7954\n",
      "Epoch 38/99\n",
      "training Loss: 0.0001 Acc: 89.8770\n",
      "validation Loss: 0.0001 Acc: 89.7697\n",
      "Epoch 39/99\n",
      "training Loss: 0.0001 Acc: 89.8092\n",
      "validation Loss: 0.0001 Acc: 89.7415\n",
      "Epoch 40/99\n",
      "training Loss: 0.0001 Acc: 89.8512\n",
      "validation Loss: 0.0001 Acc: 89.7782\n",
      "Epoch 41/99\n",
      "training Loss: 0.0001 Acc: 89.8354\n",
      "validation Loss: 0.0001 Acc: 89.5995\n",
      "Epoch 42/99\n",
      "training Loss: 0.0001 Acc: 89.8574\n",
      "validation Loss: 0.0001 Acc: 89.8443\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0001 Acc: 89.8469\n",
      "validation Loss: 0.0001 Acc: 89.7562\n",
      "Epoch 44/99\n",
      "training Loss: 0.0001 Acc: 89.8784\n",
      "validation Loss: 0.0001 Acc: 89.7764\n",
      "Epoch 45/99\n",
      "training Loss: 0.0001 Acc: 89.8773\n",
      "validation Loss: 0.0001 Acc: 89.7482\n",
      "Epoch 46/99\n",
      "training Loss: 0.0001 Acc: 89.8577\n",
      "validation Loss: 0.0001 Acc: 89.6870\n",
      "Epoch 47/99\n",
      "training Loss: 0.0001 Acc: 89.9581\n",
      "validation Loss: 0.0001 Acc: 89.8743\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0001 Acc: 89.9925\n",
      "validation Loss: 0.0001 Acc: 89.8743\n",
      "Epoch 49/99\n",
      "training Loss: 0.0001 Acc: 89.9713\n",
      "validation Loss: 0.0001 Acc: 89.8321\n",
      "Epoch 50/99\n",
      "training Loss: 0.0001 Acc: 89.9884\n",
      "validation Loss: 0.0001 Acc: 89.8719\n",
      "Epoch 51/99\n",
      "training Loss: 0.0001 Acc: 89.9884\n",
      "validation Loss: 0.0001 Acc: 89.8327\n",
      "Epoch 52/99\n",
      "training Loss: 0.0001 Acc: 89.9664\n",
      "validation Loss: 0.0001 Acc: 89.8523\n",
      "Epoch 53/99\n",
      "training Loss: 0.0001 Acc: 90.0334\n",
      "validation Loss: 0.0001 Acc: 89.8670\n",
      "Epoch 54/99\n",
      "training Loss: 0.0001 Acc: 90.0511\n",
      "validation Loss: 0.0001 Acc: 89.8859\n",
      "Saving..\n",
      "Epoch 55/99\n",
      "training Loss: 0.0001 Acc: 90.0586\n",
      "validation Loss: 0.0001 Acc: 89.9147\n",
      "Saving..\n",
      "Epoch 56/99\n",
      "training Loss: 0.0001 Acc: 90.0756\n",
      "validation Loss: 0.0001 Acc: 89.8927\n",
      "Epoch 57/99\n",
      "training Loss: 0.0001 Acc: 90.0470\n",
      "validation Loss: 0.0001 Acc: 89.8798\n",
      "Epoch 58/99\n",
      "training Loss: 0.0001 Acc: 90.0528\n",
      "validation Loss: 0.0001 Acc: 89.8804\n",
      "Epoch 59/99\n",
      "training Loss: 0.0001 Acc: 90.0591\n",
      "validation Loss: 0.0001 Acc: 89.8890\n",
      "Epoch 60/99\n",
      "training Loss: 0.0001 Acc: 90.0569\n",
      "validation Loss: 0.0001 Acc: 89.8737\n",
      "Epoch 61/99\n",
      "training Loss: 0.0001 Acc: 90.0883\n",
      "validation Loss: 0.0001 Acc: 89.9012\n",
      "Epoch 62/99\n",
      "training Loss: 0.0001 Acc: 90.1002\n",
      "validation Loss: 0.0001 Acc: 89.8908\n",
      "Epoch 63/99\n",
      "training Loss: 0.0001 Acc: 90.0964\n",
      "validation Loss: 0.0001 Acc: 89.9129\n",
      "Epoch 64/99\n",
      "training Loss: 0.0001 Acc: 90.0908\n",
      "validation Loss: 0.0001 Acc: 89.9073\n",
      "Epoch 65/99\n",
      "training Loss: 0.0001 Acc: 90.1002\n",
      "validation Loss: 0.0001 Acc: 89.9165\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0001 Acc: 90.0987\n",
      "validation Loss: 0.0001 Acc: 89.8951\n",
      "Epoch 67/99\n",
      "training Loss: 0.0001 Acc: 90.0950\n",
      "validation Loss: 0.0001 Acc: 89.9098\n",
      "Epoch 68/99\n",
      "training Loss: 0.0001 Acc: 90.1053\n",
      "validation Loss: 0.0001 Acc: 89.8908\n",
      "Epoch 69/99\n",
      "training Loss: 0.0001 Acc: 90.1162\n",
      "validation Loss: 0.0001 Acc: 89.9116\n",
      "Epoch 70/99\n",
      "training Loss: 0.0001 Acc: 90.1344\n",
      "validation Loss: 0.0001 Acc: 89.9000\n",
      "Epoch 71/99\n",
      "training Loss: 0.0001 Acc: 90.1212\n",
      "validation Loss: 0.0001 Acc: 89.9153\n",
      "Epoch 72/99\n",
      "training Loss: 0.0001 Acc: 90.1105\n",
      "validation Loss: 0.0001 Acc: 89.9122\n",
      "Epoch 73/99\n",
      "training Loss: 0.0001 Acc: 90.1318\n",
      "validation Loss: 0.0001 Acc: 89.9080\n",
      "Epoch 74/99\n",
      "training Loss: 0.0001 Acc: 90.1383\n",
      "validation Loss: 0.0001 Acc: 89.9067\n",
      "Epoch 75/99\n",
      "training Loss: 0.0001 Acc: 90.1278\n",
      "validation Loss: 0.0001 Acc: 89.9324\n",
      "Saving..\n",
      "Epoch 76/99\n",
      "training Loss: 0.0001 Acc: 90.1324\n",
      "validation Loss: 0.0001 Acc: 89.9318\n",
      "Epoch 77/99\n",
      "training Loss: 0.0001 Acc: 90.1394\n",
      "validation Loss: 0.0001 Acc: 89.9275\n",
      "Epoch 78/99\n",
      "training Loss: 0.0001 Acc: 90.1367\n",
      "validation Loss: 0.0001 Acc: 89.9239\n",
      "Epoch 79/99\n",
      "training Loss: 0.0001 Acc: 90.1110\n",
      "validation Loss: 0.0001 Acc: 89.9220\n",
      "Epoch 80/99\n",
      "training Loss: 0.0001 Acc: 90.1426\n",
      "validation Loss: 0.0001 Acc: 89.9141\n",
      "Epoch 81/99\n",
      "training Loss: 0.0001 Acc: 90.1634\n",
      "validation Loss: 0.0001 Acc: 89.9263\n",
      "Epoch 82/99\n",
      "training Loss: 0.0001 Acc: 90.1449\n",
      "validation Loss: 0.0001 Acc: 89.9178\n",
      "Epoch 83/99\n",
      "training Loss: 0.0001 Acc: 90.1415\n",
      "validation Loss: 0.0001 Acc: 89.9135\n",
      "Epoch 84/99\n",
      "training Loss: 0.0001 Acc: 90.1380\n",
      "validation Loss: 0.0001 Acc: 89.9220\n",
      "Epoch 85/99\n",
      "training Loss: 0.0001 Acc: 90.1408\n",
      "validation Loss: 0.0001 Acc: 89.9202\n",
      "Epoch 86/99\n",
      "training Loss: 0.0001 Acc: 90.1466\n",
      "validation Loss: 0.0001 Acc: 89.9324\n",
      "Epoch 87/99\n",
      "training Loss: 0.0001 Acc: 90.1342\n",
      "validation Loss: 0.0001 Acc: 89.9135\n",
      "Epoch 88/99\n",
      "training Loss: 0.0001 Acc: 90.1394\n",
      "validation Loss: 0.0001 Acc: 89.9165\n",
      "Epoch 89/99\n",
      "training Loss: 0.0001 Acc: 90.1438\n",
      "validation Loss: 0.0001 Acc: 89.9227\n",
      "Epoch 90/99\n",
      "training Loss: 0.0001 Acc: 90.1500\n",
      "validation Loss: 0.0001 Acc: 89.9227\n",
      "Epoch 91/99\n",
      "training Loss: 0.0001 Acc: 90.1460\n",
      "validation Loss: 0.0001 Acc: 89.9122\n",
      "Epoch 92/99\n",
      "training Loss: 0.0001 Acc: 90.1426\n",
      "validation Loss: 0.0001 Acc: 89.9159\n",
      "Epoch 93/99\n",
      "training Loss: 0.0001 Acc: 90.1544\n",
      "validation Loss: 0.0001 Acc: 89.9104\n",
      "Epoch 94/99\n",
      "training Loss: 0.0001 Acc: 90.1543\n",
      "validation Loss: 0.0001 Acc: 89.9080\n",
      "Epoch 95/99\n",
      "training Loss: 0.0001 Acc: 90.1334\n",
      "validation Loss: 0.0001 Acc: 89.9110\n",
      "Epoch 96/99\n",
      "training Loss: 0.0001 Acc: 90.1458\n",
      "validation Loss: 0.0001 Acc: 89.9098\n",
      "Epoch 97/99\n",
      "training Loss: 0.0001 Acc: 90.1409\n",
      "validation Loss: 0.0001 Acc: 89.9073\n",
      "Early stopped.\n",
      "Best val acc: 89.932442\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/99\n",
      "training Loss: 0.0001 Acc: 86.3573\n",
      "validation Loss: 0.0001 Acc: 88.1173\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0001 Acc: 88.4086\n",
      "validation Loss: 0.0001 Acc: 88.8511\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0001 Acc: 88.7882\n",
      "validation Loss: 0.0001 Acc: 88.4074\n",
      "Epoch 3/99\n",
      "training Loss: 0.0001 Acc: 89.0160\n",
      "validation Loss: 0.0001 Acc: 88.7672\n",
      "Epoch 4/99\n",
      "training Loss: 0.0001 Acc: 89.1292\n",
      "validation Loss: 0.0001 Acc: 89.3205\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0001 Acc: 89.2172\n",
      "validation Loss: 0.0001 Acc: 89.3811\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0001 Acc: 89.3298\n",
      "validation Loss: 0.0001 Acc: 89.3737\n",
      "Epoch 7/99\n",
      "training Loss: 0.0001 Acc: 89.3174\n",
      "validation Loss: 0.0001 Acc: 89.3425\n",
      "Epoch 8/99\n",
      "training Loss: 0.0001 Acc: 89.3838\n",
      "validation Loss: 0.0001 Acc: 89.5047\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0001 Acc: 89.3982\n",
      "validation Loss: 0.0001 Acc: 89.5114\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0001 Acc: 89.4528\n",
      "validation Loss: 0.0001 Acc: 89.4814\n",
      "Epoch 11/99\n",
      "training Loss: 0.0001 Acc: 89.4942\n",
      "validation Loss: 0.0001 Acc: 89.3572\n",
      "Epoch 12/99\n",
      "training Loss: 0.0001 Acc: 89.4554\n",
      "validation Loss: 0.0001 Acc: 89.5016\n",
      "Epoch 13/99\n",
      "training Loss: 0.0001 Acc: 89.5325\n",
      "validation Loss: 0.0001 Acc: 89.4906\n",
      "Epoch 14/99\n",
      "training Loss: 0.0001 Acc: 89.6856\n",
      "validation Loss: 0.0001 Acc: 89.6772\n",
      "Saving..\n",
      "Epoch 15/99\n",
      "training Loss: 0.0001 Acc: 89.6712\n",
      "validation Loss: 0.0001 Acc: 89.5934\n",
      "Epoch 16/99\n",
      "training Loss: 0.0001 Acc: 89.7204\n",
      "validation Loss: 0.0001 Acc: 89.6797\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0001 Acc: 89.6780\n",
      "validation Loss: 0.0001 Acc: 89.6558\n",
      "Epoch 18/99\n",
      "training Loss: 0.0001 Acc: 89.7132\n",
      "validation Loss: 0.0001 Acc: 89.7085\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0001 Acc: 89.7447\n",
      "validation Loss: 0.0001 Acc: 89.7299\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0001 Acc: 89.7216\n",
      "validation Loss: 0.0001 Acc: 89.7427\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0001 Acc: 89.7471\n",
      "validation Loss: 0.0001 Acc: 89.7421\n",
      "Epoch 22/99\n",
      "training Loss: 0.0001 Acc: 89.7877\n",
      "validation Loss: 0.0001 Acc: 89.7452\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0001 Acc: 89.7788\n",
      "validation Loss: 0.0001 Acc: 89.7323\n",
      "Epoch 24/99\n",
      "training Loss: 0.0001 Acc: 89.7629\n",
      "validation Loss: 0.0001 Acc: 89.8064\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0001 Acc: 89.7624\n",
      "validation Loss: 0.0001 Acc: 89.7513\n",
      "Epoch 26/99\n",
      "training Loss: 0.0001 Acc: 89.7588\n",
      "validation Loss: 0.0001 Acc: 89.6564\n",
      "Epoch 27/99\n",
      "training Loss: 0.0001 Acc: 89.7620\n",
      "validation Loss: 0.0001 Acc: 89.7397\n",
      "Epoch 28/99\n",
      "training Loss: 0.0001 Acc: 89.8206\n",
      "validation Loss: 0.0001 Acc: 89.7048\n",
      "Epoch 29/99\n",
      "training Loss: 0.0001 Acc: 89.9269\n",
      "validation Loss: 0.0001 Acc: 89.8149\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0001 Acc: 89.9202\n",
      "validation Loss: 0.0001 Acc: 89.8474\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0001 Acc: 89.9095\n",
      "validation Loss: 0.0001 Acc: 89.8816\n",
      "Saving..\n",
      "Epoch 32/99\n",
      "training Loss: 0.0001 Acc: 89.9384\n",
      "validation Loss: 0.0001 Acc: 89.8064\n",
      "Epoch 33/99\n",
      "training Loss: 0.0001 Acc: 89.9089\n",
      "validation Loss: 0.0001 Acc: 89.8529\n",
      "Epoch 34/99\n",
      "training Loss: 0.0001 Acc: 89.9196\n",
      "validation Loss: 0.0001 Acc: 89.8823\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0001 Acc: 89.9521\n",
      "validation Loss: 0.0001 Acc: 89.8976\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0001 Acc: 89.9358\n",
      "validation Loss: 0.0001 Acc: 89.8908\n",
      "Epoch 37/99\n",
      "training Loss: 0.0001 Acc: 89.9413\n",
      "validation Loss: 0.0001 Acc: 89.8027\n",
      "Epoch 38/99\n",
      "training Loss: 0.0001 Acc: 89.9450\n",
      "validation Loss: 0.0001 Acc: 89.8792\n",
      "Epoch 39/99\n",
      "training Loss: 0.0001 Acc: 89.9248\n",
      "validation Loss: 0.0001 Acc: 89.7654\n",
      "Epoch 40/99\n",
      "training Loss: 0.0001 Acc: 89.9525\n",
      "validation Loss: 0.0001 Acc: 89.8364\n",
      "Epoch 41/99\n",
      "training Loss: 0.0001 Acc: 89.9359\n",
      "validation Loss: 0.0001 Acc: 89.6742\n",
      "Epoch 42/99\n",
      "training Loss: 0.0001 Acc: 89.9486\n",
      "validation Loss: 0.0001 Acc: 89.8125\n",
      "Epoch 43/99\n",
      "training Loss: 0.0001 Acc: 90.0341\n",
      "validation Loss: 0.0001 Acc: 89.9312\n",
      "Saving..\n",
      "Epoch 44/99\n",
      "training Loss: 0.0001 Acc: 90.0031\n",
      "validation Loss: 0.0001 Acc: 89.8982\n",
      "Epoch 45/99\n",
      "training Loss: 0.0001 Acc: 90.0314\n",
      "validation Loss: 0.0001 Acc: 89.9202\n",
      "Epoch 46/99\n",
      "training Loss: 0.0001 Acc: 90.0348\n",
      "validation Loss: 0.0001 Acc: 89.9392\n",
      "Saving..\n",
      "Epoch 47/99\n",
      "training Loss: 0.0001 Acc: 90.0314\n",
      "validation Loss: 0.0001 Acc: 89.9153\n",
      "Epoch 48/99\n",
      "training Loss: 0.0001 Acc: 90.0340\n",
      "validation Loss: 0.0001 Acc: 89.9294\n",
      "Epoch 49/99\n",
      "training Loss: 0.0001 Acc: 90.0368\n",
      "validation Loss: 0.0001 Acc: 89.9569\n",
      "Saving..\n",
      "Epoch 50/99\n",
      "training Loss: 0.0001 Acc: 90.0412\n",
      "validation Loss: 0.0001 Acc: 89.8321\n",
      "Epoch 51/99\n",
      "training Loss: 0.0001 Acc: 90.0670\n",
      "validation Loss: 0.0001 Acc: 89.9606\n",
      "Saving..\n",
      "Epoch 52/99\n",
      "training Loss: 0.0001 Acc: 90.0846\n",
      "validation Loss: 0.0001 Acc: 89.9588\n",
      "Epoch 53/99\n",
      "training Loss: 0.0001 Acc: 90.0931\n",
      "validation Loss: 0.0001 Acc: 89.9630\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0001 Acc: 90.0846\n",
      "validation Loss: 0.0001 Acc: 89.9275\n",
      "Epoch 55/99\n",
      "training Loss: 0.0001 Acc: 90.0949\n",
      "validation Loss: 0.0001 Acc: 89.9551\n",
      "Epoch 56/99\n",
      "training Loss: 0.0001 Acc: 90.0762\n",
      "validation Loss: 0.0001 Acc: 89.9563\n",
      "Epoch 57/99\n",
      "training Loss: 0.0001 Acc: 90.0957\n",
      "validation Loss: 0.0001 Acc: 89.9459\n",
      "Epoch 58/99\n",
      "training Loss: 0.0001 Acc: 90.0791\n",
      "validation Loss: 0.0001 Acc: 89.9588\n",
      "Epoch 59/99\n",
      "training Loss: 0.0001 Acc: 90.0949\n",
      "validation Loss: 0.0001 Acc: 89.9532\n",
      "Epoch 60/99\n",
      "training Loss: 0.0001 Acc: 90.1151\n",
      "validation Loss: 0.0001 Acc: 89.9227\n",
      "Epoch 61/99\n",
      "training Loss: 0.0001 Acc: 90.1097\n",
      "validation Loss: 0.0001 Acc: 89.9508\n",
      "Epoch 62/99\n",
      "training Loss: 0.0001 Acc: 90.1113\n",
      "validation Loss: 0.0001 Acc: 89.9569\n",
      "Epoch 63/99\n",
      "training Loss: 0.0001 Acc: 90.1122\n",
      "validation Loss: 0.0001 Acc: 89.9502\n",
      "Epoch 64/99\n",
      "training Loss: 0.0001 Acc: 90.1212\n",
      "validation Loss: 0.0001 Acc: 89.9532\n",
      "Epoch 65/99\n",
      "training Loss: 0.0001 Acc: 90.1327\n",
      "validation Loss: 0.0001 Acc: 89.9447\n",
      "Epoch 66/99\n",
      "training Loss: 0.0001 Acc: 90.1155\n",
      "validation Loss: 0.0001 Acc: 89.9520\n",
      "Epoch 67/99\n",
      "training Loss: 0.0001 Acc: 90.1295\n",
      "validation Loss: 0.0001 Acc: 89.9539\n",
      "Epoch 68/99\n",
      "training Loss: 0.0001 Acc: 90.1322\n",
      "validation Loss: 0.0001 Acc: 89.9532\n",
      "Epoch 69/99\n",
      "training Loss: 0.0001 Acc: 90.1322\n",
      "validation Loss: 0.0001 Acc: 89.9459\n",
      "Epoch 70/99\n",
      "training Loss: 0.0001 Acc: 90.1178\n",
      "validation Loss: 0.0001 Acc: 89.9526\n",
      "Epoch 71/99\n",
      "training Loss: 0.0001 Acc: 90.1412\n",
      "validation Loss: 0.0001 Acc: 89.9441\n",
      "Epoch 72/99\n",
      "training Loss: 0.0001 Acc: 90.1454\n",
      "validation Loss: 0.0001 Acc: 89.9398\n",
      "Epoch 73/99\n",
      "training Loss: 0.0001 Acc: 90.1305\n",
      "validation Loss: 0.0001 Acc: 89.9496\n",
      "Epoch 74/99\n",
      "training Loss: 0.0001 Acc: 90.1203\n",
      "validation Loss: 0.0001 Acc: 89.9606\n",
      "Epoch 75/99\n",
      "training Loss: 0.0001 Acc: 90.1336\n",
      "validation Loss: 0.0001 Acc: 89.9618\n",
      "Epoch 76/99\n",
      "training Loss: 0.0001 Acc: 90.1351\n",
      "validation Loss: 0.0001 Acc: 89.9465\n",
      "Epoch 77/99\n",
      "training Loss: 0.0001 Acc: 90.1422\n",
      "validation Loss: 0.0001 Acc: 89.9508\n",
      "Epoch 78/99\n",
      "training Loss: 0.0001 Acc: 90.1539\n",
      "validation Loss: 0.0001 Acc: 89.9594\n",
      "Epoch 79/99\n",
      "training Loss: 0.0001 Acc: 90.1357\n",
      "validation Loss: 0.0001 Acc: 89.9447\n",
      "Epoch 80/99\n",
      "training Loss: 0.0001 Acc: 90.1458\n",
      "validation Loss: 0.0001 Acc: 89.9643\n",
      "Saving..\n",
      "Epoch 81/99\n",
      "training Loss: 0.0001 Acc: 90.1304\n",
      "validation Loss: 0.0001 Acc: 89.9447\n",
      "Epoch 82/99\n",
      "training Loss: 0.0001 Acc: 90.1458\n",
      "validation Loss: 0.0001 Acc: 89.9471\n",
      "Epoch 83/99\n",
      "training Loss: 0.0001 Acc: 90.1435\n",
      "validation Loss: 0.0001 Acc: 89.9532\n",
      "Epoch 84/99\n",
      "training Loss: 0.0001 Acc: 90.1501\n",
      "validation Loss: 0.0001 Acc: 89.9520\n",
      "Epoch 85/99\n",
      "training Loss: 0.0001 Acc: 90.1461\n",
      "validation Loss: 0.0001 Acc: 89.9490\n",
      "Epoch 86/99\n",
      "training Loss: 0.0001 Acc: 90.1565\n",
      "validation Loss: 0.0001 Acc: 89.9539\n",
      "Epoch 87/99\n",
      "training Loss: 0.0001 Acc: 90.1572\n",
      "validation Loss: 0.0001 Acc: 89.9508\n",
      "Epoch 88/99\n",
      "training Loss: 0.0001 Acc: 90.1770\n",
      "validation Loss: 0.0001 Acc: 89.9502\n",
      "Epoch 89/99\n",
      "training Loss: 0.0001 Acc: 90.1423\n",
      "validation Loss: 0.0001 Acc: 89.9539\n",
      "Epoch 90/99\n",
      "training Loss: 0.0001 Acc: 90.1428\n",
      "validation Loss: 0.0001 Acc: 89.9520\n",
      "Epoch 91/99\n",
      "training Loss: 0.0001 Acc: 90.1380\n",
      "validation Loss: 0.0001 Acc: 89.9514\n",
      "Epoch 92/99\n",
      "training Loss: 0.0001 Acc: 90.1461\n",
      "validation Loss: 0.0001 Acc: 89.9490\n",
      "Early stopped.\n",
      "Best val acc: 89.964264\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/99\n",
      "training Loss: 0.0001 Acc: 86.0485\n",
      "validation Loss: 0.0001 Acc: 88.1473\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0001 Acc: 88.1430\n",
      "validation Loss: 0.0001 Acc: 88.7752\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0001 Acc: 88.7837\n",
      "validation Loss: 0.0001 Acc: 88.9380\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0001 Acc: 88.9930\n",
      "validation Loss: 0.0001 Acc: 89.3272\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0001 Acc: 89.2028\n",
      "validation Loss: 0.0001 Acc: 89.3199\n",
      "Epoch 5/99\n",
      "training Loss: 0.0001 Acc: 89.2600\n",
      "validation Loss: 0.0001 Acc: 89.5108\n",
      "Saving..\n",
      "Epoch 6/99\n",
      "training Loss: 0.0001 Acc: 89.2970\n",
      "validation Loss: 0.0001 Acc: 89.3902\n",
      "Epoch 7/99\n",
      "training Loss: 0.0001 Acc: 89.3523\n",
      "validation Loss: 0.0001 Acc: 89.3333\n",
      "Epoch 8/99\n",
      "training Loss: 0.0001 Acc: 89.3807\n",
      "validation Loss: 0.0001 Acc: 89.4912\n",
      "Epoch 9/99\n",
      "training Loss: 0.0001 Acc: 89.4164\n",
      "validation Loss: 0.0001 Acc: 89.4900\n",
      "Epoch 10/99\n",
      "training Loss: 0.0001 Acc: 89.4081\n",
      "validation Loss: 0.0001 Acc: 89.2115\n",
      "Epoch 11/99\n",
      "training Loss: 0.0001 Acc: 89.4324\n",
      "validation Loss: 0.0001 Acc: 89.6350\n",
      "Saving..\n",
      "Epoch 12/99\n",
      "training Loss: 0.0001 Acc: 89.4512\n",
      "validation Loss: 0.0001 Acc: 89.4594\n",
      "Epoch 13/99\n",
      "training Loss: 0.0001 Acc: 89.5060\n",
      "validation Loss: 0.0001 Acc: 89.6601\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0001 Acc: 89.5181\n",
      "validation Loss: 0.0001 Acc: 89.5787\n",
      "Epoch 15/99\n",
      "training Loss: 0.0001 Acc: 89.4779\n",
      "validation Loss: 0.0001 Acc: 89.5487\n",
      "Epoch 16/99\n",
      "training Loss: 0.0001 Acc: 89.6631\n",
      "validation Loss: 0.0001 Acc: 89.7697\n",
      "Saving..\n",
      "Epoch 17/99\n",
      "training Loss: 0.0001 Acc: 89.6742\n",
      "validation Loss: 0.0001 Acc: 89.6197\n",
      "Epoch 18/99\n",
      "training Loss: 0.0001 Acc: 89.7048\n",
      "validation Loss: 0.0001 Acc: 89.7586\n",
      "Epoch 19/99\n",
      "training Loss: 0.0001 Acc: 89.7093\n",
      "validation Loss: 0.0001 Acc: 89.8327\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0001 Acc: 89.7271\n",
      "validation Loss: 0.0001 Acc: 89.7752\n",
      "Epoch 21/99\n",
      "training Loss: 0.0001 Acc: 89.7266\n",
      "validation Loss: 0.0001 Acc: 89.7378\n",
      "Epoch 22/99\n",
      "training Loss: 0.0001 Acc: 89.6861\n",
      "validation Loss: 0.0001 Acc: 89.6405\n",
      "Epoch 23/99\n",
      "training Loss: 0.0001 Acc: 89.7352\n",
      "validation Loss: 0.0001 Acc: 89.7935\n",
      "Epoch 24/99\n",
      "training Loss: 0.0001 Acc: 89.7551\n",
      "validation Loss: 0.0001 Acc: 89.8492\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0001 Acc: 89.7484\n",
      "validation Loss: 0.0001 Acc: 89.8039\n",
      "Epoch 26/99\n",
      "training Loss: 0.0001 Acc: 89.7812\n",
      "validation Loss: 0.0001 Acc: 89.7617\n",
      "Epoch 27/99\n",
      "training Loss: 0.0001 Acc: 89.7705\n",
      "validation Loss: 0.0001 Acc: 89.8688\n",
      "Saving..\n",
      "Epoch 28/99\n",
      "training Loss: 0.0001 Acc: 89.7705\n",
      "validation Loss: 0.0001 Acc: 89.7262\n",
      "Epoch 29/99\n",
      "training Loss: 0.0001 Acc: 89.8044\n",
      "validation Loss: 0.0001 Acc: 89.6797\n",
      "Epoch 30/99\n",
      "training Loss: 0.0001 Acc: 89.7702\n",
      "validation Loss: 0.0001 Acc: 89.8113\n",
      "Epoch 31/99\n",
      "training Loss: 0.0001 Acc: 89.7713\n",
      "validation Loss: 0.0001 Acc: 89.6815\n",
      "Epoch 32/99\n",
      "training Loss: 0.0001 Acc: 89.8810\n",
      "validation Loss: 0.0001 Acc: 89.9404\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0001 Acc: 89.9055\n",
      "validation Loss: 0.0001 Acc: 89.9312\n",
      "Epoch 34/99\n",
      "training Loss: 0.0001 Acc: 89.8867\n",
      "validation Loss: 0.0001 Acc: 89.9471\n",
      "Saving..\n",
      "Epoch 35/99\n",
      "training Loss: 0.0001 Acc: 89.9159\n",
      "validation Loss: 0.0001 Acc: 89.8951\n",
      "Epoch 36/99\n",
      "training Loss: 0.0001 Acc: 89.9050\n",
      "validation Loss: 0.0001 Acc: 89.9545\n",
      "Saving..\n",
      "Epoch 37/99\n",
      "training Loss: 0.0001 Acc: 89.9111\n",
      "validation Loss: 0.0001 Acc: 89.9012\n",
      "Epoch 38/99\n",
      "training Loss: 0.0001 Acc: 89.9234\n",
      "validation Loss: 0.0001 Acc: 89.9355\n",
      "Epoch 39/99\n",
      "training Loss: 0.0001 Acc: 89.9272\n",
      "validation Loss: 0.0001 Acc: 89.9306\n",
      "Epoch 40/99\n",
      "training Loss: 0.0001 Acc: 89.9287\n",
      "validation Loss: 0.0001 Acc: 89.8719\n",
      "Epoch 41/99\n",
      "training Loss: 0.0001 Acc: 90.0049\n",
      "validation Loss: 0.0001 Acc: 89.9716\n",
      "Saving..\n",
      "Epoch 42/99\n",
      "training Loss: 0.0001 Acc: 89.9938\n",
      "validation Loss: 0.0001 Acc: 89.9881\n",
      "Saving..\n",
      "Epoch 43/99\n",
      "training Loss: 0.0001 Acc: 90.0133\n",
      "validation Loss: 0.0001 Acc: 89.9624\n",
      "Epoch 44/99\n",
      "training Loss: 0.0001 Acc: 90.0167\n",
      "validation Loss: 0.0001 Acc: 89.9863\n",
      "Epoch 45/99\n",
      "training Loss: 0.0001 Acc: 90.0114\n",
      "validation Loss: 0.0001 Acc: 89.9575\n",
      "Epoch 46/99\n",
      "training Loss: 0.0001 Acc: 90.0187\n",
      "validation Loss: 0.0001 Acc: 89.9667\n",
      "Epoch 47/99\n",
      "training Loss: 0.0001 Acc: 90.0207\n",
      "validation Loss: 0.0001 Acc: 89.9594\n",
      "Epoch 48/99\n",
      "training Loss: 0.0001 Acc: 90.0433\n",
      "validation Loss: 0.0001 Acc: 90.0285\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0001 Acc: 90.0384\n",
      "validation Loss: 0.0001 Acc: 90.0157\n",
      "Epoch 50/99\n",
      "training Loss: 0.0001 Acc: 90.0718\n",
      "validation Loss: 0.0001 Acc: 90.0236\n",
      "Epoch 51/99\n",
      "training Loss: 0.0001 Acc: 90.0374\n",
      "validation Loss: 0.0001 Acc: 89.9985\n",
      "Epoch 52/99\n",
      "training Loss: 0.0001 Acc: 90.0612\n",
      "validation Loss: 0.0001 Acc: 90.0138\n",
      "Epoch 53/99\n",
      "training Loss: 0.0001 Acc: 90.0481\n",
      "validation Loss: 0.0001 Acc: 90.0169\n",
      "Epoch 54/99\n",
      "training Loss: 0.0001 Acc: 90.0631\n",
      "validation Loss: 0.0001 Acc: 90.0242\n",
      "Epoch 55/99\n",
      "training Loss: 0.0001 Acc: 90.0661\n",
      "validation Loss: 0.0001 Acc: 89.9985\n",
      "Epoch 56/99\n",
      "training Loss: 0.0001 Acc: 90.0745\n",
      "validation Loss: 0.0001 Acc: 90.0114\n",
      "Epoch 57/99\n",
      "training Loss: 0.0001 Acc: 90.0595\n",
      "validation Loss: 0.0001 Acc: 89.9942\n",
      "Epoch 58/99\n",
      "training Loss: 0.0001 Acc: 90.1097\n",
      "validation Loss: 0.0001 Acc: 90.0359\n",
      "Saving..\n",
      "Epoch 59/99\n",
      "training Loss: 0.0001 Acc: 90.0735\n",
      "validation Loss: 0.0001 Acc: 90.0304\n",
      "Epoch 60/99\n",
      "training Loss: 0.0001 Acc: 90.0657\n",
      "validation Loss: 0.0001 Acc: 90.0438\n",
      "Saving..\n",
      "Epoch 61/99\n",
      "training Loss: 0.0001 Acc: 90.0898\n",
      "validation Loss: 0.0001 Acc: 90.0304\n",
      "Epoch 62/99\n",
      "training Loss: 0.0001 Acc: 90.0931\n",
      "validation Loss: 0.0001 Acc: 90.0475\n",
      "Saving..\n",
      "Epoch 63/99\n",
      "training Loss: 0.0001 Acc: 90.1050\n",
      "validation Loss: 0.0001 Acc: 90.0120\n",
      "Epoch 64/99\n",
      "training Loss: 0.0001 Acc: 90.0898\n",
      "validation Loss: 0.0001 Acc: 90.0132\n",
      "Epoch 65/99\n",
      "training Loss: 0.0001 Acc: 90.0949\n",
      "validation Loss: 0.0001 Acc: 90.0218\n",
      "Epoch 66/99\n",
      "training Loss: 0.0001 Acc: 90.0915\n",
      "validation Loss: 0.0001 Acc: 90.0334\n",
      "Epoch 67/99\n",
      "training Loss: 0.0001 Acc: 90.1056\n",
      "validation Loss: 0.0001 Acc: 90.0316\n",
      "Epoch 68/99\n",
      "training Loss: 0.0001 Acc: 90.0981\n",
      "validation Loss: 0.0001 Acc: 90.0371\n",
      "Epoch 69/99\n",
      "training Loss: 0.0001 Acc: 90.1084\n",
      "validation Loss: 0.0001 Acc: 90.0457\n",
      "Epoch 70/99\n",
      "training Loss: 0.0001 Acc: 90.1162\n",
      "validation Loss: 0.0001 Acc: 90.0395\n",
      "Epoch 71/99\n",
      "training Loss: 0.0001 Acc: 90.1002\n",
      "validation Loss: 0.0001 Acc: 90.0450\n",
      "Epoch 72/99\n",
      "training Loss: 0.0001 Acc: 90.1018\n",
      "validation Loss: 0.0001 Acc: 90.0365\n",
      "Epoch 73/99\n",
      "training Loss: 0.0001 Acc: 90.1168\n",
      "validation Loss: 0.0001 Acc: 90.0481\n",
      "Saving..\n",
      "Epoch 74/99\n",
      "training Loss: 0.0001 Acc: 90.1123\n",
      "validation Loss: 0.0001 Acc: 90.0469\n",
      "Epoch 75/99\n",
      "training Loss: 0.0001 Acc: 90.1022\n",
      "validation Loss: 0.0001 Acc: 90.0401\n",
      "Epoch 76/99\n",
      "training Loss: 0.0001 Acc: 90.1149\n",
      "validation Loss: 0.0001 Acc: 90.0334\n",
      "Epoch 77/99\n",
      "training Loss: 0.0001 Acc: 90.1189\n",
      "validation Loss: 0.0001 Acc: 90.0469\n",
      "Epoch 78/99\n",
      "training Loss: 0.0001 Acc: 90.1266\n",
      "validation Loss: 0.0001 Acc: 90.0555\n",
      "Saving..\n",
      "Epoch 79/99\n",
      "training Loss: 0.0001 Acc: 90.1203\n",
      "validation Loss: 0.0001 Acc: 90.0524\n",
      "Epoch 80/99\n",
      "training Loss: 0.0001 Acc: 90.1064\n",
      "validation Loss: 0.0001 Acc: 90.0346\n",
      "Epoch 81/99\n",
      "training Loss: 0.0001 Acc: 90.1233\n",
      "validation Loss: 0.0001 Acc: 90.0328\n",
      "Epoch 82/99\n",
      "training Loss: 0.0001 Acc: 90.1333\n",
      "validation Loss: 0.0001 Acc: 90.0365\n",
      "Epoch 83/99\n",
      "training Loss: 0.0001 Acc: 90.1259\n",
      "validation Loss: 0.0001 Acc: 90.0420\n",
      "Epoch 84/99\n",
      "training Loss: 0.0001 Acc: 90.1171\n",
      "validation Loss: 0.0001 Acc: 90.0450\n",
      "Epoch 85/99\n",
      "training Loss: 0.0001 Acc: 90.1336\n",
      "validation Loss: 0.0001 Acc: 90.0334\n",
      "Epoch 86/99\n",
      "training Loss: 0.0001 Acc: 90.1163\n",
      "validation Loss: 0.0001 Acc: 90.0322\n",
      "Early stopped.\n",
      "Best val acc: 90.055450\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/99\n",
      "training Loss: 0.0001 Acc: 85.9563\n",
      "validation Loss: 0.0001 Acc: 88.0469\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0001 Acc: 88.0237\n",
      "validation Loss: 0.0001 Acc: 88.8002\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0001 Acc: 88.8072\n",
      "validation Loss: 0.0001 Acc: 88.7721\n",
      "Epoch 3/99\n",
      "training Loss: 0.0001 Acc: 89.0623\n",
      "validation Loss: 0.0001 Acc: 89.1992\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0001 Acc: 89.1499\n",
      "validation Loss: 0.0001 Acc: 89.3394\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0001 Acc: 89.2091\n",
      "validation Loss: 0.0001 Acc: 89.2451\n",
      "Epoch 6/99\n",
      "training Loss: 0.0001 Acc: 89.3601\n",
      "validation Loss: 0.0001 Acc: 89.4673\n",
      "Saving..\n",
      "Epoch 7/99\n",
      "training Loss: 0.0001 Acc: 89.3212\n",
      "validation Loss: 0.0001 Acc: 89.4055\n",
      "Epoch 8/99\n",
      "training Loss: 0.0001 Acc: 89.4078\n",
      "validation Loss: 0.0001 Acc: 89.4067\n",
      "Epoch 9/99\n",
      "training Loss: 0.0001 Acc: 89.4338\n",
      "validation Loss: 0.0001 Acc: 89.5352\n",
      "Saving..\n",
      "Epoch 10/99\n",
      "training Loss: 0.0001 Acc: 89.4337\n",
      "validation Loss: 0.0001 Acc: 89.5505\n",
      "Saving..\n",
      "Epoch 11/99\n",
      "training Loss: 0.0001 Acc: 89.4868\n",
      "validation Loss: 0.0001 Acc: 89.5377\n",
      "Epoch 12/99\n",
      "training Loss: 0.0001 Acc: 89.5120\n",
      "validation Loss: 0.0001 Acc: 89.5303\n",
      "Epoch 13/99\n",
      "training Loss: 0.0001 Acc: 89.4965\n",
      "validation Loss: 0.0001 Acc: 89.6319\n",
      "Saving..\n",
      "Epoch 14/99\n",
      "training Loss: 0.0001 Acc: 89.4955\n",
      "validation Loss: 0.0001 Acc: 89.3155\n",
      "Epoch 15/99\n",
      "training Loss: 0.0001 Acc: 89.5189\n",
      "validation Loss: 0.0001 Acc: 89.4758\n",
      "Epoch 16/99\n",
      "training Loss: 0.0001 Acc: 89.5271\n",
      "validation Loss: 0.0001 Acc: 89.4979\n",
      "Epoch 17/99\n",
      "training Loss: 0.0001 Acc: 89.5685\n",
      "validation Loss: 0.0001 Acc: 89.5383\n",
      "Epoch 18/99\n",
      "training Loss: 0.0001 Acc: 89.7040\n",
      "validation Loss: 0.0001 Acc: 89.7806\n",
      "Saving..\n",
      "Epoch 19/99\n",
      "training Loss: 0.0001 Acc: 89.7191\n",
      "validation Loss: 0.0001 Acc: 89.7708\n",
      "Epoch 20/99\n",
      "training Loss: 0.0001 Acc: 89.7003\n",
      "validation Loss: 0.0001 Acc: 89.7855\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0001 Acc: 89.7527\n",
      "validation Loss: 0.0001 Acc: 89.7818\n",
      "Epoch 22/99\n",
      "training Loss: 0.0001 Acc: 89.7487\n",
      "validation Loss: 0.0001 Acc: 89.8088\n",
      "Saving..\n",
      "Epoch 23/99\n",
      "training Loss: 0.0001 Acc: 89.7695\n",
      "validation Loss: 0.0001 Acc: 89.7824\n",
      "Epoch 24/99\n",
      "training Loss: 0.0001 Acc: 89.7767\n",
      "validation Loss: 0.0001 Acc: 89.8112\n",
      "Saving..\n",
      "Epoch 25/99\n",
      "training Loss: 0.0001 Acc: 89.7875\n",
      "validation Loss: 0.0001 Acc: 89.8314\n",
      "Saving..\n",
      "Epoch 26/99\n",
      "training Loss: 0.0001 Acc: 89.7612\n",
      "validation Loss: 0.0001 Acc: 89.7800\n",
      "Epoch 27/99\n",
      "training Loss: 0.0001 Acc: 89.7589\n",
      "validation Loss: 0.0001 Acc: 89.6552\n",
      "Epoch 28/99\n",
      "training Loss: 0.0001 Acc: 89.7626\n",
      "validation Loss: 0.0001 Acc: 89.7378\n",
      "Epoch 29/99\n",
      "training Loss: 0.0001 Acc: 89.7813\n",
      "validation Loss: 0.0001 Acc: 89.8253\n",
      "Epoch 30/99\n",
      "training Loss: 0.0001 Acc: 89.8931\n",
      "validation Loss: 0.0001 Acc: 89.9299\n",
      "Saving..\n",
      "Epoch 31/99\n",
      "training Loss: 0.0001 Acc: 89.9004\n",
      "validation Loss: 0.0001 Acc: 89.9238\n",
      "Epoch 32/99\n",
      "training Loss: 0.0001 Acc: 89.8995\n",
      "validation Loss: 0.0001 Acc: 89.8706\n",
      "Epoch 33/99\n",
      "training Loss: 0.0001 Acc: 89.9027\n",
      "validation Loss: 0.0001 Acc: 89.8797\n",
      "Epoch 34/99\n",
      "training Loss: 0.0001 Acc: 89.9222\n",
      "validation Loss: 0.0001 Acc: 89.8473\n",
      "Epoch 35/99\n",
      "training Loss: 0.0001 Acc: 89.9216\n",
      "validation Loss: 0.0001 Acc: 89.8871\n",
      "Epoch 36/99\n",
      "training Loss: 0.0001 Acc: 89.8934\n",
      "validation Loss: 0.0001 Acc: 89.8693\n",
      "Epoch 37/99\n",
      "training Loss: 0.0001 Acc: 89.8911\n",
      "validation Loss: 0.0001 Acc: 89.9342\n",
      "Saving..\n",
      "Epoch 38/99\n",
      "training Loss: 0.0001 Acc: 89.9347\n",
      "validation Loss: 0.0001 Acc: 89.8834\n",
      "Epoch 39/99\n",
      "training Loss: 0.0001 Acc: 89.9272\n",
      "validation Loss: 0.0001 Acc: 89.9226\n",
      "Epoch 40/99\n",
      "training Loss: 0.0001 Acc: 89.9338\n",
      "validation Loss: 0.0001 Acc: 89.9103\n",
      "Epoch 41/99\n",
      "training Loss: 0.0001 Acc: 89.9509\n",
      "validation Loss: 0.0001 Acc: 89.8155\n",
      "Epoch 42/99\n",
      "training Loss: 0.0001 Acc: 89.9269\n",
      "validation Loss: 0.0001 Acc: 89.8944\n",
      "Epoch 43/99\n",
      "training Loss: 0.0001 Acc: 89.9286\n",
      "validation Loss: 0.0001 Acc: 89.9305\n",
      "Epoch 44/99\n",
      "training Loss: 0.0001 Acc: 90.0005\n",
      "validation Loss: 0.0001 Acc: 89.9483\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0001 Acc: 89.9872\n",
      "validation Loss: 0.0001 Acc: 89.9116\n",
      "Epoch 46/99\n",
      "training Loss: 0.0001 Acc: 90.0080\n",
      "validation Loss: 0.0001 Acc: 89.9312\n",
      "Epoch 47/99\n",
      "training Loss: 0.0001 Acc: 90.0129\n",
      "validation Loss: 0.0001 Acc: 89.9526\n",
      "Saving..\n",
      "Epoch 48/99\n",
      "training Loss: 0.0001 Acc: 90.0010\n",
      "validation Loss: 0.0001 Acc: 89.9838\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0001 Acc: 90.0340\n",
      "validation Loss: 0.0001 Acc: 89.9624\n",
      "Epoch 50/99\n",
      "training Loss: 0.0001 Acc: 90.0270\n",
      "validation Loss: 0.0001 Acc: 89.9734\n",
      "Epoch 51/99\n",
      "training Loss: 0.0001 Acc: 90.0089\n",
      "validation Loss: 0.0001 Acc: 89.9624\n",
      "Epoch 52/99\n",
      "training Loss: 0.0001 Acc: 90.0453\n",
      "validation Loss: 0.0001 Acc: 90.0034\n",
      "Saving..\n",
      "Epoch 53/99\n",
      "training Loss: 0.0001 Acc: 90.0547\n",
      "validation Loss: 0.0001 Acc: 90.0009\n",
      "Epoch 54/99\n",
      "training Loss: 0.0001 Acc: 90.0674\n",
      "validation Loss: 0.0001 Acc: 89.9764\n",
      "Epoch 55/99\n",
      "training Loss: 0.0001 Acc: 90.0646\n",
      "validation Loss: 0.0001 Acc: 89.9850\n",
      "Epoch 56/99\n",
      "training Loss: 0.0001 Acc: 90.1007\n",
      "validation Loss: 0.0001 Acc: 90.0070\n",
      "Saving..\n",
      "Epoch 57/99\n",
      "training Loss: 0.0001 Acc: 90.0866\n",
      "validation Loss: 0.0001 Acc: 89.9972\n",
      "Epoch 58/99\n",
      "training Loss: 0.0001 Acc: 90.0905\n",
      "validation Loss: 0.0001 Acc: 89.9850\n",
      "Epoch 59/99\n",
      "training Loss: 0.0001 Acc: 90.0772\n",
      "validation Loss: 0.0001 Acc: 89.9936\n",
      "Epoch 60/99\n",
      "training Loss: 0.0001 Acc: 90.0671\n",
      "validation Loss: 0.0001 Acc: 89.9881\n",
      "Epoch 61/99\n",
      "training Loss: 0.0001 Acc: 90.0871\n",
      "validation Loss: 0.0001 Acc: 89.9972\n",
      "Epoch 62/99\n",
      "training Loss: 0.0001 Acc: 90.0658\n",
      "validation Loss: 0.0001 Acc: 89.9850\n",
      "Epoch 63/99\n",
      "training Loss: 0.0001 Acc: 90.0850\n",
      "validation Loss: 0.0001 Acc: 89.9899\n",
      "Epoch 64/99\n",
      "training Loss: 0.0001 Acc: 90.0944\n",
      "validation Loss: 0.0001 Acc: 89.9997\n",
      "Epoch 65/99\n",
      "training Loss: 0.0001 Acc: 90.0917\n",
      "validation Loss: 0.0001 Acc: 90.0046\n",
      "Epoch 66/99\n",
      "training Loss: 0.0001 Acc: 90.0796\n",
      "validation Loss: 0.0001 Acc: 89.9997\n",
      "Epoch 67/99\n",
      "training Loss: 0.0001 Acc: 90.0983\n",
      "validation Loss: 0.0001 Acc: 89.9966\n",
      "Epoch 68/99\n",
      "training Loss: 0.0001 Acc: 90.1027\n",
      "validation Loss: 0.0001 Acc: 89.9905\n",
      "Epoch 69/99\n",
      "training Loss: 0.0001 Acc: 90.0984\n",
      "validation Loss: 0.0001 Acc: 90.0113\n",
      "Saving..\n",
      "Epoch 70/99\n",
      "training Loss: 0.0001 Acc: 90.1159\n",
      "validation Loss: 0.0001 Acc: 90.0132\n",
      "Saving..\n",
      "Epoch 71/99\n",
      "training Loss: 0.0001 Acc: 90.0966\n",
      "validation Loss: 0.0001 Acc: 90.0052\n",
      "Epoch 72/99\n",
      "training Loss: 0.0001 Acc: 90.0833\n",
      "validation Loss: 0.0001 Acc: 90.0107\n",
      "Epoch 73/99\n",
      "training Loss: 0.0001 Acc: 90.0975\n",
      "validation Loss: 0.0001 Acc: 90.0028\n",
      "Epoch 74/99\n",
      "training Loss: 0.0001 Acc: 90.1003\n",
      "validation Loss: 0.0001 Acc: 90.0064\n",
      "Epoch 75/99\n",
      "training Loss: 0.0001 Acc: 90.1139\n",
      "validation Loss: 0.0001 Acc: 90.0083\n",
      "Epoch 76/99\n",
      "training Loss: 0.0001 Acc: 90.1146\n",
      "validation Loss: 0.0001 Acc: 90.0174\n",
      "Saving..\n",
      "Epoch 77/99\n",
      "training Loss: 0.0001 Acc: 90.1183\n",
      "validation Loss: 0.0001 Acc: 90.0040\n",
      "Epoch 78/99\n",
      "training Loss: 0.0001 Acc: 90.1139\n",
      "validation Loss: 0.0001 Acc: 90.0193\n",
      "Saving..\n",
      "Epoch 79/99\n",
      "training Loss: 0.0001 Acc: 90.1269\n",
      "validation Loss: 0.0001 Acc: 90.0058\n",
      "Epoch 80/99\n",
      "training Loss: 0.0001 Acc: 90.1145\n",
      "validation Loss: 0.0001 Acc: 90.0144\n",
      "Epoch 81/99\n",
      "training Loss: 0.0001 Acc: 90.1209\n",
      "validation Loss: 0.0001 Acc: 90.0040\n",
      "Epoch 82/99\n",
      "training Loss: 0.0001 Acc: 90.0949\n",
      "validation Loss: 0.0001 Acc: 90.0199\n",
      "Saving..\n",
      "Epoch 83/99\n",
      "training Loss: 0.0001 Acc: 90.0929\n",
      "validation Loss: 0.0001 Acc: 90.0064\n",
      "Epoch 84/99\n",
      "training Loss: 0.0001 Acc: 90.1081\n",
      "validation Loss: 0.0001 Acc: 90.0058\n",
      "Epoch 85/99\n",
      "training Loss: 0.0001 Acc: 90.1007\n",
      "validation Loss: 0.0001 Acc: 90.0095\n",
      "Epoch 86/99\n",
      "training Loss: 0.0001 Acc: 90.1111\n",
      "validation Loss: 0.0001 Acc: 90.0083\n",
      "Epoch 87/99\n",
      "training Loss: 0.0001 Acc: 90.1263\n",
      "validation Loss: 0.0001 Acc: 90.0125\n",
      "Epoch 88/99\n",
      "training Loss: 0.0001 Acc: 90.0990\n",
      "validation Loss: 0.0001 Acc: 90.0083\n",
      "Epoch 89/99\n",
      "training Loss: 0.0001 Acc: 90.1090\n",
      "validation Loss: 0.0001 Acc: 90.0052\n",
      "Epoch 90/99\n",
      "training Loss: 0.0001 Acc: 90.1041\n",
      "validation Loss: 0.0001 Acc: 90.0009\n",
      "Epoch 91/99\n",
      "training Loss: 0.0001 Acc: 90.1019\n",
      "validation Loss: 0.0001 Acc: 90.0034\n",
      "Early stopped.\n",
      "Best val acc: 90.019890\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/99\n",
      "training Loss: 0.0001 Acc: 85.9335\n",
      "validation Loss: 0.0001 Acc: 87.3908\n",
      "Saving..\n",
      "Epoch 1/99\n",
      "training Loss: 0.0001 Acc: 88.2068\n",
      "validation Loss: 0.0001 Acc: 88.8155\n",
      "Saving..\n",
      "Epoch 2/99\n",
      "training Loss: 0.0001 Acc: 88.8446\n",
      "validation Loss: 0.0001 Acc: 88.8675\n",
      "Saving..\n",
      "Epoch 3/99\n",
      "training Loss: 0.0001 Acc: 89.0790\n",
      "validation Loss: 0.0001 Acc: 89.2084\n",
      "Saving..\n",
      "Epoch 4/99\n",
      "training Loss: 0.0001 Acc: 89.1720\n",
      "validation Loss: 0.0001 Acc: 89.3222\n",
      "Saving..\n",
      "Epoch 5/99\n",
      "training Loss: 0.0001 Acc: 89.2791\n",
      "validation Loss: 0.0001 Acc: 89.2421\n",
      "Epoch 6/99\n",
      "training Loss: 0.0001 Acc: 89.3732\n",
      "validation Loss: 0.0001 Acc: 89.2959\n",
      "Epoch 7/99\n",
      "training Loss: 0.0001 Acc: 89.4069\n",
      "validation Loss: 0.0001 Acc: 89.2678\n",
      "Epoch 8/99\n",
      "training Loss: 0.0001 Acc: 89.4542\n",
      "validation Loss: 0.0001 Acc: 89.5468\n",
      "Saving..\n",
      "Epoch 9/99\n",
      "training Loss: 0.0001 Acc: 89.4482\n",
      "validation Loss: 0.0001 Acc: 89.1778\n",
      "Epoch 10/99\n",
      "training Loss: 0.0001 Acc: 89.5092\n",
      "validation Loss: 0.0001 Acc: 89.3002\n",
      "Epoch 11/99\n",
      "training Loss: 0.0001 Acc: 89.5177\n",
      "validation Loss: 0.0001 Acc: 89.2580\n",
      "Epoch 12/99\n",
      "training Loss: 0.0001 Acc: 89.5218\n",
      "validation Loss: 0.0001 Acc: 89.5370\n",
      "Epoch 13/99\n",
      "training Loss: 0.0001 Acc: 89.5169\n",
      "validation Loss: 0.0001 Acc: 89.4722\n",
      "Epoch 14/99\n",
      "training Loss: 0.0001 Acc: 89.5285\n",
      "validation Loss: 0.0001 Acc: 89.5413\n",
      "Epoch 15/99\n",
      "training Loss: 0.0001 Acc: 89.4975\n",
      "validation Loss: 0.0001 Acc: 89.4299\n",
      "Epoch 16/99\n",
      "training Loss: 0.0001 Acc: 89.5808\n",
      "validation Loss: 0.0001 Acc: 89.5364\n",
      "Epoch 17/99\n",
      "training Loss: 0.0001 Acc: 89.5743\n",
      "validation Loss: 0.0001 Acc: 89.4544\n",
      "Epoch 18/99\n",
      "training Loss: 0.0001 Acc: 89.6128\n",
      "validation Loss: 0.0001 Acc: 89.4104\n",
      "Epoch 19/99\n",
      "training Loss: 0.0001 Acc: 89.7349\n",
      "validation Loss: 0.0001 Acc: 89.6931\n",
      "Saving..\n",
      "Epoch 20/99\n",
      "training Loss: 0.0001 Acc: 89.7686\n",
      "validation Loss: 0.0001 Acc: 89.7421\n",
      "Saving..\n",
      "Epoch 21/99\n",
      "training Loss: 0.0001 Acc: 89.7447\n",
      "validation Loss: 0.0001 Acc: 89.7616\n",
      "Saving..\n",
      "Epoch 22/99\n",
      "training Loss: 0.0001 Acc: 89.7770\n",
      "validation Loss: 0.0001 Acc: 89.6858\n",
      "Epoch 23/99\n",
      "training Loss: 0.0001 Acc: 89.8119\n",
      "validation Loss: 0.0001 Acc: 89.7176\n",
      "Epoch 24/99\n",
      "training Loss: 0.0001 Acc: 89.7785\n",
      "validation Loss: 0.0001 Acc: 89.7586\n",
      "Epoch 25/99\n",
      "training Loss: 0.0001 Acc: 89.8347\n",
      "validation Loss: 0.0001 Acc: 89.6992\n",
      "Epoch 26/99\n",
      "training Loss: 0.0001 Acc: 89.8163\n",
      "validation Loss: 0.0001 Acc: 89.7237\n",
      "Epoch 27/99\n",
      "training Loss: 0.0001 Acc: 89.8048\n",
      "validation Loss: 0.0001 Acc: 89.7145\n",
      "Epoch 28/99\n",
      "training Loss: 0.0001 Acc: 89.8189\n",
      "validation Loss: 0.0001 Acc: 89.6564\n",
      "Epoch 29/99\n",
      "training Loss: 0.0001 Acc: 89.9035\n",
      "validation Loss: 0.0001 Acc: 89.8553\n",
      "Saving..\n",
      "Epoch 30/99\n",
      "training Loss: 0.0001 Acc: 89.9109\n",
      "validation Loss: 0.0001 Acc: 89.8253\n",
      "Epoch 31/99\n",
      "training Loss: 0.0001 Acc: 89.9255\n",
      "validation Loss: 0.0001 Acc: 89.7506\n",
      "Epoch 32/99\n",
      "training Loss: 0.0001 Acc: 89.9431\n",
      "validation Loss: 0.0001 Acc: 89.8706\n",
      "Saving..\n",
      "Epoch 33/99\n",
      "training Loss: 0.0001 Acc: 89.9286\n",
      "validation Loss: 0.0001 Acc: 89.8308\n",
      "Epoch 34/99\n",
      "training Loss: 0.0001 Acc: 90.0036\n",
      "validation Loss: 0.0001 Acc: 89.8632\n",
      "Epoch 35/99\n",
      "training Loss: 0.0001 Acc: 90.0089\n",
      "validation Loss: 0.0001 Acc: 89.8889\n",
      "Saving..\n",
      "Epoch 36/99\n",
      "training Loss: 0.0001 Acc: 90.0241\n",
      "validation Loss: 0.0001 Acc: 89.8540\n",
      "Epoch 37/99\n",
      "training Loss: 0.0001 Acc: 90.0149\n",
      "validation Loss: 0.0001 Acc: 89.8797\n",
      "Epoch 38/99\n",
      "training Loss: 0.0001 Acc: 90.0261\n",
      "validation Loss: 0.0001 Acc: 89.9214\n",
      "Saving..\n",
      "Epoch 39/99\n",
      "training Loss: 0.0001 Acc: 90.0175\n",
      "validation Loss: 0.0001 Acc: 89.8785\n",
      "Epoch 40/99\n",
      "training Loss: 0.0001 Acc: 90.0358\n",
      "validation Loss: 0.0001 Acc: 89.8779\n",
      "Epoch 41/99\n",
      "training Loss: 0.0001 Acc: 90.0498\n",
      "validation Loss: 0.0001 Acc: 89.9030\n",
      "Epoch 42/99\n",
      "training Loss: 0.0001 Acc: 90.0418\n",
      "validation Loss: 0.0001 Acc: 89.9067\n",
      "Epoch 43/99\n",
      "training Loss: 0.0001 Acc: 90.0605\n",
      "validation Loss: 0.0001 Acc: 89.9201\n",
      "Epoch 44/99\n",
      "training Loss: 0.0001 Acc: 90.0735\n",
      "validation Loss: 0.0001 Acc: 89.9385\n",
      "Saving..\n",
      "Epoch 45/99\n",
      "training Loss: 0.0001 Acc: 90.0681\n",
      "validation Loss: 0.0001 Acc: 89.9550\n",
      "Saving..\n",
      "Epoch 46/99\n",
      "training Loss: 0.0001 Acc: 90.0862\n",
      "validation Loss: 0.0001 Acc: 89.9128\n",
      "Epoch 47/99\n",
      "training Loss: 0.0001 Acc: 90.0886\n",
      "validation Loss: 0.0001 Acc: 89.9244\n",
      "Epoch 48/99\n",
      "training Loss: 0.0001 Acc: 90.0903\n",
      "validation Loss: 0.0001 Acc: 89.9569\n",
      "Saving..\n",
      "Epoch 49/99\n",
      "training Loss: 0.0001 Acc: 90.0977\n",
      "validation Loss: 0.0001 Acc: 89.9305\n",
      "Epoch 50/99\n",
      "training Loss: 0.0001 Acc: 90.0857\n",
      "validation Loss: 0.0001 Acc: 89.9416\n",
      "Epoch 51/99\n",
      "training Loss: 0.0001 Acc: 90.0998\n",
      "validation Loss: 0.0001 Acc: 89.9177\n",
      "Epoch 52/99\n",
      "training Loss: 0.0001 Acc: 90.0938\n",
      "validation Loss: 0.0001 Acc: 89.9293\n",
      "Epoch 53/99\n",
      "training Loss: 0.0001 Acc: 90.0842\n",
      "validation Loss: 0.0001 Acc: 89.9575\n",
      "Saving..\n",
      "Epoch 54/99\n",
      "training Loss: 0.0001 Acc: 90.0868\n",
      "validation Loss: 0.0001 Acc: 89.9293\n",
      "Epoch 55/99\n",
      "training Loss: 0.0001 Acc: 90.0981\n",
      "validation Loss: 0.0001 Acc: 89.9495\n",
      "Epoch 56/99\n",
      "training Loss: 0.0001 Acc: 90.0996\n",
      "validation Loss: 0.0001 Acc: 89.9312\n",
      "Epoch 57/99\n",
      "training Loss: 0.0001 Acc: 90.1047\n",
      "validation Loss: 0.0001 Acc: 89.9452\n",
      "Epoch 58/99\n",
      "training Loss: 0.0001 Acc: 90.1001\n",
      "validation Loss: 0.0001 Acc: 89.9489\n",
      "Epoch 59/99\n",
      "training Loss: 0.0001 Acc: 90.1082\n",
      "validation Loss: 0.0001 Acc: 89.9569\n",
      "Epoch 60/99\n",
      "training Loss: 0.0001 Acc: 90.0981\n",
      "validation Loss: 0.0001 Acc: 89.9544\n",
      "Epoch 61/99\n",
      "training Loss: 0.0001 Acc: 90.0897\n",
      "validation Loss: 0.0001 Acc: 89.9599\n",
      "Saving..\n",
      "Epoch 62/99\n",
      "training Loss: 0.0001 Acc: 90.1120\n",
      "validation Loss: 0.0001 Acc: 89.9562\n",
      "Epoch 63/99\n",
      "training Loss: 0.0001 Acc: 90.1097\n",
      "validation Loss: 0.0001 Acc: 89.9636\n",
      "Saving..\n",
      "Epoch 64/99\n",
      "training Loss: 0.0001 Acc: 90.1091\n",
      "validation Loss: 0.0001 Acc: 89.9520\n",
      "Epoch 65/99\n",
      "training Loss: 0.0001 Acc: 90.1090\n",
      "validation Loss: 0.0001 Acc: 89.9685\n",
      "Saving..\n",
      "Epoch 66/99\n",
      "training Loss: 0.0001 Acc: 90.1237\n",
      "validation Loss: 0.0001 Acc: 89.9507\n",
      "Epoch 67/99\n",
      "training Loss: 0.0001 Acc: 90.1319\n",
      "validation Loss: 0.0001 Acc: 89.9416\n",
      "Epoch 68/99\n",
      "training Loss: 0.0001 Acc: 90.1073\n",
      "validation Loss: 0.0001 Acc: 89.9464\n",
      "Epoch 69/99\n",
      "training Loss: 0.0001 Acc: 90.1169\n",
      "validation Loss: 0.0001 Acc: 89.9501\n",
      "Epoch 70/99\n",
      "training Loss: 0.0001 Acc: 90.1074\n",
      "validation Loss: 0.0001 Acc: 89.9501\n",
      "Epoch 71/99\n",
      "training Loss: 0.0001 Acc: 90.1107\n",
      "validation Loss: 0.0001 Acc: 89.9520\n",
      "Epoch 72/99\n",
      "training Loss: 0.0001 Acc: 90.1154\n",
      "validation Loss: 0.0001 Acc: 89.9648\n",
      "Epoch 73/99\n",
      "training Loss: 0.0001 Acc: 90.1252\n",
      "validation Loss: 0.0001 Acc: 89.9587\n",
      "Epoch 74/99\n",
      "training Loss: 0.0001 Acc: 90.1130\n",
      "validation Loss: 0.0001 Acc: 89.9587\n",
      "Epoch 75/99\n",
      "training Loss: 0.0001 Acc: 90.1081\n",
      "validation Loss: 0.0001 Acc: 89.9477\n",
      "Epoch 76/99\n",
      "training Loss: 0.0001 Acc: 90.1445\n",
      "validation Loss: 0.0001 Acc: 89.9575\n",
      "Epoch 77/99\n",
      "training Loss: 0.0001 Acc: 90.1312\n",
      "validation Loss: 0.0001 Acc: 89.9513\n",
      "Epoch 78/99\n",
      "training Loss: 0.0001 Acc: 90.1387\n",
      "validation Loss: 0.0001 Acc: 89.9495\n",
      "Epoch 79/99\n",
      "training Loss: 0.0001 Acc: 90.1081\n",
      "validation Loss: 0.0001 Acc: 89.9513\n",
      "Epoch 80/99\n",
      "training Loss: 0.0001 Acc: 90.1260\n",
      "validation Loss: 0.0001 Acc: 89.9501\n",
      "Epoch 81/99\n",
      "training Loss: 0.0001 Acc: 90.1235\n",
      "validation Loss: 0.0001 Acc: 89.9513\n",
      "Early stopped.\n",
      "Best val acc: 89.968483\n",
      "----------\n",
      "Average best_acc across k-fold: 89.98810577392578\n"
     ]
    }
   ],
   "source": [
    "space  = [\n",
    "    Integer(1, 3, name='hidden_layers'),\n",
    "    Integer(10, 500, name='initial_nodes'),\n",
    "    Real(0.01,0.9,name='dropout'),\n",
    "    Integer(2, 3, name='gru_layers'),\n",
    "    Integer(10, 500, name='gru_size'),\n",
    "    Real(0.01,0.9,name='dropout_g'),\n",
    "    Real(10**-5, 10**-1, \"log-uniform\", name='learning_rate'),\n",
    "    Integer(4000,4001,name='batch_size'),\n",
    "    # Integer(32,512,name='batch_size'),\n",
    "    Real(10**-5, 10**-4, \"log-uniform\", name='L2_reg')\n",
    "]\n",
    "# L1 reg: https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch\n",
    "# batch_size = 4000\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**X):\n",
    "    print(\"New configuration: {}\".format(X))\n",
    "    fom = []\n",
    "    for train_index, test_index in skf.split(data_hlf, label):\n",
    "        train_loader = DataLoader(\n",
    "            ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), \n",
    "            batch_size=int(X['batch_size']), \n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), \n",
    "            batch_size=int(X['batch_size']), \n",
    "            shuffle=True\n",
    "        )\n",
    "        data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "        print(train_loader)\n",
    "\n",
    "        model = InclusiveNetwork(\n",
    "            int(X['hidden_layers']), \n",
    "            int(X['initial_nodes']), \n",
    "            float(X['dropout']), \n",
    "            int(X['gru_layers']), \n",
    "            int(X['gru_size']), \n",
    "            float(X['dropout_g'])\n",
    "        ).cuda()\n",
    "        # model = InclusiveNetwork(X['hidden_layers'], X['initial_nodes'], X['dropout'], X['gru_layers'], X['gru_size'], X['dropout_g'])\n",
    "\n",
    "        optimizer = AMSGrad(model.parameters(), lr=X['learning_rate'], weight_decay=X['L2_reg'])\n",
    "        criterion= nn.NLLLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=4)\n",
    "        best_acc, train_losses, val_losses = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader)\n",
    "        fom.append(best_acc)\n",
    "    Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "    print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "    return -Y\n",
    "\n",
    "# res_gp = gp_minimize(objective, space, n_calls=30, random_state=0)\n",
    "\n",
    "# print(\"Best parameters: {}\".format(res_gp.x))\n",
    "# best_hidden_layers = int(res_gp.x[0])\n",
    "# best_initial_nodes = int(res_gp.x[1])\n",
    "# best_dropout = float(res_gp.x[2])\n",
    "# best_gru_layers = int(res_gp.x[3])\n",
    "# best_gru_size = int(res_gp.x[4])\n",
    "# best_dropout_g = float(res_gp.x[5])\n",
    "# best_learning_rate = float(res_gp.x[6])\n",
    "# best_batch_size = int(res_gp.x[7])\n",
    "# best_L2_reg = float(res_gp.x[8])\n",
    "\n",
    "# best_conf = {\"hidden_layers\": best_hidden_layers,\n",
    "#           \"initial_nodes\": best_initial_nodes,\n",
    "#           \"dropout\": best_dropout,\n",
    "#           \"gru_layers\": best_gru_layers,\n",
    "#           \"gru_size\": best_gru_size,\n",
    "#           \"dropout_g\": best_dropout_g,\n",
    "#           \"learning_rate\": best_learning_rate,\n",
    "#           \"batch_size\": best_batch_size,\n",
    "#           \"L2_reg\": best_L2_reg}\n",
    "# with open(config_file, 'w') as config:\n",
    "#     json.dump(best_conf, config)\n",
    "#     print(\"Save best configuration to {}\".format(config_file))\n",
    "\n",
    "# best_conf = {\n",
    "#     'hidden_layers': 1, 'initial_nodes': 257, 'dropout': 0.01, \n",
    "#     'gru_layers': 2, 'gru_size': 497, 'dropout_g': 0.23179373811904597, \n",
    "#     'learning_rate': 0.002616690292572499, 'batch_size': 4001\n",
    "# }\n",
    "\n",
    "\n",
    "# train_index, test_index = skf.split(data_hlf, label).next()\n",
    "# train_index, test_index = next(skf.split(data_hlf, label))\n",
    "# model = InclusiveNetwork(best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "#                         best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']).cuda()\n",
    "# # model = InclusiveNetwork(best_hidden_layers, best_initial_nodes, best_dropout, best_gru_layers, best_gru_size, best_dropout_g).cuda()\n",
    "# # model = InclusiveNetwork(best_hidden_layers, best_initial_nodes, best_dropout, best_gru_layers, best_gru_size, best_dropout_g)\n",
    "# train_loader = DataLoader(ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), batch_size = best_conf['batch_size'], shuffle=True)\n",
    "# val_loader = DataLoader(ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), batch_size = best_conf['batch_size'], shuffle=True)\n",
    "# # train_loader = DataLoader(ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), batch_size = best_batch_size, shuffle=True)\n",
    "# # val_loader = DataLoader(ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), batch_size = best_batch_size, shuffle=True)\n",
    "# data_loader = {\"training\": train_loader, \"validation\": val_loader}\n",
    "# optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'])\n",
    "# # optimizer = AMSGrad(model.parameters(), lr=best_learning_rate)\n",
    "# criterion= nn.NLLLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=3)\n",
    "# EPOCHS = 70\n",
    "# best_acc, train_losses, val_losses = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader)\n",
    "# torch.save(model.state_dict(), model_file)\n",
    "\n",
    "\n",
    "with open(config_file) as f:\n",
    "    best_conf = json.load(f)\n",
    "    print(best_conf)\n",
    "print(\"Loaded best configuration from {}\".format(config_file))\n",
    "# model = InclusiveNetwork(best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']).cuda()\n",
    "\n",
    "\n",
    "fom = []\n",
    "# model = InclusiveNetwork(best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "                        # best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']).cuda()\n",
    "# optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "criterion= nn.NLLLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=3)\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "EPOCHS = 100\n",
    "for fold_idx, (train_index, test_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    print('='*50)\n",
    "    print(f'Fold {fold_idx}')\n",
    "    print('='*50)\n",
    "    model = InclusiveNetwork(best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), \n",
    "        batch_size=best_conf['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), \n",
    "        batch_size=best_conf['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader)\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    model_i_file = f'ReallyTopclassStyle_{fold_idx}.torch'\n",
    "    torch.save(model.state_dict(), model_i_file)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.01, 'gru_layers': 2, 'gru_size': 442, 'dropout_g': 0.01, 'learning_rate': 0.003297552560160522, 'batch_size': 4000, 'L2_reg': 4.783663281646104e-05}\n",
      "Loaded best configuration from BestConfigReallyTopclass.json\n"
     ]
    }
   ],
   "source": [
    "with open(config_file) as f:\n",
    "    bestconf = json.load(f)\n",
    "    print(bestconf)\n",
    "print(\"Loaded best configuration from {}\".format(config_file))\n",
    "model = InclusiveNetwork(bestconf['hidden_layers'], bestconf['initial_nodes'], bestconf['dropout'], bestconf['gru_layers'], bestconf['gru_size'], bestconf['dropout_g']).cuda()\n",
    "# model = InclusiveNetwork(bestconf['hidden_layers'], bestconf['initial_nodes'], bestconf['dropout'], bestconf['gru_layers'], bestconf['gru_size'], bestconf['dropout_g'])\n",
    "# model.load_state_dict(torch.load(model_file))\n",
    "# print(\"Loaded best model parameter from {}\".format(model_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size=1, shuffle=False, req_grad=True):\n",
    "    # train_index, test_index = next(skf.split(data_hlf, label))\n",
    "    # full_test_data = DataLoader(ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), batch_size=batch_size, shuffle=shuffle)\n",
    "    full_test_data = DataLoader(ParticleHLF(data_list_test, data_hlf_test, label_test), batch_size=bestconf['batch_size'], shuffle=False)\n",
    "\n",
    "    for (particles_data, hlf_data, y_data) in full_test_data:\n",
    "\n",
    "        particles_data = particles_data.numpy()\n",
    "        # print(particles_data)\n",
    "        arr = np.sum(particles_data!=0, axis=1)[:,0] # the number of particles in each batch\n",
    "        arr = [1 if x==0 else x for x in arr]\n",
    "        arr = np.array(arr)\n",
    "        sorted_indices_la= np.argsort(-arr)\n",
    "        particles_data = torch.from_numpy(particles_data[sorted_indices_la]).float()\n",
    "        hlf_data = hlf_data[sorted_indices_la]\n",
    "        y_data = y_data[sorted_indices_la]\n",
    "        particles_data.requires_grad = req_grad\n",
    "        particles_data = particles_data.cuda()\n",
    "        # t_seq_length= [arr[i] for i in sorted_indices_la]\n",
    "        # particles_data = torch.nn.utils.rnn.pack_padded_sequence(particles_data, t_seq_length, batch_first=True)\n",
    "\n",
    "        hlf_data.requires_grad = req_grad\n",
    "        hlf_data = hlf_data.cuda()\n",
    "\n",
    "        y_data = Variable(y_data, requires_grad=False).cuda()\n",
    "\n",
    "        trial_input = (\n",
    "            particles_data, \n",
    "            hlf_data\n",
    "        )\n",
    "    return trial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module of type <class 'torch.nn.modules.rnn.GRU'> has no rule defined and nodefault rule exists for this module type. Please, set a ruleexplicitly for this module and assure that it is appropriatefor this type of layer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m trial_input_lrp \u001b[38;5;241m=\u001b[39m get_data()\n\u001b[1;32m      5\u001b[0m lrp \u001b[38;5;241m=\u001b[39m LRP(model)\n\u001b[0;32m----> 6\u001b[0m attribution \u001b[38;5;241m=\u001b[39m \u001b[43mlrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_input_lrp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(attribution)\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/captum/attr/_core/lrp.py:193\u001b[0m, in \u001b[0;36mLRP.attribute\u001b[0;34m(self, inputs, target, additional_forward_args, return_convergence_delta, verbose)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: List[Module] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_layers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_attach_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_handles: List[RemovableHandle] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_handles: List[RemovableHandle] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/captum/attr/_core/lrp.py:294\u001b[0m, in \u001b[0;36mLRP._check_and_attach_rules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m     layer\u001b[38;5;241m.\u001b[39mrule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    295\u001b[0m         (\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no rule defined and no\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault rule exists for this module type. Please, set a rule\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplicitly for this module and assure that it is appropriate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this type of layer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         )\n\u001b[1;32m    301\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Module of type <class 'torch.nn.modules.rnn.GRU'> has no rule defined and nodefault rule exists for this module type. Please, set a ruleexplicitly for this module and assure that it is appropriatefor this type of layer."
     ]
    }
   ],
   "source": [
    "from captum.attr import LRP\n",
    "\n",
    "trial_input_lrp = get_data()\n",
    "\n",
    "lrp = LRP(model)\n",
    "attribution = lrp.attribute(trial_input_lrp, target=1)\n",
    "print(attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x Gradient (captum.attr.InputXGradient)\n",
      "--------------------RNN particles--------------------\n",
      "4 arrays = 4 max particles: l1, l2, dipho, MET\n",
      "6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET\n",
      "tensor([[ 1.0862e+00, -4.7879e-01, -6.9271e-01, -8.6824e-02, -4.6987e-02,\n",
      "          1.4419e-01],\n",
      "        [-1.1658e+00, -3.4647e-01,  5.9264e-01, -1.3464e-01, -1.1869e-01,\n",
      "          2.2870e-01],\n",
      "        [-1.3662e+00,  3.0545e-01,  1.0882e+00, -6.1627e-02, -1.1924e-02,\n",
      "          1.7489e-01],\n",
      "        [ 5.1425e-01,  2.5074e-03, -3.1560e-01,  1.6140e-02, -3.0173e-04,\n",
      "          1.1100e-03]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "--------------------DNN High Level Features--------------------\n",
      "9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \n",
      "    n_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj\n",
      "tensor([ 0.0799, -0.2580, -0.1419, -0.0764,  0.0240, -0.1035, -0.2513,  0.0443,\n",
      "        -0.0560], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import InputXGradient\n",
    "\n",
    "trial_input_ixg = get_data(batch_size=bestconf['batch_size'])\n",
    "# print('input')\n",
    "# print('-'*50)\n",
    "# print('-'*20 + 'RNN particles' + '-'*20)\n",
    "# print('4 arrays = 4 max particles: l1, l2, dipho, MET')\n",
    "# print('6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET')\n",
    "# print(trial_input_ixg[0])\n",
    "# print('-'*20 + 'DNN High Level Features' + '-'*20)\n",
    "# print('9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \\nn_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj')\n",
    "# print(trial_input_ixg[1])\n",
    "# print('='*50)\n",
    "\n",
    "model.train()\n",
    "output_rnn_ixg, output_dnn_ixg = None, None\n",
    "for fold_idx in range(skf.get_n_splits()):\n",
    "    model.load_state_dict(torch.load(f'ReallyTopclassStyle_{fold_idx}.torch'))\n",
    "    input_x_grad = InputXGradient(model)\n",
    "    attribution_ixg = input_x_grad.attribute(trial_input_ixg, target=1)\n",
    "    \n",
    "    if fold_idx == 0:\n",
    "        output_rnn_ixg = torch.mean(attribution_ixg[0], 0)\n",
    "        output_dnn_ixg = torch.mean(attribution_ixg[1], 0)\n",
    "    else:\n",
    "        output_rnn_ixg += torch.mean(attribution_ixg[0], 0)\n",
    "        output_dnn_ixg += torch.mean(attribution_ixg[1], 0)\n",
    "    \n",
    "\n",
    "print('Input x Gradient (captum.attr.InputXGradient)')\n",
    "print('-'*20 + 'RNN particles' + '-'*20)\n",
    "print('4 arrays = 4 max particles: l1, l2, dipho, MET')\n",
    "print('6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET')\n",
    "print(output_rnn_ixg / skf.get_n_splits())\n",
    "print('-'*20 + 'DNN High Level Features' + '-'*20)\n",
    "print('9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \\n    n_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj')\n",
    "print(output_dnn_ixg / skf.get_n_splits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Permutation (captum.attr.FeaturePermutation)\n",
      "--------------------RNN particles--------------------\n",
      "4 arrays = 4 max particles: l1, l2, dipho, MET\n",
      "6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET\n",
      "tensor([[ 1.0803e+00, -4.7619e-01, -6.9067e-01, -8.7585e-02, -4.8117e-02,\n",
      "          1.4357e-01],\n",
      "        [-1.1782e+00, -3.5606e-01,  5.9954e-01, -1.3248e-01, -1.1878e-01,\n",
      "          2.3600e-01],\n",
      "        [-1.3530e+00,  3.0737e-01,  1.0784e+00, -6.2604e-02, -1.1893e-02,\n",
      "          1.7370e-01],\n",
      "        [ 5.1794e-01,  3.0706e-03, -3.1958e-01,  1.5590e-02, -3.1101e-04,\n",
      "          1.1548e-03]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "--------------------DNN High Level Features--------------------\n",
      "9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \n",
      "    n_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj\n",
      "tensor([ 0.0812, -0.2633, -0.1487, -0.0765,  0.0245, -0.1023, -0.2515,  0.0450,\n",
      "        -0.0577], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import FeaturePermutation\n",
    "\n",
    "trial_input_fp = get_data(batch_size=bestconf['batch_size'], req_grad=False)\n",
    "\n",
    "# feat_perm = FeaturePermutation(model)\n",
    "# attribution_fp = feat_perm.attribute(trial_input_fp, target=1)\n",
    "# print('Feature Permutation (captum.attr.FeaturePermutation)')\n",
    "# print('-'*20 + 'RNN particles' + '-'*20)\n",
    "# print('4 arrays = 4 max particles: l1, l2, dipho, MET')\n",
    "# print('6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET')\n",
    "# # print(attribution_fp[0])\n",
    "# print(torch.mean(attribution_fp[0], 0))\n",
    "# print('-'*20 + 'DNN High Level Features' + '-'*20)\n",
    "# print('9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \\n    n_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj')\n",
    "# # print(attribution_fp[1])\n",
    "# print(torch.mean(attribution_fp[1], 0))\n",
    "\n",
    "model.eval()\n",
    "output_rnn_fp, output_dnn_fp = None, None\n",
    "for fold_idx in range(skf.get_n_splits()):\n",
    "    model.load_state_dict(torch.load(f'ReallyTopclassStyle_{fold_idx}.torch'))\n",
    "    feat_perm = FeaturePermutation(model)\n",
    "    attribution_fp = feat_perm.attribute(trial_input_fp, target=1)\n",
    "    \n",
    "    if fold_idx == 0:\n",
    "        output_rnn_fp = torch.mean(attribution_ixg[0], 0)\n",
    "        output_dnn_fp = torch.mean(attribution_ixg[1], 0)\n",
    "    else:\n",
    "        output_rnn_fp += torch.mean(attribution_ixg[0], 0)\n",
    "        output_dnn_fp += torch.mean(attribution_ixg[1], 0)\n",
    "\n",
    "print('Feature Permutation (captum.attr.FeaturePermutation)')\n",
    "print('-'*20 + 'RNN particles' + '-'*20)\n",
    "print('4 arrays = 4 max particles: l1, l2, dipho, MET')\n",
    "print('6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET')\n",
    "print(output_rnn_fp / skf.get_n_splits())\n",
    "print('-'*20 + 'DNN High Level Features' + '-'*20)\n",
    "print('9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \\n    n_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj')\n",
    "print(output_dnn_fp / skf.get_n_splits())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 11.90 GiB of which 28.62 MiB is free. Process 520612 has 256.00 MiB memory in use. Process 2824814 has 324.00 MiB memory in use. Process 3333400 has 256.00 MiB memory in use. Process 221809 has 324.00 MiB memory in use. Process 257008 has 324.00 MiB memory in use. Process 2382724 has 324.00 MiB memory in use. Process 706646 has 324.00 MiB memory in use. Process 3089060 has 324.00 MiB memory in use. Process 3089575 has 324.00 MiB memory in use. Process 3772068 has 8.77 GiB memory in use. Including non-PyTorch memory, this process has 396.00 MiB memory in use. Of the allocated memory 77.86 MiB is allocated by PyTorch, and 22.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m t_seq_length\u001b[38;5;241m=\u001b[39m [arr[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m sorted_indices_la]\n\u001b[1;32m     35\u001b[0m particles_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpack_padded_sequence(particles_data, t_seq_length, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhlf_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Unsort the predictions (to match the original data order)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# https://stackoverflow.com/questions/34159608/how-to-unsort-a-np-array-given-the-argsort\u001b[39;00m\n\u001b[1;32m     41\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(sorted_indices_la)\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m, in \u001b[0;36mInclusiveNetwork.forward\u001b[0;34m(self, particles, hlf)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, particles, hlf):\n\u001b[0;32m---> 35\u001b[0m     _, hgru \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     hgru \u001b[38;5;241m=\u001b[39m hgru[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# Get the last hidden layer\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((hlf,hgru), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1105\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m   1103\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1105\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1108\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 146.00 MiB. GPU 0 has a total capacty of 11.90 GiB of which 28.62 MiB is free. Process 520612 has 256.00 MiB memory in use. Process 2824814 has 324.00 MiB memory in use. Process 3333400 has 256.00 MiB memory in use. Process 221809 has 324.00 MiB memory in use. Process 257008 has 324.00 MiB memory in use. Process 2382724 has 324.00 MiB memory in use. Process 706646 has 324.00 MiB memory in use. Process 3089060 has 324.00 MiB memory in use. Process 3089575 has 324.00 MiB memory in use. Process 3772068 has 8.77 GiB memory in use. Including non-PyTorch memory, this process has 396.00 MiB memory in use. Of the allocated memory 77.86 MiB is allocated by PyTorch, and 22.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "def fill_array(array_to_fill, value, index, batch_size):\n",
    "    array_to_fill[index*batch_size:min((index+1)*batch_size, array_to_fill.shape[0])] = value  \n",
    "\n",
    "TPR_thresholds = [0.96, 0.935, 0.9, 0.7, 0.5, 0.3]\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "\n",
    "fprs = []\n",
    "base_tpr = np.linspace(0, 1, 5000)\n",
    "thresholds = []\n",
    "# volatile=True\n",
    "best_batch_size = bestconf['batch_size']\n",
    "# for train_index, test_index in skf.split(data_hlf, label):\n",
    "val_loader = DataLoader(ParticleHLF(data_list_test, data_hlf_test, label_test), batch_size=bestconf['batch_size'], shuffle=False)\n",
    "all_pred = np.zeros(shape=(len(data_hlf_test),2))\n",
    "all_label = np.zeros(shape=(len(data_hlf_test)))\n",
    "criterion= nn.NLLLoss()\n",
    "\n",
    "for fold_idx in range(skf.get_n_splits()):\n",
    "    model.load_state_dict(torch.load(f'ReallyTopclassStyle_{fold_idx}.torch'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (particles_data, hlf_data, y_data) in enumerate(val_loader):\n",
    "            particles_data = particles_data.numpy()\n",
    "            arr = np.sum(particles_data!=0, axis=1)[:,0] # the number of particles in the whole batch\n",
    "            arr = [1 if x==0 else x for x in arr]\n",
    "            arr = np.array(arr)\n",
    "            sorted_indices_la= np.argsort(-arr)\n",
    "            particles_data = torch.from_numpy(particles_data[sorted_indices_la]).float()\n",
    "            hlf_data = hlf_data[sorted_indices_la]\n",
    "            particles_data = Variable(particles_data).cuda()\n",
    "            hlf_data = Variable(hlf_data).cuda()\n",
    "            # particles_data = Variable(particles_data)\n",
    "            # hlf_data = Variable(hlf_data)\n",
    "            t_seq_length= [arr[i] for i in sorted_indices_la]\n",
    "            particles_data = torch.nn.utils.rnn.pack_padded_sequence(particles_data, t_seq_length, batch_first=True)\n",
    "\n",
    "            outputs = model(particles_data, hlf_data)\n",
    "\n",
    "            # Unsort the predictions (to match the original data order)\n",
    "            # https://stackoverflow.com/questions/34159608/how-to-unsort-a-np-array-given-the-argsort\n",
    "            b = np.argsort(sorted_indices_la)\n",
    "            unsorted_pred = outputs[b].data.cpu().numpy()\n",
    "\n",
    "            fill_array(all_pred, unsorted_pred, batch_idx, best_batch_size)\n",
    "            fill_array(all_label, y_data.numpy(), batch_idx, best_batch_size)\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(all_label, np.exp(all_pred)[:,1])\n",
    "\n",
    "    fpr = np.interp(base_tpr, tpr, fpr)\n",
    "    threshold = np.interp(base_tpr, tpr, threshold)\n",
    "    fpr[0] = 0.0\n",
    "    fprs.append(fpr)\n",
    "    thresholds.append(threshold)\n",
    "\n",
    "thresholds = np.array(thresholds)\n",
    "mean_thresholds = thresholds.mean(axis=0)\n",
    "\n",
    "fprs = np.array(fprs)\n",
    "mean_fprs = fprs.mean(axis=0)\n",
    "std_fprs = fprs.std(axis=0)\n",
    "fprs_right = np.minimum(mean_fprs + std_fprs, 1)\n",
    "fprs_left = np.maximum(mean_fprs - std_fprs,0)\n",
    "\n",
    "mean_area = auc(mean_fprs, base_tpr)\n",
    "\n",
    "# [tl.tolist() for tl in train_losses_arr]\n",
    "\n",
    "\n",
    "# IN_perf = {\n",
    "#     'train losses': train_losses_arr,\n",
    "#     'val losses': val_losses_arr,\n",
    "#     'fprs': fprs.tolist(),\n",
    "#     'thresholds': thresholds.tolist(),\n",
    "#     'mean_fprs': mean_fprs.tolist(),\n",
    "#     'mean_thresholds': mean_thresholds.tolist(),\n",
    "#     'base_tpr': base_tpr.tolist(),\n",
    "#     'mean_area': float(mean_area),\n",
    "#     'all_pred': all_pred.tolist(),\n",
    "#     'all_label': all_label.tolist()\n",
    "# }\n",
    "\n",
    "# with open('IN_perf.json', 'w') as f:\n",
    "#     json.dump(IN_perf, f)\n",
    "\n",
    "with open('IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(base_tpr>TPR_threshold)\n",
    "    NNtable.add_row([mean_thresholds[thres_idx], base_tpr[thres_idx], \"{:.4f} +/- {:.4f}\".format(mean_fprs[thres_idx], std_fprs[thres_idx])])\n",
    "print(NNtable)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "for fold_idx in range(skf.get_n_splits()):\n",
    "    plt.plot(range(len(IN_perf['train_losses_arr'][fold_idx])), IN_perf['train_losses_arr'][fold_idx], label=f\"Train data losses - fold {fold_idx}\", alpha=0.7)\n",
    "    plt.plot(range(len(IN_perf['train_losses_arr'][fold_idx])), IN_perf['val_losses_arr'][fold_idx], label=f\"Validation data losses - fold {fold_idx}\", alpha=0.7)\n",
    "# plt.plot(range(len(train_losses_arr[0])), train_losses_arr[0], label=\"Train data losses\")\n",
    "# plt.plot(range(len(train_losses_arr[0])), val_losses_arr[0], label=\"Validation data losses\")\n",
    "# plt.fill_betweenx(base_tpr, fprs_left, fprs_right, color='grey', alpha=0.4)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('EPOCH')\n",
    "plt.ylabel('Data Loss')\n",
    "\n",
    "# plt.figure(figsize=(9,7))\n",
    "# plt.plot(range(EPOCHS), val_losses, label=\"val losses vs. epoch\")\n",
    "# plt.fill_betweenx(base_tpr, fprs_left, fprs_right, color='grey', alpha=0.4)\n",
    "# plt.legend(loc='best')\n",
    "# plt.xlabel('EPOCH')\n",
    "# plt.ylabel('Validation data Loss')\n",
    "\n",
    "run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(IN_perf['mean_fprs'], IN_perf['base_tpr'],label=\"Run3 NN AUC (test data) = %.4f\" % mean_area)\n",
    "plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "# plt.fill_betweenx(base_tpr, fprs_left, fprs_right, color='grey', alpha=0.4)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Background contamination')\n",
    "plt.ylabel('Signal efficiency')\n",
    "#plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "#plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist(np.exp(all_pred)[all_label==0,1], bins=60, label='ttH background',alpha=0.5, density=True)\n",
    "plt.hist(np.exp(all_pred)[all_label==1,1], bins=60, label='HH signal', alpha=0.5, density=True)\n",
    "#plt.axvline(thresholds[thres_idx], ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Threshold\", fontsize=18)\n",
    "\n",
    "with h5py.File(\"ReallyInclusive_ROC.h5\",\"w\") as out:\n",
    "    out['FPR'] = mean_fprs\n",
    "    out['dFPR'] = std_fprs\n",
    "    out['TPR'] = base_tpr\n",
    "    out['Thresholds'] = mean_thresholds\n",
    "    print(\"Saved ROC.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_array(array_to_fill, value, index, batch_size):\n",
    "    array_to_fill[index*batch_size:min((index+1)*batch_size, array_to_fill.shape[0]),...] = value\n",
    "\n",
    "particles_val = np.concatenate((normed_sig_list, normed_bkg_list))\n",
    "hlf_val = np.concatenate((normed_sig_hlf, normed_bkg_hlf))\n",
    "y_val = np.concatenate((np.ones(len(normed_sig_hlf)),np.zeros(len(normed_bkg_hlf))))\n",
    "print(particles_val.shape)\n",
    "print(hlf_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "test_batch_size = 300\n",
    "\n",
    "all_pred = np.zeros(shape=(len(y_val),2))\n",
    "test_loader = DataLoader(ParticleHLF(particles_val, hlf_val, y_val), batch_size = test_batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "# volatile=True\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (particles_data, hlf_data, y_data) in enumerate(test_loader):\n",
    "        particles_data = particles_data.numpy()\n",
    "        arr = np.sum(particles_data!=0, axis=1)[:,0] # the number of particles in the whole batch\n",
    "        arr = [1 if x==0 else x for x in arr]\n",
    "        arr = np.array(arr)\n",
    "        sorted_indices_la= np.argsort(-arr)\n",
    "        particles_data = torch.from_numpy(particles_data[sorted_indices_la]).float()\n",
    "        hlf_data = hlf_data[sorted_indices_la]\n",
    "        particles_data = Variable(particles_data).cuda()\n",
    "        hlf_data = Variable(hlf_data).cuda()\n",
    "        # particles_data = Variable(particles_data)\n",
    "        # hlf_data = Variable(hlf_data)\n",
    "        t_seq_length= [arr[i] for i in sorted_indices_la]\n",
    "        particles_data = torch.nn.utils.rnn.pack_padded_sequence(particles_data, t_seq_length, batch_first=True)\n",
    "    \n",
    "        outputs = model(particles_data, hlf_data)\n",
    "    \n",
    "        # Unsort the predictions (to match the original data order)\n",
    "        # https://stackoverflow.com/questions/34159608/how-to-unsort-a-np-array-given-the-argsort\n",
    "        b = np.argsort(sorted_indices_la)\n",
    "        unsorted_pred = outputs[b].data.cpu().numpy()\n",
    "        fill_array(all_pred, unsorted_pred, batch_idx, test_batch_size)\n",
    "    \n",
    "fpr, tpr, thresholds = roc_curve(y_val, np.exp(all_pred)[:,1])\n",
    "area = auc(fpr, tpr)\n",
    "\n",
    "TPR_thresholds = [0.96, 0.935, 0.9, 0.7, 0.5, 0.3]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    NNtable.add_row([thresholds[thres_idx], tpr[thres_idx],  fpr[thres_idx]])\n",
    "print(NNtable)\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(fpr,tpr,label=\"NN AUC = {}\".format(area))\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Background contamination')\n",
    "plt.ylabel('Signal efficiency')\n",
    "#plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "#plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist(np.exp(all_pred)[y_val==0,1], bins=60, label='ttH background',alpha=0.5, density=True)\n",
    "plt.hist(np.exp(all_pred)[y_val==1,1], bins=60, label='HH signal', alpha=0.5, density=True)\n",
    "#plt.axvline(thresholds[thres_idx], ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Threshold\", fontsize=18)\n",
    "\n",
    "sig_frame = pd.DataFrame.from_records(pandas_samples['sig'])\n",
    "bkg_frame = pd.DataFrame.from_records(pandas_samples['bkg'])\n",
    "\n",
    "sig_frame['NN_score'] = pd.Series(np.exp(all_pred[:len(normed_sig_hlf),1]), index=sig_frame.index)\n",
    "bkg_frame['NN_score'] = pd.Series(np.exp(all_pred[len(normed_sig_hlf):,1]), index=bkg_frame.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check mistagged samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/261008/deep-learning-how-do-i-know-which-variables-are-important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_check = {\n",
    "    'puppiMET_sumEt': [40, 0, 2500],\n",
    "    'puppiMET_pt': [40, 0, 500],\n",
    "    'puppiMET_eta': [40, -5, 5],\n",
    "    'puppiMET_phi': [40, -3.15, 3.15],\n",
    "    'DeltaPhi_j1MET': [40, -3.15, 3,15],\n",
    "    'DeltaPhi_j2MET': [40, -3.15, 3,15],\n",
    "    'DeltaR_jg_min': [40, 0, 5],\n",
    "    'n_jets': [12, 0, 12],\n",
    "    'chi_t0': [40, 0, 1000],\n",
    "    'chi_t1': [40, 0, 1000],\n",
    "    'lepton1_pt': [40, 0, 500],\n",
    "    'lepton2_pt': [40, 0, 500],\n",
    "    # 'ptmu1': [40, 0, 500],\n",
    "    # 'ptmu2': [40, 0, 500],\n",
    "    'pt': [40, 0, 1500],\n",
    "    'lepton1_eta': [40, -5, 5],\n",
    "    'lepton2_eta': [40, -5, 5],\n",
    "    # 'etamu1': [40, -3.15, 3.15],\n",
    "    # 'etamu2': [40, -3.15, 3.15],\n",
    "    'eta': [40, -5, 5],\n",
    "    'lepton1_phi': [40, -3.15, 3.15],\n",
    "    'lepton2_phi': [40, -3.15, 3.15],\n",
    "    # 'phimu1': [40, -3.15, 3.15],\n",
    "    # 'phimu2': [40, -3.15, 3.15],\n",
    "    'phi': [40, -3.15, 3.15],\n",
    "    'abs_CosThetaStar_CS': [40, 0, 1],\n",
    "    'abs_CosThetaStar_jj': [40, 0, 1]\n",
    "}\n",
    "\n",
    "# print(thresholds)\n",
    "background_mistag_thres = thresholds[np.argmax(fpr>0.005)]\n",
    "signal_mistag_thres = thresholds[np.argmax(tpr>0.97)]\n",
    "print(\"Threshold for bkg mistag: {}\".format(background_mistag_thres))\n",
    "print(\"Threshold for signal mistag: {}\".format(signal_mistag_thres))\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist(np.exp(all_pred)[y_val==0,1], bins=60, label='ttH background',alpha=0.5, density=True)\n",
    "plt.hist(np.exp(all_pred)[y_val==1,1], bins=60, label='HH signal', alpha=0.5, density=True)\n",
    "plt.axvline(background_mistag_thres, ls='--',color='tab:gray')\n",
    "plt.axvline(signal_mistag_thres, ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Threshold\", fontsize=18)\n",
    "\n",
    "\n",
    "for feat in features_to_check:\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.hist(bkg_frame[feat], bins=features_to_check[feat][0], \n",
    "             range=(features_to_check[feat][1], features_to_check[feat][2]),\n",
    "             label='Full background',\n",
    "             histtype='stepfilled',\n",
    "             alpha=0.4,\n",
    "             density=True\n",
    "             )\n",
    "    plt.hist(sig_frame[feat], bins=features_to_check[feat][0], \n",
    "             range=(features_to_check[feat][1], features_to_check[feat][2]),\n",
    "              label='Full signal',\n",
    "              histtype='stepfilled',\n",
    "             alpha=0.4,\n",
    "             density=True,\n",
    "            )\n",
    "    plt.hist(sig_frame[feat][sig_frame['NN_score']<signal_mistag_thres], bins=features_to_check[feat][0], \n",
    "             range=(features_to_check[feat][1], features_to_check[feat][2]),\n",
    "             label='Signal predicted as bkg',\n",
    "            histtype='step',\n",
    "            linewidth=3,\n",
    "             density=True,\n",
    "            )\n",
    "    plt.hist(bkg_frame[feat][bkg_frame['NN_score']>background_mistag_thres], bins=features_to_check[feat][0], \n",
    "             range=(features_to_check[feat][1], features_to_check[feat][2]),\n",
    "             histtype='step',\n",
    "             linewidth=3,\n",
    "             label='Bkg predicted as signal',\n",
    "             density=True\n",
    "            )\n",
    "    plt.xlabel(feat, fontsize=15)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2),\n",
    "          ncol=2, fancybox=True, fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=25\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(bkg_aux_frame['dijet_mass'], bins=100, histtype='step', label='No cut',density=True)\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(bkg_aux_frame['dijet_mass'][bkg_frame['NN_score']>thresholds[thres_idx]], \n",
    "             bins=100, histtype='step', \n",
    "             label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             density=True)\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Background events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{bb}$',fontsize=fontsize)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(sig_aux_frame['dijet_mass'], bins=100, histtype='step', label='No cut', density=True)\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(sig_aux_frame['dijet_mass'][sig_frame['NN_score']>thresholds[thres_idx]], \n",
    "             bins=100, histtype='step', label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             density=True)\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Signal events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{bb}$',fontsize=fontsize)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(bkg_aux_frame['mass'], bins=100, range=(115,135), histtype='step', label='No cut', density=True)\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(bkg_aux_frame['mass'][bkg_frame['NN_score']>thresholds[thres_idx]], \n",
    "             bins=100, histtype='step', \n",
    "             range=(115,135),\n",
    "             label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             density=True)\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Background events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{\\gamma \\gamma}$',fontsize=fontsize)\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.hist(sig_aux_frame['mass'], bins=100, range=(115,135), histtype='step', label='No cut', density=True)\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(tpr>TPR_threshold)\n",
    "    #print(\"NN Signal efficiency = {} @ {} ttH background contamination\".format(tpr[thres_idx], fpr[thres_idx]))\n",
    "    plt.hist(sig_aux_frame['mass'][sig_frame['NN_score']>thresholds[thres_idx]], \n",
    "             bins=100, \n",
    "             histtype='step', \n",
    "             range=(115,135),\n",
    "             label= \"{:.0f}% signal eff\".format(100*tpr[thres_idx]),\n",
    "             density=True)\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Signal events',fontsize=fontsize)\n",
    "plt.xlabel(r'$m_{\\gamma \\gamma}$',fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"DNN_ROC.h5\",\"r\") as rocfile:\n",
    "    fpr_dnn = rocfile['FPR'][:]\n",
    "    dfpr_dnn = rocfile['dFPR'][:]\n",
    "    tpr_dnn = rocfile['TPR'][:]\n",
    "    thres_dnn = rocfile['Thresholds'][:]\n",
    "    area_dnn = auc(fpr_dnn, tpr_dnn)\n",
    "    fprs_dnn_right = np.minimum(fpr_dnn + dfpr_dnn, 1)\n",
    "    fprs_dnn_left = np.maximum(fpr_dnn - dfpr_dnn,0)\n",
    "    darea_dnn = (1-auc(tpr_dnn,fprs_dnn_left))-area_dnn\n",
    "    \n",
    "with h5py.File(\"BDT_ROC.h5\",\"r\") as rocfile:\n",
    "    fpr_bdt = rocfile['FPR'][:]\n",
    "    tpr_bdt = rocfile['TPR'][:]\n",
    "    thres_bdt = rocfile['Thresholds'][:]\n",
    "    area_bdt = auc(fpr_bdt, tpr_bdt)\n",
    "\n",
    "with h5py.File(\"ReallyInclusive_ROC.h5\",\"r\") as rocfile:\n",
    "    fpr_inc = rocfile['FPR'][:]\n",
    "    dfpr_inc = rocfile['dFPR'][:]\n",
    "    tpr_inc = rocfile['TPR'][:]\n",
    "    thres_inc = rocfile['Thresholds'][:]\n",
    "    area_inc = auc(fpr_inc, tpr_inc)\n",
    "    fprs_inc_right = np.minimum(fpr_inc + dfpr_dnn, 1)\n",
    "    fprs_inc_left = np.maximum(fpr_inc - dfpr_dnn,0)\n",
    "    darea_inc = (1-auc(tpr_inc,fprs_inc_left))-area_inc\n",
    "\n",
    "### Compare\n",
    "plt.figure(figsize=(9,7))\n",
    "\n",
    "plt.plot(fpr_bdt,tpr_bdt,color='blue',label=\"BDT AUC = {:.4f}\".format(area_bdt))\n",
    "\n",
    "\n",
    "plt.plot(fpr_dnn,tpr_dnn,color='green',label=r\"VanillaDNN AUC = {:.4f}$\\pm${:.4f}\".format(area_dnn, darea_dnn))\n",
    "plt.fill_betweenx(tpr_dnn, fprs_dnn_left, fprs_dnn_right, color='green', alpha=0.3)\n",
    "\n",
    "\n",
    "plt.plot(fpr_inc,tpr_inc,color='orange',label=r\"InclusiveNet AUC = {:.4f}$\\pm${:.4f}\".format(area_inc, darea_inc))\n",
    "plt.fill_betweenx(tpr_inc, fprs_inc_left, fprs_inc_right, color='orange', alpha=0.3)\n",
    "\n",
    "plt.xlabel('Background contamination',fontsize=18)\n",
    "plt.ylabel('Signal efficiency',fontsize=18)\n",
    "# # plt.xlim(0.01,0.6)\n",
    "# # plt.ylim(0.2,1)\n",
    "# plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "# plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "plt.legend(loc='best',fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR_thresholds = [0.96, 0.935, 0.9, 0.7, 0.5]\n",
    "\n",
    "table = PrettyTable(['Signal Efficiency','BDT','VanillaDNN','InclusiveNet'])\n",
    "table.get_string(title=\"Background contamination at different signal efficiencies\")\n",
    "table.float_format = \".2\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_dnn = np.argmax(tpr_dnn>TPR_threshold)\n",
    "    thres_bdt = np.argmax(tpr_bdt>TPR_threshold)\n",
    "    thres_inc = np.argmax(tpr_inc>TPR_threshold)\n",
    "\n",
    "    table.add_row([\"{:.2f}%\".format(100*(tpr_dnn[thres_dnn])),  \"{:.2f}%\".format(100*fpr_bdt[thres_bdt]), \"({:.2f} +/- {:.2f})%\".format(100*fpr_dnn[thres_dnn], 100*dfpr_dnn[thres_dnn]), \"({:.2f} +/- {:.2f})%\".format(100*fpr_inc[thres_inc], 100*dfpr_inc[thres_inc])])\n",
    "print(table.get_string(title=\"Background contamination at different signal efficiencies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'puppiMET_sumEt', 'puppiMET_pt', 'puppiMET_eta', 'puppiMET_phi', # MET variables\n",
    "    'DeltaPhi_j1MET', 'DeltaPhi_j2MET', # jet-MET variables\n",
    "    'DeltaR_jg_min', 'n_jets', 'chi_t0', 'chi_t1', # jet variables\n",
    "    'lepton1_pt' ,'lepton2_pt', 'pt', # lepton and diphoton pt\n",
    "    'lepton1_eta', 'lepton2_eta', 'eta', # lepton and diphoton eta\n",
    "    'lepton1_phi', 'lepton2_phi', 'phi', # lepton and diphoton phi\n",
    "    'abs_CosThetaStar_CS', 'abs_CosThetaStar_jj' # angular variables\n",
    "]\n",
    "\n",
    "for fea in features:\n",
    "    plt.figure()\n",
    "    plt.hist(sig_frame[fea][sig_frame[fea]!=0], bins=40, density=True, \n",
    "             histtype='stepfilled', alpha=0.3, label='GluGluToHHTo2B2G Signal')\n",
    "    plt.hist(bkg_frame[fea][bkg_frame[fea]!=0], bins=40, density=True, \n",
    "             histtype='stepfilled', alpha=0.45, label='ttHToGG Background')\n",
    "    plt.xlabel(fea,fontsize=15)\n",
    "    plt.legend(loc='best',fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "puppiMET_sumEt\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 4, 6)\n",
      "background number: (170754, 4, 6)\n",
      "Data list: (341508, 4, 6)\n",
      "Data HLF: (341508, 8)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.6753\n",
      "validation Loss: 0.0001 Acc: 84.6311\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 84.6918\n",
      "validation Loss: 0.0001 Acc: 84.9650\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 84.4054\n",
      "validation Loss: 0.0001 Acc: 85.4081\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 84.9778\n",
      "validation Loss: 0.0001 Acc: 85.9234\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.1306\n",
      "validation Loss: 0.0001 Acc: 86.4778\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 86.4937\n",
      "validation Loss: 0.0001 Acc: 86.8858\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 86.5552\n",
      "validation Loss: 0.0001 Acc: 86.0289\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 86.5840\n",
      "validation Loss: 0.0001 Acc: 86.8351\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 86.6947\n",
      "validation Loss: 0.0001 Acc: 86.9092\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 86.8172\n",
      "validation Loss: 0.0001 Acc: 87.1279\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 86.7894\n",
      "validation Loss: 0.0001 Acc: 87.0439\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 86.8504\n",
      "validation Loss: 0.0001 Acc: 87.1259\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 86.8753\n",
      "validation Loss: 0.0001 Acc: 87.0557\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 86.8773\n",
      "validation Loss: 0.0001 Acc: 87.0361\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 86.9471\n",
      "validation Loss: 0.0001 Acc: 87.1279\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 86.9671\n",
      "validation Loss: 0.0001 Acc: 87.0088\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 86.9871\n",
      "validation Loss: 0.0001 Acc: 87.0908\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 86.8904\n",
      "validation Loss: 0.0001 Acc: 87.1240\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.0398\n",
      "validation Loss: 0.0001 Acc: 86.9307\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.0739\n",
      "validation Loss: 0.0001 Acc: 87.2138\n",
      "Saving..\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.1403\n",
      "validation Loss: 0.0001 Acc: 87.3699\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.1315\n",
      "validation Loss: 0.0001 Acc: 87.3465\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.1613\n",
      "validation Loss: 0.0001 Acc: 87.3387\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.1803\n",
      "validation Loss: 0.0001 Acc: 87.3778\n",
      "Saving..\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.1667\n",
      "validation Loss: 0.0001 Acc: 87.3250\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.2745\n",
      "validation Loss: 0.0001 Acc: 87.3895\n",
      "Saving..\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.2604\n",
      "validation Loss: 0.0001 Acc: 87.3172\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.2487\n",
      "validation Loss: 0.0001 Acc: 87.2235\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.2545\n",
      "validation Loss: 0.0001 Acc: 87.3270\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.2692\n",
      "validation Loss: 0.0001 Acc: 87.3094\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.3082\n",
      "validation Loss: 0.0001 Acc: 87.3875\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.3136\n",
      "validation Loss: 0.0001 Acc: 87.4207\n",
      "Saving..\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.3019\n",
      "validation Loss: 0.0001 Acc: 87.4168\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.3263\n",
      "validation Loss: 0.0001 Acc: 87.3465\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.3243\n",
      "validation Loss: 0.0001 Acc: 87.3895\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.3321\n",
      "validation Loss: 0.0001 Acc: 87.4129\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.3346\n",
      "validation Loss: 0.0001 Acc: 87.3992\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.3180\n",
      "validation Loss: 0.0001 Acc: 87.4187\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 87.3468\n",
      "validation Loss: 0.0001 Acc: 87.3895\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 87.3433\n",
      "validation Loss: 0.0001 Acc: 87.3621\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 87.3575\n",
      "validation Loss: 0.0001 Acc: 87.4246\n",
      "Saving..\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.3404\n",
      "validation Loss: 0.0001 Acc: 87.4324\n",
      "Saving..\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 87.3316\n",
      "validation Loss: 0.0001 Acc: 87.4109\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.3067\n",
      "validation Loss: 0.0001 Acc: 87.4344\n",
      "Saving..\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.3956\n",
      "validation Loss: 0.0001 Acc: 87.4636\n",
      "Saving..\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.3736\n",
      "validation Loss: 0.0001 Acc: 87.4480\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.3643\n",
      "validation Loss: 0.0001 Acc: 87.3895\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 87.3263\n",
      "validation Loss: 0.0001 Acc: 87.4344\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 87.3677\n",
      "validation Loss: 0.0001 Acc: 87.4383\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 87.3487\n",
      "validation Loss: 0.0001 Acc: 87.4109\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.3614\n",
      "validation Loss: 0.0001 Acc: 87.3992\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.3575\n",
      "validation Loss: 0.0001 Acc: 87.4090\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 87.3599\n",
      "validation Loss: 0.0001 Acc: 87.4148\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.3526\n",
      "validation Loss: 0.0001 Acc: 87.4187\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 87.3995\n",
      "validation Loss: 0.0001 Acc: 87.4070\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 87.3707\n",
      "validation Loss: 0.0001 Acc: 87.4207\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.3814\n",
      "validation Loss: 0.0001 Acc: 87.4305\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 87.3843\n",
      "validation Loss: 0.0001 Acc: 87.4187\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 87.3648\n",
      "validation Loss: 0.0001 Acc: 87.4031\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 87.3760\n",
      "validation Loss: 0.0001 Acc: 87.4109\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 87.3882\n",
      "validation Loss: 0.0001 Acc: 87.3992\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 87.3858\n",
      "validation Loss: 0.0001 Acc: 87.4187\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 87.3775\n",
      "validation Loss: 0.0001 Acc: 87.4090\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 87.3565\n",
      "validation Loss: 0.0001 Acc: 87.4012\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 87.3951\n",
      "validation Loss: 0.0001 Acc: 87.3992\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 87.4004\n",
      "validation Loss: 0.0001 Acc: 87.4012\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 87.3965\n",
      "validation Loss: 0.0001 Acc: 87.4109\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 87.4039\n",
      "validation Loss: 0.0001 Acc: 87.4148\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 87.4185\n",
      "validation Loss: 0.0001 Acc: 87.4109\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 87.3868\n",
      "validation Loss: 0.0001 Acc: 87.4090\n",
      "Best val acc: 87.463646\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.3907\n",
      "validation Loss: 0.0001 Acc: 87.4204\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.3673\n",
      "validation Loss: 0.0001 Acc: 87.4204\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.3824\n",
      "validation Loss: 0.0001 Acc: 87.4204\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.3781\n",
      "validation Loss: 0.0001 Acc: 87.4204\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.4039\n",
      "validation Loss: 0.0001 Acc: 87.4244\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.3527\n",
      "validation Loss: 0.0001 Acc: 87.4165\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.4078\n",
      "validation Loss: 0.0001 Acc: 87.4087\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.4064\n",
      "validation Loss: 0.0001 Acc: 87.4146\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.3839\n",
      "validation Loss: 0.0001 Acc: 87.4165\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.3805\n",
      "validation Loss: 0.0001 Acc: 87.4165\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.3571\n",
      "validation Loss: 0.0001 Acc: 87.4165\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.3698\n",
      "validation Loss: 0.0001 Acc: 87.4165\n",
      "Early stopped.\n",
      "Best val acc: 87.424355\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.3927\n",
      "validation Loss: 0.0001 Acc: 87.4517\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.3888\n",
      "validation Loss: 0.0001 Acc: 87.4536\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.3649\n",
      "validation Loss: 0.0001 Acc: 87.4517\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.3873\n",
      "validation Loss: 0.0001 Acc: 87.4575\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.4117\n",
      "validation Loss: 0.0001 Acc: 87.4497\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.3917\n",
      "validation Loss: 0.0001 Acc: 87.4478\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.3966\n",
      "validation Loss: 0.0001 Acc: 87.4556\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.3878\n",
      "validation Loss: 0.0001 Acc: 87.4497\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.4161\n",
      "validation Loss: 0.0001 Acc: 87.4517\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.4029\n",
      "validation Loss: 0.0001 Acc: 87.4536\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.3698\n",
      "validation Loss: 0.0001 Acc: 87.4478\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.3990\n",
      "validation Loss: 0.0001 Acc: 87.4497\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.3971\n",
      "validation Loss: 0.0001 Acc: 87.4478\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.3737\n",
      "validation Loss: 0.0001 Acc: 87.4497\n",
      "Early stopped.\n",
      "Best val acc: 87.457535\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.3502\n",
      "validation Loss: 0.0001 Acc: 87.4790\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.3707\n",
      "validation Loss: 0.0001 Acc: 87.4790\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.3732\n",
      "validation Loss: 0.0001 Acc: 87.4771\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.3634\n",
      "validation Loss: 0.0001 Acc: 87.4751\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.3673\n",
      "validation Loss: 0.0001 Acc: 87.4732\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.3951\n",
      "validation Loss: 0.0001 Acc: 87.4693\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.3776\n",
      "validation Loss: 0.0001 Acc: 87.4693\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.3781\n",
      "validation Loss: 0.0001 Acc: 87.4693\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.3785\n",
      "validation Loss: 0.0001 Acc: 87.4693\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.3829\n",
      "validation Loss: 0.0001 Acc: 87.4693\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.4103\n",
      "validation Loss: 0.0001 Acc: 87.4693\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.4108\n",
      "validation Loss: 0.0001 Acc: 87.4712\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.3659\n",
      "validation Loss: 0.0001 Acc: 87.4712\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.3629\n",
      "validation Loss: 0.0001 Acc: 87.4712\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.4200\n",
      "validation Loss: 0.0001 Acc: 87.4712\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.3541\n",
      "validation Loss: 0.0001 Acc: 87.4712\n",
      "Early stopped.\n",
      "Best val acc: 87.479012\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.4337\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.4049\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.3854\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.3966\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.4186\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.4015\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.4249\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.4156\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.4298\n",
      "validation Loss: 0.0001 Acc: 87.3307\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.3986\n",
      "validation Loss: 0.0001 Acc: 87.3307\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.3912\n",
      "validation Loss: 0.0001 Acc: 87.3307\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.4025\n",
      "validation Loss: 0.0001 Acc: 87.3307\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.4449\n",
      "validation Loss: 0.0001 Acc: 87.3307\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.4171\n",
      "validation Loss: 0.0001 Acc: 87.3307\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.3844\n",
      "validation Loss: 0.0001 Acc: 87.3307\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.4098\n",
      "validation Loss: 0.0001 Acc: 87.3307\n",
      "Early stopped.\n",
      "Best val acc: 87.332603\n",
      "----------\n",
      "Average best_acc across k-fold: 87.43142700195312\n",
      "puppiMET_pt\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.3069\n",
      "validation Loss: 0.0001 Acc: 84.4867\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 84.6269\n",
      "validation Loss: 0.0001 Acc: 83.6024\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.8148\n",
      "validation Loss: 0.0001 Acc: 86.5247\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 86.8080\n",
      "validation Loss: 0.0001 Acc: 87.0635\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0554\n",
      "validation Loss: 0.0001 Acc: 87.1220\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 86.9968\n",
      "validation Loss: 0.0001 Acc: 87.3192\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.0969\n",
      "validation Loss: 0.0001 Acc: 87.0654\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.1764\n",
      "validation Loss: 0.0001 Acc: 87.0752\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.1969\n",
      "validation Loss: 0.0001 Acc: 87.0498\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.2003\n",
      "validation Loss: 0.0001 Acc: 87.4597\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.2872\n",
      "validation Loss: 0.0001 Acc: 87.3641\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.2540\n",
      "validation Loss: 0.0001 Acc: 87.4988\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.2535\n",
      "validation Loss: 0.0001 Acc: 87.3895\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.3638\n",
      "validation Loss: 0.0001 Acc: 87.5339\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.3497\n",
      "validation Loss: 0.0001 Acc: 87.5281\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.3570\n",
      "validation Loss: 0.0001 Acc: 87.4656\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.4619\n",
      "validation Loss: 0.0001 Acc: 87.5359\n",
      "Saving..\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.5239\n",
      "validation Loss: 0.0001 Acc: 87.3699\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.4917\n",
      "validation Loss: 0.0001 Acc: 87.5808\n",
      "Saving..\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.5898\n",
      "validation Loss: 0.0001 Acc: 87.6686\n",
      "Saving..\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.5566\n",
      "validation Loss: 0.0001 Acc: 87.6706\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.5278\n",
      "validation Loss: 0.0001 Acc: 87.6374\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.6074\n",
      "validation Loss: 0.0001 Acc: 87.7057\n",
      "Saving..\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.6127\n",
      "validation Loss: 0.0001 Acc: 87.7369\n",
      "Saving..\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.5893\n",
      "validation Loss: 0.0001 Acc: 87.7428\n",
      "Saving..\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.7099\n",
      "validation Loss: 0.0001 Acc: 87.7818\n",
      "Saving..\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.7504\n",
      "validation Loss: 0.0001 Acc: 87.8365\n",
      "Saving..\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.7489\n",
      "validation Loss: 0.0001 Acc: 87.9263\n",
      "Saving..\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.7831\n",
      "validation Loss: 0.0001 Acc: 87.7701\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.7616\n",
      "validation Loss: 0.0001 Acc: 87.5105\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.7484\n",
      "validation Loss: 0.0001 Acc: 87.8990\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.8397\n",
      "validation Loss: 0.0001 Acc: 87.9400\n",
      "Saving..\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.8367\n",
      "validation Loss: 0.0001 Acc: 87.9321\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.8826\n",
      "validation Loss: 0.0001 Acc: 87.8872\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.8689\n",
      "validation Loss: 0.0001 Acc: 87.8580\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.8524\n",
      "validation Loss: 0.0001 Acc: 87.8287\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.8792\n",
      "validation Loss: 0.0001 Acc: 87.9751\n",
      "Saving..\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.8748\n",
      "validation Loss: 0.0001 Acc: 87.9400\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 87.9246\n",
      "validation Loss: 0.0001 Acc: 87.8677\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 87.8929\n",
      "validation Loss: 0.0001 Acc: 87.9439\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 87.8870\n",
      "validation Loss: 0.0001 Acc: 87.9029\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.9348\n",
      "validation Loss: 0.0001 Acc: 87.9400\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 87.9158\n",
      "validation Loss: 0.0001 Acc: 87.9888\n",
      "Saving..\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.9197\n",
      "validation Loss: 0.0001 Acc: 88.0063\n",
      "Saving..\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.9480\n",
      "validation Loss: 0.0001 Acc: 88.0122\n",
      "Saving..\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.9319\n",
      "validation Loss: 0.0001 Acc: 87.9400\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.9134\n",
      "validation Loss: 0.0001 Acc: 88.0200\n",
      "Saving..\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 87.9485\n",
      "validation Loss: 0.0001 Acc: 88.0395\n",
      "Saving..\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 87.9422\n",
      "validation Loss: 0.0001 Acc: 87.9809\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 87.9734\n",
      "validation Loss: 0.0001 Acc: 88.0102\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.9680\n",
      "validation Loss: 0.0001 Acc: 87.9653\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.9749\n",
      "validation Loss: 0.0001 Acc: 88.0473\n",
      "Saving..\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 87.9719\n",
      "validation Loss: 0.0001 Acc: 88.0454\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.9661\n",
      "validation Loss: 0.0001 Acc: 88.0473\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 87.9602\n",
      "validation Loss: 0.0001 Acc: 88.0278\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 87.9558\n",
      "validation Loss: 0.0001 Acc: 88.0376\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.9612\n",
      "validation Loss: 0.0001 Acc: 88.0239\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 87.9954\n",
      "validation Loss: 0.0001 Acc: 87.9790\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 87.9827\n",
      "validation Loss: 0.0001 Acc: 87.9868\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 87.9978\n",
      "validation Loss: 0.0001 Acc: 88.0571\n",
      "Saving..\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 87.9905\n",
      "validation Loss: 0.0001 Acc: 88.0317\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 87.9841\n",
      "validation Loss: 0.0001 Acc: 88.0317\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.0515\n",
      "validation Loss: 0.0001 Acc: 88.0454\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 87.9666\n",
      "validation Loss: 0.0001 Acc: 88.0376\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 87.9807\n",
      "validation Loss: 0.0001 Acc: 88.0532\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 88.0041\n",
      "validation Loss: 0.0001 Acc: 88.0473\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 87.9905\n",
      "validation Loss: 0.0001 Acc: 88.0805\n",
      "Saving..\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 87.9841\n",
      "validation Loss: 0.0001 Acc: 88.0395\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 88.0041\n",
      "validation Loss: 0.0001 Acc: 88.0473\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 88.0212\n",
      "validation Loss: 0.0001 Acc: 88.0376\n",
      "Best val acc: 88.080505\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9915\n",
      "validation Loss: 0.0001 Acc: 87.9592\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0208\n",
      "validation Loss: 0.0001 Acc: 87.9319\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.9886\n",
      "validation Loss: 0.0001 Acc: 87.9495\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.0188\n",
      "validation Loss: 0.0001 Acc: 87.9631\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.9891\n",
      "validation Loss: 0.0001 Acc: 87.9651\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.0266\n",
      "validation Loss: 0.0001 Acc: 87.9378\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9739\n",
      "validation Loss: 0.0001 Acc: 87.9768\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.0135\n",
      "validation Loss: 0.0001 Acc: 87.9631\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9925\n",
      "validation Loss: 0.0001 Acc: 87.9651\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9749\n",
      "validation Loss: 0.0001 Acc: 87.9690\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0120\n",
      "validation Loss: 0.0001 Acc: 87.9729\n",
      "Early stopped.\n",
      "Best val acc: 87.976807\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9652\n",
      "validation Loss: 0.0001 Acc: 88.1740\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.9486\n",
      "validation Loss: 0.0001 Acc: 88.1447\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.9896\n",
      "validation Loss: 0.0001 Acc: 88.1701\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.9866\n",
      "validation Loss: 0.0001 Acc: 88.1388\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.9695\n",
      "validation Loss: 0.0001 Acc: 88.1525\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9227\n",
      "validation Loss: 0.0001 Acc: 88.1466\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9881\n",
      "validation Loss: 0.0001 Acc: 88.1427\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.9534\n",
      "validation Loss: 0.0001 Acc: 88.1466\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9822\n",
      "validation Loss: 0.0001 Acc: 88.1525\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9730\n",
      "validation Loss: 0.0001 Acc: 88.1564\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.9954\n",
      "validation Loss: 0.0001 Acc: 88.1505\n",
      "Early stopped.\n",
      "Best val acc: 88.173973\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.0569\n",
      "validation Loss: 0.0001 Acc: 87.9417\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0018\n",
      "validation Loss: 0.0001 Acc: 87.9397\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.9974\n",
      "validation Loss: 0.0001 Acc: 87.9339\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.0174\n",
      "validation Loss: 0.0001 Acc: 87.9300\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.0252\n",
      "validation Loss: 0.0001 Acc: 87.9319\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.0237\n",
      "validation Loss: 0.0001 Acc: 87.9495\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9944\n",
      "validation Loss: 0.0001 Acc: 87.9456\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.9920\n",
      "validation Loss: 0.0001 Acc: 87.9475\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9822\n",
      "validation Loss: 0.0001 Acc: 87.9417\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9671\n",
      "validation Loss: 0.0001 Acc: 87.9378\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0032\n",
      "validation Loss: 0.0001 Acc: 87.9397\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.0125\n",
      "validation Loss: 0.0001 Acc: 87.9397\n",
      "Early stopped.\n",
      "Best val acc: 87.949478\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9705\n",
      "validation Loss: 0.0001 Acc: 88.1096\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0105\n",
      "validation Loss: 0.0001 Acc: 88.1056\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.9622\n",
      "validation Loss: 0.0001 Acc: 88.1076\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.9691\n",
      "validation Loss: 0.0001 Acc: 88.1076\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.9778\n",
      "validation Loss: 0.0001 Acc: 88.1076\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9910\n",
      "validation Loss: 0.0001 Acc: 88.1096\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9876\n",
      "validation Loss: 0.0001 Acc: 88.1076\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.9661\n",
      "validation Loss: 0.0001 Acc: 88.1076\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.0110\n",
      "validation Loss: 0.0001 Acc: 88.1096\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9891\n",
      "validation Loss: 0.0001 Acc: 88.1076\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.9842\n",
      "validation Loss: 0.0001 Acc: 88.1076\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.9764\n",
      "validation Loss: 0.0001 Acc: 88.1076\n",
      "Early stopped.\n",
      "Best val acc: 88.109550\n",
      "----------\n",
      "Average best_acc across k-fold: 88.05806732177734\n",
      "puppiMET_eta\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.1853\n",
      "validation Loss: 0.0001 Acc: 83.9323\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 84.6772\n",
      "validation Loss: 0.0001 Acc: 84.6468\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.9002\n",
      "validation Loss: 0.0001 Acc: 85.8336\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 86.7460\n",
      "validation Loss: 0.0001 Acc: 86.4954\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.9793\n",
      "validation Loss: 0.0001 Acc: 87.2333\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.1325\n",
      "validation Loss: 0.0001 Acc: 87.1825\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.1471\n",
      "validation Loss: 0.0001 Acc: 87.3153\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.2804\n",
      "validation Loss: 0.0001 Acc: 87.1415\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.2535\n",
      "validation Loss: 0.0001 Acc: 87.3231\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.3214\n",
      "validation Loss: 0.0001 Acc: 87.3172\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.3716\n",
      "validation Loss: 0.0001 Acc: 87.3075\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.3614\n",
      "validation Loss: 0.0001 Acc: 87.3699\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.4131\n",
      "validation Loss: 0.0001 Acc: 87.5437\n",
      "Saving..\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.4078\n",
      "validation Loss: 0.0001 Acc: 87.4402\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.4536\n",
      "validation Loss: 0.0001 Acc: 87.3856\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.3629\n",
      "validation Loss: 0.0001 Acc: 87.2762\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.5122\n",
      "validation Loss: 0.0001 Acc: 87.4968\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.3380\n",
      "validation Loss: 0.0001 Acc: 87.5769\n",
      "Saving..\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.5210\n",
      "validation Loss: 0.0001 Acc: 87.6315\n",
      "Saving..\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.5527\n",
      "validation Loss: 0.0001 Acc: 87.6491\n",
      "Saving..\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.5444\n",
      "validation Loss: 0.0001 Acc: 87.5827\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.5488\n",
      "validation Loss: 0.0001 Acc: 87.6296\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.5708\n",
      "validation Loss: 0.0001 Acc: 87.3582\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.5415\n",
      "validation Loss: 0.0001 Acc: 87.5593\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.6449\n",
      "validation Loss: 0.0001 Acc: 87.6510\n",
      "Saving..\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.5805\n",
      "validation Loss: 0.0001 Acc: 87.7135\n",
      "Saving..\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.7050\n",
      "validation Loss: 0.0001 Acc: 87.8502\n",
      "Saving..\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.7552\n",
      "validation Loss: 0.0001 Acc: 87.8580\n",
      "Saving..\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.7899\n",
      "validation Loss: 0.0001 Acc: 87.8463\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.7718\n",
      "validation Loss: 0.0001 Acc: 87.8443\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.8265\n",
      "validation Loss: 0.0001 Acc: 87.7311\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.7474\n",
      "validation Loss: 0.0001 Acc: 87.6471\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.8519\n",
      "validation Loss: 0.0001 Acc: 88.0044\n",
      "Saving..\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.8895\n",
      "validation Loss: 0.0001 Acc: 87.9595\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.8811\n",
      "validation Loss: 0.0001 Acc: 87.8892\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.8924\n",
      "validation Loss: 0.0001 Acc: 87.8833\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.8924\n",
      "validation Loss: 0.0001 Acc: 87.8931\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.9070\n",
      "validation Loss: 0.0001 Acc: 87.9770\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 87.9158\n",
      "validation Loss: 0.0001 Acc: 87.9595\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 87.9534\n",
      "validation Loss: 0.0001 Acc: 87.9927\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 87.9422\n",
      "validation Loss: 0.0001 Acc: 87.9927\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.9280\n",
      "validation Loss: 0.0001 Acc: 88.0161\n",
      "Saving..\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 87.9597\n",
      "validation Loss: 0.0001 Acc: 88.0278\n",
      "Saving..\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.9636\n",
      "validation Loss: 0.0001 Acc: 88.0395\n",
      "Saving..\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.9763\n",
      "validation Loss: 0.0001 Acc: 88.0239\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.9548\n",
      "validation Loss: 0.0001 Acc: 88.0317\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.9719\n",
      "validation Loss: 0.0001 Acc: 88.0395\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 87.9553\n",
      "validation Loss: 0.0001 Acc: 88.0102\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 87.9749\n",
      "validation Loss: 0.0001 Acc: 88.0688\n",
      "Saving..\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 87.9485\n",
      "validation Loss: 0.0001 Acc: 88.0317\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.9958\n",
      "validation Loss: 0.0001 Acc: 87.9946\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.9749\n",
      "validation Loss: 0.0001 Acc: 88.0434\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 87.9578\n",
      "validation Loss: 0.0001 Acc: 88.0063\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.9861\n",
      "validation Loss: 0.0001 Acc: 88.0337\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.0129\n",
      "validation Loss: 0.0001 Acc: 88.0786\n",
      "Saving..\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 87.9807\n",
      "validation Loss: 0.0001 Acc: 88.0317\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.9612\n",
      "validation Loss: 0.0001 Acc: 88.0415\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 87.9295\n",
      "validation Loss: 0.0001 Acc: 88.0493\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 87.9783\n",
      "validation Loss: 0.0001 Acc: 88.0161\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.0217\n",
      "validation Loss: 0.0001 Acc: 88.0493\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 87.9851\n",
      "validation Loss: 0.0001 Acc: 88.0434\n",
      "Early stopped.\n",
      "Best val acc: 88.078552\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.0037\n",
      "validation Loss: 0.0001 Acc: 88.0881\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.9813\n",
      "validation Loss: 0.0001 Acc: 88.1037\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.9622\n",
      "validation Loss: 0.0001 Acc: 88.0842\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.0013\n",
      "validation Loss: 0.0001 Acc: 88.0861\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.0037\n",
      "validation Loss: 0.0001 Acc: 88.0939\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9486\n",
      "validation Loss: 0.0001 Acc: 88.0920\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9822\n",
      "validation Loss: 0.0001 Acc: 88.0744\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.9652\n",
      "validation Loss: 0.0001 Acc: 88.1056\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9691\n",
      "validation Loss: 0.0001 Acc: 88.0861\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.0271\n",
      "validation Loss: 0.0001 Acc: 88.0764\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0018\n",
      "validation Loss: 0.0001 Acc: 88.0783\n",
      "Early stopped.\n",
      "Best val acc: 88.105644\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.0325\n",
      "validation Loss: 0.0001 Acc: 87.9788\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0022\n",
      "validation Loss: 0.0001 Acc: 87.9514\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.0188\n",
      "validation Loss: 0.0001 Acc: 87.9456\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.0305\n",
      "validation Loss: 0.0001 Acc: 87.9631\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.9944\n",
      "validation Loss: 0.0001 Acc: 87.9514\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.0384\n",
      "validation Loss: 0.0001 Acc: 87.9456\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.0242\n",
      "validation Loss: 0.0001 Acc: 87.9397\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.0223\n",
      "validation Loss: 0.0001 Acc: 87.9456\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.0525\n",
      "validation Loss: 0.0001 Acc: 87.9456\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.0271\n",
      "validation Loss: 0.0001 Acc: 87.9456\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0086\n",
      "validation Loss: 0.0001 Acc: 87.9475\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.0335\n",
      "validation Loss: 0.0001 Acc: 87.9417\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.0291\n",
      "validation Loss: 0.0001 Acc: 87.9436\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.0281\n",
      "validation Loss: 0.0001 Acc: 87.9417\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.0227\n",
      "validation Loss: 0.0001 Acc: 87.9436\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.0325\n",
      "validation Loss: 0.0001 Acc: 87.9397\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.0149\n",
      "validation Loss: 0.0001 Acc: 87.9436\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.0388\n",
      "validation Loss: 0.0001 Acc: 87.9417\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.0408\n",
      "validation Loss: 0.0001 Acc: 87.9417\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 88.0003\n",
      "validation Loss: 0.0001 Acc: 87.9417\n",
      "Early stopped.\n",
      "Best val acc: 87.978760\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9876\n",
      "validation Loss: 0.0001 Acc: 88.0529\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0047\n",
      "validation Loss: 0.0001 Acc: 88.0529\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.0315\n",
      "validation Loss: 0.0001 Acc: 88.0529\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.9676\n",
      "validation Loss: 0.0001 Acc: 88.0549\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.0115\n",
      "validation Loss: 0.0001 Acc: 88.0568\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.0135\n",
      "validation Loss: 0.0001 Acc: 88.0549\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.0086\n",
      "validation Loss: 0.0001 Acc: 88.0549\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.0154\n",
      "validation Loss: 0.0001 Acc: 88.0549\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9871\n",
      "validation Loss: 0.0001 Acc: 88.0549\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9881\n",
      "validation Loss: 0.0001 Acc: 88.0549\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0042\n",
      "validation Loss: 0.0001 Acc: 88.0549\n",
      "Early stopped.\n",
      "Best val acc: 88.056839\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9608\n",
      "validation Loss: 0.0001 Acc: 88.1837\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0086\n",
      "validation Loss: 0.0001 Acc: 88.1837\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.9544\n",
      "validation Loss: 0.0001 Acc: 88.1837\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.9505\n",
      "validation Loss: 0.0001 Acc: 88.1837\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.9832\n",
      "validation Loss: 0.0001 Acc: 88.1837\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9388\n",
      "validation Loss: 0.0001 Acc: 88.1837\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9320\n",
      "validation Loss: 0.0001 Acc: 88.1837\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.9803\n",
      "validation Loss: 0.0001 Acc: 88.1837\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9813\n",
      "validation Loss: 0.0001 Acc: 88.1857\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9759\n",
      "validation Loss: 0.0001 Acc: 88.1857\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.9896\n",
      "validation Loss: 0.0001 Acc: 88.1837\n",
      "Early stopped.\n",
      "Best val acc: 88.185684\n",
      "----------\n",
      "Average best_acc across k-fold: 88.08109283447266\n",
      "puppiMET_phi\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 82.2741\n",
      "validation Loss: 0.0001 Acc: 84.4340\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.0969\n",
      "validation Loss: 0.0001 Acc: 86.0074\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 86.5366\n",
      "validation Loss: 0.0001 Acc: 86.8487\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 86.9353\n",
      "validation Loss: 0.0001 Acc: 86.3822\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.8529\n",
      "validation Loss: 0.0001 Acc: 86.7043\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.1476\n",
      "validation Loss: 0.0001 Acc: 86.8839\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.1667\n",
      "validation Loss: 0.0001 Acc: 86.6496\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.1154\n",
      "validation Loss: 0.0001 Acc: 86.7258\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.2111\n",
      "validation Loss: 0.0001 Acc: 86.9346\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.1389\n",
      "validation Loss: 0.0001 Acc: 86.9678\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.2560\n",
      "validation Loss: 0.0001 Acc: 87.1845\n",
      "Saving..\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.3941\n",
      "validation Loss: 0.0001 Acc: 87.1396\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.3799\n",
      "validation Loss: 0.0001 Acc: 87.2333\n",
      "Saving..\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.4317\n",
      "validation Loss: 0.0001 Acc: 87.2684\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.4146\n",
      "validation Loss: 0.0001 Acc: 87.2021\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.4492\n",
      "validation Loss: 0.0001 Acc: 87.2392\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.4439\n",
      "validation Loss: 0.0001 Acc: 87.3036\n",
      "Saving..\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.4575\n",
      "validation Loss: 0.0001 Acc: 87.4695\n",
      "Saving..\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.4263\n",
      "validation Loss: 0.0001 Acc: 87.5105\n",
      "Saving..\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.5405\n",
      "validation Loss: 0.0001 Acc: 87.5359\n",
      "Saving..\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.5869\n",
      "validation Loss: 0.0001 Acc: 87.3094\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.5683\n",
      "validation Loss: 0.0001 Acc: 87.3856\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.5303\n",
      "validation Loss: 0.0001 Acc: 87.5203\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.6327\n",
      "validation Loss: 0.0001 Acc: 87.2118\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.6952\n",
      "validation Loss: 0.0001 Acc: 87.6100\n",
      "Saving..\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.7762\n",
      "validation Loss: 0.0001 Acc: 87.6296\n",
      "Saving..\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.7591\n",
      "validation Loss: 0.0001 Acc: 87.6100\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.8387\n",
      "validation Loss: 0.0001 Acc: 87.6471\n",
      "Saving..\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.7294\n",
      "validation Loss: 0.0001 Acc: 87.6706\n",
      "Saving..\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.7713\n",
      "validation Loss: 0.0001 Acc: 87.6569\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.8211\n",
      "validation Loss: 0.0001 Acc: 87.6042\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.8397\n",
      "validation Loss: 0.0001 Acc: 87.6959\n",
      "Saving..\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.8084\n",
      "validation Loss: 0.0001 Acc: 87.6354\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.7806\n",
      "validation Loss: 0.0001 Acc: 87.7018\n",
      "Saving..\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.8319\n",
      "validation Loss: 0.0001 Acc: 87.4851\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.7870\n",
      "validation Loss: 0.0001 Acc: 87.6667\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.9382\n",
      "validation Loss: 0.0001 Acc: 87.8111\n",
      "Saving..\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.9573\n",
      "validation Loss: 0.0001 Acc: 87.8267\n",
      "Saving..\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 87.9007\n",
      "validation Loss: 0.0001 Acc: 87.7174\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 87.9202\n",
      "validation Loss: 0.0001 Acc: 87.8345\n",
      "Saving..\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 87.9392\n",
      "validation Loss: 0.0001 Acc: 87.7447\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.9285\n",
      "validation Loss: 0.0001 Acc: 87.8209\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 87.9451\n",
      "validation Loss: 0.0001 Acc: 87.7721\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.9138\n",
      "validation Loss: 0.0001 Acc: 87.7740\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.9519\n",
      "validation Loss: 0.0001 Acc: 87.6881\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.8328\n",
      "validation Loss: 0.0001 Acc: 87.7467\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.9539\n",
      "validation Loss: 0.0001 Acc: 87.7662\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 87.9627\n",
      "validation Loss: 0.0001 Acc: 87.7779\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 87.9939\n",
      "validation Loss: 0.0001 Acc: 87.8404\n",
      "Saving..\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.0188\n",
      "validation Loss: 0.0001 Acc: 87.8384\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.9900\n",
      "validation Loss: 0.0001 Acc: 87.8384\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.9758\n",
      "validation Loss: 0.0001 Acc: 87.9029\n",
      "Saving..\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.0188\n",
      "validation Loss: 0.0001 Acc: 87.8365\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.9812\n",
      "validation Loss: 0.0001 Acc: 87.8365\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.0183\n",
      "validation Loss: 0.0001 Acc: 87.7896\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.0290\n",
      "validation Loss: 0.0001 Acc: 87.8736\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.9700\n",
      "validation Loss: 0.0001 Acc: 87.8658\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.0510\n",
      "validation Loss: 0.0001 Acc: 87.8267\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.0134\n",
      "validation Loss: 0.0001 Acc: 87.8326\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.0373\n",
      "validation Loss: 0.0001 Acc: 87.8833\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.0442\n",
      "validation Loss: 0.0001 Acc: 87.8248\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 87.9851\n",
      "validation Loss: 0.0001 Acc: 87.8306\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.0544\n",
      "validation Loss: 0.0001 Acc: 87.8150\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.0183\n",
      "validation Loss: 0.0001 Acc: 87.8716\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 88.0578\n",
      "validation Loss: 0.0001 Acc: 87.8521\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 88.0812\n",
      "validation Loss: 0.0001 Acc: 87.8697\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 88.0520\n",
      "validation Loss: 0.0001 Acc: 87.8560\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 88.0515\n",
      "validation Loss: 0.0001 Acc: 87.8482\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 88.0329\n",
      "validation Loss: 0.0001 Acc: 87.8990\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 88.0798\n",
      "validation Loss: 0.0001 Acc: 87.8736\n",
      "Best val acc: 87.902863\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9408\n",
      "validation Loss: 0.0001 Acc: 88.1701\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.9632\n",
      "validation Loss: 0.0001 Acc: 88.1271\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.9769\n",
      "validation Loss: 0.0001 Acc: 88.1525\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.9871\n",
      "validation Loss: 0.0001 Acc: 88.1271\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.9749\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9886\n",
      "validation Loss: 0.0001 Acc: 88.1271\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.0027\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.9949\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9949\n",
      "validation Loss: 0.0001 Acc: 88.1232\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9671\n",
      "validation Loss: 0.0001 Acc: 88.1174\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0105\n",
      "validation Loss: 0.0001 Acc: 88.1213\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.0203\n",
      "validation Loss: 0.0001 Acc: 88.1408\n",
      "Early stopped.\n",
      "Best val acc: 88.170067\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.0027\n",
      "validation Loss: 0.0001 Acc: 88.1291\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.9725\n",
      "validation Loss: 0.0001 Acc: 88.1056\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.0096\n",
      "validation Loss: 0.0001 Acc: 88.0881\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.9778\n",
      "validation Loss: 0.0001 Acc: 88.1115\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.0032\n",
      "validation Loss: 0.0001 Acc: 88.1037\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9954\n",
      "validation Loss: 0.0001 Acc: 88.0978\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.0096\n",
      "validation Loss: 0.0001 Acc: 88.0998\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.9930\n",
      "validation Loss: 0.0001 Acc: 88.0803\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.0061\n",
      "validation Loss: 0.0001 Acc: 88.0861\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9881\n",
      "validation Loss: 0.0001 Acc: 88.0842\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0032\n",
      "validation Loss: 0.0001 Acc: 88.0861\n",
      "Early stopped.\n",
      "Best val acc: 88.129074\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.0037\n",
      "validation Loss: 0.0001 Acc: 88.0803\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0374\n",
      "validation Loss: 0.0001 Acc: 88.0725\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.0164\n",
      "validation Loss: 0.0001 Acc: 88.0686\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.0384\n",
      "validation Loss: 0.0001 Acc: 88.0686\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.0086\n",
      "validation Loss: 0.0001 Acc: 88.0666\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.0101\n",
      "validation Loss: 0.0001 Acc: 88.0627\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.0262\n",
      "validation Loss: 0.0001 Acc: 88.0686\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.0154\n",
      "validation Loss: 0.0001 Acc: 88.0686\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.0105\n",
      "validation Loss: 0.0001 Acc: 88.0686\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9964\n",
      "validation Loss: 0.0001 Acc: 88.0647\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0047\n",
      "validation Loss: 0.0001 Acc: 88.0627\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.0061\n",
      "validation Loss: 0.0001 Acc: 88.0627\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.0262\n",
      "validation Loss: 0.0001 Acc: 88.0647\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.0115\n",
      "validation Loss: 0.0001 Acc: 88.0647\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.9856\n",
      "validation Loss: 0.0001 Acc: 88.0647\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.0320\n",
      "validation Loss: 0.0001 Acc: 88.0607\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.9993\n",
      "validation Loss: 0.0001 Acc: 88.0647\n",
      "Early stopped.\n",
      "Best val acc: 88.080269\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9896\n",
      "validation Loss: 0.0001 Acc: 88.2403\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.9979\n",
      "validation Loss: 0.0001 Acc: 88.2423\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.9622\n",
      "validation Loss: 0.0001 Acc: 88.2482\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.9769\n",
      "validation Loss: 0.0001 Acc: 88.2442\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.9803\n",
      "validation Loss: 0.0001 Acc: 88.2442\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9876\n",
      "validation Loss: 0.0001 Acc: 88.2462\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9930\n",
      "validation Loss: 0.0001 Acc: 88.2423\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.0061\n",
      "validation Loss: 0.0001 Acc: 88.2423\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9695\n",
      "validation Loss: 0.0001 Acc: 88.2403\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9793\n",
      "validation Loss: 0.0001 Acc: 88.2384\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.9881\n",
      "validation Loss: 0.0001 Acc: 88.2403\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.9969\n",
      "validation Loss: 0.0001 Acc: 88.2403\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.9652\n",
      "validation Loss: 0.0001 Acc: 88.2403\n",
      "Early stopped.\n",
      "Best val acc: 88.248154\n",
      "----------\n",
      "Average best_acc across k-fold: 88.10609436035156\n",
      "DeltaPhi_j1MET\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 4, 6)\n",
      "background number: (170754, 4, 6)\n",
      "Data list: (341508, 4, 6)\n",
      "Data HLF: (341508, 8)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 82.4035\n",
      "validation Loss: 0.0001 Acc: 84.5941\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.0540\n",
      "validation Loss: 0.0001 Acc: 85.0372\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.7402\n",
      "validation Loss: 0.0001 Acc: 86.3470\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 86.9798\n",
      "validation Loss: 0.0001 Acc: 86.9366\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.2413\n",
      "validation Loss: 0.0001 Acc: 86.9112\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.3526\n",
      "validation Loss: 0.0001 Acc: 86.7921\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.3741\n",
      "validation Loss: 0.0001 Acc: 87.0518\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.4063\n",
      "validation Loss: 0.0001 Acc: 86.9210\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.4244\n",
      "validation Loss: 0.0001 Acc: 87.2704\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5264\n",
      "validation Loss: 0.0001 Acc: 86.8429\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5059\n",
      "validation Loss: 0.0001 Acc: 87.2431\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5107\n",
      "validation Loss: 0.0001 Acc: 87.4266\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.6205\n",
      "validation Loss: 0.0001 Acc: 87.4695\n",
      "Saving..\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.6723\n",
      "validation Loss: 0.0001 Acc: 87.5769\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.6459\n",
      "validation Loss: 0.0001 Acc: 87.5339\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.6513\n",
      "validation Loss: 0.0001 Acc: 87.5320\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.6571\n",
      "validation Loss: 0.0001 Acc: 87.3699\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.6615\n",
      "validation Loss: 0.0001 Acc: 87.4636\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.7923\n",
      "validation Loss: 0.0001 Acc: 87.5905\n",
      "Saving..\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.8021\n",
      "validation Loss: 0.0001 Acc: 87.5222\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.7962\n",
      "validation Loss: 0.0001 Acc: 87.4851\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.7933\n",
      "validation Loss: 0.0001 Acc: 87.5203\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.7855\n",
      "validation Loss: 0.0001 Acc: 87.4051\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.8792\n",
      "validation Loss: 0.0001 Acc: 87.6296\n",
      "Saving..\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.8685\n",
      "validation Loss: 0.0001 Acc: 87.6686\n",
      "Saving..\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.8865\n",
      "validation Loss: 0.0001 Acc: 87.7233\n",
      "Saving..\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.9138\n",
      "validation Loss: 0.0001 Acc: 87.6100\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.9553\n",
      "validation Loss: 0.0001 Acc: 87.7057\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.8909\n",
      "validation Loss: 0.0001 Acc: 87.6276\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.8977\n",
      "validation Loss: 0.0001 Acc: 87.5886\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.8943\n",
      "validation Loss: 0.0001 Acc: 87.6452\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.8958\n",
      "validation Loss: 0.0001 Acc: 87.6686\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.9260\n",
      "validation Loss: 0.0001 Acc: 87.5866\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.9192\n",
      "validation Loss: 0.0001 Acc: 87.6140\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.8758\n",
      "validation Loss: 0.0001 Acc: 87.6237\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.9485\n",
      "validation Loss: 0.0001 Acc: 87.6335\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.9783\n",
      "validation Loss: 0.0001 Acc: 87.6940\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.9339\n",
      "validation Loss: 0.0001 Acc: 87.6920\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 88.0071\n",
      "validation Loss: 0.0001 Acc: 87.7018\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 88.0002\n",
      "validation Loss: 0.0001 Acc: 87.7096\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 88.0139\n",
      "validation Loss: 0.0001 Acc: 87.6881\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.9812\n",
      "validation Loss: 0.0001 Acc: 87.7155\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 88.0022\n",
      "validation Loss: 0.0001 Acc: 87.7155\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.9822\n",
      "validation Loss: 0.0001 Acc: 87.7135\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.9622\n",
      "validation Loss: 0.0001 Acc: 87.6803\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.9997\n",
      "validation Loss: 0.0001 Acc: 87.6901\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.9802\n",
      "validation Loss: 0.0001 Acc: 87.7272\n",
      "Saving..\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 88.0110\n",
      "validation Loss: 0.0001 Acc: 87.7155\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 88.0198\n",
      "validation Loss: 0.0001 Acc: 87.7389\n",
      "Saving..\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.0046\n",
      "validation Loss: 0.0001 Acc: 87.7233\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 88.0427\n",
      "validation Loss: 0.0001 Acc: 87.7565\n",
      "Saving..\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 88.0359\n",
      "validation Loss: 0.0001 Acc: 87.6764\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.0305\n",
      "validation Loss: 0.0001 Acc: 87.8267\n",
      "Saving..\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 88.0061\n",
      "validation Loss: 0.0001 Acc: 87.6667\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.0437\n",
      "validation Loss: 0.0001 Acc: 87.7155\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.0388\n",
      "validation Loss: 0.0001 Acc: 87.7174\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 88.0388\n",
      "validation Loss: 0.0001 Acc: 87.7838\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.0193\n",
      "validation Loss: 0.0001 Acc: 87.7447\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.0344\n",
      "validation Loss: 0.0001 Acc: 87.7486\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.0422\n",
      "validation Loss: 0.0001 Acc: 87.8053\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.0695\n",
      "validation Loss: 0.0001 Acc: 87.7311\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.0036\n",
      "validation Loss: 0.0001 Acc: 87.7662\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.0759\n",
      "validation Loss: 0.0001 Acc: 87.7721\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.0647\n",
      "validation Loss: 0.0001 Acc: 87.7896\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 88.0866\n",
      "validation Loss: 0.0001 Acc: 87.7545\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 88.0544\n",
      "validation Loss: 0.0001 Acc: 87.7213\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 88.0388\n",
      "validation Loss: 0.0001 Acc: 87.7760\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 88.0876\n",
      "validation Loss: 0.0001 Acc: 87.7662\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 88.0578\n",
      "validation Loss: 0.0001 Acc: 87.7623\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 88.1027\n",
      "validation Loss: 0.0001 Acc: 87.7565\n",
      "Best val acc: 87.826736\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9613\n",
      "validation Loss: 0.0001 Acc: 88.1701\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.9666\n",
      "validation Loss: 0.0001 Acc: 88.1837\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.9832\n",
      "validation Loss: 0.0001 Acc: 88.1154\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.0003\n",
      "validation Loss: 0.0001 Acc: 88.1681\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.0037\n",
      "validation Loss: 0.0001 Acc: 88.1388\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9583\n",
      "validation Loss: 0.0001 Acc: 88.1408\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9627\n",
      "validation Loss: 0.0001 Acc: 88.1603\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.9959\n",
      "validation Loss: 0.0001 Acc: 88.1388\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9886\n",
      "validation Loss: 0.0001 Acc: 88.1662\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9998\n",
      "validation Loss: 0.0001 Acc: 88.1681\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.9720\n",
      "validation Loss: 0.0001 Acc: 88.1486\n",
      "Early stopped.\n",
      "Best val acc: 88.183731\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9383\n",
      "validation Loss: 0.0001 Acc: 88.2931\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.9818\n",
      "validation Loss: 0.0001 Acc: 88.2931\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.9491\n",
      "validation Loss: 0.0001 Acc: 88.2755\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.9432\n",
      "validation Loss: 0.0001 Acc: 88.2599\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.9598\n",
      "validation Loss: 0.0001 Acc: 88.2755\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9471\n",
      "validation Loss: 0.0001 Acc: 88.2618\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9417\n",
      "validation Loss: 0.0001 Acc: 88.2638\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.9530\n",
      "validation Loss: 0.0001 Acc: 88.2755\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9554\n",
      "validation Loss: 0.0001 Acc: 88.2716\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9481\n",
      "validation Loss: 0.0001 Acc: 88.2813\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.9398\n",
      "validation Loss: 0.0001 Acc: 88.2813\n",
      "Early stopped.\n",
      "Best val acc: 88.293053\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9827\n",
      "validation Loss: 0.0001 Acc: 88.1232\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0027\n",
      "validation Loss: 0.0001 Acc: 88.1213\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.0057\n",
      "validation Loss: 0.0001 Acc: 88.1232\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.9896\n",
      "validation Loss: 0.0001 Acc: 88.1193\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.9456\n",
      "validation Loss: 0.0001 Acc: 88.1252\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9974\n",
      "validation Loss: 0.0001 Acc: 88.1252\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9871\n",
      "validation Loss: 0.0001 Acc: 88.1252\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.0066\n",
      "validation Loss: 0.0001 Acc: 88.1252\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.9891\n",
      "validation Loss: 0.0001 Acc: 88.1252\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.9920\n",
      "validation Loss: 0.0001 Acc: 88.1232\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0022\n",
      "validation Loss: 0.0001 Acc: 88.1232\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.9783\n",
      "validation Loss: 0.0001 Acc: 88.1232\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.0066\n",
      "validation Loss: 0.0001 Acc: 88.1232\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.9896\n",
      "validation Loss: 0.0001 Acc: 88.1232\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.9691\n",
      "validation Loss: 0.0001 Acc: 88.1232\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.9715\n",
      "validation Loss: 0.0001 Acc: 88.1232\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.9627\n",
      "validation Loss: 0.0001 Acc: 88.1252\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.9642\n",
      "validation Loss: 0.0001 Acc: 88.1252\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.9876\n",
      "validation Loss: 0.0001 Acc: 88.1252\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.9861\n",
      "validation Loss: 0.0001 Acc: 88.1252\n",
      "Early stopped.\n",
      "Best val acc: 88.125168\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.0784\n",
      "validation Loss: 0.0001 Acc: 87.8148\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0623\n",
      "validation Loss: 0.0001 Acc: 87.8148\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.0559\n",
      "validation Loss: 0.0001 Acc: 87.8167\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.0906\n",
      "validation Loss: 0.0001 Acc: 87.8167\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.0740\n",
      "validation Loss: 0.0001 Acc: 87.8167\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.0720\n",
      "validation Loss: 0.0001 Acc: 87.8167\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.0686\n",
      "validation Loss: 0.0001 Acc: 87.8167\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.0950\n",
      "validation Loss: 0.0001 Acc: 87.8187\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.0457\n",
      "validation Loss: 0.0001 Acc: 87.8206\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.0662\n",
      "validation Loss: 0.0001 Acc: 87.8206\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0774\n",
      "validation Loss: 0.0001 Acc: 87.8206\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.0564\n",
      "validation Loss: 0.0001 Acc: 87.8206\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.0711\n",
      "validation Loss: 0.0001 Acc: 87.8206\n",
      "Early stopped.\n",
      "Best val acc: 87.820633\n",
      "----------\n",
      "Average best_acc across k-fold: 88.04986572265625\n",
      "DeltaPhi_j2MET\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 4, 6)\n",
      "background number: (170754, 4, 6)\n",
      "Data list: (341508, 4, 6)\n",
      "Data HLF: (341508, 8)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 82.0721\n",
      "validation Loss: 0.0001 Acc: 85.0957\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 84.9944\n",
      "validation Loss: 0.0001 Acc: 85.7302\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.4224\n",
      "validation Loss: 0.0001 Acc: 86.2982\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 86.5649\n",
      "validation Loss: 0.0001 Acc: 87.3075\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.1208\n",
      "validation Loss: 0.0001 Acc: 87.2723\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.2487\n",
      "validation Loss: 0.0001 Acc: 87.3114\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.3799\n",
      "validation Loss: 0.0001 Acc: 87.5573\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.3331\n",
      "validation Loss: 0.0001 Acc: 87.3211\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.4927\n",
      "validation Loss: 0.0001 Acc: 87.5554\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.4522\n",
      "validation Loss: 0.0001 Acc: 87.7350\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5181\n",
      "validation Loss: 0.0001 Acc: 87.6647\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5127\n",
      "validation Loss: 0.0001 Acc: 87.8033\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.5869\n",
      "validation Loss: 0.0001 Acc: 87.6628\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.6845\n",
      "validation Loss: 0.0001 Acc: 87.6745\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.6640\n",
      "validation Loss: 0.0001 Acc: 87.8716\n",
      "Saving..\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.6000\n",
      "validation Loss: 0.0001 Acc: 87.7740\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.6830\n",
      "validation Loss: 0.0001 Acc: 87.8638\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.6615\n",
      "validation Loss: 0.0001 Acc: 87.8638\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.7347\n",
      "validation Loss: 0.0001 Acc: 87.9692\n",
      "Saving..\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.7425\n",
      "validation Loss: 0.0001 Acc: 88.0024\n",
      "Saving..\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.7318\n",
      "validation Loss: 0.0001 Acc: 88.0102\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.8201\n",
      "validation Loss: 0.0001 Acc: 87.7604\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.7406\n",
      "validation Loss: 0.0001 Acc: 88.0844\n",
      "Saving..\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.8138\n",
      "validation Loss: 0.0001 Acc: 87.9048\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.7650\n",
      "validation Loss: 0.0001 Acc: 87.8131\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.7635\n",
      "validation Loss: 0.0001 Acc: 87.7174\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.7953\n",
      "validation Loss: 0.0001 Acc: 87.7760\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.8607\n",
      "validation Loss: 0.0001 Acc: 88.0571\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.9592\n",
      "validation Loss: 0.0001 Acc: 87.9907\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.9334\n",
      "validation Loss: 0.0001 Acc: 88.0180\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.9182\n",
      "validation Loss: 0.0001 Acc: 87.8833\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.9099\n",
      "validation Loss: 0.0001 Acc: 88.2113\n",
      "Saving..\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.9968\n",
      "validation Loss: 0.0001 Acc: 88.0746\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.9758\n",
      "validation Loss: 0.0001 Acc: 88.1566\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.9065\n",
      "validation Loss: 0.0001 Acc: 88.0629\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.9485\n",
      "validation Loss: 0.0001 Acc: 87.9556\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.9163\n",
      "validation Loss: 0.0001 Acc: 87.9556\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.9954\n",
      "validation Loss: 0.0001 Acc: 88.0376\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 88.0481\n",
      "validation Loss: 0.0001 Acc: 88.1781\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 88.0476\n",
      "validation Loss: 0.0001 Acc: 88.1254\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 88.0437\n",
      "validation Loss: 0.0001 Acc: 88.0786\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 88.1213\n",
      "validation Loss: 0.0001 Acc: 88.1957\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 88.0642\n",
      "validation Loss: 0.0001 Acc: 88.0649\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 88.0715\n",
      "validation Loss: 0.0001 Acc: 88.1918\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 88.0788\n",
      "validation Loss: 0.0001 Acc: 88.1859\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 88.0764\n",
      "validation Loss: 0.0001 Acc: 88.2425\n",
      "Saving..\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 88.1003\n",
      "validation Loss: 0.0001 Acc: 88.2074\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 88.1203\n",
      "validation Loss: 0.0001 Acc: 88.1313\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 88.1266\n",
      "validation Loss: 0.0001 Acc: 88.1547\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.1183\n",
      "validation Loss: 0.0001 Acc: 88.1137\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 88.1047\n",
      "validation Loss: 0.0001 Acc: 88.2211\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 88.0900\n",
      "validation Loss: 0.0001 Acc: 88.1918\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.1066\n",
      "validation Loss: 0.0001 Acc: 88.1137\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 88.1164\n",
      "validation Loss: 0.0001 Acc: 88.1664\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.1359\n",
      "validation Loss: 0.0001 Acc: 88.2542\n",
      "Saving..\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.1359\n",
      "validation Loss: 0.0001 Acc: 88.1859\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 88.1413\n",
      "validation Loss: 0.0001 Acc: 88.2074\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.1315\n",
      "validation Loss: 0.0001 Acc: 88.2269\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.1632\n",
      "validation Loss: 0.0001 Acc: 88.1801\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.1681\n",
      "validation Loss: 0.0001 Acc: 88.1371\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.1823\n",
      "validation Loss: 0.0001 Acc: 88.2132\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.1471\n",
      "validation Loss: 0.0001 Acc: 88.1625\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.1613\n",
      "validation Loss: 0.0001 Acc: 88.2035\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.1701\n",
      "validation Loss: 0.0001 Acc: 88.1976\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 88.1696\n",
      "validation Loss: 0.0001 Acc: 88.1664\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 88.1710\n",
      "validation Loss: 0.0001 Acc: 88.2386\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 88.1706\n",
      "validation Loss: 0.0001 Acc: 88.2523\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 88.1637\n",
      "validation Loss: 0.0001 Acc: 88.2406\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 88.1686\n",
      "validation Loss: 0.0001 Acc: 88.2054\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 88.1535\n",
      "validation Loss: 0.0001 Acc: 88.1957\n",
      "Best val acc: 88.254242\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2092\n",
      "validation Loss: 0.0001 Acc: 88.0842\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2228\n",
      "validation Loss: 0.0001 Acc: 88.0822\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2228\n",
      "validation Loss: 0.0001 Acc: 88.0744\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2228\n",
      "validation Loss: 0.0001 Acc: 88.0510\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2209\n",
      "validation Loss: 0.0001 Acc: 88.0198\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.1994\n",
      "validation Loss: 0.0001 Acc: 88.0725\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2141\n",
      "validation Loss: 0.0001 Acc: 88.0588\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2326\n",
      "validation Loss: 0.0001 Acc: 88.0705\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2180\n",
      "validation Loss: 0.0001 Acc: 88.0725\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2243\n",
      "validation Loss: 0.0001 Acc: 88.0588\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2394\n",
      "validation Loss: 0.0001 Acc: 88.0686\n",
      "Early stopped.\n",
      "Best val acc: 88.084175\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.1667\n",
      "validation Loss: 0.0001 Acc: 88.2521\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.1765\n",
      "validation Loss: 0.0001 Acc: 88.2521\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.1721\n",
      "validation Loss: 0.0001 Acc: 88.2657\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2023\n",
      "validation Loss: 0.0001 Acc: 88.2579\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.1838\n",
      "validation Loss: 0.0001 Acc: 88.2521\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.1838\n",
      "validation Loss: 0.0001 Acc: 88.2482\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2087\n",
      "validation Loss: 0.0001 Acc: 88.2442\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2004\n",
      "validation Loss: 0.0001 Acc: 88.2482\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.1594\n",
      "validation Loss: 0.0001 Acc: 88.2521\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.1809\n",
      "validation Loss: 0.0001 Acc: 88.2521\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.1657\n",
      "validation Loss: 0.0001 Acc: 88.2540\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.2043\n",
      "validation Loss: 0.0001 Acc: 88.2540\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.1897\n",
      "validation Loss: 0.0001 Acc: 88.2560\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.1857\n",
      "validation Loss: 0.0001 Acc: 88.2521\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.2116\n",
      "validation Loss: 0.0001 Acc: 88.2540\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.1921\n",
      "validation Loss: 0.0001 Acc: 88.2540\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.1936\n",
      "validation Loss: 0.0001 Acc: 88.2501\n",
      "Early stopped.\n",
      "Best val acc: 88.265724\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.1940\n",
      "validation Loss: 0.0001 Acc: 88.1740\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2141\n",
      "validation Loss: 0.0001 Acc: 88.1857\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2306\n",
      "validation Loss: 0.0001 Acc: 88.1876\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2043\n",
      "validation Loss: 0.0001 Acc: 88.1759\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.1843\n",
      "validation Loss: 0.0001 Acc: 88.1798\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.1677\n",
      "validation Loss: 0.0001 Acc: 88.1779\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2350\n",
      "validation Loss: 0.0001 Acc: 88.1779\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2404\n",
      "validation Loss: 0.0001 Acc: 88.1798\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2267\n",
      "validation Loss: 0.0001 Acc: 88.1759\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.1716\n",
      "validation Loss: 0.0001 Acc: 88.1759\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2058\n",
      "validation Loss: 0.0001 Acc: 88.1798\n",
      "Early stopped.\n",
      "Best val acc: 88.187637\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.1423\n",
      "validation Loss: 0.0001 Acc: 88.3985\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.1272\n",
      "validation Loss: 0.0001 Acc: 88.4004\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.1677\n",
      "validation Loss: 0.0001 Acc: 88.4004\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.1731\n",
      "validation Loss: 0.0001 Acc: 88.4043\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.1467\n",
      "validation Loss: 0.0001 Acc: 88.4024\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.1262\n",
      "validation Loss: 0.0001 Acc: 88.3926\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.1462\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.1330\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.1423\n",
      "validation Loss: 0.0001 Acc: 88.3887\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.1384\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.1340\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.1277\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.1345\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.1330\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.1550\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.1448\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.1306\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.1633\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.1594\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 88.1570\n",
      "validation Loss: 0.0001 Acc: 88.3848\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 88.1521\n",
      "validation Loss: 0.0001 Acc: 88.3848\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.1652\n",
      "validation Loss: 0.0001 Acc: 88.3848\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.1857\n",
      "validation Loss: 0.0001 Acc: 88.3848\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.1413\n",
      "validation Loss: 0.0001 Acc: 88.3848\n",
      "Early stopped.\n",
      "Best val acc: 88.404320\n",
      "----------\n",
      "Average best_acc across k-fold: 88.23921203613281\n",
      "DeltaR_jg_min\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 4, 6)\n",
      "background number: (170754, 4, 6)\n",
      "Data list: (341508, 4, 6)\n",
      "Data HLF: (341508, 8)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.0360\n",
      "validation Loss: 0.0001 Acc: 84.1705\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 84.3859\n",
      "validation Loss: 0.0001 Acc: 83.7839\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.3522\n",
      "validation Loss: 0.0001 Acc: 86.3451\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 85.7767\n",
      "validation Loss: 0.0001 Acc: 86.7453\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.7338\n",
      "validation Loss: 0.0001 Acc: 87.1240\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.0017\n",
      "validation Loss: 0.0001 Acc: 87.2372\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.1525\n",
      "validation Loss: 0.0001 Acc: 86.7453\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.2496\n",
      "validation Loss: 0.0001 Acc: 87.5495\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.3033\n",
      "validation Loss: 0.0001 Acc: 87.4890\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5478\n",
      "validation Loss: 0.0001 Acc: 87.5866\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.4634\n",
      "validation Loss: 0.0001 Acc: 87.6549\n",
      "Saving..\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.6479\n",
      "validation Loss: 0.0001 Acc: 87.2645\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.5517\n",
      "validation Loss: 0.0001 Acc: 87.6549\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.6230\n",
      "validation Loss: 0.0001 Acc: 87.6940\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.6552\n",
      "validation Loss: 0.0001 Acc: 87.2157\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.6703\n",
      "validation Loss: 0.0001 Acc: 87.7896\n",
      "Saving..\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.6435\n",
      "validation Loss: 0.0001 Acc: 87.7760\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.7440\n",
      "validation Loss: 0.0001 Acc: 87.4344\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.6537\n",
      "validation Loss: 0.0001 Acc: 87.5183\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.7025\n",
      "validation Loss: 0.0001 Acc: 87.6803\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.8631\n",
      "validation Loss: 0.0001 Acc: 87.9907\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.9685\n",
      "validation Loss: 0.0001 Acc: 88.0376\n",
      "Saving..\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.9641\n",
      "validation Loss: 0.0001 Acc: 88.0356\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.9656\n",
      "validation Loss: 0.0001 Acc: 87.9263\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.9095\n",
      "validation Loss: 0.0001 Acc: 87.8287\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.9339\n",
      "validation Loss: 0.0001 Acc: 87.9849\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.9529\n",
      "validation Loss: 0.0001 Acc: 87.8951\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.0061\n",
      "validation Loss: 0.0001 Acc: 88.0258\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.0544\n",
      "validation Loss: 0.0001 Acc: 88.1313\n",
      "Saving..\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.0422\n",
      "validation Loss: 0.0001 Acc: 88.0258\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 88.0734\n",
      "validation Loss: 0.0001 Acc: 88.0864\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 88.0788\n",
      "validation Loss: 0.0001 Acc: 88.0883\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 88.0422\n",
      "validation Loss: 0.0001 Acc: 88.0688\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 88.0554\n",
      "validation Loss: 0.0001 Acc: 88.0278\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 88.0578\n",
      "validation Loss: 0.0001 Acc: 88.1410\n",
      "Saving..\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 88.0978\n",
      "validation Loss: 0.0001 Acc: 88.0297\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 88.1725\n",
      "validation Loss: 0.0001 Acc: 88.1605\n",
      "Saving..\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 88.1471\n",
      "validation Loss: 0.0001 Acc: 88.0649\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 88.1325\n",
      "validation Loss: 0.0001 Acc: 88.1605\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 88.1369\n",
      "validation Loss: 0.0001 Acc: 88.1039\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 88.1613\n",
      "validation Loss: 0.0001 Acc: 88.1684\n",
      "Saving..\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 88.1452\n",
      "validation Loss: 0.0001 Acc: 88.1879\n",
      "Saving..\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 88.1413\n",
      "validation Loss: 0.0001 Acc: 88.2015\n",
      "Saving..\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 88.1730\n",
      "validation Loss: 0.0001 Acc: 88.1430\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 88.1505\n",
      "validation Loss: 0.0001 Acc: 88.0942\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 88.1520\n",
      "validation Loss: 0.0001 Acc: 88.1313\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 88.1242\n",
      "validation Loss: 0.0001 Acc: 88.1976\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 88.1681\n",
      "validation Loss: 0.0001 Acc: 88.1644\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 88.1535\n",
      "validation Loss: 0.0001 Acc: 88.1371\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.1354\n",
      "validation Loss: 0.0001 Acc: 88.2074\n",
      "Saving..\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 88.1920\n",
      "validation Loss: 0.0001 Acc: 88.1469\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 88.1232\n",
      "validation Loss: 0.0001 Acc: 88.1762\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.1886\n",
      "validation Loss: 0.0001 Acc: 88.1801\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 88.1754\n",
      "validation Loss: 0.0001 Acc: 88.1410\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.1823\n",
      "validation Loss: 0.0001 Acc: 88.1762\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.1676\n",
      "validation Loss: 0.0001 Acc: 88.1840\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 88.1740\n",
      "validation Loss: 0.0001 Acc: 88.1703\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.1569\n",
      "validation Loss: 0.0001 Acc: 88.1352\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.1979\n",
      "validation Loss: 0.0001 Acc: 88.1449\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.1540\n",
      "validation Loss: 0.0001 Acc: 88.1391\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.1911\n",
      "validation Loss: 0.0001 Acc: 88.1703\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.2457\n",
      "validation Loss: 0.0001 Acc: 88.1391\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.2174\n",
      "validation Loss: 0.0001 Acc: 88.1703\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.2042\n",
      "validation Loss: 0.0001 Acc: 88.2093\n",
      "Saving..\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 88.1993\n",
      "validation Loss: 0.0001 Acc: 88.1918\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 88.2213\n",
      "validation Loss: 0.0001 Acc: 88.1527\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 88.2130\n",
      "validation Loss: 0.0001 Acc: 88.1254\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 88.2433\n",
      "validation Loss: 0.0001 Acc: 88.1703\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 88.2096\n",
      "validation Loss: 0.0001 Acc: 88.1957\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 88.2164\n",
      "validation Loss: 0.0001 Acc: 88.1586\n",
      "Best val acc: 88.209343\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2058\n",
      "validation Loss: 0.0001 Acc: 88.3848\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.1901\n",
      "validation Loss: 0.0001 Acc: 88.3438\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2048\n",
      "validation Loss: 0.0001 Acc: 88.3516\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2038\n",
      "validation Loss: 0.0001 Acc: 88.3458\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.1687\n",
      "validation Loss: 0.0001 Acc: 88.3301\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.1940\n",
      "validation Loss: 0.0001 Acc: 88.3321\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.1911\n",
      "validation Loss: 0.0001 Acc: 88.3380\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.1906\n",
      "validation Loss: 0.0001 Acc: 88.3497\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2375\n",
      "validation Loss: 0.0001 Acc: 88.3477\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.1828\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2048\n",
      "validation Loss: 0.0001 Acc: 88.3555\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.1936\n",
      "validation Loss: 0.0001 Acc: 88.3340\n",
      "Early stopped.\n",
      "Best val acc: 88.384804\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2404\n",
      "validation Loss: 0.0001 Acc: 88.2735\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2175\n",
      "validation Loss: 0.0001 Acc: 88.2677\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2228\n",
      "validation Loss: 0.0001 Acc: 88.2755\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2463\n",
      "validation Loss: 0.0001 Acc: 88.2638\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2326\n",
      "validation Loss: 0.0001 Acc: 88.2696\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2585\n",
      "validation Loss: 0.0001 Acc: 88.2696\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2604\n",
      "validation Loss: 0.0001 Acc: 88.2657\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2292\n",
      "validation Loss: 0.0001 Acc: 88.2657\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2365\n",
      "validation Loss: 0.0001 Acc: 88.2618\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.1897\n",
      "validation Loss: 0.0001 Acc: 88.2657\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2389\n",
      "validation Loss: 0.0001 Acc: 88.2696\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.2507\n",
      "validation Loss: 0.0001 Acc: 88.2638\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.2180\n",
      "validation Loss: 0.0001 Acc: 88.2735\n",
      "Early stopped.\n",
      "Best val acc: 88.275482\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2306\n",
      "validation Loss: 0.0001 Acc: 88.3380\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2116\n",
      "validation Loss: 0.0001 Acc: 88.3321\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2526\n",
      "validation Loss: 0.0001 Acc: 88.3262\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.1901\n",
      "validation Loss: 0.0001 Acc: 88.3262\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2175\n",
      "validation Loss: 0.0001 Acc: 88.3282\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2141\n",
      "validation Loss: 0.0001 Acc: 88.3301\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2262\n",
      "validation Loss: 0.0001 Acc: 88.3301\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2101\n",
      "validation Loss: 0.0001 Acc: 88.3301\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.1970\n",
      "validation Loss: 0.0001 Acc: 88.3321\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.1887\n",
      "validation Loss: 0.0001 Acc: 88.3340\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.1892\n",
      "validation Loss: 0.0001 Acc: 88.3321\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.2126\n",
      "validation Loss: 0.0001 Acc: 88.3340\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.2199\n",
      "validation Loss: 0.0001 Acc: 88.3340\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.2267\n",
      "validation Loss: 0.0001 Acc: 88.3340\n",
      "Early stopped.\n",
      "Best val acc: 88.337952\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.1682\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.1472\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.1945\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.1965\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.1906\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.1652\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2233\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2331\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.1950\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2106\n",
      "validation Loss: 0.0001 Acc: 88.3848\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.1735\n",
      "validation Loss: 0.0001 Acc: 88.3848\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.1613\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.1926\n",
      "validation Loss: 0.0001 Acc: 88.3848\n",
      "Early stopped.\n",
      "Best val acc: 88.390656\n",
      "----------\n",
      "Average best_acc across k-fold: 88.31964874267578\n",
      "n_jets\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 4, 6)\n",
      "background number: (170754, 4, 6)\n",
      "Data list: (341508, 4, 6)\n",
      "Data HLF: (341508, 8)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 76.4846\n",
      "validation Loss: 0.0001 Acc: 81.3243\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 82.2780\n",
      "validation Loss: 0.0001 Acc: 84.1060\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 84.7631\n",
      "validation Loss: 0.0001 Acc: 84.3832\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 85.6425\n",
      "validation Loss: 0.0001 Acc: 86.2670\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.5654\n",
      "validation Loss: 0.0001 Acc: 87.0127\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 86.9612\n",
      "validation Loss: 0.0001 Acc: 86.8643\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 86.8016\n",
      "validation Loss: 0.0001 Acc: 87.1552\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0154\n",
      "validation Loss: 0.0001 Acc: 86.7414\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 86.9485\n",
      "validation Loss: 0.0001 Acc: 87.0596\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.1637\n",
      "validation Loss: 0.0001 Acc: 87.2216\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.2701\n",
      "validation Loss: 0.0001 Acc: 86.6945\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.3136\n",
      "validation Loss: 0.0001 Acc: 86.8722\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.2267\n",
      "validation Loss: 0.0001 Acc: 85.9215\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.2926\n",
      "validation Loss: 0.0001 Acc: 86.9210\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.4819\n",
      "validation Loss: 0.0001 Acc: 87.5944\n",
      "Saving..\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.5112\n",
      "validation Loss: 0.0001 Acc: 87.5242\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.5698\n",
      "validation Loss: 0.0001 Acc: 87.6042\n",
      "Saving..\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.5630\n",
      "validation Loss: 0.0001 Acc: 87.5905\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.5908\n",
      "validation Loss: 0.0001 Acc: 87.6276\n",
      "Saving..\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.6772\n",
      "validation Loss: 0.0001 Acc: 87.6003\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.5810\n",
      "validation Loss: 0.0001 Acc: 87.6959\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.6894\n",
      "validation Loss: 0.0001 Acc: 87.5925\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.6918\n",
      "validation Loss: 0.0001 Acc: 87.6647\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.6698\n",
      "validation Loss: 0.0001 Acc: 87.6120\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.6937\n",
      "validation Loss: 0.0001 Acc: 87.6140\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.6357\n",
      "validation Loss: 0.0001 Acc: 87.4051\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.6933\n",
      "validation Loss: 0.0001 Acc: 87.7408\n",
      "Saving..\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.7655\n",
      "validation Loss: 0.0001 Acc: 87.6569\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.6674\n",
      "validation Loss: 0.0001 Acc: 87.7096\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.5864\n",
      "validation Loss: 0.0001 Acc: 87.6842\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.7548\n",
      "validation Loss: 0.0001 Acc: 87.5652\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.7382\n",
      "validation Loss: 0.0001 Acc: 87.7447\n",
      "Saving..\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.7767\n",
      "validation Loss: 0.0001 Acc: 87.6686\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.8211\n",
      "validation Loss: 0.0001 Acc: 87.7428\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.8431\n",
      "validation Loss: 0.0001 Acc: 87.7291\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.8494\n",
      "validation Loss: 0.0001 Acc: 87.8736\n",
      "Saving..\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.8699\n",
      "validation Loss: 0.0001 Acc: 87.7428\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.8758\n",
      "validation Loss: 0.0001 Acc: 87.6940\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 87.8450\n",
      "validation Loss: 0.0001 Acc: 87.8541\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 87.8831\n",
      "validation Loss: 0.0001 Acc: 87.8658\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 87.8694\n",
      "validation Loss: 0.0001 Acc: 87.6647\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.8924\n",
      "validation Loss: 0.0001 Acc: 87.8990\n",
      "Saving..\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 87.9295\n",
      "validation Loss: 0.0001 Acc: 87.8248\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.8631\n",
      "validation Loss: 0.0001 Acc: 87.7935\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.9095\n",
      "validation Loss: 0.0001 Acc: 87.8306\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.9265\n",
      "validation Loss: 0.0001 Acc: 87.8658\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.9075\n",
      "validation Loss: 0.0001 Acc: 87.9029\n",
      "Saving..\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 87.9631\n",
      "validation Loss: 0.0001 Acc: 87.8580\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 87.9583\n",
      "validation Loss: 0.0001 Acc: 87.9556\n",
      "Saving..\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 87.9573\n",
      "validation Loss: 0.0001 Acc: 87.9048\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.9436\n",
      "validation Loss: 0.0001 Acc: 87.8463\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.9539\n",
      "validation Loss: 0.0001 Acc: 87.9224\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 87.8855\n",
      "validation Loss: 0.0001 Acc: 87.9302\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.9343\n",
      "validation Loss: 0.0001 Acc: 87.9204\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 87.9300\n",
      "validation Loss: 0.0001 Acc: 87.9302\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 87.8821\n",
      "validation Loss: 0.0001 Acc: 87.9185\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.9587\n",
      "validation Loss: 0.0001 Acc: 87.8209\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 87.9465\n",
      "validation Loss: 0.0001 Acc: 87.8697\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 87.9651\n",
      "validation Loss: 0.0001 Acc: 87.9907\n",
      "Saving..\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.0173\n",
      "validation Loss: 0.0001 Acc: 87.9302\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 87.9875\n",
      "validation Loss: 0.0001 Acc: 87.9517\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.0056\n",
      "validation Loss: 0.0001 Acc: 87.9809\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.0256\n",
      "validation Loss: 0.0001 Acc: 87.9321\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.0446\n",
      "validation Loss: 0.0001 Acc: 87.9243\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 88.0642\n",
      "validation Loss: 0.0001 Acc: 87.9204\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 88.0554\n",
      "validation Loss: 0.0001 Acc: 87.9361\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 88.0251\n",
      "validation Loss: 0.0001 Acc: 87.9868\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 88.0324\n",
      "validation Loss: 0.0001 Acc: 87.8951\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 88.0573\n",
      "validation Loss: 0.0001 Acc: 87.9614\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 88.0578\n",
      "validation Loss: 0.0001 Acc: 87.9731\n",
      "Best val acc: 87.990707\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9783\n",
      "validation Loss: 0.0001 Acc: 88.1857\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0032\n",
      "validation Loss: 0.0001 Acc: 88.2033\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.0008\n",
      "validation Loss: 0.0001 Acc: 88.1681\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.0291\n",
      "validation Loss: 0.0001 Acc: 88.1798\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.0052\n",
      "validation Loss: 0.0001 Acc: 88.1466\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.9900\n",
      "validation Loss: 0.0001 Acc: 88.1447\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.9925\n",
      "validation Loss: 0.0001 Acc: 88.1564\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.0208\n",
      "validation Loss: 0.0001 Acc: 88.1720\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.0369\n",
      "validation Loss: 0.0001 Acc: 88.1408\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.0237\n",
      "validation Loss: 0.0001 Acc: 88.1584\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0496\n",
      "validation Loss: 0.0001 Acc: 88.1505\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.0057\n",
      "validation Loss: 0.0001 Acc: 88.1720\n",
      "Early stopped.\n",
      "Best val acc: 88.203255\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.9915\n",
      "validation Loss: 0.0001 Acc: 88.2638\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.9847\n",
      "validation Loss: 0.0001 Acc: 88.2638\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.0071\n",
      "validation Loss: 0.0001 Acc: 88.2521\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.9983\n",
      "validation Loss: 0.0001 Acc: 88.2521\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.9959\n",
      "validation Loss: 0.0001 Acc: 88.2657\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.0188\n",
      "validation Loss: 0.0001 Acc: 88.2716\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.0086\n",
      "validation Loss: 0.0001 Acc: 88.2638\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.0442\n",
      "validation Loss: 0.0001 Acc: 88.2677\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.0208\n",
      "validation Loss: 0.0001 Acc: 88.2599\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.0120\n",
      "validation Loss: 0.0001 Acc: 88.2735\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0388\n",
      "validation Loss: 0.0001 Acc: 88.2638\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.0486\n",
      "validation Loss: 0.0001 Acc: 88.2657\n",
      "Early stopped.\n",
      "Best val acc: 88.273529\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.0632\n",
      "validation Loss: 0.0001 Acc: 88.0529\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0711\n",
      "validation Loss: 0.0001 Acc: 88.0607\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.0706\n",
      "validation Loss: 0.0001 Acc: 88.0529\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.0608\n",
      "validation Loss: 0.0001 Acc: 88.0490\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.0725\n",
      "validation Loss: 0.0001 Acc: 88.0490\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.0999\n",
      "validation Loss: 0.0001 Acc: 88.0451\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.0340\n",
      "validation Loss: 0.0001 Acc: 88.0354\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.1106\n",
      "validation Loss: 0.0001 Acc: 88.0412\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.1135\n",
      "validation Loss: 0.0001 Acc: 88.0334\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.0418\n",
      "validation Loss: 0.0001 Acc: 88.0276\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0750\n",
      "validation Loss: 0.0001 Acc: 88.0256\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.0735\n",
      "validation Loss: 0.0001 Acc: 88.0256\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.0491\n",
      "validation Loss: 0.0001 Acc: 88.0295\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.0745\n",
      "validation Loss: 0.0001 Acc: 88.0256\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.0916\n",
      "validation Loss: 0.0001 Acc: 88.0256\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.0574\n",
      "validation Loss: 0.0001 Acc: 88.0237\n",
      "Early stopped.\n",
      "Best val acc: 88.060745\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.0120\n",
      "validation Loss: 0.0001 Acc: 88.2931\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.0237\n",
      "validation Loss: 0.0001 Acc: 88.2950\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.0218\n",
      "validation Loss: 0.0001 Acc: 88.2950\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.0047\n",
      "validation Loss: 0.0001 Acc: 88.2931\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.0149\n",
      "validation Loss: 0.0001 Acc: 88.2891\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.0227\n",
      "validation Loss: 0.0001 Acc: 88.2891\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.0301\n",
      "validation Loss: 0.0001 Acc: 88.2891\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.0125\n",
      "validation Loss: 0.0001 Acc: 88.2891\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.0037\n",
      "validation Loss: 0.0001 Acc: 88.2891\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.0203\n",
      "validation Loss: 0.0001 Acc: 88.2891\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.0271\n",
      "validation Loss: 0.0001 Acc: 88.2891\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.0296\n",
      "validation Loss: 0.0001 Acc: 88.2891\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.0218\n",
      "validation Loss: 0.0001 Acc: 88.2891\n",
      "Early stopped.\n",
      "Best val acc: 88.295006\n",
      "----------\n",
      "Average best_acc across k-fold: 88.16465759277344\n",
      "chi_t0\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 4, 6)\n",
      "background number: (170754, 4, 6)\n",
      "Data list: (341508, 4, 6)\n",
      "Data HLF: (341508, 8)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 82.1180\n",
      "validation Loss: 0.0001 Acc: 84.7366\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.0218\n",
      "validation Loss: 0.0001 Acc: 84.7092\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 86.0339\n",
      "validation Loss: 0.0001 Acc: 83.3779\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 86.5054\n",
      "validation Loss: 0.0001 Acc: 87.0693\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.2184\n",
      "validation Loss: 0.0001 Acc: 87.0518\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.2418\n",
      "validation Loss: 0.0001 Acc: 86.9971\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.4082\n",
      "validation Loss: 0.0001 Acc: 87.2977\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.4746\n",
      "validation Loss: 0.0001 Acc: 87.1279\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5595\n",
      "validation Loss: 0.0001 Acc: 87.4090\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5961\n",
      "validation Loss: 0.0001 Acc: 87.4929\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.4683\n",
      "validation Loss: 0.0001 Acc: 87.4031\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5288\n",
      "validation Loss: 0.0001 Acc: 87.1630\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.6981\n",
      "validation Loss: 0.0001 Acc: 87.5085\n",
      "Saving..\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.6903\n",
      "validation Loss: 0.0001 Acc: 87.5632\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.7225\n",
      "validation Loss: 0.0001 Acc: 87.6315\n",
      "Saving..\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.7997\n",
      "validation Loss: 0.0001 Acc: 87.6354\n",
      "Saving..\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.8528\n",
      "validation Loss: 0.0001 Acc: 87.3953\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.7650\n",
      "validation Loss: 0.0001 Acc: 87.7935\n",
      "Saving..\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.8294\n",
      "validation Loss: 0.0001 Acc: 87.6120\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.8094\n",
      "validation Loss: 0.0001 Acc: 87.5085\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.8782\n",
      "validation Loss: 0.0001 Acc: 87.4617\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.8636\n",
      "validation Loss: 0.0001 Acc: 87.7994\n",
      "Saving..\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.8953\n",
      "validation Loss: 0.0001 Acc: 87.4949\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.9509\n",
      "validation Loss: 0.0001 Acc: 87.2216\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.8338\n",
      "validation Loss: 0.0001 Acc: 87.8248\n",
      "Saving..\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.9197\n",
      "validation Loss: 0.0001 Acc: 87.4441\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.9841\n",
      "validation Loss: 0.0001 Acc: 87.6257\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.0134\n",
      "validation Loss: 0.0001 Acc: 87.9712\n",
      "Saving..\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.0485\n",
      "validation Loss: 0.0001 Acc: 87.9224\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.0866\n",
      "validation Loss: 0.0001 Acc: 87.9517\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 88.1027\n",
      "validation Loss: 0.0001 Acc: 87.9224\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 88.0725\n",
      "validation Loss: 0.0001 Acc: 87.7838\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 88.0407\n",
      "validation Loss: 0.0001 Acc: 87.9263\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 88.0705\n",
      "validation Loss: 0.0001 Acc: 87.7916\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 88.0754\n",
      "validation Loss: 0.0001 Acc: 88.0454\n",
      "Saving..\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 88.0510\n",
      "validation Loss: 0.0001 Acc: 87.9361\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 88.1344\n",
      "validation Loss: 0.0001 Acc: 87.8912\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 88.0998\n",
      "validation Loss: 0.0001 Acc: 87.9907\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 88.1061\n",
      "validation Loss: 0.0001 Acc: 87.9400\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 88.1720\n",
      "validation Loss: 0.0001 Acc: 88.0727\n",
      "Saving..\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 88.1935\n",
      "validation Loss: 0.0001 Acc: 88.0688\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 88.2096\n",
      "validation Loss: 0.0001 Acc: 87.9536\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 88.1911\n",
      "validation Loss: 0.0001 Acc: 88.0219\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 88.1774\n",
      "validation Loss: 0.0001 Acc: 88.0063\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 88.2013\n",
      "validation Loss: 0.0001 Acc: 87.9966\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 88.1896\n",
      "validation Loss: 0.0001 Acc: 88.0415\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 88.2013\n",
      "validation Loss: 0.0001 Acc: 88.0337\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 88.2535\n",
      "validation Loss: 0.0001 Acc: 88.0376\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 88.2345\n",
      "validation Loss: 0.0001 Acc: 88.0668\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.2399\n",
      "validation Loss: 0.0001 Acc: 88.0161\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 88.2389\n",
      "validation Loss: 0.0001 Acc: 88.1527\n",
      "Saving..\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 88.3043\n",
      "validation Loss: 0.0001 Acc: 88.0551\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.2862\n",
      "validation Loss: 0.0001 Acc: 88.1352\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 88.2726\n",
      "validation Loss: 0.0001 Acc: 88.1547\n",
      "Saving..\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.3023\n",
      "validation Loss: 0.0001 Acc: 88.0825\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.3062\n",
      "validation Loss: 0.0001 Acc: 88.1176\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 88.2750\n",
      "validation Loss: 0.0001 Acc: 88.1117\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.2901\n",
      "validation Loss: 0.0001 Acc: 88.1235\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.3048\n",
      "validation Loss: 0.0001 Acc: 88.1391\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.3082\n",
      "validation Loss: 0.0001 Acc: 88.1313\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.3194\n",
      "validation Loss: 0.0001 Acc: 88.1371\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.3350\n",
      "validation Loss: 0.0001 Acc: 88.1059\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.3287\n",
      "validation Loss: 0.0001 Acc: 88.1508\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.3511\n",
      "validation Loss: 0.0001 Acc: 88.1274\n",
      "Early stopped.\n",
      "Best val acc: 88.154686\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2887\n",
      "validation Loss: 0.0001 Acc: 88.4492\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2711\n",
      "validation Loss: 0.0001 Acc: 88.4199\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2389\n",
      "validation Loss: 0.0001 Acc: 88.4453\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2511\n",
      "validation Loss: 0.0001 Acc: 88.4395\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2716\n",
      "validation Loss: 0.0001 Acc: 88.4375\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2760\n",
      "validation Loss: 0.0001 Acc: 88.4063\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2921\n",
      "validation Loss: 0.0001 Acc: 88.4141\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2877\n",
      "validation Loss: 0.0001 Acc: 88.4414\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2511\n",
      "validation Loss: 0.0001 Acc: 88.4121\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.3082\n",
      "validation Loss: 0.0001 Acc: 88.4063\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2682\n",
      "validation Loss: 0.0001 Acc: 88.4082\n",
      "Early stopped.\n",
      "Best val acc: 88.449219\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2853\n",
      "validation Loss: 0.0001 Acc: 88.4766\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2824\n",
      "validation Loss: 0.0001 Acc: 88.4668\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2619\n",
      "validation Loss: 0.0001 Acc: 88.4726\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2755\n",
      "validation Loss: 0.0001 Acc: 88.4824\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2648\n",
      "validation Loss: 0.0001 Acc: 88.4726\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2926\n",
      "validation Loss: 0.0001 Acc: 88.4648\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.3014\n",
      "validation Loss: 0.0001 Acc: 88.4785\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2614\n",
      "validation Loss: 0.0001 Acc: 88.4648\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2697\n",
      "validation Loss: 0.0001 Acc: 88.4629\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2790\n",
      "validation Loss: 0.0001 Acc: 88.4590\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2594\n",
      "validation Loss: 0.0001 Acc: 88.4570\n",
      "Early stopped.\n",
      "Best val acc: 88.482407\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.3180\n",
      "validation Loss: 0.0001 Acc: 88.3594\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2946\n",
      "validation Loss: 0.0001 Acc: 88.3672\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2833\n",
      "validation Loss: 0.0001 Acc: 88.3692\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2702\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2448\n",
      "validation Loss: 0.0001 Acc: 88.3711\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2858\n",
      "validation Loss: 0.0001 Acc: 88.3711\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.3326\n",
      "validation Loss: 0.0001 Acc: 88.3692\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2838\n",
      "validation Loss: 0.0001 Acc: 88.3711\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2951\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2916\n",
      "validation Loss: 0.0001 Acc: 88.3711\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2765\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.2663\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.3009\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.2877\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.2882\n",
      "validation Loss: 0.0001 Acc: 88.3575\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.2507\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Early stopped.\n",
      "Best val acc: 88.371140\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.3019\n",
      "validation Loss: 0.0001 Acc: 88.4160\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2960\n",
      "validation Loss: 0.0001 Acc: 88.4121\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2546\n",
      "validation Loss: 0.0001 Acc: 88.4121\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2346\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2687\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.3209\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2726\n",
      "validation Loss: 0.0001 Acc: 88.4082\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.3156\n",
      "validation Loss: 0.0001 Acc: 88.4082\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2877\n",
      "validation Loss: 0.0001 Acc: 88.4082\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2721\n",
      "validation Loss: 0.0001 Acc: 88.4082\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.3073\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.2863\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.2697\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.2589\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.2804\n",
      "validation Loss: 0.0001 Acc: 88.4082\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.3092\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.3092\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.2877\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.3170\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 88.2912\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 88.2912\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.3068\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.3043\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.2853\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.2955\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.2619\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.2853\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.2687\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.2751\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.2936\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Early stopped.\n",
      "Best val acc: 88.416039\n",
      "----------\n",
      "Average best_acc across k-fold: 88.37470245361328\n",
      "chi_t1\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 4, 6)\n",
      "background number: (170754, 4, 6)\n",
      "Data list: (341508, 4, 6)\n",
      "Data HLF: (341508, 8)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.1790\n",
      "validation Loss: 0.0001 Acc: 84.7971\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.1340\n",
      "validation Loss: 0.0001 Acc: 85.2558\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.9729\n",
      "validation Loss: 0.0001 Acc: 84.5765\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 86.9090\n",
      "validation Loss: 0.0001 Acc: 85.0782\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0388\n",
      "validation Loss: 0.0001 Acc: 87.3270\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.3536\n",
      "validation Loss: 0.0001 Acc: 87.4246\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.5034\n",
      "validation Loss: 0.0001 Acc: 87.2372\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.4858\n",
      "validation Loss: 0.0001 Acc: 87.5183\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.4551\n",
      "validation Loss: 0.0001 Acc: 87.5847\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.6274\n",
      "validation Loss: 0.0001 Acc: 87.2333\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5595\n",
      "validation Loss: 0.0001 Acc: 87.6296\n",
      "Saving..\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.6772\n",
      "validation Loss: 0.0001 Acc: 87.7428\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.7152\n",
      "validation Loss: 0.0001 Acc: 87.7252\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.7626\n",
      "validation Loss: 0.0001 Acc: 87.3973\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.7806\n",
      "validation Loss: 0.0001 Acc: 87.4441\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.8563\n",
      "validation Loss: 0.0001 Acc: 87.7291\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.8641\n",
      "validation Loss: 0.0001 Acc: 87.8267\n",
      "Saving..\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.8636\n",
      "validation Loss: 0.0001 Acc: 87.7291\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.9104\n",
      "validation Loss: 0.0001 Acc: 87.8072\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.8602\n",
      "validation Loss: 0.0001 Acc: 87.6862\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.9187\n",
      "validation Loss: 0.0001 Acc: 87.7096\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.0315\n",
      "validation Loss: 0.0001 Acc: 87.7096\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.0061\n",
      "validation Loss: 0.0001 Acc: 87.8990\n",
      "Saving..\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.0788\n",
      "validation Loss: 0.0001 Acc: 87.9966\n",
      "Saving..\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.0378\n",
      "validation Loss: 0.0001 Acc: 87.9673\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.1061\n",
      "validation Loss: 0.0001 Acc: 88.0571\n",
      "Saving..\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.1071\n",
      "validation Loss: 0.0001 Acc: 87.9575\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.0739\n",
      "validation Loss: 0.0001 Acc: 88.0297\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.1349\n",
      "validation Loss: 0.0001 Acc: 88.0337\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.1715\n",
      "validation Loss: 0.0001 Acc: 88.0532\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 88.1911\n",
      "validation Loss: 0.0001 Acc: 88.0024\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 88.2023\n",
      "validation Loss: 0.0001 Acc: 88.0864\n",
      "Saving..\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 88.1808\n",
      "validation Loss: 0.0001 Acc: 87.9888\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 88.2574\n",
      "validation Loss: 0.0001 Acc: 88.1117\n",
      "Saving..\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 88.2242\n",
      "validation Loss: 0.0001 Acc: 88.0473\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 88.2418\n",
      "validation Loss: 0.0001 Acc: 88.0512\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 88.2364\n",
      "validation Loss: 0.0001 Acc: 88.0707\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 88.2057\n",
      "validation Loss: 0.0001 Acc: 88.0688\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 88.2233\n",
      "validation Loss: 0.0001 Acc: 88.1039\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 88.1964\n",
      "validation Loss: 0.0001 Acc: 88.0629\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 88.2477\n",
      "validation Loss: 0.0001 Acc: 88.0493\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 88.2384\n",
      "validation Loss: 0.0001 Acc: 88.0668\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 88.2194\n",
      "validation Loss: 0.0001 Acc: 88.0551\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 88.2574\n",
      "validation Loss: 0.0001 Acc: 88.1195\n",
      "Saving..\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 88.2457\n",
      "validation Loss: 0.0001 Acc: 88.0942\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 88.2311\n",
      "validation Loss: 0.0001 Acc: 88.0903\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 88.2467\n",
      "validation Loss: 0.0001 Acc: 88.1078\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 88.2569\n",
      "validation Loss: 0.0001 Acc: 88.0610\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 88.2574\n",
      "validation Loss: 0.0001 Acc: 88.0883\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.2350\n",
      "validation Loss: 0.0001 Acc: 88.0786\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 88.2794\n",
      "validation Loss: 0.0001 Acc: 88.1235\n",
      "Saving..\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 88.2804\n",
      "validation Loss: 0.0001 Acc: 88.0746\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.2682\n",
      "validation Loss: 0.0001 Acc: 88.1000\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 88.2677\n",
      "validation Loss: 0.0001 Acc: 88.0746\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.2755\n",
      "validation Loss: 0.0001 Acc: 88.1117\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.2491\n",
      "validation Loss: 0.0001 Acc: 88.1176\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 88.2667\n",
      "validation Loss: 0.0001 Acc: 88.1020\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.3048\n",
      "validation Loss: 0.0001 Acc: 88.0688\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.2682\n",
      "validation Loss: 0.0001 Acc: 88.0786\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.2428\n",
      "validation Loss: 0.0001 Acc: 88.1039\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.2457\n",
      "validation Loss: 0.0001 Acc: 88.1020\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.2852\n",
      "validation Loss: 0.0001 Acc: 88.0961\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.2521\n",
      "validation Loss: 0.0001 Acc: 88.1020\n",
      "Early stopped.\n",
      "Best val acc: 88.123451\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2248\n",
      "validation Loss: 0.0001 Acc: 88.3048\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2424\n",
      "validation Loss: 0.0001 Acc: 88.3145\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2316\n",
      "validation Loss: 0.0001 Acc: 88.3184\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2204\n",
      "validation Loss: 0.0001 Acc: 88.3145\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2297\n",
      "validation Loss: 0.0001 Acc: 88.3223\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2389\n",
      "validation Loss: 0.0001 Acc: 88.3165\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2658\n",
      "validation Loss: 0.0001 Acc: 88.3204\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2736\n",
      "validation Loss: 0.0001 Acc: 88.3223\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2180\n",
      "validation Loss: 0.0001 Acc: 88.3087\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2707\n",
      "validation Loss: 0.0001 Acc: 88.3223\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2760\n",
      "validation Loss: 0.0001 Acc: 88.3165\n",
      "Early stopped.\n",
      "Best val acc: 88.322334\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2155\n",
      "validation Loss: 0.0001 Acc: 88.4219\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2282\n",
      "validation Loss: 0.0001 Acc: 88.4277\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2043\n",
      "validation Loss: 0.0001 Acc: 88.4219\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.1755\n",
      "validation Loss: 0.0001 Acc: 88.4199\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2067\n",
      "validation Loss: 0.0001 Acc: 88.4180\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2238\n",
      "validation Loss: 0.0001 Acc: 88.4199\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.1872\n",
      "validation Loss: 0.0001 Acc: 88.4199\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2253\n",
      "validation Loss: 0.0001 Acc: 88.4180\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.1921\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2433\n",
      "validation Loss: 0.0001 Acc: 88.4024\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.1989\n",
      "validation Loss: 0.0001 Acc: 88.4082\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.2370\n",
      "validation Loss: 0.0001 Acc: 88.4024\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.2209\n",
      "validation Loss: 0.0001 Acc: 88.4082\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.2516\n",
      "validation Loss: 0.0001 Acc: 88.4082\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.2165\n",
      "validation Loss: 0.0001 Acc: 88.4082\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.1882\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.2194\n",
      "validation Loss: 0.0001 Acc: 88.4102\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.1984\n",
      "validation Loss: 0.0001 Acc: 88.4024\n",
      "Early stopped.\n",
      "Best val acc: 88.427750\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.1535\n",
      "validation Loss: 0.0001 Acc: 88.5976\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.1457\n",
      "validation Loss: 0.0001 Acc: 88.5995\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.1677\n",
      "validation Loss: 0.0001 Acc: 88.5976\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.1482\n",
      "validation Loss: 0.0001 Acc: 88.6034\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.1970\n",
      "validation Loss: 0.0001 Acc: 88.6015\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.1731\n",
      "validation Loss: 0.0001 Acc: 88.5976\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.1594\n",
      "validation Loss: 0.0001 Acc: 88.5995\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.1960\n",
      "validation Loss: 0.0001 Acc: 88.5995\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.1482\n",
      "validation Loss: 0.0001 Acc: 88.6034\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.1687\n",
      "validation Loss: 0.0001 Acc: 88.6034\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.1735\n",
      "validation Loss: 0.0001 Acc: 88.6073\n",
      "Saving..\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.1692\n",
      "validation Loss: 0.0001 Acc: 88.6054\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.1516\n",
      "validation Loss: 0.0001 Acc: 88.6073\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.1472\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.1706\n",
      "validation Loss: 0.0001 Acc: 88.6073\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.1667\n",
      "validation Loss: 0.0001 Acc: 88.6073\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.1589\n",
      "validation Loss: 0.0001 Acc: 88.6073\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.1862\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.1887\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 88.1657\n",
      "validation Loss: 0.0001 Acc: 88.6073\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 88.1609\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.1692\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.1379\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.1657\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.1579\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.1799\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.1526\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.1853\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.2038\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.1706\n",
      "validation Loss: 0.0001 Acc: 88.6093\n",
      "Early stopped.\n",
      "Best val acc: 88.609299\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.3082\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2594\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2892\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2672\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2594\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2882\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2936\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2516\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2819\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2770\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2711\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.2775\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.3253\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.3024\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.3063\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.2711\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.2794\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.2877\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.2838\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 88.2629\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 88.2833\n",
      "validation Loss: 0.0001 Acc: 88.1369\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.3038\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.2711\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.3024\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.2833\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.2570\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.2721\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.2682\n",
      "validation Loss: 0.0001 Acc: 88.1349\n",
      "Early stopped.\n",
      "Best val acc: 88.136879\n",
      "----------\n",
      "Average best_acc across k-fold: 88.32394409179688\n",
      "lepton1_pt\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 82.7592\n",
      "validation Loss: 0.0001 Acc: 85.3124\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.3893\n",
      "validation Loss: 0.0001 Acc: 85.7243\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.9359\n",
      "validation Loss: 0.0001 Acc: 85.7770\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 86.0906\n",
      "validation Loss: 0.0001 Acc: 86.1167\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.1481\n",
      "validation Loss: 0.0001 Acc: 86.2670\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 86.3024\n",
      "validation Loss: 0.0001 Acc: 86.3158\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 86.2418\n",
      "validation Loss: 0.0001 Acc: 86.4154\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 86.3248\n",
      "validation Loss: 0.0001 Acc: 86.4056\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 86.4146\n",
      "validation Loss: 0.0001 Acc: 86.3607\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 86.4400\n",
      "validation Loss: 0.0001 Acc: 86.4154\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 86.4649\n",
      "validation Loss: 0.0001 Acc: 86.4368\n",
      "Saving..\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 86.4156\n",
      "validation Loss: 0.0001 Acc: 86.5520\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 86.5752\n",
      "validation Loss: 0.0001 Acc: 86.5364\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 86.6049\n",
      "validation Loss: 0.0001 Acc: 86.5540\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 86.5898\n",
      "validation Loss: 0.0001 Acc: 86.4193\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 86.7128\n",
      "validation Loss: 0.0001 Acc: 86.4134\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 86.6489\n",
      "validation Loss: 0.0001 Acc: 86.3841\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 86.7948\n",
      "validation Loss: 0.0001 Acc: 86.7902\n",
      "Saving..\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 86.8089\n",
      "validation Loss: 0.0001 Acc: 86.6613\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 86.7616\n",
      "validation Loss: 0.0001 Acc: 86.7433\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 86.8441\n",
      "validation Loss: 0.0001 Acc: 86.8331\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 86.8573\n",
      "validation Loss: 0.0001 Acc: 86.8429\n",
      "Saving..\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 86.7343\n",
      "validation Loss: 0.0001 Acc: 86.7648\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 86.9051\n",
      "validation Loss: 0.0001 Acc: 86.8351\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 86.8621\n",
      "validation Loss: 0.0001 Acc: 86.6965\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 86.8724\n",
      "validation Loss: 0.0001 Acc: 86.6223\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 86.9109\n",
      "validation Loss: 0.0001 Acc: 86.8936\n",
      "Saving..\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 86.9534\n",
      "validation Loss: 0.0001 Acc: 86.8702\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 86.9758\n",
      "validation Loss: 0.0001 Acc: 86.8683\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 86.9227\n",
      "validation Loss: 0.0001 Acc: 86.8214\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 86.9719\n",
      "validation Loss: 0.0001 Acc: 86.9171\n",
      "Saving..\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 86.9934\n",
      "validation Loss: 0.0001 Acc: 86.9053\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 86.9871\n",
      "validation Loss: 0.0001 Acc: 86.9385\n",
      "Saving..\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 86.9729\n",
      "validation Loss: 0.0001 Acc: 86.9776\n",
      "Saving..\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.0139\n",
      "validation Loss: 0.0001 Acc: 86.8819\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.0427\n",
      "validation Loss: 0.0001 Acc: 86.8897\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 86.9856\n",
      "validation Loss: 0.0001 Acc: 86.8917\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.0183\n",
      "validation Loss: 0.0001 Acc: 86.9132\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 87.0300\n",
      "validation Loss: 0.0001 Acc: 86.9171\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 87.0232\n",
      "validation Loss: 0.0001 Acc: 86.9132\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 87.0803\n",
      "validation Loss: 0.0001 Acc: 86.9502\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.0881\n",
      "validation Loss: 0.0001 Acc: 86.8975\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 87.1047\n",
      "validation Loss: 0.0001 Acc: 86.9873\n",
      "Saving..\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.0847\n",
      "validation Loss: 0.0001 Acc: 86.9210\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.0617\n",
      "validation Loss: 0.0001 Acc: 86.9502\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.0695\n",
      "validation Loss: 0.0001 Acc: 86.9717\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.1032\n",
      "validation Loss: 0.0001 Acc: 86.9639\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 87.0783\n",
      "validation Loss: 0.0001 Acc: 86.9541\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 87.0637\n",
      "validation Loss: 0.0001 Acc: 86.9756\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 87.1071\n",
      "validation Loss: 0.0001 Acc: 86.9951\n",
      "Saving..\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.0944\n",
      "validation Loss: 0.0001 Acc: 86.9483\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.0466\n",
      "validation Loss: 0.0001 Acc: 86.9581\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 87.0979\n",
      "validation Loss: 0.0001 Acc: 86.8975\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.0832\n",
      "validation Loss: 0.0001 Acc: 86.9776\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 87.1125\n",
      "validation Loss: 0.0001 Acc: 86.9776\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 87.0930\n",
      "validation Loss: 0.0001 Acc: 86.9424\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.1345\n",
      "validation Loss: 0.0001 Acc: 86.9834\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 87.0930\n",
      "validation Loss: 0.0001 Acc: 86.9620\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 87.1462\n",
      "validation Loss: 0.0001 Acc: 86.9678\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 87.1545\n",
      "validation Loss: 0.0001 Acc: 86.9600\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 87.1144\n",
      "validation Loss: 0.0001 Acc: 86.9815\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 87.1437\n",
      "validation Loss: 0.0001 Acc: 86.9659\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 87.1642\n",
      "validation Loss: 0.0001 Acc: 86.9600\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 87.1501\n",
      "validation Loss: 0.0001 Acc: 86.9678\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 87.0993\n",
      "validation Loss: 0.0001 Acc: 86.9932\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 87.1413\n",
      "validation Loss: 0.0001 Acc: 86.9698\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 87.1325\n",
      "validation Loss: 0.0001 Acc: 86.9893\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 87.1066\n",
      "validation Loss: 0.0001 Acc: 86.9834\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 87.1525\n",
      "validation Loss: 0.0001 Acc: 86.9620\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 87.1164\n",
      "validation Loss: 0.0001 Acc: 86.9834\n",
      "Best val acc: 86.995140\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.0516\n",
      "validation Loss: 0.0001 Acc: 87.1979\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.0691\n",
      "validation Loss: 0.0001 Acc: 87.1667\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0491\n",
      "validation Loss: 0.0001 Acc: 87.1842\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0501\n",
      "validation Loss: 0.0001 Acc: 87.1881\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.1057\n",
      "validation Loss: 0.0001 Acc: 87.1706\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.0921\n",
      "validation Loss: 0.0001 Acc: 87.1667\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.0838\n",
      "validation Loss: 0.0001 Acc: 87.1823\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0525\n",
      "validation Loss: 0.0001 Acc: 87.1725\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0872\n",
      "validation Loss: 0.0001 Acc: 87.1686\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.0843\n",
      "validation Loss: 0.0001 Acc: 87.1764\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0745\n",
      "validation Loss: 0.0001 Acc: 87.1725\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0838\n",
      "validation Loss: 0.0001 Acc: 87.1647\n",
      "Early stopped.\n",
      "Best val acc: 87.197906\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.1213\n",
      "validation Loss: 0.0001 Acc: 87.0944\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.1170\n",
      "validation Loss: 0.0001 Acc: 87.0925\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0960\n",
      "validation Loss: 0.0001 Acc: 87.1003\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.1223\n",
      "validation Loss: 0.0001 Acc: 87.0905\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.1643\n",
      "validation Loss: 0.0001 Acc: 87.0944\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.1033\n",
      "validation Loss: 0.0001 Acc: 87.0847\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.1350\n",
      "validation Loss: 0.0001 Acc: 87.0886\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.1135\n",
      "validation Loss: 0.0001 Acc: 87.0925\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0867\n",
      "validation Loss: 0.0001 Acc: 87.0925\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.0989\n",
      "validation Loss: 0.0001 Acc: 87.0944\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.1336\n",
      "validation Loss: 0.0001 Acc: 87.0905\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.1238\n",
      "validation Loss: 0.0001 Acc: 87.0964\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.1028\n",
      "validation Loss: 0.0001 Acc: 87.0925\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.0843\n",
      "validation Loss: 0.0001 Acc: 87.0944\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0882\n",
      "validation Loss: 0.0001 Acc: 87.0944\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.0774\n",
      "validation Loss: 0.0001 Acc: 87.0944\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.0940\n",
      "validation Loss: 0.0001 Acc: 87.0983\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.1131\n",
      "validation Loss: 0.0001 Acc: 87.0983\n",
      "Early stopped.\n",
      "Best val acc: 87.100296\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.0721\n",
      "validation Loss: 0.0001 Acc: 87.2330\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.0750\n",
      "validation Loss: 0.0001 Acc: 87.2350\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0784\n",
      "validation Loss: 0.0001 Acc: 87.2350\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0486\n",
      "validation Loss: 0.0001 Acc: 87.2350\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0833\n",
      "validation Loss: 0.0001 Acc: 87.2350\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.0594\n",
      "validation Loss: 0.0001 Acc: 87.2330\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.0725\n",
      "validation Loss: 0.0001 Acc: 87.2330\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0706\n",
      "validation Loss: 0.0001 Acc: 87.2330\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.1043\n",
      "validation Loss: 0.0001 Acc: 87.2311\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.1062\n",
      "validation Loss: 0.0001 Acc: 87.2330\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0926\n",
      "validation Loss: 0.0001 Acc: 87.2311\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0652\n",
      "validation Loss: 0.0001 Acc: 87.2311\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.0828\n",
      "validation Loss: 0.0001 Acc: 87.2311\n",
      "Early stopped.\n",
      "Best val acc: 87.234993\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.0086\n",
      "validation Loss: 0.0001 Acc: 87.3385\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.0481\n",
      "validation Loss: 0.0001 Acc: 87.3365\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0418\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0506\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0467\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.0384\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.0477\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0506\n",
      "validation Loss: 0.0001 Acc: 87.3326\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0735\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.0638\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0184\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0462\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.0438\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.0540\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0096\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.0511\n",
      "validation Loss: 0.0001 Acc: 87.3346\n",
      "Early stopped.\n",
      "Best val acc: 87.338455\n",
      "----------\n",
      "Average best_acc across k-fold: 87.17335510253906\n",
      "lepton2_pt\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.8037\n",
      "validation Loss: 0.0001 Acc: 84.8634\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.2672\n",
      "validation Loss: 0.0001 Acc: 85.2558\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.6572\n",
      "validation Loss: 0.0001 Acc: 85.8434\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 86.0247\n",
      "validation Loss: 0.0001 Acc: 86.0406\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.1208\n",
      "validation Loss: 0.0001 Acc: 86.0659\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 86.1721\n",
      "validation Loss: 0.0001 Acc: 86.2006\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 86.3116\n",
      "validation Loss: 0.0001 Acc: 86.1928\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 86.4776\n",
      "validation Loss: 0.0001 Acc: 86.2787\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 86.4766\n",
      "validation Loss: 0.0001 Acc: 86.4290\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 86.6079\n",
      "validation Loss: 0.0001 Acc: 86.6516\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 86.6757\n",
      "validation Loss: 0.0001 Acc: 86.5715\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 86.8407\n",
      "validation Loss: 0.0001 Acc: 86.1440\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 86.9026\n",
      "validation Loss: 0.0001 Acc: 86.7453\n",
      "Saving..\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 86.8709\n",
      "validation Loss: 0.0001 Acc: 86.6848\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0012\n",
      "validation Loss: 0.0001 Acc: 86.8390\n",
      "Saving..\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 86.9378\n",
      "validation Loss: 0.0001 Acc: 86.9132\n",
      "Saving..\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.0627\n",
      "validation Loss: 0.0001 Acc: 86.9229\n",
      "Saving..\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 86.9973\n",
      "validation Loss: 0.0001 Acc: 86.5032\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.0290\n",
      "validation Loss: 0.0001 Acc: 87.1181\n",
      "Saving..\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.0486\n",
      "validation Loss: 0.0001 Acc: 86.6965\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.0964\n",
      "validation Loss: 0.0001 Acc: 86.8663\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.0666\n",
      "validation Loss: 0.0001 Acc: 86.9659\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.1140\n",
      "validation Loss: 0.0001 Acc: 86.9073\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.2882\n",
      "validation Loss: 0.0001 Acc: 87.2099\n",
      "Saving..\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.3067\n",
      "validation Loss: 0.0001 Acc: 87.2216\n",
      "Saving..\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.3380\n",
      "validation Loss: 0.0001 Acc: 87.2040\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.4165\n",
      "validation Loss: 0.0001 Acc: 86.9795\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.3741\n",
      "validation Loss: 0.0001 Acc: 87.0869\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.4326\n",
      "validation Loss: 0.0001 Acc: 87.3192\n",
      "Saving..\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.4780\n",
      "validation Loss: 0.0001 Acc: 87.1708\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.4497\n",
      "validation Loss: 0.0001 Acc: 87.2880\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.4883\n",
      "validation Loss: 0.0001 Acc: 87.3270\n",
      "Saving..\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.4858\n",
      "validation Loss: 0.0001 Acc: 87.2196\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.4976\n",
      "validation Loss: 0.0001 Acc: 87.3192\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.4834\n",
      "validation Loss: 0.0001 Acc: 87.3153\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.4902\n",
      "validation Loss: 0.0001 Acc: 87.1943\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.5093\n",
      "validation Loss: 0.0001 Acc: 87.2372\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.5142\n",
      "validation Loss: 0.0001 Acc: 87.3153\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 87.4912\n",
      "validation Loss: 0.0001 Acc: 87.2138\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 87.5708\n",
      "validation Loss: 0.0001 Acc: 87.3485\n",
      "Saving..\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 87.5390\n",
      "validation Loss: 0.0001 Acc: 87.2509\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.5298\n",
      "validation Loss: 0.0001 Acc: 87.2899\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 87.5595\n",
      "validation Loss: 0.0001 Acc: 87.2880\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.5547\n",
      "validation Loss: 0.0001 Acc: 87.3738\n",
      "Saving..\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.5654\n",
      "validation Loss: 0.0001 Acc: 87.2704\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.5600\n",
      "validation Loss: 0.0001 Acc: 87.2489\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.5327\n",
      "validation Loss: 0.0001 Acc: 87.3856\n",
      "Saving..\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 87.5312\n",
      "validation Loss: 0.0001 Acc: 87.3602\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 87.5961\n",
      "validation Loss: 0.0001 Acc: 87.2801\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 87.5893\n",
      "validation Loss: 0.0001 Acc: 87.3680\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.5688\n",
      "validation Loss: 0.0001 Acc: 87.3114\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.5503\n",
      "validation Loss: 0.0001 Acc: 87.2782\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 87.5878\n",
      "validation Loss: 0.0001 Acc: 87.3582\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.6025\n",
      "validation Loss: 0.0001 Acc: 87.3387\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 87.5542\n",
      "validation Loss: 0.0001 Acc: 87.3504\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 87.5917\n",
      "validation Loss: 0.0001 Acc: 87.3250\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.5883\n",
      "validation Loss: 0.0001 Acc: 87.2880\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 87.5781\n",
      "validation Loss: 0.0001 Acc: 87.3699\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 87.6259\n",
      "validation Loss: 0.0001 Acc: 87.3407\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 87.5795\n",
      "validation Loss: 0.0001 Acc: 87.3309\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 87.6318\n",
      "validation Loss: 0.0001 Acc: 87.3543\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 87.6040\n",
      "validation Loss: 0.0001 Acc: 87.3309\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 87.5795\n",
      "validation Loss: 0.0001 Acc: 87.3621\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 87.5888\n",
      "validation Loss: 0.0001 Acc: 87.3309\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 87.6098\n",
      "validation Loss: 0.0001 Acc: 87.3524\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 87.6196\n",
      "validation Loss: 0.0001 Acc: 87.3582\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 87.6069\n",
      "validation Loss: 0.0001 Acc: 87.3524\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 87.6015\n",
      "validation Loss: 0.0001 Acc: 87.3524\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 87.6069\n",
      "validation Loss: 0.0001 Acc: 87.3446\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 87.6127\n",
      "validation Loss: 0.0001 Acc: 87.3504\n",
      "Best val acc: 87.385559\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.4947\n",
      "validation Loss: 0.0001 Acc: 87.7094\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.5201\n",
      "validation Loss: 0.0001 Acc: 87.6723\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.5303\n",
      "validation Loss: 0.0001 Acc: 87.6762\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.5210\n",
      "validation Loss: 0.0001 Acc: 87.6820\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.5079\n",
      "validation Loss: 0.0001 Acc: 87.6937\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5171\n",
      "validation Loss: 0.0001 Acc: 87.6859\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.5210\n",
      "validation Loss: 0.0001 Acc: 87.6996\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.5328\n",
      "validation Loss: 0.0001 Acc: 87.6996\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5484\n",
      "validation Loss: 0.0001 Acc: 87.6859\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5484\n",
      "validation Loss: 0.0001 Acc: 87.6957\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5347\n",
      "validation Loss: 0.0001 Acc: 87.6996\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5010\n",
      "validation Loss: 0.0001 Acc: 87.7074\n",
      "Early stopped.\n",
      "Best val acc: 87.709366\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.5411\n",
      "validation Loss: 0.0001 Acc: 87.6703\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.5040\n",
      "validation Loss: 0.0001 Acc: 87.6762\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.5201\n",
      "validation Loss: 0.0001 Acc: 87.6684\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.5689\n",
      "validation Loss: 0.0001 Acc: 87.6684\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.4918\n",
      "validation Loss: 0.0001 Acc: 87.6879\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5391\n",
      "validation Loss: 0.0001 Acc: 87.6859\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.5362\n",
      "validation Loss: 0.0001 Acc: 87.6703\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.5352\n",
      "validation Loss: 0.0001 Acc: 87.6762\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5137\n",
      "validation Loss: 0.0001 Acc: 87.6820\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5210\n",
      "validation Loss: 0.0001 Acc: 87.6781\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5337\n",
      "validation Loss: 0.0001 Acc: 87.6762\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5118\n",
      "validation Loss: 0.0001 Acc: 87.6781\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.5245\n",
      "validation Loss: 0.0001 Acc: 87.6801\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.5489\n",
      "validation Loss: 0.0001 Acc: 87.6820\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.5445\n",
      "validation Loss: 0.0001 Acc: 87.6820\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.5074\n",
      "validation Loss: 0.0001 Acc: 87.6801\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.5269\n",
      "validation Loss: 0.0001 Acc: 87.6840\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.5215\n",
      "validation Loss: 0.0001 Acc: 87.6840\n",
      "Early stopped.\n",
      "Best val acc: 87.687889\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.5533\n",
      "validation Loss: 0.0001 Acc: 87.4946\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.5415\n",
      "validation Loss: 0.0001 Acc: 87.4946\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.5645\n",
      "validation Loss: 0.0001 Acc: 87.4868\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.5772\n",
      "validation Loss: 0.0001 Acc: 87.4868\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.5669\n",
      "validation Loss: 0.0001 Acc: 87.4849\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5694\n",
      "validation Loss: 0.0001 Acc: 87.4849\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.5450\n",
      "validation Loss: 0.0001 Acc: 87.4829\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.5669\n",
      "validation Loss: 0.0001 Acc: 87.4849\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5518\n",
      "validation Loss: 0.0001 Acc: 87.4829\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5542\n",
      "validation Loss: 0.0001 Acc: 87.4829\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5718\n",
      "validation Loss: 0.0001 Acc: 87.4829\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5616\n",
      "validation Loss: 0.0001 Acc: 87.4829\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.5464\n",
      "validation Loss: 0.0001 Acc: 87.4829\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.5645\n",
      "validation Loss: 0.0001 Acc: 87.4829\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.5533\n",
      "validation Loss: 0.0001 Acc: 87.4849\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.5972\n",
      "validation Loss: 0.0001 Acc: 87.4849\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.5533\n",
      "validation Loss: 0.0001 Acc: 87.4849\n",
      "Early stopped.\n",
      "Best val acc: 87.494629\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.5084\n",
      "validation Loss: 0.0001 Acc: 87.7640\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.5147\n",
      "validation Loss: 0.0001 Acc: 87.7640\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.5240\n",
      "validation Loss: 0.0001 Acc: 87.7640\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.4825\n",
      "validation Loss: 0.0001 Acc: 87.7640\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.5059\n",
      "validation Loss: 0.0001 Acc: 87.7640\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5162\n",
      "validation Loss: 0.0001 Acc: 87.7640\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.5079\n",
      "validation Loss: 0.0001 Acc: 87.7621\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.4786\n",
      "validation Loss: 0.0001 Acc: 87.7621\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5035\n",
      "validation Loss: 0.0001 Acc: 87.7621\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.4815\n",
      "validation Loss: 0.0001 Acc: 87.7621\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5225\n",
      "validation Loss: 0.0001 Acc: 87.7621\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5113\n",
      "validation Loss: 0.0001 Acc: 87.7621\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.5084\n",
      "validation Loss: 0.0001 Acc: 87.7621\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.5059\n",
      "validation Loss: 0.0001 Acc: 87.7640\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.5157\n",
      "validation Loss: 0.0001 Acc: 87.7640\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.4664\n",
      "validation Loss: 0.0001 Acc: 87.7640\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.5118\n",
      "validation Loss: 0.0001 Acc: 87.7660\n",
      "Saving..\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.4927\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Saving..\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.4810\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.5069\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.5279\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.4927\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.5001\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.4962\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.5332\n",
      "validation Loss: 0.0001 Acc: 87.7660\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.4927\n",
      "validation Loss: 0.0001 Acc: 87.7660\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.5367\n",
      "validation Loss: 0.0001 Acc: 87.7660\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.5054\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.5279\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.4747\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.5196\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.4727\n",
      "validation Loss: 0.0001 Acc: 87.7679\n",
      "Early stopped.\n",
      "Best val acc: 87.767929\n",
      "----------\n",
      "Average best_acc across k-fold: 87.60906982421875\n",
      "pt\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.5172\n",
      "validation Loss: 0.0001 Acc: 85.0684\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.3414\n",
      "validation Loss: 0.0001 Acc: 86.2260\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 86.6401\n",
      "validation Loss: 0.0001 Acc: 86.3744\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0061\n",
      "validation Loss: 0.0001 Acc: 86.4759\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.2867\n",
      "validation Loss: 0.0001 Acc: 87.3016\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.4175\n",
      "validation Loss: 0.0001 Acc: 87.3621\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.4810\n",
      "validation Loss: 0.0001 Acc: 87.5554\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.5713\n",
      "validation Loss: 0.0001 Acc: 87.5983\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5551\n",
      "validation Loss: 0.0001 Acc: 87.6081\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5376\n",
      "validation Loss: 0.0001 Acc: 87.2821\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5229\n",
      "validation Loss: 0.0001 Acc: 87.6179\n",
      "Saving..\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.6967\n",
      "validation Loss: 0.0001 Acc: 87.7213\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.8084\n",
      "validation Loss: 0.0001 Acc: 87.5300\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.7474\n",
      "validation Loss: 0.0001 Acc: 87.7350\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.8343\n",
      "validation Loss: 0.0001 Acc: 87.9282\n",
      "Saving..\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.8221\n",
      "validation Loss: 0.0001 Acc: 87.9692\n",
      "Saving..\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.8655\n",
      "validation Loss: 0.0001 Acc: 87.6061\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.8358\n",
      "validation Loss: 0.0001 Acc: 87.9751\n",
      "Saving..\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.8592\n",
      "validation Loss: 0.0001 Acc: 87.9497\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.8665\n",
      "validation Loss: 0.0001 Acc: 87.9809\n",
      "Saving..\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.8665\n",
      "validation Loss: 0.0001 Acc: 88.1039\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.9914\n",
      "validation Loss: 0.0001 Acc: 88.0141\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.9749\n",
      "validation Loss: 0.0001 Acc: 88.1352\n",
      "Saving..\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.9583\n",
      "validation Loss: 0.0001 Acc: 88.0122\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.9090\n",
      "validation Loss: 0.0001 Acc: 88.0415\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.0754\n",
      "validation Loss: 0.0001 Acc: 88.1684\n",
      "Saving..\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.1183\n",
      "validation Loss: 0.0001 Acc: 88.2230\n",
      "Saving..\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.1398\n",
      "validation Loss: 0.0001 Acc: 88.3206\n",
      "Saving..\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.1520\n",
      "validation Loss: 0.0001 Acc: 88.2640\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.1598\n",
      "validation Loss: 0.0001 Acc: 88.2757\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 88.1681\n",
      "validation Loss: 0.0001 Acc: 88.2933\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 88.1828\n",
      "validation Loss: 0.0001 Acc: 88.2874\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 88.2169\n",
      "validation Loss: 0.0001 Acc: 88.2738\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 88.1676\n",
      "validation Loss: 0.0001 Acc: 88.1410\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 88.2477\n",
      "validation Loss: 0.0001 Acc: 88.3265\n",
      "Saving..\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 88.3028\n",
      "validation Loss: 0.0001 Acc: 88.3909\n",
      "Saving..\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 88.3184\n",
      "validation Loss: 0.0001 Acc: 88.3831\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 88.2774\n",
      "validation Loss: 0.0001 Acc: 88.3265\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 88.3023\n",
      "validation Loss: 0.0001 Acc: 88.3479\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 88.3365\n",
      "validation Loss: 0.0001 Acc: 88.3714\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 88.3462\n",
      "validation Loss: 0.0001 Acc: 88.3343\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 88.3414\n",
      "validation Loss: 0.0001 Acc: 88.2601\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 88.3155\n",
      "validation Loss: 0.0001 Acc: 88.3909\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 88.3389\n",
      "validation Loss: 0.0001 Acc: 88.3401\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 88.3565\n",
      "validation Loss: 0.0001 Acc: 88.4514\n",
      "Saving..\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 88.3526\n",
      "validation Loss: 0.0001 Acc: 88.3753\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 88.4156\n",
      "validation Loss: 0.0001 Acc: 88.4202\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 88.3877\n",
      "validation Loss: 0.0001 Acc: 88.4358\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 88.3853\n",
      "validation Loss: 0.0001 Acc: 88.3811\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.3521\n",
      "validation Loss: 0.0001 Acc: 88.3772\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 88.4443\n",
      "validation Loss: 0.0001 Acc: 88.4085\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 88.4180\n",
      "validation Loss: 0.0001 Acc: 88.4182\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.4180\n",
      "validation Loss: 0.0001 Acc: 88.3987\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 88.3731\n",
      "validation Loss: 0.0001 Acc: 88.4494\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.4136\n",
      "validation Loss: 0.0001 Acc: 88.3909\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.4482\n",
      "validation Loss: 0.0001 Acc: 88.4338\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 88.4517\n",
      "validation Loss: 0.0001 Acc: 88.4163\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.4029\n",
      "validation Loss: 0.0001 Acc: 88.3928\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.4697\n",
      "validation Loss: 0.0001 Acc: 88.3889\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.4331\n",
      "validation Loss: 0.0001 Acc: 88.3948\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.4180\n",
      "validation Loss: 0.0001 Acc: 88.4104\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.4565\n",
      "validation Loss: 0.0001 Acc: 88.4085\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.4570\n",
      "validation Loss: 0.0001 Acc: 88.4085\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.4424\n",
      "validation Loss: 0.0001 Acc: 88.3928\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 88.4526\n",
      "validation Loss: 0.0001 Acc: 88.4221\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 88.4234\n",
      "validation Loss: 0.0001 Acc: 88.3987\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 88.4517\n",
      "validation Loss: 0.0001 Acc: 88.4319\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 88.4346\n",
      "validation Loss: 0.0001 Acc: 88.3909\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 88.4580\n",
      "validation Loss: 0.0001 Acc: 88.3928\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 88.4253\n",
      "validation Loss: 0.0001 Acc: 88.3772\n",
      "Best val acc: 88.451401\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.3771\n",
      "validation Loss: 0.0001 Acc: 88.5156\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.4166\n",
      "validation Loss: 0.0001 Acc: 88.5605\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.4634\n",
      "validation Loss: 0.0001 Acc: 88.5293\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.4429\n",
      "validation Loss: 0.0001 Acc: 88.5312\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.4556\n",
      "validation Loss: 0.0001 Acc: 88.5312\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.4171\n",
      "validation Loss: 0.0001 Acc: 88.5175\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.4581\n",
      "validation Loss: 0.0001 Acc: 88.5390\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.4576\n",
      "validation Loss: 0.0001 Acc: 88.5332\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.4498\n",
      "validation Loss: 0.0001 Acc: 88.5097\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.4371\n",
      "validation Loss: 0.0001 Acc: 88.5097\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.4673\n",
      "validation Loss: 0.0001 Acc: 88.5175\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.4732\n",
      "validation Loss: 0.0001 Acc: 88.5254\n",
      "Early stopped.\n",
      "Best val acc: 88.560493\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.5576\n",
      "validation Loss: 0.0001 Acc: 88.4258\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.4961\n",
      "validation Loss: 0.0001 Acc: 88.4063\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.5074\n",
      "validation Loss: 0.0001 Acc: 88.4024\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.5205\n",
      "validation Loss: 0.0001 Acc: 88.4063\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.5088\n",
      "validation Loss: 0.0001 Acc: 88.3926\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.5030\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.4888\n",
      "validation Loss: 0.0001 Acc: 88.3965\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.4786\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.4898\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.4952\n",
      "validation Loss: 0.0001 Acc: 88.3926\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.4844\n",
      "validation Loss: 0.0001 Acc: 88.3887\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.5069\n",
      "validation Loss: 0.0001 Acc: 88.3926\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.4917\n",
      "validation Loss: 0.0001 Acc: 88.3965\n",
      "Early stopped.\n",
      "Best val acc: 88.425797\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.3790\n",
      "validation Loss: 0.0001 Acc: 88.7674\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.3653\n",
      "validation Loss: 0.0001 Acc: 88.7596\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.3702\n",
      "validation Loss: 0.0001 Acc: 88.7635\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.3839\n",
      "validation Loss: 0.0001 Acc: 88.7674\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.3946\n",
      "validation Loss: 0.0001 Acc: 88.7733\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.3810\n",
      "validation Loss: 0.0001 Acc: 88.7694\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.3917\n",
      "validation Loss: 0.0001 Acc: 88.7772\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.4283\n",
      "validation Loss: 0.0001 Acc: 88.7674\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.3688\n",
      "validation Loss: 0.0001 Acc: 88.7674\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.3936\n",
      "validation Loss: 0.0001 Acc: 88.7733\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.3976\n",
      "validation Loss: 0.0001 Acc: 88.7713\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.3868\n",
      "validation Loss: 0.0001 Acc: 88.7674\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.4005\n",
      "validation Loss: 0.0001 Acc: 88.7674\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.4278\n",
      "validation Loss: 0.0001 Acc: 88.7674\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.3893\n",
      "validation Loss: 0.0001 Acc: 88.7674\n",
      "Early stopped.\n",
      "Best val acc: 88.777184\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.4766\n",
      "validation Loss: 0.0001 Acc: 88.5878\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.4649\n",
      "validation Loss: 0.0001 Acc: 88.5878\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.4512\n",
      "validation Loss: 0.0001 Acc: 88.5839\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.4629\n",
      "validation Loss: 0.0001 Acc: 88.5859\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.4337\n",
      "validation Loss: 0.0001 Acc: 88.5820\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.4298\n",
      "validation Loss: 0.0001 Acc: 88.5800\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.4371\n",
      "validation Loss: 0.0001 Acc: 88.5800\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.4498\n",
      "validation Loss: 0.0001 Acc: 88.5800\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.4503\n",
      "validation Loss: 0.0001 Acc: 88.5820\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.4239\n",
      "validation Loss: 0.0001 Acc: 88.5820\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.4747\n",
      "validation Loss: 0.0001 Acc: 88.5820\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.4210\n",
      "validation Loss: 0.0001 Acc: 88.5820\n",
      "Early stopped.\n",
      "Best val acc: 88.587822\n",
      "----------\n",
      "Average best_acc across k-fold: 88.56053924560547\n",
      "lepton1_eta\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.7778\n",
      "validation Loss: 0.0001 Acc: 84.9298\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.2941\n",
      "validation Loss: 0.0001 Acc: 85.5506\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.8094\n",
      "validation Loss: 0.0001 Acc: 85.9254\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 86.0647\n",
      "validation Loss: 0.0001 Acc: 85.9957\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.1945\n",
      "validation Loss: 0.0001 Acc: 86.0523\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 86.2428\n",
      "validation Loss: 0.0001 Acc: 86.0269\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 86.2170\n",
      "validation Loss: 0.0001 Acc: 85.7321\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 86.2726\n",
      "validation Loss: 0.0001 Acc: 86.1987\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 86.3946\n",
      "validation Loss: 0.0001 Acc: 86.1655\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 86.3682\n",
      "validation Loss: 0.0001 Acc: 86.0913\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 86.4136\n",
      "validation Loss: 0.0001 Acc: 86.3314\n",
      "Saving..\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 86.5400\n",
      "validation Loss: 0.0001 Acc: 86.4329\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 86.4400\n",
      "validation Loss: 0.0001 Acc: 86.1616\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 86.5142\n",
      "validation Loss: 0.0001 Acc: 86.4349\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 86.6864\n",
      "validation Loss: 0.0001 Acc: 86.4310\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 86.6084\n",
      "validation Loss: 0.0001 Acc: 86.3139\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 86.7318\n",
      "validation Loss: 0.0001 Acc: 86.6145\n",
      "Saving..\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 86.7806\n",
      "validation Loss: 0.0001 Acc: 86.6633\n",
      "Saving..\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 86.8124\n",
      "validation Loss: 0.0001 Acc: 86.6262\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 86.8119\n",
      "validation Loss: 0.0001 Acc: 86.4895\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 86.7660\n",
      "validation Loss: 0.0001 Acc: 86.5208\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 86.7924\n",
      "validation Loss: 0.0001 Acc: 86.6711\n",
      "Saving..\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 86.8602\n",
      "validation Loss: 0.0001 Acc: 86.5637\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 86.8768\n",
      "validation Loss: 0.0001 Acc: 86.6672\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 86.8431\n",
      "validation Loss: 0.0001 Acc: 86.8077\n",
      "Saving..\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 86.8646\n",
      "validation Loss: 0.0001 Acc: 86.5832\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 86.8558\n",
      "validation Loss: 0.0001 Acc: 86.5930\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 86.8978\n",
      "validation Loss: 0.0001 Acc: 86.6867\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 86.9529\n",
      "validation Loss: 0.0001 Acc: 86.6906\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.0022\n",
      "validation Loss: 0.0001 Acc: 86.8292\n",
      "Saving..\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 86.9890\n",
      "validation Loss: 0.0001 Acc: 86.7101\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.0242\n",
      "validation Loss: 0.0001 Acc: 86.7941\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.0300\n",
      "validation Loss: 0.0001 Acc: 86.7004\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.0105\n",
      "validation Loss: 0.0001 Acc: 86.6574\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.0617\n",
      "validation Loss: 0.0001 Acc: 86.7589\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.0983\n",
      "validation Loss: 0.0001 Acc: 86.8429\n",
      "Saving..\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.1042\n",
      "validation Loss: 0.0001 Acc: 86.8663\n",
      "Saving..\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.1022\n",
      "validation Loss: 0.0001 Acc: 86.9014\n",
      "Saving..\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 87.0544\n",
      "validation Loss: 0.0001 Acc: 86.8175\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 87.0700\n",
      "validation Loss: 0.0001 Acc: 86.8351\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 87.0940\n",
      "validation Loss: 0.0001 Acc: 86.8604\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.0452\n",
      "validation Loss: 0.0001 Acc: 86.7941\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 87.1130\n",
      "validation Loss: 0.0001 Acc: 86.8507\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.1291\n",
      "validation Loss: 0.0001 Acc: 86.8487\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.0852\n",
      "validation Loss: 0.0001 Acc: 86.8448\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.0647\n",
      "validation Loss: 0.0001 Acc: 86.8487\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.1149\n",
      "validation Loss: 0.0001 Acc: 86.8624\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 87.1193\n",
      "validation Loss: 0.0001 Acc: 86.9014\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 87.1008\n",
      "validation Loss: 0.0001 Acc: 86.8897\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 87.1301\n",
      "validation Loss: 0.0001 Acc: 86.8683\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.1511\n",
      "validation Loss: 0.0001 Acc: 86.8839\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.1408\n",
      "validation Loss: 0.0001 Acc: 86.8995\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 87.1286\n",
      "validation Loss: 0.0001 Acc: 86.9346\n",
      "Saving..\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.1174\n",
      "validation Loss: 0.0001 Acc: 86.8800\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 87.1374\n",
      "validation Loss: 0.0001 Acc: 86.8956\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 87.1462\n",
      "validation Loss: 0.0001 Acc: 86.9307\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.1135\n",
      "validation Loss: 0.0001 Acc: 86.8585\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 87.1403\n",
      "validation Loss: 0.0001 Acc: 86.9210\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 87.1633\n",
      "validation Loss: 0.0001 Acc: 86.8917\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 87.1681\n",
      "validation Loss: 0.0001 Acc: 86.8858\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 87.1306\n",
      "validation Loss: 0.0001 Acc: 86.8800\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 87.1525\n",
      "validation Loss: 0.0001 Acc: 86.8761\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 87.1511\n",
      "validation Loss: 0.0001 Acc: 86.8858\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 87.1384\n",
      "validation Loss: 0.0001 Acc: 86.8975\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 87.1374\n",
      "validation Loss: 0.0001 Acc: 86.8956\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 87.1618\n",
      "validation Loss: 0.0001 Acc: 86.8663\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 87.1511\n",
      "validation Loss: 0.0001 Acc: 86.8741\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 87.1618\n",
      "validation Loss: 0.0001 Acc: 86.8741\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 87.1579\n",
      "validation Loss: 0.0001 Acc: 86.8897\n",
      "Early stopped.\n",
      "Best val acc: 86.934624\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.0823\n",
      "validation Loss: 0.0001 Acc: 87.1589\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.0706\n",
      "validation Loss: 0.0001 Acc: 87.1589\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0838\n",
      "validation Loss: 0.0001 Acc: 87.1452\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0882\n",
      "validation Loss: 0.0001 Acc: 87.1452\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0823\n",
      "validation Loss: 0.0001 Acc: 87.1511\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.0711\n",
      "validation Loss: 0.0001 Acc: 87.1569\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.1018\n",
      "validation Loss: 0.0001 Acc: 87.1589\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0804\n",
      "validation Loss: 0.0001 Acc: 87.1608\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0711\n",
      "validation Loss: 0.0001 Acc: 87.1647\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.0891\n",
      "validation Loss: 0.0001 Acc: 87.1608\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0506\n",
      "validation Loss: 0.0001 Acc: 87.1550\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0847\n",
      "validation Loss: 0.0001 Acc: 87.1725\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.0511\n",
      "validation Loss: 0.0001 Acc: 87.1550\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.0599\n",
      "validation Loss: 0.0001 Acc: 87.1530\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0862\n",
      "validation Loss: 0.0001 Acc: 87.1550\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.0955\n",
      "validation Loss: 0.0001 Acc: 87.1530\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.0872\n",
      "validation Loss: 0.0001 Acc: 87.1589\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.0652\n",
      "validation Loss: 0.0001 Acc: 87.1569\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.0760\n",
      "validation Loss: 0.0001 Acc: 87.1589\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.0599\n",
      "validation Loss: 0.0001 Acc: 87.1589\n",
      "Early stopped.\n",
      "Best val acc: 87.172523\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.0545\n",
      "validation Loss: 0.0001 Acc: 87.2057\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.0779\n",
      "validation Loss: 0.0001 Acc: 87.2096\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0828\n",
      "validation Loss: 0.0001 Acc: 87.2096\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0784\n",
      "validation Loss: 0.0001 Acc: 87.2057\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0520\n",
      "validation Loss: 0.0001 Acc: 87.2057\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.0745\n",
      "validation Loss: 0.0001 Acc: 87.2038\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.0564\n",
      "validation Loss: 0.0001 Acc: 87.2057\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0442\n",
      "validation Loss: 0.0001 Acc: 87.2096\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0462\n",
      "validation Loss: 0.0001 Acc: 87.2057\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.0530\n",
      "validation Loss: 0.0001 Acc: 87.2077\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0706\n",
      "validation Loss: 0.0001 Acc: 87.2077\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0760\n",
      "validation Loss: 0.0001 Acc: 87.2096\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.0408\n",
      "validation Loss: 0.0001 Acc: 87.2057\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.1052\n",
      "validation Loss: 0.0001 Acc: 87.2096\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0808\n",
      "validation Loss: 0.0001 Acc: 87.2096\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.0032\n",
      "validation Loss: 0.0001 Acc: 87.2096\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.0760\n",
      "validation Loss: 0.0001 Acc: 87.2077\n",
      "Early stopped.\n",
      "Best val acc: 87.209618\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.1213\n",
      "validation Loss: 0.0001 Acc: 87.0710\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.1048\n",
      "validation Loss: 0.0001 Acc: 87.0710\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.1204\n",
      "validation Loss: 0.0001 Acc: 87.0710\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0872\n",
      "validation Loss: 0.0001 Acc: 87.0691\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0823\n",
      "validation Loss: 0.0001 Acc: 87.0691\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.1033\n",
      "validation Loss: 0.0001 Acc: 87.0691\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.1135\n",
      "validation Loss: 0.0001 Acc: 87.0691\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.1135\n",
      "validation Loss: 0.0001 Acc: 87.0691\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0652\n",
      "validation Loss: 0.0001 Acc: 87.0691\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.1038\n",
      "validation Loss: 0.0001 Acc: 87.0691\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0755\n",
      "validation Loss: 0.0001 Acc: 87.0691\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0906\n",
      "validation Loss: 0.0001 Acc: 87.0691\n",
      "Early stopped.\n",
      "Best val acc: 87.071014\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 86.9901\n",
      "validation Loss: 0.0001 Acc: 87.4009\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.0320\n",
      "validation Loss: 0.0001 Acc: 87.4009\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0330\n",
      "validation Loss: 0.0001 Acc: 87.4009\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0213\n",
      "validation Loss: 0.0001 Acc: 87.4009\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0140\n",
      "validation Loss: 0.0001 Acc: 87.4009\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.0579\n",
      "validation Loss: 0.0001 Acc: 87.4009\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.0457\n",
      "validation Loss: 0.0001 Acc: 87.4009\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0125\n",
      "validation Loss: 0.0001 Acc: 87.4009\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0384\n",
      "validation Loss: 0.0001 Acc: 87.4009\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.0472\n",
      "validation Loss: 0.0001 Acc: 87.4009\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0047\n",
      "validation Loss: 0.0001 Acc: 87.4029\n",
      "Saving..\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0457\n",
      "validation Loss: 0.0001 Acc: 87.4029\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.0296\n",
      "validation Loss: 0.0001 Acc: 87.4029\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 86.9862\n",
      "validation Loss: 0.0001 Acc: 87.4029\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0189\n",
      "validation Loss: 0.0001 Acc: 87.4029\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.0457\n",
      "validation Loss: 0.0001 Acc: 87.4048\n",
      "Saving..\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.0174\n",
      "validation Loss: 0.0001 Acc: 87.4048\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.0228\n",
      "validation Loss: 0.0001 Acc: 87.4048\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.0150\n",
      "validation Loss: 0.0001 Acc: 87.4048\n",
      "Early stopped.\n",
      "Best val acc: 87.404831\n",
      "----------\n",
      "Average best_acc across k-fold: 87.15852355957031\n",
      "lepton2_eta\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 80.1595\n",
      "validation Loss: 0.0001 Acc: 85.0177\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.0979\n",
      "validation Loss: 0.0001 Acc: 85.5330\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.4229\n",
      "validation Loss: 0.0001 Acc: 85.8512\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 85.6660\n",
      "validation Loss: 0.0001 Acc: 85.8805\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 85.9276\n",
      "validation Loss: 0.0001 Acc: 86.0640\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 86.0130\n",
      "validation Loss: 0.0001 Acc: 85.9976\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 86.0945\n",
      "validation Loss: 0.0001 Acc: 86.3880\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 86.2897\n",
      "validation Loss: 0.0001 Acc: 86.3627\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 86.3517\n",
      "validation Loss: 0.0001 Acc: 86.7921\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 86.4727\n",
      "validation Loss: 0.0001 Acc: 86.8273\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 86.6367\n",
      "validation Loss: 0.0001 Acc: 86.1909\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 86.5757\n",
      "validation Loss: 0.0001 Acc: 86.8800\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 86.7304\n",
      "validation Loss: 0.0001 Acc: 86.7882\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 86.7992\n",
      "validation Loss: 0.0001 Acc: 86.8702\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 86.8041\n",
      "validation Loss: 0.0001 Acc: 86.8839\n",
      "Saving..\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 86.8817\n",
      "validation Loss: 0.0001 Acc: 86.4856\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 86.8660\n",
      "validation Loss: 0.0001 Acc: 87.2196\n",
      "Saving..\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 86.8738\n",
      "validation Loss: 0.0001 Acc: 86.9678\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 86.9944\n",
      "validation Loss: 0.0001 Acc: 87.1611\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 86.9988\n",
      "validation Loss: 0.0001 Acc: 87.1201\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 86.9871\n",
      "validation Loss: 0.0001 Acc: 87.3387\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.0164\n",
      "validation Loss: 0.0001 Acc: 87.2470\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.0642\n",
      "validation Loss: 0.0001 Acc: 87.1259\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 86.9954\n",
      "validation Loss: 0.0001 Acc: 87.3465\n",
      "Saving..\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.0993\n",
      "validation Loss: 0.0001 Acc: 87.2997\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.1750\n",
      "validation Loss: 0.0001 Acc: 87.3543\n",
      "Saving..\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.1242\n",
      "validation Loss: 0.0001 Acc: 87.2177\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.0764\n",
      "validation Loss: 0.0001 Acc: 87.2431\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.1530\n",
      "validation Loss: 0.0001 Acc: 87.3504\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.1389\n",
      "validation Loss: 0.0001 Acc: 87.2255\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.2409\n",
      "validation Loss: 0.0001 Acc: 87.3270\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.2316\n",
      "validation Loss: 0.0001 Acc: 87.4207\n",
      "Saving..\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.2726\n",
      "validation Loss: 0.0001 Acc: 87.4812\n",
      "Saving..\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.2477\n",
      "validation Loss: 0.0001 Acc: 87.4187\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.2799\n",
      "validation Loss: 0.0001 Acc: 87.4480\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.2823\n",
      "validation Loss: 0.0001 Acc: 87.4070\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.2545\n",
      "validation Loss: 0.0001 Acc: 87.4929\n",
      "Saving..\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.3048\n",
      "validation Loss: 0.0001 Acc: 87.4793\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 87.2999\n",
      "validation Loss: 0.0001 Acc: 87.5320\n",
      "Saving..\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 87.2892\n",
      "validation Loss: 0.0001 Acc: 87.4441\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 87.3131\n",
      "validation Loss: 0.0001 Acc: 87.4968\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.2940\n",
      "validation Loss: 0.0001 Acc: 87.4773\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 87.3389\n",
      "validation Loss: 0.0001 Acc: 87.5554\n",
      "Saving..\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.3126\n",
      "validation Loss: 0.0001 Acc: 87.3875\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.3570\n",
      "validation Loss: 0.0001 Acc: 87.4988\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.4136\n",
      "validation Loss: 0.0001 Acc: 87.5222\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.4156\n",
      "validation Loss: 0.0001 Acc: 87.5769\n",
      "Saving..\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 87.3663\n",
      "validation Loss: 0.0001 Acc: 87.5456\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 87.3814\n",
      "validation Loss: 0.0001 Acc: 87.5554\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 87.4375\n",
      "validation Loss: 0.0001 Acc: 87.6081\n",
      "Saving..\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.4541\n",
      "validation Loss: 0.0001 Acc: 87.6354\n",
      "Saving..\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.4048\n",
      "validation Loss: 0.0001 Acc: 87.5710\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 87.4239\n",
      "validation Loss: 0.0001 Acc: 87.5944\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.4156\n",
      "validation Loss: 0.0001 Acc: 87.5359\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 87.4419\n",
      "validation Loss: 0.0001 Acc: 87.5476\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 87.4161\n",
      "validation Loss: 0.0001 Acc: 87.5749\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.4341\n",
      "validation Loss: 0.0001 Acc: 87.6432\n",
      "Saving..\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 87.4502\n",
      "validation Loss: 0.0001 Acc: 87.5671\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 87.5073\n",
      "validation Loss: 0.0001 Acc: 87.6335\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 87.4819\n",
      "validation Loss: 0.0001 Acc: 87.6198\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 87.4497\n",
      "validation Loss: 0.0001 Acc: 87.5495\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 87.4634\n",
      "validation Loss: 0.0001 Acc: 87.6100\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 87.4341\n",
      "validation Loss: 0.0001 Acc: 87.6198\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 87.5259\n",
      "validation Loss: 0.0001 Acc: 87.6432\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 87.4990\n",
      "validation Loss: 0.0001 Acc: 87.6296\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 87.5049\n",
      "validation Loss: 0.0001 Acc: 87.6276\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 87.5146\n",
      "validation Loss: 0.0001 Acc: 87.6432\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 87.5088\n",
      "validation Loss: 0.0001 Acc: 87.6432\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 87.5102\n",
      "validation Loss: 0.0001 Acc: 87.6432\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 87.4902\n",
      "validation Loss: 0.0001 Acc: 87.6120\n",
      "Best val acc: 87.643234\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.5596\n",
      "validation Loss: 0.0001 Acc: 87.3287\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.5777\n",
      "validation Loss: 0.0001 Acc: 87.3482\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.6099\n",
      "validation Loss: 0.0001 Acc: 87.3443\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.5645\n",
      "validation Loss: 0.0001 Acc: 87.3014\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.5962\n",
      "validation Loss: 0.0001 Acc: 87.3189\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.6021\n",
      "validation Loss: 0.0001 Acc: 87.3111\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.6113\n",
      "validation Loss: 0.0001 Acc: 87.3072\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.5962\n",
      "validation Loss: 0.0001 Acc: 87.2916\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5567\n",
      "validation Loss: 0.0001 Acc: 87.3209\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5762\n",
      "validation Loss: 0.0001 Acc: 87.3131\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5694\n",
      "validation Loss: 0.0001 Acc: 87.3365\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5825\n",
      "validation Loss: 0.0001 Acc: 87.3209\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.6040\n",
      "validation Loss: 0.0001 Acc: 87.3248\n",
      "Early stopped.\n",
      "Best val acc: 87.348221\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.5045\n",
      "validation Loss: 0.0001 Acc: 87.7230\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.5093\n",
      "validation Loss: 0.0001 Acc: 87.7172\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.5108\n",
      "validation Loss: 0.0001 Acc: 87.7133\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.5074\n",
      "validation Loss: 0.0001 Acc: 87.7230\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.5206\n",
      "validation Loss: 0.0001 Acc: 87.6957\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5059\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.4781\n",
      "validation Loss: 0.0001 Acc: 87.7113\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.4727\n",
      "validation Loss: 0.0001 Acc: 87.7094\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5040\n",
      "validation Loss: 0.0001 Acc: 87.7035\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.4927\n",
      "validation Loss: 0.0001 Acc: 87.7113\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5328\n",
      "validation Loss: 0.0001 Acc: 87.7133\n",
      "Early stopped.\n",
      "Best val acc: 87.723030\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.5201\n",
      "validation Loss: 0.0001 Acc: 87.6391\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.4971\n",
      "validation Loss: 0.0001 Acc: 87.6371\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.5171\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.5362\n",
      "validation Loss: 0.0001 Acc: 87.6254\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.5191\n",
      "validation Loss: 0.0001 Acc: 87.6313\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5152\n",
      "validation Loss: 0.0001 Acc: 87.6293\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.5274\n",
      "validation Loss: 0.0001 Acc: 87.6332\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.5474\n",
      "validation Loss: 0.0001 Acc: 87.6332\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5123\n",
      "validation Loss: 0.0001 Acc: 87.6352\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5191\n",
      "validation Loss: 0.0001 Acc: 87.6293\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5171\n",
      "validation Loss: 0.0001 Acc: 87.6352\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5450\n",
      "validation Loss: 0.0001 Acc: 87.6313\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.5201\n",
      "validation Loss: 0.0001 Acc: 87.6293\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.5181\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.5108\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.5181\n",
      "validation Loss: 0.0001 Acc: 87.6293\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.5425\n",
      "validation Loss: 0.0001 Acc: 87.6293\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.5196\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.4913\n",
      "validation Loss: 0.0001 Acc: 87.6293\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.5015\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.5118\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.5396\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.5474\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.5040\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.5367\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.5191\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.5093\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.5347\n",
      "validation Loss: 0.0001 Acc: 87.6274\n",
      "Early stopped.\n",
      "Best val acc: 87.639084\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.5274\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.5079\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.4947\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.5298\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.5064\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5230\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.4713\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.5191\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5303\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.4561\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5064\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.4981\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.4981\n",
      "validation Loss: 0.0001 Acc: 87.7055\n",
      "Early stopped.\n",
      "Best val acc: 87.705460\n",
      "----------\n",
      "Average best_acc across k-fold: 87.61180114746094\n",
      "eta\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 82.7085\n",
      "validation Loss: 0.0001 Acc: 85.1504\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.9193\n",
      "validation Loss: 0.0001 Acc: 86.4954\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 86.9568\n",
      "validation Loss: 0.0001 Acc: 87.3231\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.3633\n",
      "validation Loss: 0.0001 Acc: 86.9659\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.4361\n",
      "validation Loss: 0.0001 Acc: 87.5281\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5142\n",
      "validation Loss: 0.0001 Acc: 87.6706\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.5991\n",
      "validation Loss: 0.0001 Acc: 87.7467\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.6269\n",
      "validation Loss: 0.0001 Acc: 87.5808\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.6274\n",
      "validation Loss: 0.0001 Acc: 87.7565\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.7455\n",
      "validation Loss: 0.0001 Acc: 87.5769\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.6752\n",
      "validation Loss: 0.0001 Acc: 86.7453\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.6664\n",
      "validation Loss: 0.0001 Acc: 88.0102\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.8153\n",
      "validation Loss: 0.0001 Acc: 87.9966\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.8768\n",
      "validation Loss: 0.0001 Acc: 88.0376\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.9651\n",
      "validation Loss: 0.0001 Acc: 88.0473\n",
      "Saving..\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.9221\n",
      "validation Loss: 0.0001 Acc: 88.0805\n",
      "Saving..\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.0007\n",
      "validation Loss: 0.0001 Acc: 88.0473\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.0237\n",
      "validation Loss: 0.0001 Acc: 88.1703\n",
      "Saving..\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.0061\n",
      "validation Loss: 0.0001 Acc: 88.0883\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.9797\n",
      "validation Loss: 0.0001 Acc: 87.9888\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 88.0061\n",
      "validation Loss: 0.0001 Acc: 88.1801\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.1120\n",
      "validation Loss: 0.0001 Acc: 88.0766\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.0022\n",
      "validation Loss: 0.0001 Acc: 88.2523\n",
      "Saving..\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.0378\n",
      "validation Loss: 0.0001 Acc: 88.2093\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.1071\n",
      "validation Loss: 0.0001 Acc: 88.2542\n",
      "Saving..\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.1671\n",
      "validation Loss: 0.0001 Acc: 88.1723\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.1588\n",
      "validation Loss: 0.0001 Acc: 88.2132\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.1339\n",
      "validation Loss: 0.0001 Acc: 88.1274\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.0930\n",
      "validation Loss: 0.0001 Acc: 88.1527\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.1501\n",
      "validation Loss: 0.0001 Acc: 88.2132\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 88.1637\n",
      "validation Loss: 0.0001 Acc: 88.2796\n",
      "Saving..\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 88.0988\n",
      "validation Loss: 0.0001 Acc: 88.2699\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 88.1530\n",
      "validation Loss: 0.0001 Acc: 88.3655\n",
      "Saving..\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 88.2155\n",
      "validation Loss: 0.0001 Acc: 88.3284\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 88.1754\n",
      "validation Loss: 0.0001 Acc: 88.2660\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 88.2096\n",
      "validation Loss: 0.0001 Acc: 88.3362\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 88.2340\n",
      "validation Loss: 0.0001 Acc: 88.3694\n",
      "Saving..\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 88.1886\n",
      "validation Loss: 0.0001 Acc: 88.1684\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 88.1423\n",
      "validation Loss: 0.0001 Acc: 88.2952\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 88.2413\n",
      "validation Loss: 0.0001 Acc: 88.4065\n",
      "Saving..\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 88.2223\n",
      "validation Loss: 0.0001 Acc: 88.4202\n",
      "Saving..\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 88.3053\n",
      "validation Loss: 0.0001 Acc: 88.3109\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 88.2682\n",
      "validation Loss: 0.0001 Acc: 88.3850\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 88.2438\n",
      "validation Loss: 0.0001 Acc: 88.4436\n",
      "Saving..\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 88.2677\n",
      "validation Loss: 0.0001 Acc: 88.3655\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 88.3755\n",
      "validation Loss: 0.0001 Acc: 88.3401\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 88.4160\n",
      "validation Loss: 0.0001 Acc: 88.4358\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 88.4399\n",
      "validation Loss: 0.0001 Acc: 88.4670\n",
      "Saving..\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 88.4077\n",
      "validation Loss: 0.0001 Acc: 88.4163\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.4033\n",
      "validation Loss: 0.0001 Acc: 88.5178\n",
      "Saving..\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 88.4116\n",
      "validation Loss: 0.0001 Acc: 88.4631\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 88.4068\n",
      "validation Loss: 0.0001 Acc: 88.4202\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.4321\n",
      "validation Loss: 0.0001 Acc: 88.3792\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 88.4058\n",
      "validation Loss: 0.0001 Acc: 88.3831\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.4844\n",
      "validation Loss: 0.0001 Acc: 88.4651\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.5195\n",
      "validation Loss: 0.0001 Acc: 88.5119\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 88.4878\n",
      "validation Loss: 0.0001 Acc: 88.4007\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.4931\n",
      "validation Loss: 0.0001 Acc: 88.4631\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.4980\n",
      "validation Loss: 0.0001 Acc: 88.5412\n",
      "Saving..\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.4800\n",
      "validation Loss: 0.0001 Acc: 88.4768\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.5566\n",
      "validation Loss: 0.0001 Acc: 88.5373\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.5395\n",
      "validation Loss: 0.0001 Acc: 88.5275\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.5732\n",
      "validation Loss: 0.0001 Acc: 88.4885\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.5180\n",
      "validation Loss: 0.0001 Acc: 88.5471\n",
      "Saving..\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 88.5624\n",
      "validation Loss: 0.0001 Acc: 88.5334\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 88.5868\n",
      "validation Loss: 0.0001 Acc: 88.4397\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 88.5351\n",
      "validation Loss: 0.0001 Acc: 88.5802\n",
      "Saving..\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 88.5590\n",
      "validation Loss: 0.0001 Acc: 88.5763\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 88.5854\n",
      "validation Loss: 0.0001 Acc: 88.5197\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 88.6210\n",
      "validation Loss: 0.0001 Acc: 88.5744\n",
      "Best val acc: 88.580238\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.5361\n",
      "validation Loss: 0.0001 Acc: 88.9002\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.5640\n",
      "validation Loss: 0.0001 Acc: 88.8865\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.4976\n",
      "validation Loss: 0.0001 Acc: 88.8377\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.5210\n",
      "validation Loss: 0.0001 Acc: 88.8963\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.5474\n",
      "validation Loss: 0.0001 Acc: 88.8787\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.5508\n",
      "validation Loss: 0.0001 Acc: 88.8709\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.5576\n",
      "validation Loss: 0.0001 Acc: 88.8533\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.5591\n",
      "validation Loss: 0.0001 Acc: 88.8806\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.5986\n",
      "validation Loss: 0.0001 Acc: 88.8572\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.5654\n",
      "validation Loss: 0.0001 Acc: 88.8787\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.5693\n",
      "validation Loss: 0.0001 Acc: 88.8533\n",
      "Early stopped.\n",
      "Best val acc: 88.900162\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.6323\n",
      "validation Loss: 0.0001 Acc: 88.4883\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.6294\n",
      "validation Loss: 0.0001 Acc: 88.4766\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.6733\n",
      "validation Loss: 0.0001 Acc: 88.4707\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.6416\n",
      "validation Loss: 0.0001 Acc: 88.4531\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.6469\n",
      "validation Loss: 0.0001 Acc: 88.4668\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.6274\n",
      "validation Loss: 0.0001 Acc: 88.4453\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.6533\n",
      "validation Loss: 0.0001 Acc: 88.4687\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.6294\n",
      "validation Loss: 0.0001 Acc: 88.4629\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.6406\n",
      "validation Loss: 0.0001 Acc: 88.4590\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.6513\n",
      "validation Loss: 0.0001 Acc: 88.4473\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.6547\n",
      "validation Loss: 0.0001 Acc: 88.4531\n",
      "Early stopped.\n",
      "Best val acc: 88.488266\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.5474\n",
      "validation Loss: 0.0001 Acc: 88.7987\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.5737\n",
      "validation Loss: 0.0001 Acc: 88.8065\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.6313\n",
      "validation Loss: 0.0001 Acc: 88.8065\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.5854\n",
      "validation Loss: 0.0001 Acc: 88.8045\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.5474\n",
      "validation Loss: 0.0001 Acc: 88.8065\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.5732\n",
      "validation Loss: 0.0001 Acc: 88.8201\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.5981\n",
      "validation Loss: 0.0001 Acc: 88.8240\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.5903\n",
      "validation Loss: 0.0001 Acc: 88.8240\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.5713\n",
      "validation Loss: 0.0001 Acc: 88.8162\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.5854\n",
      "validation Loss: 0.0001 Acc: 88.8143\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.5757\n",
      "validation Loss: 0.0001 Acc: 88.8182\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.5835\n",
      "validation Loss: 0.0001 Acc: 88.8104\n",
      "Early stopped.\n",
      "Best val acc: 88.824028\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.6464\n",
      "validation Loss: 0.0001 Acc: 88.6776\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.6733\n",
      "validation Loss: 0.0001 Acc: 88.6776\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.6162\n",
      "validation Loss: 0.0001 Acc: 88.6757\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.6211\n",
      "validation Loss: 0.0001 Acc: 88.6718\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.6098\n",
      "validation Loss: 0.0001 Acc: 88.6737\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.6299\n",
      "validation Loss: 0.0001 Acc: 88.6737\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.6406\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.6377\n",
      "validation Loss: 0.0001 Acc: 88.6718\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.6357\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.6147\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.5776\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.6035\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.6425\n",
      "validation Loss: 0.0001 Acc: 88.6679\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.6284\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.6216\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.6050\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.6177\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.6206\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.6084\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 88.6064\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 88.6323\n",
      "validation Loss: 0.0001 Acc: 88.6698\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.6225\n",
      "validation Loss: 0.0001 Acc: 88.6718\n",
      "Early stopped.\n",
      "Best val acc: 88.677620\n",
      "----------\n",
      "Average best_acc across k-fold: 88.6940689086914\n",
      "lepton1_phi\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.6919\n",
      "validation Loss: 0.0001 Acc: 85.2539\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.3317\n",
      "validation Loss: 0.0001 Acc: 85.8024\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.7753\n",
      "validation Loss: 0.0001 Acc: 85.8942\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 85.8880\n",
      "validation Loss: 0.0001 Acc: 86.2709\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.0178\n",
      "validation Loss: 0.0001 Acc: 86.2026\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 86.0652\n",
      "validation Loss: 0.0001 Acc: 86.0386\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 86.0710\n",
      "validation Loss: 0.0001 Acc: 86.5891\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 86.2404\n",
      "validation Loss: 0.0001 Acc: 86.3217\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 86.3014\n",
      "validation Loss: 0.0001 Acc: 86.7316\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 86.3887\n",
      "validation Loss: 0.0001 Acc: 86.6320\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 86.4122\n",
      "validation Loss: 0.0001 Acc: 86.4525\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 86.3292\n",
      "validation Loss: 0.0001 Acc: 86.6145\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 86.3482\n",
      "validation Loss: 0.0001 Acc: 86.7101\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 86.4463\n",
      "validation Loss: 0.0001 Acc: 86.6594\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 86.5434\n",
      "validation Loss: 0.0001 Acc: 86.3802\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 86.4717\n",
      "validation Loss: 0.0001 Acc: 86.5384\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 86.5293\n",
      "validation Loss: 0.0001 Acc: 86.8526\n",
      "Saving..\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 86.6342\n",
      "validation Loss: 0.0001 Acc: 86.7492\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 86.5532\n",
      "validation Loss: 0.0001 Acc: 86.5832\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 86.5986\n",
      "validation Loss: 0.0001 Acc: 86.8643\n",
      "Saving..\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 86.6225\n",
      "validation Loss: 0.0001 Acc: 86.9444\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 86.6289\n",
      "validation Loss: 0.0001 Acc: 86.8331\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 86.6918\n",
      "validation Loss: 0.0001 Acc: 86.6574\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 86.6982\n",
      "validation Loss: 0.0001 Acc: 86.6320\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 86.5840\n",
      "validation Loss: 0.0001 Acc: 86.9073\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 86.6376\n",
      "validation Loss: 0.0001 Acc: 86.6691\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 86.7338\n",
      "validation Loss: 0.0001 Acc: 86.7726\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 86.7387\n",
      "validation Loss: 0.0001 Acc: 86.9522\n",
      "Saving..\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 86.7148\n",
      "validation Loss: 0.0001 Acc: 86.5344\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 86.6860\n",
      "validation Loss: 0.0001 Acc: 86.7589\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 86.6552\n",
      "validation Loss: 0.0001 Acc: 86.8956\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 86.7562\n",
      "validation Loss: 0.0001 Acc: 86.3431\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 86.8133\n",
      "validation Loss: 0.0001 Acc: 86.9795\n",
      "Saving..\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 86.9109\n",
      "validation Loss: 0.0001 Acc: 86.9990\n",
      "Saving..\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 86.8826\n",
      "validation Loss: 0.0001 Acc: 87.0908\n",
      "Saving..\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 86.8499\n",
      "validation Loss: 0.0001 Acc: 87.0518\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 86.9134\n",
      "validation Loss: 0.0001 Acc: 87.0244\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 86.9105\n",
      "validation Loss: 0.0001 Acc: 87.0342\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 86.8216\n",
      "validation Loss: 0.0001 Acc: 86.9932\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 86.8987\n",
      "validation Loss: 0.0001 Acc: 86.9581\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 86.8338\n",
      "validation Loss: 0.0001 Acc: 86.8956\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 86.8587\n",
      "validation Loss: 0.0001 Acc: 87.0478\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 86.9710\n",
      "validation Loss: 0.0001 Acc: 87.1142\n",
      "Saving..\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 86.9685\n",
      "validation Loss: 0.0001 Acc: 87.0771\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 86.9778\n",
      "validation Loss: 0.0001 Acc: 87.0830\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.0398\n",
      "validation Loss: 0.0001 Acc: 86.9620\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 86.9719\n",
      "validation Loss: 0.0001 Acc: 87.0166\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 86.9837\n",
      "validation Loss: 0.0001 Acc: 87.0400\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 86.9695\n",
      "validation Loss: 0.0001 Acc: 87.1162\n",
      "Saving..\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 86.9861\n",
      "validation Loss: 0.0001 Acc: 87.0908\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.0012\n",
      "validation Loss: 0.0001 Acc: 87.0732\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.0632\n",
      "validation Loss: 0.0001 Acc: 87.0908\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 87.0310\n",
      "validation Loss: 0.0001 Acc: 87.0049\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.0237\n",
      "validation Loss: 0.0001 Acc: 87.1318\n",
      "Saving..\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 86.9622\n",
      "validation Loss: 0.0001 Acc: 87.0927\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 87.0159\n",
      "validation Loss: 0.0001 Acc: 87.0771\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.0115\n",
      "validation Loss: 0.0001 Acc: 87.0342\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 87.0217\n",
      "validation Loss: 0.0001 Acc: 87.1064\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 87.0305\n",
      "validation Loss: 0.0001 Acc: 87.0478\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 87.0525\n",
      "validation Loss: 0.0001 Acc: 87.1064\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 87.0339\n",
      "validation Loss: 0.0001 Acc: 87.1279\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 87.0222\n",
      "validation Loss: 0.0001 Acc: 87.1162\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 87.0368\n",
      "validation Loss: 0.0001 Acc: 87.0771\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 87.0827\n",
      "validation Loss: 0.0001 Acc: 87.0732\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 87.0525\n",
      "validation Loss: 0.0001 Acc: 87.0888\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 87.0530\n",
      "validation Loss: 0.0001 Acc: 87.1123\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 87.0149\n",
      "validation Loss: 0.0001 Acc: 87.1123\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 87.0881\n",
      "validation Loss: 0.0001 Acc: 87.1220\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 87.0491\n",
      "validation Loss: 0.0001 Acc: 87.0810\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 87.0705\n",
      "validation Loss: 0.0001 Acc: 87.0966\n",
      "Best val acc: 87.131790\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 86.9940\n",
      "validation Loss: 0.0001 Acc: 87.1452\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.0525\n",
      "validation Loss: 0.0001 Acc: 87.1374\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0272\n",
      "validation Loss: 0.0001 Acc: 87.1393\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0008\n",
      "validation Loss: 0.0001 Acc: 87.1647\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0408\n",
      "validation Loss: 0.0001 Acc: 87.1276\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.0452\n",
      "validation Loss: 0.0001 Acc: 87.1550\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.0520\n",
      "validation Loss: 0.0001 Acc: 87.1471\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0311\n",
      "validation Loss: 0.0001 Acc: 87.1257\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0203\n",
      "validation Loss: 0.0001 Acc: 87.1491\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.0496\n",
      "validation Loss: 0.0001 Acc: 87.1335\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0799\n",
      "validation Loss: 0.0001 Acc: 87.1335\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0247\n",
      "validation Loss: 0.0001 Acc: 87.1393\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.0628\n",
      "validation Loss: 0.0001 Acc: 87.1335\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.0584\n",
      "validation Loss: 0.0001 Acc: 87.1511\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0272\n",
      "validation Loss: 0.0001 Acc: 87.1432\n",
      "Early stopped.\n",
      "Best val acc: 87.164719\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.0564\n",
      "validation Loss: 0.0001 Acc: 87.1569\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.0472\n",
      "validation Loss: 0.0001 Acc: 87.1608\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0511\n",
      "validation Loss: 0.0001 Acc: 87.1628\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0506\n",
      "validation Loss: 0.0001 Acc: 87.1686\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0691\n",
      "validation Loss: 0.0001 Acc: 87.1628\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.0355\n",
      "validation Loss: 0.0001 Acc: 87.1667\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.0374\n",
      "validation Loss: 0.0001 Acc: 87.1647\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0428\n",
      "validation Loss: 0.0001 Acc: 87.1667\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0384\n",
      "validation Loss: 0.0001 Acc: 87.1647\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.0413\n",
      "validation Loss: 0.0001 Acc: 87.1686\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0408\n",
      "validation Loss: 0.0001 Acc: 87.1686\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0359\n",
      "validation Loss: 0.0001 Acc: 87.1686\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.0403\n",
      "validation Loss: 0.0001 Acc: 87.1667\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.0179\n",
      "validation Loss: 0.0001 Acc: 87.1647\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0145\n",
      "validation Loss: 0.0001 Acc: 87.1647\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.0496\n",
      "validation Loss: 0.0001 Acc: 87.1647\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.0706\n",
      "validation Loss: 0.0001 Acc: 87.1628\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.0491\n",
      "validation Loss: 0.0001 Acc: 87.1686\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.0540\n",
      "validation Loss: 0.0001 Acc: 87.1628\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.0335\n",
      "validation Loss: 0.0001 Acc: 87.1667\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.0325\n",
      "validation Loss: 0.0001 Acc: 87.1667\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.0355\n",
      "validation Loss: 0.0001 Acc: 87.1647\n",
      "Early stopped.\n",
      "Best val acc: 87.168625\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.0369\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.0755\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0633\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0764\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0350\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.0638\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.0467\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0706\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0647\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.0457\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0662\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0442\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.0330\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.0579\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0477\n",
      "validation Loss: 0.0001 Acc: 87.0866\n",
      "Early stopped.\n",
      "Best val acc: 87.086632\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.0735\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.0906\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.0999\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0926\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.0926\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.1023\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.0989\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.0652\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.0794\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.0501\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.0901\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.0457\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.0843\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.0530\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0779\n",
      "validation Loss: 0.0001 Acc: 87.0378\n",
      "Early stopped.\n",
      "Best val acc: 87.037827\n",
      "----------\n",
      "Average best_acc across k-fold: 87.117919921875\n",
      "lepton2_phi\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 82.2287\n",
      "validation Loss: 0.0001 Acc: 85.0470\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.3663\n",
      "validation Loss: 0.0001 Acc: 85.2304\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.8665\n",
      "validation Loss: 0.0001 Acc: 85.2578\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 85.9715\n",
      "validation Loss: 0.0001 Acc: 85.6013\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.1818\n",
      "validation Loss: 0.0001 Acc: 86.2045\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 86.3775\n",
      "validation Loss: 0.0001 Acc: 86.1733\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 86.2877\n",
      "validation Loss: 0.0001 Acc: 86.0074\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 86.4468\n",
      "validation Loss: 0.0001 Acc: 86.4076\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 86.6010\n",
      "validation Loss: 0.0001 Acc: 86.4290\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 86.6796\n",
      "validation Loss: 0.0001 Acc: 86.0913\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 86.7162\n",
      "validation Loss: 0.0001 Acc: 86.8292\n",
      "Saving..\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 86.8929\n",
      "validation Loss: 0.0001 Acc: 86.7707\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 86.9544\n",
      "validation Loss: 0.0001 Acc: 86.7960\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 86.9627\n",
      "validation Loss: 0.0001 Acc: 86.9737\n",
      "Saving..\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.0534\n",
      "validation Loss: 0.0001 Acc: 87.0810\n",
      "Saving..\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.0964\n",
      "validation Loss: 0.0001 Acc: 87.0322\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.1413\n",
      "validation Loss: 0.0001 Acc: 87.0888\n",
      "Saving..\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.1393\n",
      "validation Loss: 0.0001 Acc: 87.0303\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.0666\n",
      "validation Loss: 0.0001 Acc: 87.0869\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.1467\n",
      "validation Loss: 0.0001 Acc: 87.0908\n",
      "Saving..\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.2267\n",
      "validation Loss: 0.0001 Acc: 87.1533\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.2740\n",
      "validation Loss: 0.0001 Acc: 87.1045\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.2745\n",
      "validation Loss: 0.0001 Acc: 87.2645\n",
      "Saving..\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.2555\n",
      "validation Loss: 0.0001 Acc: 86.8214\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 87.2340\n",
      "validation Loss: 0.0001 Acc: 86.9737\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.2916\n",
      "validation Loss: 0.0001 Acc: 86.9444\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 87.3560\n",
      "validation Loss: 0.0001 Acc: 87.1123\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.3428\n",
      "validation Loss: 0.0001 Acc: 87.2528\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.4151\n",
      "validation Loss: 0.0001 Acc: 87.3368\n",
      "Saving..\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.3960\n",
      "validation Loss: 0.0001 Acc: 87.2958\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 87.4502\n",
      "validation Loss: 0.0001 Acc: 87.3602\n",
      "Saving..\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 87.4590\n",
      "validation Loss: 0.0001 Acc: 87.2274\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 87.4375\n",
      "validation Loss: 0.0001 Acc: 87.2684\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 87.4317\n",
      "validation Loss: 0.0001 Acc: 87.2723\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 87.4414\n",
      "validation Loss: 0.0001 Acc: 87.2040\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 87.5234\n",
      "validation Loss: 0.0001 Acc: 87.4168\n",
      "Saving..\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 87.5268\n",
      "validation Loss: 0.0001 Acc: 87.4070\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 87.5410\n",
      "validation Loss: 0.0001 Acc: 87.3992\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 87.5468\n",
      "validation Loss: 0.0001 Acc: 87.4168\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 87.5342\n",
      "validation Loss: 0.0001 Acc: 87.3719\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 87.5542\n",
      "validation Loss: 0.0001 Acc: 87.3856\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 87.5717\n",
      "validation Loss: 0.0001 Acc: 87.3602\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 87.5835\n",
      "validation Loss: 0.0001 Acc: 87.4832\n",
      "Saving..\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 87.6230\n",
      "validation Loss: 0.0001 Acc: 87.4187\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 87.6152\n",
      "validation Loss: 0.0001 Acc: 87.4695\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 87.5673\n",
      "validation Loss: 0.0001 Acc: 87.4148\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 87.5869\n",
      "validation Loss: 0.0001 Acc: 87.4324\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 87.6259\n",
      "validation Loss: 0.0001 Acc: 87.4324\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 87.6103\n",
      "validation Loss: 0.0001 Acc: 87.4773\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 87.6391\n",
      "validation Loss: 0.0001 Acc: 87.4656\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 87.6571\n",
      "validation Loss: 0.0001 Acc: 87.4461\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 87.6098\n",
      "validation Loss: 0.0001 Acc: 87.4656\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 87.6454\n",
      "validation Loss: 0.0001 Acc: 87.4871\n",
      "Saving..\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 87.6371\n",
      "validation Loss: 0.0001 Acc: 87.4929\n",
      "Saving..\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 87.6493\n",
      "validation Loss: 0.0001 Acc: 87.4715\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 87.6449\n",
      "validation Loss: 0.0001 Acc: 87.5046\n",
      "Saving..\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 87.6244\n",
      "validation Loss: 0.0001 Acc: 87.4832\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 87.6283\n",
      "validation Loss: 0.0001 Acc: 87.4773\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 87.6557\n",
      "validation Loss: 0.0001 Acc: 87.4812\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 87.6259\n",
      "validation Loss: 0.0001 Acc: 87.4988\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 87.6825\n",
      "validation Loss: 0.0001 Acc: 87.4715\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 87.6308\n",
      "validation Loss: 0.0001 Acc: 87.4675\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 87.6435\n",
      "validation Loss: 0.0001 Acc: 87.4461\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 87.6547\n",
      "validation Loss: 0.0001 Acc: 87.4675\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 87.6488\n",
      "validation Loss: 0.0001 Acc: 87.4519\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 87.6498\n",
      "validation Loss: 0.0001 Acc: 87.4675\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 87.6235\n",
      "validation Loss: 0.0001 Acc: 87.4402\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 87.7177\n",
      "validation Loss: 0.0001 Acc: 87.4890\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 87.6376\n",
      "validation Loss: 0.0001 Acc: 87.4968\n",
      "Early stopped.\n",
      "Best val acc: 87.504639\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.6631\n",
      "validation Loss: 0.0001 Acc: 87.4653\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.6787\n",
      "validation Loss: 0.0001 Acc: 87.4322\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.6606\n",
      "validation Loss: 0.0001 Acc: 87.4458\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.6963\n",
      "validation Loss: 0.0001 Acc: 87.4302\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.6704\n",
      "validation Loss: 0.0001 Acc: 87.4322\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.6655\n",
      "validation Loss: 0.0001 Acc: 87.4224\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.6689\n",
      "validation Loss: 0.0001 Acc: 87.4224\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.6323\n",
      "validation Loss: 0.0001 Acc: 87.4302\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.6660\n",
      "validation Loss: 0.0001 Acc: 87.4204\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.6933\n",
      "validation Loss: 0.0001 Acc: 87.4185\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.6831\n",
      "validation Loss: 0.0001 Acc: 87.4185\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.6587\n",
      "validation Loss: 0.0001 Acc: 87.4322\n",
      "Early stopped.\n",
      "Best val acc: 87.465347\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.5635\n",
      "validation Loss: 0.0001 Acc: 87.8226\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.5928\n",
      "validation Loss: 0.0001 Acc: 87.8128\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.5669\n",
      "validation Loss: 0.0001 Acc: 87.8050\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.5728\n",
      "validation Loss: 0.0001 Acc: 87.8070\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.5645\n",
      "validation Loss: 0.0001 Acc: 87.8128\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5840\n",
      "validation Loss: 0.0001 Acc: 87.8050\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.5752\n",
      "validation Loss: 0.0001 Acc: 87.8206\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.5650\n",
      "validation Loss: 0.0001 Acc: 87.8226\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5694\n",
      "validation Loss: 0.0001 Acc: 87.8167\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5781\n",
      "validation Loss: 0.0001 Acc: 87.8148\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5933\n",
      "validation Loss: 0.0001 Acc: 87.8089\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5982\n",
      "validation Loss: 0.0001 Acc: 87.8226\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.6011\n",
      "validation Loss: 0.0001 Acc: 87.8187\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.5938\n",
      "validation Loss: 0.0001 Acc: 87.8187\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.6079\n",
      "validation Loss: 0.0001 Acc: 87.8128\n",
      "Early stopped.\n",
      "Best val acc: 87.822586\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.5591\n",
      "validation Loss: 0.0001 Acc: 87.8811\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.5591\n",
      "validation Loss: 0.0001 Acc: 87.8851\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.5903\n",
      "validation Loss: 0.0001 Acc: 87.8851\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.5767\n",
      "validation Loss: 0.0001 Acc: 87.8792\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.5411\n",
      "validation Loss: 0.0001 Acc: 87.8831\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5518\n",
      "validation Loss: 0.0001 Acc: 87.8890\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.5498\n",
      "validation Loss: 0.0001 Acc: 87.8870\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.5674\n",
      "validation Loss: 0.0001 Acc: 87.8870\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5332\n",
      "validation Loss: 0.0001 Acc: 87.8851\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5415\n",
      "validation Loss: 0.0001 Acc: 87.8851\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.5791\n",
      "validation Loss: 0.0001 Acc: 87.8851\n",
      "Early stopped.\n",
      "Best val acc: 87.888962\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 87.5674\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 87.6289\n",
      "validation Loss: 0.0001 Acc: 87.7230\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 87.5972\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.5386\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.6348\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.5879\n",
      "validation Loss: 0.0001 Acc: 87.7230\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.6035\n",
      "validation Loss: 0.0001 Acc: 87.7230\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.5947\n",
      "validation Loss: 0.0001 Acc: 87.7230\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5967\n",
      "validation Loss: 0.0001 Acc: 87.7230\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5957\n",
      "validation Loss: 0.0001 Acc: 87.7230\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.6274\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5967\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.6011\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.6206\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.5816\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.5830\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.5913\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.5952\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.6025\n",
      "validation Loss: 0.0001 Acc: 87.7250\n",
      "Early stopped.\n",
      "Best val acc: 87.724983\n",
      "----------\n",
      "Average best_acc across k-fold: 87.6812973022461\n",
      "phi\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 3, 6)\n",
      "background number: (170754, 3, 6)\n",
      "Data list: (341508, 3, 6)\n",
      "Data HLF: (341508, 9)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.4333\n",
      "validation Loss: 0.0001 Acc: 84.9240\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.3980\n",
      "validation Loss: 0.0001 Acc: 85.3202\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 86.4053\n",
      "validation Loss: 0.0001 Acc: 87.1552\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0095\n",
      "validation Loss: 0.0001 Acc: 87.2606\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.2467\n",
      "validation Loss: 0.0001 Acc: 87.3524\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.4688\n",
      "validation Loss: 0.0001 Acc: 87.2079\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.4175\n",
      "validation Loss: 0.0001 Acc: 87.5222\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.4990\n",
      "validation Loss: 0.0001 Acc: 87.5593\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.5493\n",
      "validation Loss: 0.0001 Acc: 87.2235\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.6020\n",
      "validation Loss: 0.0001 Acc: 87.8248\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.6801\n",
      "validation Loss: 0.0001 Acc: 87.4715\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.6986\n",
      "validation Loss: 0.0001 Acc: 87.7935\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.7943\n",
      "validation Loss: 0.0001 Acc: 87.6803\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.7914\n",
      "validation Loss: 0.0001 Acc: 87.7233\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.8143\n",
      "validation Loss: 0.0001 Acc: 87.8150\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.8885\n",
      "validation Loss: 0.0001 Acc: 87.9653\n",
      "Saving..\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.8558\n",
      "validation Loss: 0.0001 Acc: 87.8658\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.8973\n",
      "validation Loss: 0.0001 Acc: 87.9224\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.8748\n",
      "validation Loss: 0.0001 Acc: 88.1684\n",
      "Saving..\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.9387\n",
      "validation Loss: 0.0001 Acc: 88.0473\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.9217\n",
      "validation Loss: 0.0001 Acc: 88.0356\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.0383\n",
      "validation Loss: 0.0001 Acc: 87.9868\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.0163\n",
      "validation Loss: 0.0001 Acc: 88.0102\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.1339\n",
      "validation Loss: 0.0001 Acc: 88.1527\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.1632\n",
      "validation Loss: 0.0001 Acc: 88.0571\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.1671\n",
      "validation Loss: 0.0001 Acc: 88.1235\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.1749\n",
      "validation Loss: 0.0001 Acc: 88.1039\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.1627\n",
      "validation Loss: 0.0001 Acc: 88.0707\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.2623\n",
      "validation Loss: 0.0001 Acc: 88.2152\n",
      "Saving..\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.2296\n",
      "validation Loss: 0.0001 Acc: 88.2308\n",
      "Saving..\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 88.2867\n",
      "validation Loss: 0.0001 Acc: 88.3011\n",
      "Saving..\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 88.3116\n",
      "validation Loss: 0.0001 Acc: 88.2289\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 88.3009\n",
      "validation Loss: 0.0001 Acc: 88.2367\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 88.2823\n",
      "validation Loss: 0.0001 Acc: 88.3128\n",
      "Saving..\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 88.2506\n",
      "validation Loss: 0.0001 Acc: 88.2445\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 88.3643\n",
      "validation Loss: 0.0001 Acc: 88.3187\n",
      "Saving..\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 88.3541\n",
      "validation Loss: 0.0001 Acc: 88.2972\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 88.3384\n",
      "validation Loss: 0.0001 Acc: 88.2523\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 88.3638\n",
      "validation Loss: 0.0001 Acc: 88.2035\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 88.3555\n",
      "validation Loss: 0.0001 Acc: 88.2542\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 88.3428\n",
      "validation Loss: 0.0001 Acc: 88.3109\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 88.3775\n",
      "validation Loss: 0.0001 Acc: 88.3128\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 88.3462\n",
      "validation Loss: 0.0001 Acc: 88.2620\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 88.3785\n",
      "validation Loss: 0.0001 Acc: 88.2835\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 88.3316\n",
      "validation Loss: 0.0001 Acc: 88.3167\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 88.3760\n",
      "validation Loss: 0.0001 Acc: 88.2952\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 88.3897\n",
      "validation Loss: 0.0001 Acc: 88.3050\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 88.4097\n",
      "validation Loss: 0.0001 Acc: 88.3050\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 88.4273\n",
      "validation Loss: 0.0001 Acc: 88.2972\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.4029\n",
      "validation Loss: 0.0001 Acc: 88.3011\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 88.3877\n",
      "validation Loss: 0.0001 Acc: 88.3401\n",
      "Saving..\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 88.3658\n",
      "validation Loss: 0.0001 Acc: 88.3343\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.3599\n",
      "validation Loss: 0.0001 Acc: 88.2972\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 88.4038\n",
      "validation Loss: 0.0001 Acc: 88.2913\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.3960\n",
      "validation Loss: 0.0001 Acc: 88.3109\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.3736\n",
      "validation Loss: 0.0001 Acc: 88.3050\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 88.3672\n",
      "validation Loss: 0.0001 Acc: 88.2894\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.3887\n",
      "validation Loss: 0.0001 Acc: 88.2933\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.3853\n",
      "validation Loss: 0.0001 Acc: 88.2874\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.3477\n",
      "validation Loss: 0.0001 Acc: 88.3030\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.4019\n",
      "validation Loss: 0.0001 Acc: 88.3089\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.4063\n",
      "validation Loss: 0.0001 Acc: 88.2913\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.4024\n",
      "validation Loss: 0.0001 Acc: 88.3187\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.4033\n",
      "validation Loss: 0.0001 Acc: 88.3109\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 88.4204\n",
      "validation Loss: 0.0001 Acc: 88.3187\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 88.4219\n",
      "validation Loss: 0.0001 Acc: 88.2952\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 88.3902\n",
      "validation Loss: 0.0001 Acc: 88.3050\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 88.4112\n",
      "validation Loss: 0.0001 Acc: 88.2874\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 88.3780\n",
      "validation Loss: 0.0001 Acc: 88.2991\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 88.4029\n",
      "validation Loss: 0.0001 Acc: 88.2913\n",
      "Best val acc: 88.340134\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.3346\n",
      "validation Loss: 0.0001 Acc: 88.4863\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.3590\n",
      "validation Loss: 0.0001 Acc: 88.4785\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.3863\n",
      "validation Loss: 0.0001 Acc: 88.4590\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.3575\n",
      "validation Loss: 0.0001 Acc: 88.4629\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.3702\n",
      "validation Loss: 0.0001 Acc: 88.4668\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.3941\n",
      "validation Loss: 0.0001 Acc: 88.4648\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.3907\n",
      "validation Loss: 0.0001 Acc: 88.4668\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.4098\n",
      "validation Loss: 0.0001 Acc: 88.4531\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.3810\n",
      "validation Loss: 0.0001 Acc: 88.4590\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.3775\n",
      "validation Loss: 0.0001 Acc: 88.4648\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.3649\n",
      "validation Loss: 0.0001 Acc: 88.4570\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.3595\n",
      "validation Loss: 0.0001 Acc: 88.4551\n",
      "Early stopped.\n",
      "Best val acc: 88.486313\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.3434\n",
      "validation Loss: 0.0001 Acc: 88.5937\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.3414\n",
      "validation Loss: 0.0001 Acc: 88.5937\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.3609\n",
      "validation Loss: 0.0001 Acc: 88.6015\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.3346\n",
      "validation Loss: 0.0001 Acc: 88.5976\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.3404\n",
      "validation Loss: 0.0001 Acc: 88.5995\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.3629\n",
      "validation Loss: 0.0001 Acc: 88.5898\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.3531\n",
      "validation Loss: 0.0001 Acc: 88.5976\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.3683\n",
      "validation Loss: 0.0001 Acc: 88.5898\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.3243\n",
      "validation Loss: 0.0001 Acc: 88.5956\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.3751\n",
      "validation Loss: 0.0001 Acc: 88.5956\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.3619\n",
      "validation Loss: 0.0001 Acc: 88.5937\n",
      "Early stopped.\n",
      "Best val acc: 88.601486\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.3380\n",
      "validation Loss: 0.0001 Acc: 88.5175\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.3541\n",
      "validation Loss: 0.0001 Acc: 88.5215\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.3746\n",
      "validation Loss: 0.0001 Acc: 88.5195\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.3746\n",
      "validation Loss: 0.0001 Acc: 88.5195\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.3434\n",
      "validation Loss: 0.0001 Acc: 88.5215\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.3585\n",
      "validation Loss: 0.0001 Acc: 88.5215\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.4015\n",
      "validation Loss: 0.0001 Acc: 88.5195\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.3707\n",
      "validation Loss: 0.0001 Acc: 88.5215\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.4078\n",
      "validation Loss: 0.0001 Acc: 88.5215\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.3917\n",
      "validation Loss: 0.0001 Acc: 88.5195\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.3888\n",
      "validation Loss: 0.0001 Acc: 88.5175\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.3751\n",
      "validation Loss: 0.0001 Acc: 88.5156\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.3766\n",
      "validation Loss: 0.0001 Acc: 88.5175\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.4224\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.3556\n",
      "validation Loss: 0.0001 Acc: 88.5156\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.3522\n",
      "validation Loss: 0.0001 Acc: 88.5117\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.3805\n",
      "validation Loss: 0.0001 Acc: 88.5117\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.3712\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.3795\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 88.3961\n",
      "validation Loss: 0.0001 Acc: 88.5117\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 88.3551\n",
      "validation Loss: 0.0001 Acc: 88.5117\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.3800\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.3688\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.3741\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.3780\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.3678\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.3497\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.3551\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.3409\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.3629\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 88.3487\n",
      "validation Loss: 0.0001 Acc: 88.5136\n",
      "Early stopped.\n",
      "Best val acc: 88.521454\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.3829\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.4244\n",
      "validation Loss: 0.0001 Acc: 88.3594\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.3873\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.3771\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.3893\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.4073\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.4327\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.4249\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.3814\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.4195\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.4268\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.4259\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.3795\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.4034\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.4093\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.4210\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.3819\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.4200\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.4166\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 88.3863\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 88.4366\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.4039\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.4161\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.4054\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.4039\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.4332\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.3951\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.4127\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.3893\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.4200\n",
      "validation Loss: 0.0001 Acc: 88.3653\n",
      "Early stopped.\n",
      "Best val acc: 88.365280\n",
      "----------\n",
      "Average best_acc across k-fold: 88.46292877197266\n",
      "abs_CosThetaStar_CS\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 4, 6)\n",
      "background number: (170754, 4, 6)\n",
      "Data list: (341508, 4, 6)\n",
      "Data HLF: (341508, 8)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.9608\n",
      "validation Loss: 0.0001 Acc: 84.9298\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.1008\n",
      "validation Loss: 0.0001 Acc: 85.7477\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.2677\n",
      "validation Loss: 0.0001 Acc: 84.0709\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 85.9920\n",
      "validation Loss: 0.0001 Acc: 86.5208\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 86.9949\n",
      "validation Loss: 0.0001 Acc: 86.7394\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.1579\n",
      "validation Loss: 0.0001 Acc: 87.3329\n",
      "Saving..\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.3785\n",
      "validation Loss: 0.0001 Acc: 87.4910\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.4463\n",
      "validation Loss: 0.0001 Acc: 87.3582\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.4483\n",
      "validation Loss: 0.0001 Acc: 87.6628\n",
      "Saving..\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.5844\n",
      "validation Loss: 0.0001 Acc: 87.5573\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.6064\n",
      "validation Loss: 0.0001 Acc: 87.6374\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.5737\n",
      "validation Loss: 0.0001 Acc: 87.0576\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.5839\n",
      "validation Loss: 0.0001 Acc: 87.7682\n",
      "Saving..\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.6864\n",
      "validation Loss: 0.0001 Acc: 87.7428\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.7001\n",
      "validation Loss: 0.0001 Acc: 87.8990\n",
      "Saving..\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.6947\n",
      "validation Loss: 0.0001 Acc: 87.8794\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.7299\n",
      "validation Loss: 0.0001 Acc: 87.5027\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.6933\n",
      "validation Loss: 0.0001 Acc: 87.9146\n",
      "Saving..\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.8797\n",
      "validation Loss: 0.0001 Acc: 87.7135\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.8865\n",
      "validation Loss: 0.0001 Acc: 87.9029\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.9690\n",
      "validation Loss: 0.0001 Acc: 87.9497\n",
      "Saving..\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 87.9358\n",
      "validation Loss: 0.0001 Acc: 88.0844\n",
      "Saving..\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 87.9573\n",
      "validation Loss: 0.0001 Acc: 87.9185\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 87.9607\n",
      "validation Loss: 0.0001 Acc: 87.9321\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.0090\n",
      "validation Loss: 0.0001 Acc: 87.9204\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 87.9456\n",
      "validation Loss: 0.0001 Acc: 88.0844\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.0066\n",
      "validation Loss: 0.0001 Acc: 88.0961\n",
      "Saving..\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 87.9197\n",
      "validation Loss: 0.0001 Acc: 87.7467\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 87.9885\n",
      "validation Loss: 0.0001 Acc: 88.1000\n",
      "Saving..\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 87.9993\n",
      "validation Loss: 0.0001 Acc: 88.0649\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 88.0969\n",
      "validation Loss: 0.0001 Acc: 88.1410\n",
      "Saving..\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 88.1081\n",
      "validation Loss: 0.0001 Acc: 88.1840\n",
      "Saving..\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 88.1159\n",
      "validation Loss: 0.0001 Acc: 88.2211\n",
      "Saving..\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 88.1535\n",
      "validation Loss: 0.0001 Acc: 88.1898\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 88.1462\n",
      "validation Loss: 0.0001 Acc: 88.1957\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 88.1623\n",
      "validation Loss: 0.0001 Acc: 88.1274\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 88.1696\n",
      "validation Loss: 0.0001 Acc: 88.1605\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 88.1847\n",
      "validation Loss: 0.0001 Acc: 88.2172\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 88.1720\n",
      "validation Loss: 0.0001 Acc: 88.1976\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 88.2252\n",
      "validation Loss: 0.0001 Acc: 88.1410\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 88.2228\n",
      "validation Loss: 0.0001 Acc: 88.1625\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 88.1774\n",
      "validation Loss: 0.0001 Acc: 88.1801\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 88.1959\n",
      "validation Loss: 0.0001 Acc: 88.2132\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 88.2115\n",
      "validation Loss: 0.0001 Acc: 88.1762\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 88.2272\n",
      "validation Loss: 0.0001 Acc: 88.1937\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 88.2111\n",
      "validation Loss: 0.0001 Acc: 88.1742\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 88.2555\n",
      "validation Loss: 0.0001 Acc: 88.2211\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 88.2213\n",
      "validation Loss: 0.0001 Acc: 88.2425\n",
      "Saving..\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 88.2189\n",
      "validation Loss: 0.0001 Acc: 88.2347\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.2247\n",
      "validation Loss: 0.0001 Acc: 88.2406\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 88.2164\n",
      "validation Loss: 0.0001 Acc: 88.1469\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 88.2921\n",
      "validation Loss: 0.0001 Acc: 88.1703\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.2604\n",
      "validation Loss: 0.0001 Acc: 88.1918\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 88.2076\n",
      "validation Loss: 0.0001 Acc: 88.1976\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.2555\n",
      "validation Loss: 0.0001 Acc: 88.2308\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.2745\n",
      "validation Loss: 0.0001 Acc: 88.2328\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 88.2765\n",
      "validation Loss: 0.0001 Acc: 88.2503\n",
      "Saving..\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.2355\n",
      "validation Loss: 0.0001 Acc: 88.1937\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.2994\n",
      "validation Loss: 0.0001 Acc: 88.2640\n",
      "Saving..\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.3038\n",
      "validation Loss: 0.0001 Acc: 88.2562\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.2794\n",
      "validation Loss: 0.0001 Acc: 88.1898\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.2828\n",
      "validation Loss: 0.0001 Acc: 88.2464\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.2970\n",
      "validation Loss: 0.0001 Acc: 88.2328\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.2921\n",
      "validation Loss: 0.0001 Acc: 88.2620\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 88.3253\n",
      "validation Loss: 0.0001 Acc: 88.2699\n",
      "Saving..\n",
      "Epoch 65/69\n",
      "training Loss: 0.0001 Acc: 88.3155\n",
      "validation Loss: 0.0001 Acc: 88.2425\n",
      "Epoch 66/69\n",
      "training Loss: 0.0001 Acc: 88.3048\n",
      "validation Loss: 0.0001 Acc: 88.2308\n",
      "Epoch 67/69\n",
      "training Loss: 0.0001 Acc: 88.3184\n",
      "validation Loss: 0.0001 Acc: 88.2425\n",
      "Epoch 68/69\n",
      "training Loss: 0.0001 Acc: 88.2833\n",
      "validation Loss: 0.0001 Acc: 88.2347\n",
      "Epoch 69/69\n",
      "training Loss: 0.0001 Acc: 88.2916\n",
      "validation Loss: 0.0001 Acc: 88.2230\n",
      "Best val acc: 88.269859\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2770\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2741\n",
      "validation Loss: 0.0001 Acc: 88.3594\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2804\n",
      "validation Loss: 0.0001 Acc: 88.3770\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2946\n",
      "validation Loss: 0.0001 Acc: 88.3633\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.3058\n",
      "validation Loss: 0.0001 Acc: 88.3770\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2565\n",
      "validation Loss: 0.0001 Acc: 88.3555\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2819\n",
      "validation Loss: 0.0001 Acc: 88.3829\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.3048\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Saving..\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2580\n",
      "validation Loss: 0.0001 Acc: 88.3848\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2438\n",
      "validation Loss: 0.0001 Acc: 88.3809\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2936\n",
      "validation Loss: 0.0001 Acc: 88.3770\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.3234\n",
      "validation Loss: 0.0001 Acc: 88.3711\n",
      "Early stopped.\n",
      "Best val acc: 88.386757\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2697\n",
      "validation Loss: 0.0001 Acc: 88.3594\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.3224\n",
      "validation Loss: 0.0001 Acc: 88.3594\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2775\n",
      "validation Loss: 0.0001 Acc: 88.3614\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.3200\n",
      "validation Loss: 0.0001 Acc: 88.3497\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2873\n",
      "validation Loss: 0.0001 Acc: 88.3672\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.3200\n",
      "validation Loss: 0.0001 Acc: 88.3555\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2467\n",
      "validation Loss: 0.0001 Acc: 88.3536\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2707\n",
      "validation Loss: 0.0001 Acc: 88.3575\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2941\n",
      "validation Loss: 0.0001 Acc: 88.3536\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2624\n",
      "validation Loss: 0.0001 Acc: 88.3555\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2458\n",
      "validation Loss: 0.0001 Acc: 88.3555\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.2794\n",
      "validation Loss: 0.0001 Acc: 88.3555\n",
      "Early stopped.\n",
      "Best val acc: 88.367233\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2990\n",
      "validation Loss: 0.0001 Acc: 88.3184\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2916\n",
      "validation Loss: 0.0001 Acc: 88.3126\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2716\n",
      "validation Loss: 0.0001 Acc: 88.3067\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2819\n",
      "validation Loss: 0.0001 Acc: 88.3067\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.3063\n",
      "validation Loss: 0.0001 Acc: 88.3009\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2951\n",
      "validation Loss: 0.0001 Acc: 88.2950\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2960\n",
      "validation Loss: 0.0001 Acc: 88.2950\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.3146\n",
      "validation Loss: 0.0001 Acc: 88.2950\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.3185\n",
      "validation Loss: 0.0001 Acc: 88.2931\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.3268\n",
      "validation Loss: 0.0001 Acc: 88.3009\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2546\n",
      "validation Loss: 0.0001 Acc: 88.2989\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.3200\n",
      "validation Loss: 0.0001 Acc: 88.2950\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.2965\n",
      "validation Loss: 0.0001 Acc: 88.2950\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.2892\n",
      "validation Loss: 0.0001 Acc: 88.2950\n",
      "Early stopped.\n",
      "Best val acc: 88.318428\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.2614\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.2687\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.2614\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.2780\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.2507\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.2428\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.2541\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.2458\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.2658\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.2638\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.2477\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.2575\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.2868\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.2882\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.2531\n",
      "validation Loss: 0.0001 Acc: 88.4902\n",
      "Early stopped.\n",
      "Best val acc: 88.490219\n",
      "----------\n",
      "Average best_acc across k-fold: 88.36650085449219\n",
      "abs_CosThetaStar_jj\n",
      "Mean and std calculated.\n",
      "Mean and std calculated for particle list.\n",
      "signal number: (170754, 4, 6)\n",
      "background number: (170754, 4, 6)\n",
      "Data list: (341508, 4, 6)\n",
      "Data HLF: (341508, 8)\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 81.8569\n",
      "validation Loss: 0.0001 Acc: 84.7229\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 85.1487\n",
      "validation Loss: 0.0001 Acc: 85.5994\n",
      "Saving..\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 85.7924\n",
      "validation Loss: 0.0001 Acc: 86.3529\n",
      "Saving..\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 87.0359\n",
      "validation Loss: 0.0001 Acc: 87.1084\n",
      "Saving..\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 87.3194\n",
      "validation Loss: 0.0001 Acc: 87.3836\n",
      "Saving..\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 87.3507\n",
      "validation Loss: 0.0001 Acc: 87.0537\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 87.4121\n",
      "validation Loss: 0.0001 Acc: 87.5691\n",
      "Saving..\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 87.6532\n",
      "validation Loss: 0.0001 Acc: 87.3055\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 87.6654\n",
      "validation Loss: 0.0001 Acc: 87.4402\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 87.6625\n",
      "validation Loss: 0.0001 Acc: 87.7662\n",
      "Saving..\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 87.7416\n",
      "validation Loss: 0.0001 Acc: 87.7701\n",
      "Saving..\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 87.7953\n",
      "validation Loss: 0.0001 Acc: 87.7838\n",
      "Saving..\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 87.7767\n",
      "validation Loss: 0.0001 Acc: 87.7643\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 87.8016\n",
      "validation Loss: 0.0001 Acc: 87.6764\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 87.7787\n",
      "validation Loss: 0.0001 Acc: 87.5105\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 87.8558\n",
      "validation Loss: 0.0001 Acc: 87.5417\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 87.8382\n",
      "validation Loss: 0.0001 Acc: 87.5788\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 87.9602\n",
      "validation Loss: 0.0001 Acc: 87.7213\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 87.8626\n",
      "validation Loss: 0.0001 Acc: 87.8131\n",
      "Saving..\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 87.9851\n",
      "validation Loss: 0.0001 Acc: 87.9634\n",
      "Saving..\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 87.9954\n",
      "validation Loss: 0.0001 Acc: 87.8287\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.0295\n",
      "validation Loss: 0.0001 Acc: 87.9731\n",
      "Saving..\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.0237\n",
      "validation Loss: 0.0001 Acc: 87.6725\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.0295\n",
      "validation Loss: 0.0001 Acc: 87.8345\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.0954\n",
      "validation Loss: 0.0001 Acc: 88.0180\n",
      "Saving..\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.0154\n",
      "validation Loss: 0.0001 Acc: 87.9107\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.2330\n",
      "validation Loss: 0.0001 Acc: 87.9946\n",
      "Epoch 27/69\n",
      "training Loss: 0.0001 Acc: 88.2247\n",
      "validation Loss: 0.0001 Acc: 88.1410\n",
      "Saving..\n",
      "Epoch 28/69\n",
      "training Loss: 0.0001 Acc: 88.2164\n",
      "validation Loss: 0.0001 Acc: 88.0805\n",
      "Epoch 29/69\n",
      "training Loss: 0.0001 Acc: 88.2394\n",
      "validation Loss: 0.0001 Acc: 88.1371\n",
      "Epoch 30/69\n",
      "training Loss: 0.0001 Acc: 88.2813\n",
      "validation Loss: 0.0001 Acc: 88.0707\n",
      "Epoch 31/69\n",
      "training Loss: 0.0001 Acc: 88.2804\n",
      "validation Loss: 0.0001 Acc: 88.1293\n",
      "Epoch 32/69\n",
      "training Loss: 0.0001 Acc: 88.2940\n",
      "validation Loss: 0.0001 Acc: 88.1000\n",
      "Epoch 33/69\n",
      "training Loss: 0.0001 Acc: 88.3140\n",
      "validation Loss: 0.0001 Acc: 88.1488\n",
      "Saving..\n",
      "Epoch 34/69\n",
      "training Loss: 0.0001 Acc: 88.3013\n",
      "validation Loss: 0.0001 Acc: 88.1098\n",
      "Epoch 35/69\n",
      "training Loss: 0.0001 Acc: 88.3497\n",
      "validation Loss: 0.0001 Acc: 88.1840\n",
      "Saving..\n",
      "Epoch 36/69\n",
      "training Loss: 0.0001 Acc: 88.3736\n",
      "validation Loss: 0.0001 Acc: 88.1781\n",
      "Epoch 37/69\n",
      "training Loss: 0.0001 Acc: 88.3785\n",
      "validation Loss: 0.0001 Acc: 88.1996\n",
      "Saving..\n",
      "Epoch 38/69\n",
      "training Loss: 0.0001 Acc: 88.3760\n",
      "validation Loss: 0.0001 Acc: 88.2464\n",
      "Saving..\n",
      "Epoch 39/69\n",
      "training Loss: 0.0001 Acc: 88.3775\n",
      "validation Loss: 0.0001 Acc: 88.1742\n",
      "Epoch 40/69\n",
      "training Loss: 0.0001 Acc: 88.4004\n",
      "validation Loss: 0.0001 Acc: 88.1371\n",
      "Epoch 41/69\n",
      "training Loss: 0.0001 Acc: 88.3301\n",
      "validation Loss: 0.0001 Acc: 88.0766\n",
      "Epoch 42/69\n",
      "training Loss: 0.0001 Acc: 88.3980\n",
      "validation Loss: 0.0001 Acc: 88.1156\n",
      "Epoch 43/69\n",
      "training Loss: 0.0001 Acc: 88.4141\n",
      "validation Loss: 0.0001 Acc: 88.1625\n",
      "Epoch 44/69\n",
      "training Loss: 0.0001 Acc: 88.4458\n",
      "validation Loss: 0.0001 Acc: 88.2152\n",
      "Epoch 45/69\n",
      "training Loss: 0.0001 Acc: 88.4165\n",
      "validation Loss: 0.0001 Acc: 88.1723\n",
      "Epoch 46/69\n",
      "training Loss: 0.0001 Acc: 88.4370\n",
      "validation Loss: 0.0001 Acc: 88.2542\n",
      "Saving..\n",
      "Epoch 47/69\n",
      "training Loss: 0.0001 Acc: 88.4370\n",
      "validation Loss: 0.0001 Acc: 88.0883\n",
      "Epoch 48/69\n",
      "training Loss: 0.0001 Acc: 88.4097\n",
      "validation Loss: 0.0001 Acc: 88.1508\n",
      "Epoch 49/69\n",
      "training Loss: 0.0001 Acc: 88.4194\n",
      "validation Loss: 0.0001 Acc: 88.2074\n",
      "Epoch 50/69\n",
      "training Loss: 0.0001 Acc: 88.4370\n",
      "validation Loss: 0.0001 Acc: 88.1508\n",
      "Epoch 51/69\n",
      "training Loss: 0.0001 Acc: 88.4219\n",
      "validation Loss: 0.0001 Acc: 88.2386\n",
      "Epoch 52/69\n",
      "training Loss: 0.0001 Acc: 88.4570\n",
      "validation Loss: 0.0001 Acc: 88.2250\n",
      "Epoch 53/69\n",
      "training Loss: 0.0001 Acc: 88.4614\n",
      "validation Loss: 0.0001 Acc: 88.2406\n",
      "Epoch 54/69\n",
      "training Loss: 0.0001 Acc: 88.4673\n",
      "validation Loss: 0.0001 Acc: 88.2015\n",
      "Epoch 55/69\n",
      "training Loss: 0.0001 Acc: 88.4775\n",
      "validation Loss: 0.0001 Acc: 88.2562\n",
      "Saving..\n",
      "Epoch 56/69\n",
      "training Loss: 0.0001 Acc: 88.4761\n",
      "validation Loss: 0.0001 Acc: 88.2620\n",
      "Saving..\n",
      "Epoch 57/69\n",
      "training Loss: 0.0001 Acc: 88.4268\n",
      "validation Loss: 0.0001 Acc: 88.2308\n",
      "Epoch 58/69\n",
      "training Loss: 0.0001 Acc: 88.4580\n",
      "validation Loss: 0.0001 Acc: 88.2269\n",
      "Epoch 59/69\n",
      "training Loss: 0.0001 Acc: 88.4643\n",
      "validation Loss: 0.0001 Acc: 88.2406\n",
      "Epoch 60/69\n",
      "training Loss: 0.0001 Acc: 88.4595\n",
      "validation Loss: 0.0001 Acc: 88.1859\n",
      "Epoch 61/69\n",
      "training Loss: 0.0001 Acc: 88.4683\n",
      "validation Loss: 0.0001 Acc: 88.2503\n",
      "Epoch 62/69\n",
      "training Loss: 0.0001 Acc: 88.4858\n",
      "validation Loss: 0.0001 Acc: 88.2738\n",
      "Saving..\n",
      "Epoch 63/69\n",
      "training Loss: 0.0001 Acc: 88.5063\n",
      "validation Loss: 0.0001 Acc: 88.2328\n",
      "Epoch 64/69\n",
      "training Loss: 0.0001 Acc: 88.4795\n",
      "validation Loss: 0.0001 Acc: 88.2308\n",
      "Early stopped.\n",
      "Best val acc: 88.273766\n",
      "----------\n",
      "==================================================\n",
      "Fold 1\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.4176\n",
      "validation Loss: 0.0001 Acc: 88.5449\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.4117\n",
      "validation Loss: 0.0001 Acc: 88.4980\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.4010\n",
      "validation Loss: 0.0001 Acc: 88.4883\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.4688\n",
      "validation Loss: 0.0001 Acc: 88.5156\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.4190\n",
      "validation Loss: 0.0001 Acc: 88.5097\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.4517\n",
      "validation Loss: 0.0001 Acc: 88.5039\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.4410\n",
      "validation Loss: 0.0001 Acc: 88.4922\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.3922\n",
      "validation Loss: 0.0001 Acc: 88.4824\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.4405\n",
      "validation Loss: 0.0001 Acc: 88.5000\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.4751\n",
      "validation Loss: 0.0001 Acc: 88.4844\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.4385\n",
      "validation Loss: 0.0001 Acc: 88.4668\n",
      "Early stopped.\n",
      "Best val acc: 88.544876\n",
      "----------\n",
      "==================================================\n",
      "Fold 2\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.4542\n",
      "validation Loss: 0.0001 Acc: 88.4063\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.5098\n",
      "validation Loss: 0.0001 Acc: 88.3965\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.4527\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.4410\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.4595\n",
      "validation Loss: 0.0001 Acc: 88.3926\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.4634\n",
      "validation Loss: 0.0001 Acc: 88.3926\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.4678\n",
      "validation Loss: 0.0001 Acc: 88.3868\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.4634\n",
      "validation Loss: 0.0001 Acc: 88.3887\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.4747\n",
      "validation Loss: 0.0001 Acc: 88.3887\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.4688\n",
      "validation Loss: 0.0001 Acc: 88.3809\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.4698\n",
      "validation Loss: 0.0001 Acc: 88.3829\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.4561\n",
      "validation Loss: 0.0001 Acc: 88.3829\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.4961\n",
      "validation Loss: 0.0001 Acc: 88.3907\n",
      "Early stopped.\n",
      "Best val acc: 88.406273\n",
      "----------\n",
      "==================================================\n",
      "Fold 3\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.3609\n",
      "validation Loss: 0.0001 Acc: 88.8221\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.3478\n",
      "validation Loss: 0.0001 Acc: 88.8162\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.3487\n",
      "validation Loss: 0.0001 Acc: 88.8182\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.3839\n",
      "validation Loss: 0.0001 Acc: 88.8123\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.3692\n",
      "validation Loss: 0.0001 Acc: 88.8123\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.3258\n",
      "validation Loss: 0.0001 Acc: 88.8084\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.3424\n",
      "validation Loss: 0.0001 Acc: 88.8123\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.3395\n",
      "validation Loss: 0.0001 Acc: 88.8104\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.3688\n",
      "validation Loss: 0.0001 Acc: 88.8104\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.3658\n",
      "validation Loss: 0.0001 Acc: 88.8006\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.4015\n",
      "validation Loss: 0.0001 Acc: 88.8045\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.3775\n",
      "validation Loss: 0.0001 Acc: 88.8026\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.3536\n",
      "validation Loss: 0.0001 Acc: 88.8065\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.3649\n",
      "validation Loss: 0.0001 Acc: 88.8084\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.3575\n",
      "validation Loss: 0.0001 Acc: 88.8065\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.3824\n",
      "validation Loss: 0.0001 Acc: 88.8123\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.3678\n",
      "validation Loss: 0.0001 Acc: 88.8045\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.3683\n",
      "validation Loss: 0.0001 Acc: 88.8045\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.3863\n",
      "validation Loss: 0.0001 Acc: 88.8045\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 88.3248\n",
      "validation Loss: 0.0001 Acc: 88.8084\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 88.3702\n",
      "validation Loss: 0.0001 Acc: 88.8084\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.3795\n",
      "validation Loss: 0.0001 Acc: 88.8065\n",
      "Epoch 22/69\n",
      "training Loss: 0.0001 Acc: 88.3556\n",
      "validation Loss: 0.0001 Acc: 88.8065\n",
      "Epoch 23/69\n",
      "training Loss: 0.0001 Acc: 88.3731\n",
      "validation Loss: 0.0001 Acc: 88.8065\n",
      "Epoch 24/69\n",
      "training Loss: 0.0001 Acc: 88.3605\n",
      "validation Loss: 0.0001 Acc: 88.8045\n",
      "Epoch 25/69\n",
      "training Loss: 0.0001 Acc: 88.3595\n",
      "validation Loss: 0.0001 Acc: 88.8045\n",
      "Epoch 26/69\n",
      "training Loss: 0.0001 Acc: 88.3639\n",
      "validation Loss: 0.0001 Acc: 88.8045\n",
      "Early stopped.\n",
      "Best val acc: 88.822083\n",
      "----------\n",
      "==================================================\n",
      "Fold 4\n",
      "==================================================\n",
      "Epoch 0/69\n",
      "training Loss: 0.0001 Acc: 88.4239\n",
      "validation Loss: 0.0001 Acc: 88.5429\n",
      "Saving..\n",
      "Epoch 1/69\n",
      "training Loss: 0.0001 Acc: 88.4688\n",
      "validation Loss: 0.0001 Acc: 88.5429\n",
      "Epoch 2/69\n",
      "training Loss: 0.0001 Acc: 88.4234\n",
      "validation Loss: 0.0001 Acc: 88.5429\n",
      "Epoch 3/69\n",
      "training Loss: 0.0001 Acc: 88.4029\n",
      "validation Loss: 0.0001 Acc: 88.5429\n",
      "Epoch 4/69\n",
      "training Loss: 0.0001 Acc: 88.4669\n",
      "validation Loss: 0.0001 Acc: 88.5429\n",
      "Epoch 5/69\n",
      "training Loss: 0.0001 Acc: 88.4307\n",
      "validation Loss: 0.0001 Acc: 88.5429\n",
      "Epoch 6/69\n",
      "training Loss: 0.0001 Acc: 88.3961\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 7/69\n",
      "training Loss: 0.0001 Acc: 88.4327\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 8/69\n",
      "training Loss: 0.0001 Acc: 88.4229\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 9/69\n",
      "training Loss: 0.0001 Acc: 88.4288\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 10/69\n",
      "training Loss: 0.0001 Acc: 88.4268\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 11/69\n",
      "training Loss: 0.0001 Acc: 88.3897\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 12/69\n",
      "training Loss: 0.0001 Acc: 88.4054\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 13/69\n",
      "training Loss: 0.0001 Acc: 88.4561\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 14/69\n",
      "training Loss: 0.0001 Acc: 88.4093\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 15/69\n",
      "training Loss: 0.0001 Acc: 88.4273\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 16/69\n",
      "training Loss: 0.0001 Acc: 88.4283\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 17/69\n",
      "training Loss: 0.0001 Acc: 88.4327\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 18/69\n",
      "training Loss: 0.0001 Acc: 88.4146\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 19/69\n",
      "training Loss: 0.0001 Acc: 88.4425\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 20/69\n",
      "training Loss: 0.0001 Acc: 88.4268\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Epoch 21/69\n",
      "training Loss: 0.0001 Acc: 88.4381\n",
      "validation Loss: 0.0001 Acc: 88.5410\n",
      "Early stopped.\n",
      "Best val acc: 88.542923\n",
      "----------\n",
      "Average best_acc across k-fold: 88.51798248291016\n",
      "Save output file to output_dict.json\n"
     ]
    }
   ],
   "source": [
    "high_level_fields = [\n",
    "    'puppiMET_sumEt', 'puppiMET_pt', 'puppiMET_eta', 'puppiMET_phi', # MET variables\n",
    "    'DeltaPhi_j1MET', 'DeltaPhi_j2MET', # jet-MET variables\n",
    "    'DeltaR_jg_min', 'n_jets', 'chi_t0', 'chi_t1', # jet variables\n",
    "    'lepton1_pt' ,'lepton2_pt', 'pt', # lepton and diphoton pt\n",
    "    'lepton1_eta', 'lepton2_eta', 'eta', # lepton and diphoton eta\n",
    "    'lepton1_phi', 'lepton2_phi', 'phi', # lepton and diphoton phi\n",
    "    'abs_CosThetaStar_CS', 'abs_CosThetaStar_jj' # angular variables\n",
    "]\n",
    "\n",
    "pandas_aux_samples = {}\n",
    "high_level_aux_fields = {\n",
    "    'mass', 'dijet_mass' # diphoton and bb-dijet mass\n",
    "} # https://stackoverflow.com/questions/67003141/how-to-remove-a-field-from-a-collection-of-records-created-by-awkward-zip\n",
    "\n",
    "output_dict = {}\n",
    "\n",
    "for i in range(len(high_level_fields)):\n",
    "    print(high_level_fields[i])\n",
    "    model_file = f\"IN_no_{high_level_fields[i]}\"\n",
    "    \n",
    "    pandas_samples = {}\n",
    "    for sample_name, sample in samples.items():\n",
    "        pandas_samples[sample_name] = {\n",
    "            field: ak.to_numpy(sample[field], allow_missing=False) for field in high_level_fields\n",
    "        }\n",
    "        pandas_aux_samples[sample_name] = {\n",
    "            field: ak.to_numpy(sample[field], allow_missing=False) for field in high_level_aux_fields\n",
    "        }\n",
    "\n",
    "    sig_frame = pd.DataFrame(pandas_samples['sig'])\n",
    "    sig_aux_frame = pd.DataFrame(pandas_aux_samples['sig'])\n",
    "    bkg_frame = pd.DataFrame(pandas_samples['bkg'])\n",
    "    bkg_aux_frame = pd.DataFrame(pandas_aux_samples['bkg'])\n",
    "\n",
    "    zero_entries = (bkg_frame == -999) # now pad with -999 instead of 0\n",
    "    masked_x_sample = np.ma.array(bkg_frame, mask=zero_entries)\n",
    "    x_mean = masked_x_sample.mean(axis=0)\n",
    "    x_std = masked_x_sample.std(axis=0)\n",
    "    print(\"Mean and std calculated.\")\n",
    "\n",
    "    # Standardize background\n",
    "    normed_bkg = (masked_x_sample - x_mean)/x_std\n",
    "\n",
    "    # Standardize signal\n",
    "    zero_entries = (sig_frame == -999) # now pad with -999 instead of 0\n",
    "    masked_x_sample = np.ma.array(sig_frame, mask=zero_entries)\n",
    "    normed_sig = (masked_x_sample - x_mean)/x_std\n",
    "\n",
    "    normed_bkg_frame = pd.DataFrame(normed_bkg.filled(0), columns=list(bkg_frame))\n",
    "    normed_bkg_frame.head()\n",
    "\n",
    "    normed_sig_frame = pd.DataFrame(normed_sig.filled(0), columns=list(sig_frame))\n",
    "    normed_sig_frame.head()\n",
    "\n",
    "\n",
    "    sig_list = to_p_list(sig_frame)\n",
    "    bkg_list = to_p_list(bkg_frame)\n",
    "\n",
    "\n",
    "    if re.match('lepton1', high_level_fields[i]) is not None:\n",
    "        sig_list = np.delete(sig_list, 0, axis=1)\n",
    "        bkg_list = np.delete(bkg_list, 0, axis=1)\n",
    "    elif re.match('lepton2', high_level_fields[i]) is not None:\n",
    "        sig_list = np.delete(sig_list, 1, axis=1)\n",
    "        bkg_list = np.delete(bkg_list, 1, axis=1)\n",
    "    elif re.match('puppiMET', high_level_fields[i]) is not None and re.search('sumEt', high_level_fields[i]) is None:\n",
    "        sig_list = np.delete(sig_list, 2, axis=1)\n",
    "        bkg_list = np.delete(bkg_list, 2, axis=1)\n",
    "    elif re.search('_', high_level_fields[i]) is None:\n",
    "        sig_list = np.delete(sig_list, 3, axis=1)\n",
    "        bkg_list = np.delete(bkg_list, 3, axis=1)\n",
    "\n",
    "    # Standardize the particle list\n",
    "    x_sample = bkg_list[:,:,:3] # don't standardize boolean flags\n",
    "    # Flatten out\n",
    "    x_flat = x_sample.reshape((x_sample.shape[0]*x_sample.shape[1], x_sample.shape[2]))\n",
    "    # Masked out zero\n",
    "    zero_entries = x_flat == 0 \n",
    "    masked_x_sample = np.ma.array(x_flat, mask=zero_entries)\n",
    "    x_list_mean = masked_x_sample.mean(axis=0)\n",
    "    x_list_std = masked_x_sample.std(axis=0)\n",
    "    print(\"Mean and std calculated for particle list.\")\n",
    "    del x_sample, x_flat, zero_entries, masked_x_sample # release the memory\n",
    "        \n",
    "    normed_sig_list = standardize_p_list(sig_list)\n",
    "    normed_bkg_list = standardize_p_list(bkg_list)\n",
    "\n",
    "    all_HLFs = {'puppiMET_sumEt','DeltaPhi_j1MET','DeltaPhi_j2MET','DeltaR_jg_min','n_jets','chi_t0',\n",
    "                    'chi_t1','abs_CosThetaStar_CS','abs_CosThetaStar_jj'}\n",
    "    if high_level_fields[i] in all_HLFs:\n",
    "        all_HLFs.remove(high_level_fields[i])\n",
    "    dnn_input=len(all_HLFs)\n",
    "\n",
    "    normed_sig_hlf = normed_sig_frame[list(all_HLFs)].values\n",
    "\n",
    "    normed_bkg_hlf = normed_bkg_frame[list(all_HLFs)].values\n",
    "        \n",
    "    # Shuffle before splitting into train-val\n",
    "    randix = np.arange(len(normed_bkg_list))\n",
    "    np.random.shuffle(randix)\n",
    "\n",
    "    background_list = normed_bkg_list[randix]\n",
    "    background_list = background_list[:len(normed_sig_list)] # downsampling\n",
    "    print(f'signal number: {normed_sig_list.shape}')\n",
    "    print(f'background number: {background_list.shape}')\n",
    "\n",
    "    background_hlf = normed_bkg_hlf[randix]\n",
    "    background_hlf = background_hlf[:len(normed_sig_hlf)]\n",
    "\n",
    "    sig_label = np.ones(len(normed_sig_hlf))\n",
    "    bkg_label = np.zeros(len(background_hlf))\n",
    "\n",
    "    data_list_full = np.concatenate((normed_sig_list,background_list))\n",
    "    data_hlf_full = np.concatenate((normed_sig_hlf,background_hlf))\n",
    "    label_full = np.concatenate((sig_label,bkg_label))\n",
    "\n",
    "    print(\"Data list: {}\".format(data_list_full.shape))\n",
    "    print(\"Data HLF: {}\".format(data_hlf_full.shape))\n",
    "\n",
    "    data_list, data_list_test, data_hlf, data_hlf_test, label, label_test = train_test_split(\n",
    "        data_list_full, data_hlf_full, label_full, test_size=0.25, random_state=None)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    # skf.get_n_splits(data_hlf, label)\n",
    "\n",
    "\n",
    "    fom = []\n",
    "    model = InclusiveNetwork(best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "                            best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], dnn_input=dnn_input).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'])\n",
    "    criterion= nn.NLLLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=3)\n",
    "    train_losses_arr, val_losses_arr = [], []\n",
    "    EPOCHS = 70\n",
    "    for fold_idx, (train_index, test_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        print('='*50)\n",
    "        print(f'Fold {fold_idx}')\n",
    "        print('='*50)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), \n",
    "            batch_size=best_conf['batch_size'], \n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), \n",
    "            batch_size=best_conf['batch_size'], \n",
    "            shuffle=True\n",
    "        )\n",
    "        data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "        best_acc, train_losses, val_losses = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader)\n",
    "        train_losses_arr.append(train_losses)\n",
    "        val_losses_arr.append(val_losses)\n",
    "\n",
    "        model_i_file = model_file + f'_{fold_idx}.torch'\n",
    "        torch.save(model.state_dict(), model_i_file)\n",
    "\n",
    "        fom.append(best_acc)\n",
    "\n",
    "    Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "    print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "\n",
    "    TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "\n",
    "    fprs = []\n",
    "    base_tpr = np.linspace(0, 1, 5000)\n",
    "    thresholds = []\n",
    "    # volatile=True\n",
    "    best_batch_size = bestconf['batch_size']\n",
    "    # for train_index, test_index in skf.split(data_hlf, label):\n",
    "    val_loader = DataLoader(ParticleHLF(data_list_test, data_hlf_test, label_test), batch_size=bestconf['batch_size'], shuffle=False)\n",
    "    all_pred = np.zeros(shape=(len(data_hlf_test),2))\n",
    "    all_label = np.zeros(shape=(len(data_hlf_test)))\n",
    "    criterion= nn.NLLLoss()\n",
    "\n",
    "    for fold_idx in range(skf.get_n_splits()):\n",
    "        model.load_state_dict(torch.load(f'{model_file}_{fold_idx}.torch'))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (particles_data, hlf_data, y_data) in enumerate(val_loader):\n",
    "                particles_data = particles_data.numpy()\n",
    "                arr = np.sum(particles_data!=0, axis=1)[:,0] # the number of particles in the whole batch\n",
    "                arr = [1 if x==0 else x for x in arr]\n",
    "                arr = np.array(arr)\n",
    "                sorted_indices_la= np.argsort(-arr)\n",
    "                particles_data = torch.from_numpy(particles_data[sorted_indices_la]).float()\n",
    "                hlf_data = hlf_data[sorted_indices_la]\n",
    "                particles_data = Variable(particles_data).cuda()\n",
    "                hlf_data = Variable(hlf_data).cuda()\n",
    "                # particles_data = Variable(particles_data)\n",
    "                # hlf_data = Variable(hlf_data)\n",
    "                t_seq_length= [arr[i] for i in sorted_indices_la]\n",
    "                particles_data = torch.nn.utils.rnn.pack_padded_sequence(particles_data, t_seq_length, batch_first=True)\n",
    "\n",
    "                outputs = model(particles_data, hlf_data)\n",
    "\n",
    "                # Unsort the predictions (to match the original data order)\n",
    "                # https://stackoverflow.com/questions/34159608/how-to-unsort-a-np-array-given-the-argsort\n",
    "                b = np.argsort(sorted_indices_la)\n",
    "                unsorted_pred = outputs[b].data.cpu().numpy()\n",
    "\n",
    "                fill_array(all_pred, unsorted_pred, batch_idx, best_batch_size)\n",
    "                fill_array(all_label, y_data.numpy(), batch_idx, best_batch_size)\n",
    "\n",
    "        fpr, tpr, threshold = roc_curve(all_label, np.exp(all_pred)[:,1])\n",
    "\n",
    "        fpr = np.interp(base_tpr, tpr, fpr)\n",
    "        threshold = np.interp(base_tpr, tpr, threshold)\n",
    "        fpr[0] = 0.0\n",
    "        fprs.append(fpr)\n",
    "        thresholds.append(threshold)\n",
    "\n",
    "    thresholds = np.array(thresholds)\n",
    "    mean_thresholds = thresholds.mean(axis=0)\n",
    "\n",
    "    fprs = np.array(fprs)\n",
    "    mean_fprs = fprs.mean(axis=0)\n",
    "    std_fprs = fprs.std(axis=0)\n",
    "    fprs_right = np.minimum(mean_fprs + std_fprs, 1)\n",
    "    fprs_left = np.maximum(mean_fprs - std_fprs,0)\n",
    "\n",
    "    mean_area = auc(mean_fprs, base_tpr)\n",
    "\n",
    "    output_dict[high_level_fields[i]] = {\n",
    "        'thresholds' : thresholds.tolist(),\n",
    "        'mean_thresholds': mean_thresholds.tolist(),\n",
    "        'fprs': fprs.tolist(),\n",
    "        'mean_fprs': mean_fprs.tolist(),\n",
    "        'std_fprs': std_fprs.tolist(),\n",
    "        'fprs_right': fprs_right.tolist(),\n",
    "        'fprs_left': fprs_left.tolist(),\n",
    "        'base_tpr': base_tpr.tolist(),\n",
    "        'mean_area': mean_area.tolist(),\n",
    "        'train_losses_arr': train_losses_arr,\n",
    "        'val_losses_arr': val_losses_arr\n",
    "        # 'train_losses_arr': [loss_arr.tolist() for loss_arr in train_losses_arr],\n",
    "        # 'val_losses_arr': [loss_arr.tolist() for loss_arr in val_losses_arr]\n",
    "    }\n",
    "\n",
    "with open('output_dict.json', 'w') as output:\n",
    "    json.dump(output_dict, output)\n",
    "    print(\"Save output file to {}\".format('output_dict.json'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(\"Neural network performance\")\n",
    "# NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "# NNtable.float_format = \".4\"\n",
    "# for TPR_threshold in TPR_thresholds:\n",
    "#     thres_idx = np.argmax(base_tpr>TPR_threshold)\n",
    "#     NNtable.add_row([mean_thresholds[thres_idx], base_tpr[thres_idx], \"{:.4f} +/- {:.4f}\".format(mean_fprs[thres_idx], std_fprs[thres_idx])])\n",
    "# print(NNtable)\n",
    "\n",
    "# plt.figure(figsize=(9,7))\n",
    "# for fold_idx in range(skf.get_n_splits()):\n",
    "#     plt.plot(range(len(train_losses_arr[fold_idx])), train_losses_arr[fold_idx], label=\"Train data losses\")\n",
    "#     plt.plot(range(len(train_losses_arr[fold_idx])), val_losses_arr[fold_idx], label=\"Validation data losses\")\n",
    "# # plt.fill_betweenx(base_tpr, fprs_left, fprs_right, color='grey', alpha=0.4)\n",
    "# plt.legend(loc='best')\n",
    "# plt.xlabel('EPOCH')\n",
    "# plt.ylabel('Data Loss')\n",
    "\n",
    "# run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "# run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "# run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "check_set = {\n",
    "    'puppiMET_eta', 'puppiMET_phi',\n",
    "    'lepton1_eta', 'lepton2_eta', 'eta', # lepton and diphoton eta\n",
    "    'lepton1_phi', 'lepton2_phi', 'phi', # lepton and diphoton phi\n",
    "}\n",
    "other_set = {\n",
    "    'puppiMET_pt': 'puppiMET', 'lepton1_pt': 'lepton1', 'lepton2_pt': 'lepton2', 'pt': 'diphoton'\n",
    "}\n",
    "plt.figure(figsize=(9,7))\n",
    "for key, field_dict in output_dict.items():\n",
    "    if key in check_set:\n",
    "        continue\n",
    "    elif key in other_set:\n",
    "        print_key = other_set[key]\n",
    "    else:\n",
    "        print_key = key\n",
    "    plt.plot(field_dict['mean_fprs'], field_dict['base_tpr'], label=\"AUC (without %s) = %.4f\" % (print_key, field_dict['mean_area']))\n",
    "    plt.fill_betweenx(field_dict['base_tpr'], field_dict['fprs_left'], field_dict['fprs_right'], color='grey', alpha=0.4)\n",
    "# plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Background contamination (log)')\n",
    "plt.ylabel('Signal efficiency (log)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "#plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "#plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "\n",
    "# plt.figure(figsize=(9,7))\n",
    "# plt.hist(np.exp(all_pred)[all_label==0,1], bins=60, label='ttH background',alpha=0.5, density=True)\n",
    "# plt.hist(np.exp(all_pred)[all_label==1,1], bins=60, label='HH signal', alpha=0.5, density=True)\n",
    "# #plt.axvline(thresholds[thres_idx], ls='--',color='tab:gray')\n",
    "# plt.legend(loc='best')\n",
    "# plt.xlabel(\"Threshold\", fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna-hhbbgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
