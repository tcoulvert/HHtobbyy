{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu1.fnal.gov      Sat Aug 17 17:58:19 2024  555.42.02\n",
      "[0] Tesla P100-PCIE-12GB | 42Â°C,   0 % |     2 / 12288 MB |\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "SIGNAL_FILEPATHS = [lpc_fileprefix+\"/Run3_2022preEE_merged/GluGluToHH/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/GluGluToHH/nominal/*\"]\n",
    "SIGNAL_FILEPATHS = [\n",
    "    lpc_fileprefix+\"/Run3_2022preEE_merged/GluGluToHH/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/GluGluToHH/nominal/*\",\n",
    "    lpc_fileprefix+\"/Run3_2022preEE_merged/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [lpc_fileprefix+\"/Run3_2022preEE_merged/ttHToGG/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/ttHToGG/nominal/*\"]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "# OUTPUT_DIRPATH = CURRENT_DIRPATH + \"/model_outputs/v1/extra_vars/\"\n",
    "# CURRENT_TIME = '2024-08-10_13-16-12'\n",
    "# CURRENT_TIME = '2024-08-17_11-45-34'\n",
    "OUTPUT_DIRPATH = CURRENT_DIRPATH + \"/model_outputs/v1/base_vars/\"\n",
    "# CURRENT_TIME = '2024-08-10_10-29-50'\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False\n",
    "PRE_STD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(IN_info, plot_prefix, plot_postfix='', method='arr', labels=None):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(IN_info, plot_prefix, plot_postfix='', method='std', labels=None, yscale='linear', run2=True):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            IN_info['mean_fprs'], IN_info['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.plot(\n",
    "                IN_info['fprs'][fold_idx], IN_info['base_tpr'],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            IN_info['mean_fprs'], IN_info['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            plt.plot(\n",
    "                IN_info[i]['mean_fprs'], IN_info[i]['base_tpr'], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(IN_info, plot_prefix, plot_postfix='', method='arr', labels=None):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.hist(np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['all_labels']) == 0,1\n",
    "        ], bins=60, label='ttH background', histtype='step', alpha=0.5, density=True)\n",
    "        plt.hist(np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['all_labels']) == 1,1\n",
    "        ], bins=60, label='HH signal', histtype='step', alpha=0.5, density=True)\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.hist(np.exp(\n",
    "                    IN_info['all_preds'][fold_idx]\n",
    "                )[\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0,1\n",
    "                ], bins=60, label='ttH background'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                histtype='step', alpha=0.5, density=True, \n",
    "                linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            plt.hist(\n",
    "                np.exp(\n",
    "                    IN_info['all_preds'][fold_idx]\n",
    "                )[\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1,1\n",
    "                ], bins=60, label='HH signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                histtype='step', alpha=0.5, density=True, \n",
    "                linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "        plt.hist(np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ], bins=60, label='ttH background - avg. over folds', histtype='step', alpha=0.8, density=True)\n",
    "        plt.hist(np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ], bins=60, label='HH signal - avg. over folds', histtype='step', alpha=0.8, density=True)\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            plt.hist(\n",
    "                np.exp(\n",
    "                    IN_info[i]['all_preds'][0]\n",
    "                )[\n",
    "                    np.array(IN_info[i]['all_labels'][0]) == 0, 1\n",
    "                ], bins=60, \n",
    "                label='ttH background'+(' - '+labels[i] if labels is not None else ''), \n",
    "                histtype='step', linestyle=linestyles[i], alpha=0.75, density=True\n",
    "            )\n",
    "            plt.hist(\n",
    "                np.exp(\n",
    "                    IN_info[i]['all_preds'][0]\n",
    "                )[\n",
    "                    np.array(IN_info[i]['all_labels'][0]) == 1, 1\n",
    "                ], bins=60, \n",
    "                label='HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                histtype='step', linestyle=linestyles[i], alpha=0.75, density=True\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200, name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200, name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'jet1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'jet2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(40, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'lepton1_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, var_name, index_map, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label == 1\n",
    "    sig_test_mask = label_test == 1\n",
    "    bkg_mask = label == 0\n",
    "    bkg_test_mask = label_test == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "            index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2, index3]\n",
    "            sig_val_np = data[sig_val_mask, index2, index3]\n",
    "            sig_test_np = data_test[sig_test_mask, index2, index3]\n",
    "            bkg_train_np = data[bkg_train_mask, index2, index3]\n",
    "            bkg_val_np = data[bkg_val_mask, index2, index3]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2, index3]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns[var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_val_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_val_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "            index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[sig_mask, index2, index3]\n",
    "            sig_test_np = data_test[sig_test_mask, index2, index3]\n",
    "            bkg_train_np = data[bkg_mask, index2, index3]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2, index3]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns[var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict):\n",
    "    sig_train_mask = (label == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux.loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux.loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, plot_prefix='', plot_postfix=''):\n",
    "    fig, ax = plt.subplots()\n",
    "    if fold_idx is not None:\n",
    "        linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "    else:\n",
    "        linestyles=[\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "    linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "    linestyles = linestyles[:len(hist_list)]\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=0.75\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir = output_dir + \"fold/\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (543639, 4, 6)\n",
      "Data HLF: (543639, 9)\n",
      "Data list test: (545732, 4, 6)\n",
      "Data HLF test: (545732, 9)\n"
     ]
    }
   ],
   "source": [
    "if PRE_STD:\n",
    "    (\n",
    "        data_df, data_test_df, \n",
    "        data_list, data_hlf, label, \n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        high_level_fields, input_hlf_vars, hlf_vars_columns,\n",
    "        data_aux, data_test_aux\n",
    "    ) = process_data(\n",
    "        SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED, return_pre_std=True\n",
    "    )\n",
    "else:\n",
    "    (\n",
    "        data_list, data_hlf, label, \n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        high_level_fields, input_hlf_vars, hlf_vars_columns,\n",
    "        data_aux, data_test_aux\n",
    "    ) = process_data(\n",
    "        SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED\n",
    "    )\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.01, 'gru_layers': 2, 'gru_size': 442, 'dropout_g': 0.01, 'learning_rate': 0.003297552560160522, 'batch_size': 4000, 'L2_reg': 4.783663281646104e-05}\n",
      "Epoch 0/149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha) (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706619781071/work/torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0001 Acc: 84.6909\n",
      "validation Loss: 0.0001 Acc: 85.7534\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0001 Acc: 86.3590\n",
      "validation Loss: 0.0001 Acc: 86.8626\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0001 Acc: 87.3765\n",
      "validation Loss: 0.0001 Acc: 87.0263\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0001 Acc: 87.5984\n",
      "validation Loss: 0.0001 Acc: 88.3360\n",
      "Saving..\n",
      "Epoch 4/149\n",
      "training Loss: 0.0001 Acc: 87.7577\n",
      "validation Loss: 0.0001 Acc: 88.3655\n",
      "Saving..\n",
      "Epoch 5/149\n",
      "training Loss: 0.0001 Acc: 87.9019\n",
      "validation Loss: 0.0001 Acc: 87.4954\n",
      "Epoch 6/149\n",
      "training Loss: 0.0001 Acc: 87.9854\n",
      "validation Loss: 0.0001 Acc: 88.4961\n",
      "Saving..\n",
      "Epoch 7/149\n",
      "training Loss: 0.0001 Acc: 88.0316\n",
      "validation Loss: 0.0001 Acc: 87.3841\n",
      "Epoch 8/149\n",
      "training Loss: 0.0001 Acc: 88.1307\n",
      "validation Loss: 0.0001 Acc: 87.0135\n",
      "Epoch 9/149\n",
      "training Loss: 0.0001 Acc: 88.0723\n",
      "validation Loss: 0.0001 Acc: 88.3949\n",
      "Epoch 10/149\n",
      "training Loss: 0.0001 Acc: 88.1162\n",
      "validation Loss: 0.0001 Acc: 88.4455\n",
      "Epoch 11/149\n",
      "training Loss: 0.0001 Acc: 88.1491\n",
      "validation Loss: 0.0001 Acc: 88.7987\n",
      "Saving..\n",
      "Epoch 12/149\n",
      "training Loss: 0.0001 Acc: 88.1859\n",
      "validation Loss: 0.0001 Acc: 88.6533\n",
      "Epoch 13/149\n",
      "training Loss: 0.0001 Acc: 88.2737\n",
      "validation Loss: 0.0001 Acc: 87.6766\n",
      "Epoch 14/149\n",
      "training Loss: 0.0001 Acc: 88.2712\n",
      "validation Loss: 0.0001 Acc: 88.8906\n",
      "Saving..\n",
      "Epoch 15/149\n",
      "training Loss: 0.0001 Acc: 88.4530\n",
      "validation Loss: 0.0001 Acc: 88.3038\n",
      "Epoch 16/149\n",
      "training Loss: 0.0001 Acc: 88.5333\n",
      "validation Loss: 0.0001 Acc: 88.6119\n",
      "Epoch 17/149\n",
      "training Loss: 0.0001 Acc: 88.5834\n",
      "validation Loss: 0.0001 Acc: 88.3075\n",
      "Epoch 18/149\n",
      "training Loss: 0.0001 Acc: 88.5841\n",
      "validation Loss: 0.0001 Acc: 88.2579\n",
      "Epoch 19/149\n",
      "training Loss: 0.0001 Acc: 88.5303\n",
      "validation Loss: 0.0001 Acc: 88.7094\n",
      "Epoch 20/149\n",
      "training Loss: 0.0001 Acc: 88.5262\n",
      "validation Loss: 0.0001 Acc: 88.0859\n",
      "Epoch 21/149\n",
      "training Loss: 0.0001 Acc: 88.6377\n",
      "validation Loss: 0.0001 Acc: 88.8915\n",
      "Saving..\n",
      "Epoch 22/149\n",
      "training Loss: 0.0001 Acc: 88.6958\n",
      "validation Loss: 0.0001 Acc: 88.4997\n",
      "Epoch 23/149\n",
      "training Loss: 0.0001 Acc: 88.7018\n",
      "validation Loss: 0.0001 Acc: 88.3204\n",
      "Epoch 24/149\n",
      "training Loss: 0.0001 Acc: 88.7198\n",
      "validation Loss: 0.0001 Acc: 88.7021\n",
      "Epoch 25/149\n",
      "training Loss: 0.0001 Acc: 88.7338\n",
      "validation Loss: 0.0001 Acc: 88.8152\n",
      "Epoch 26/149\n",
      "training Loss: 0.0001 Acc: 88.7101\n",
      "validation Loss: 0.0001 Acc: 88.5503\n",
      "Epoch 27/149\n",
      "training Loss: 0.0001 Acc: 88.7140\n",
      "validation Loss: 0.0001 Acc: 88.3498\n",
      "Epoch 28/149\n",
      "training Loss: 0.0001 Acc: 88.7108\n",
      "validation Loss: 0.0001 Acc: 88.9090\n",
      "Saving..\n",
      "Epoch 29/149\n",
      "training Loss: 0.0001 Acc: 88.7342\n",
      "validation Loss: 0.0001 Acc: 88.6414\n",
      "Epoch 30/149\n",
      "training Loss: 0.0001 Acc: 88.7476\n",
      "validation Loss: 0.0001 Acc: 89.1417\n",
      "Saving..\n",
      "Epoch 31/149\n",
      "training Loss: 0.0001 Acc: 88.7860\n",
      "validation Loss: 0.0001 Acc: 88.7674\n",
      "Epoch 32/149\n",
      "training Loss: 0.0001 Acc: 88.8120\n",
      "validation Loss: 0.0001 Acc: 88.6202\n",
      "Epoch 33/149\n",
      "training Loss: 0.0001 Acc: 88.8294\n",
      "validation Loss: 0.0001 Acc: 88.5292\n",
      "Epoch 34/149\n",
      "training Loss: 0.0001 Acc: 88.8115\n",
      "validation Loss: 0.0001 Acc: 88.8750\n",
      "Epoch 35/149\n",
      "training Loss: 0.0001 Acc: 88.8764\n",
      "validation Loss: 0.0001 Acc: 88.6506\n",
      "Epoch 36/149\n",
      "training Loss: 0.0001 Acc: 88.8750\n",
      "validation Loss: 0.0001 Acc: 88.7251\n",
      "Epoch 37/149\n",
      "training Loss: 0.0001 Acc: 88.8681\n",
      "validation Loss: 0.0001 Acc: 88.7646\n",
      "Epoch 38/149\n",
      "training Loss: 0.0001 Acc: 88.8363\n",
      "validation Loss: 0.0001 Acc: 88.7711\n",
      "Epoch 39/149\n",
      "training Loss: 0.0001 Acc: 88.8924\n",
      "validation Loss: 0.0001 Acc: 88.5798\n",
      "Epoch 40/149\n",
      "training Loss: 0.0001 Acc: 88.8559\n",
      "validation Loss: 0.0001 Acc: 88.8603\n",
      "Epoch 41/149\n",
      "training Loss: 0.0001 Acc: 88.8628\n",
      "validation Loss: 0.0001 Acc: 88.6883\n",
      "Epoch 42/149\n",
      "training Loss: 0.0001 Acc: 88.8777\n",
      "validation Loss: 0.0001 Acc: 88.7683\n",
      "Epoch 43/149\n",
      "training Loss: 0.0001 Acc: 88.8786\n",
      "validation Loss: 0.0001 Acc: 88.7950\n",
      "Epoch 44/149\n",
      "training Loss: 0.0001 Acc: 88.8658\n",
      "validation Loss: 0.0001 Acc: 88.7058\n",
      "Epoch 45/149\n",
      "training Loss: 0.0001 Acc: 88.8646\n",
      "validation Loss: 0.0001 Acc: 88.6947\n",
      "Epoch 46/149\n",
      "training Loss: 0.0001 Acc: 88.9069\n",
      "validation Loss: 0.0001 Acc: 88.6818\n",
      "Epoch 47/149\n",
      "training Loss: 0.0001 Acc: 88.8906\n",
      "validation Loss: 0.0001 Acc: 88.7793\n",
      "Epoch 48/149\n",
      "training Loss: 0.0001 Acc: 88.8844\n",
      "validation Loss: 0.0001 Acc: 88.9495\n",
      "Epoch 49/149\n",
      "training Loss: 0.0001 Acc: 88.9288\n",
      "validation Loss: 0.0001 Acc: 88.6745\n",
      "Epoch 50/149\n",
      "training Loss: 0.0001 Acc: 88.9251\n",
      "validation Loss: 0.0001 Acc: 88.6818\n",
      "Epoch 51/149\n",
      "training Loss: 0.0001 Acc: 88.9152\n",
      "validation Loss: 0.0001 Acc: 88.8952\n",
      "Epoch 52/149\n",
      "training Loss: 0.0001 Acc: 88.9104\n",
      "validation Loss: 0.0001 Acc: 88.9421\n",
      "Epoch 53/149\n",
      "training Loss: 0.0001 Acc: 88.9338\n",
      "validation Loss: 0.0001 Acc: 88.6984\n",
      "Epoch 54/149\n",
      "training Loss: 0.0001 Acc: 88.9237\n",
      "validation Loss: 0.0001 Acc: 88.7931\n",
      "Early stopped.\n",
      "Best val acc: 89.141708\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 84.3145\n",
      "validation Loss: 0.0001 Acc: 85.9383\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0001 Acc: 86.4628\n",
      "validation Loss: 0.0001 Acc: 88.0049\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0001 Acc: 87.3905\n",
      "validation Loss: 0.0001 Acc: 87.9755\n",
      "Epoch 3/149\n",
      "training Loss: 0.0001 Acc: 87.6211\n",
      "validation Loss: 0.0001 Acc: 87.6573\n",
      "Epoch 4/149\n",
      "training Loss: 0.0001 Acc: 87.7724\n",
      "validation Loss: 0.0001 Acc: 88.2257\n",
      "Saving..\n",
      "Epoch 5/149\n",
      "training Loss: 0.0001 Acc: 87.9081\n",
      "validation Loss: 0.0001 Acc: 88.3369\n",
      "Saving..\n",
      "Epoch 6/149\n",
      "training Loss: 0.0001 Acc: 88.0065\n",
      "validation Loss: 0.0001 Acc: 88.0987\n",
      "Epoch 7/149\n",
      "training Loss: 0.0001 Acc: 87.9288\n",
      "validation Loss: 0.0001 Acc: 87.4862\n",
      "Epoch 8/149\n",
      "training Loss: 0.0001 Acc: 88.0923\n",
      "validation Loss: 0.0001 Acc: 87.3151\n",
      "Epoch 9/149\n",
      "training Loss: 0.0001 Acc: 88.2728\n",
      "validation Loss: 0.0001 Acc: 88.1640\n",
      "Epoch 10/149\n",
      "training Loss: 0.0001 Acc: 88.3645\n",
      "validation Loss: 0.0001 Acc: 88.1677\n",
      "Epoch 11/149\n",
      "training Loss: 0.0001 Acc: 88.3705\n",
      "validation Loss: 0.0001 Acc: 87.7198\n",
      "Epoch 12/149\n",
      "training Loss: 0.0001 Acc: 88.4057\n",
      "validation Loss: 0.0001 Acc: 88.6423\n",
      "Saving..\n",
      "Epoch 13/149\n",
      "training Loss: 0.0001 Acc: 88.4346\n",
      "validation Loss: 0.0001 Acc: 88.8327\n",
      "Saving..\n",
      "Epoch 14/149\n",
      "training Loss: 0.0001 Acc: 88.4038\n",
      "validation Loss: 0.0001 Acc: 89.0755\n",
      "Saving..\n",
      "Epoch 15/149\n",
      "training Loss: 0.0001 Acc: 88.4767\n",
      "validation Loss: 0.0001 Acc: 88.6405\n",
      "Epoch 16/149\n",
      "training Loss: 0.0001 Acc: 88.4728\n",
      "validation Loss: 0.0001 Acc: 88.6368\n",
      "Epoch 17/149\n",
      "training Loss: 0.0001 Acc: 88.4169\n",
      "validation Loss: 0.0001 Acc: 88.4436\n",
      "Epoch 18/149\n",
      "training Loss: 0.0001 Acc: 88.4712\n",
      "validation Loss: 0.0001 Acc: 88.9072\n",
      "Epoch 19/149\n",
      "training Loss: 0.0001 Acc: 88.4776\n",
      "validation Loss: 0.0001 Acc: 88.3958\n",
      "Epoch 20/149\n",
      "training Loss: 0.0001 Acc: 88.5137\n",
      "validation Loss: 0.0001 Acc: 88.5402\n",
      "Epoch 21/149\n",
      "training Loss: 0.0001 Acc: 88.6409\n",
      "validation Loss: 0.0001 Acc: 88.1788\n",
      "Epoch 22/149\n",
      "training Loss: 0.0001 Acc: 88.6191\n",
      "validation Loss: 0.0001 Acc: 88.8667\n",
      "Epoch 23/149\n",
      "training Loss: 0.0001 Acc: 88.6710\n",
      "validation Loss: 0.0001 Acc: 89.2622\n",
      "Saving..\n",
      "Epoch 24/149\n",
      "training Loss: 0.0001 Acc: 88.6616\n",
      "validation Loss: 0.0001 Acc: 88.4538\n",
      "Epoch 25/149\n",
      "training Loss: 0.0001 Acc: 88.6393\n",
      "validation Loss: 0.0001 Acc: 88.9090\n",
      "Epoch 26/149\n",
      "training Loss: 0.0001 Acc: 88.6758\n",
      "validation Loss: 0.0001 Acc: 88.7784\n",
      "Epoch 27/149\n",
      "training Loss: 0.0001 Acc: 88.6834\n",
      "validation Loss: 0.0001 Acc: 88.5172\n",
      "Epoch 28/149\n",
      "training Loss: 0.0001 Acc: 88.7365\n",
      "validation Loss: 0.0001 Acc: 88.5871\n",
      "Epoch 29/149\n",
      "training Loss: 0.0001 Acc: 88.7002\n",
      "validation Loss: 0.0001 Acc: 88.8042\n",
      "Epoch 30/149\n",
      "training Loss: 0.0001 Acc: 88.7177\n",
      "validation Loss: 0.0001 Acc: 88.7094\n",
      "Epoch 31/149\n",
      "training Loss: 0.0001 Acc: 88.6873\n",
      "validation Loss: 0.0001 Acc: 88.4519\n",
      "Epoch 32/149\n",
      "training Loss: 0.0001 Acc: 88.7432\n",
      "validation Loss: 0.0001 Acc: 88.6828\n",
      "Epoch 33/149\n",
      "training Loss: 0.0001 Acc: 88.7972\n",
      "validation Loss: 0.0001 Acc: 88.7407\n",
      "Epoch 34/149\n",
      "training Loss: 0.0001 Acc: 88.7924\n",
      "validation Loss: 0.0001 Acc: 88.4565\n",
      "Epoch 35/149\n",
      "training Loss: 0.0001 Acc: 88.7894\n",
      "validation Loss: 0.0001 Acc: 88.8364\n",
      "Epoch 36/149\n",
      "training Loss: 0.0001 Acc: 88.8048\n",
      "validation Loss: 0.0001 Acc: 88.8768\n",
      "Epoch 37/149\n",
      "training Loss: 0.0001 Acc: 88.8764\n",
      "validation Loss: 0.0001 Acc: 88.5237\n",
      "Epoch 38/149\n",
      "training Loss: 0.0001 Acc: 88.8674\n",
      "validation Loss: 0.0001 Acc: 88.7499\n",
      "Epoch 39/149\n",
      "training Loss: 0.0001 Acc: 88.8752\n",
      "validation Loss: 0.0001 Acc: 88.6340\n",
      "Epoch 40/149\n",
      "training Loss: 0.0001 Acc: 88.8839\n",
      "validation Loss: 0.0001 Acc: 88.8649\n",
      "Epoch 41/149\n",
      "training Loss: 0.0001 Acc: 88.8423\n",
      "validation Loss: 0.0001 Acc: 88.9578\n",
      "Epoch 42/149\n",
      "training Loss: 0.0001 Acc: 88.8683\n",
      "validation Loss: 0.0001 Acc: 88.8520\n",
      "Epoch 43/149\n",
      "training Loss: 0.0001 Acc: 88.9005\n",
      "validation Loss: 0.0001 Acc: 88.8594\n",
      "Epoch 44/149\n",
      "training Loss: 0.0001 Acc: 88.8959\n",
      "validation Loss: 0.0001 Acc: 88.9228\n",
      "Epoch 45/149\n",
      "training Loss: 0.0001 Acc: 88.8945\n",
      "validation Loss: 0.0001 Acc: 88.8235\n",
      "Epoch 46/149\n",
      "training Loss: 0.0001 Acc: 88.8897\n",
      "validation Loss: 0.0001 Acc: 88.8281\n",
      "Epoch 47/149\n",
      "training Loss: 0.0001 Acc: 88.8924\n",
      "validation Loss: 0.0001 Acc: 88.8419\n",
      "Epoch 48/149\n",
      "training Loss: 0.0001 Acc: 88.8694\n",
      "validation Loss: 0.0001 Acc: 88.7674\n",
      "Epoch 49/149\n",
      "training Loss: 0.0001 Acc: 88.9228\n",
      "validation Loss: 0.0001 Acc: 88.9762\n",
      "Epoch 50/149\n",
      "training Loss: 0.0001 Acc: 88.9012\n",
      "validation Loss: 0.0001 Acc: 88.7904\n",
      "Epoch 51/149\n",
      "training Loss: 0.0001 Acc: 88.9320\n",
      "validation Loss: 0.0001 Acc: 88.7260\n",
      "Epoch 52/149\n",
      "training Loss: 0.0001 Acc: 88.9193\n",
      "validation Loss: 0.0001 Acc: 88.8382\n",
      "Epoch 53/149\n",
      "training Loss: 0.0001 Acc: 88.9065\n",
      "validation Loss: 0.0001 Acc: 88.9578\n",
      "Epoch 54/149\n",
      "training Loss: 0.0001 Acc: 88.9573\n",
      "validation Loss: 0.0001 Acc: 88.9201\n",
      "Epoch 55/149\n",
      "training Loss: 0.0001 Acc: 88.9088\n",
      "validation Loss: 0.0001 Acc: 89.0083\n",
      "Epoch 56/149\n",
      "training Loss: 0.0001 Acc: 88.9233\n",
      "validation Loss: 0.0001 Acc: 88.8511\n",
      "Epoch 57/149\n",
      "training Loss: 0.0001 Acc: 88.9566\n",
      "validation Loss: 0.0001 Acc: 88.8474\n",
      "Epoch 58/149\n",
      "training Loss: 0.0001 Acc: 88.9393\n",
      "validation Loss: 0.0001 Acc: 88.8281\n",
      "Epoch 59/149\n",
      "training Loss: 0.0001 Acc: 88.9302\n",
      "validation Loss: 0.0001 Acc: 88.9099\n",
      "Epoch 60/149\n",
      "training Loss: 0.0001 Acc: 88.9370\n",
      "validation Loss: 0.0001 Acc: 88.8658\n",
      "Epoch 61/149\n",
      "training Loss: 0.0001 Acc: 88.9476\n",
      "validation Loss: 0.0001 Acc: 88.7950\n",
      "Epoch 62/149\n",
      "training Loss: 0.0001 Acc: 88.9221\n",
      "validation Loss: 0.0001 Acc: 88.8943\n",
      "Epoch 63/149\n",
      "training Loss: 0.0001 Acc: 88.9702\n",
      "validation Loss: 0.0001 Acc: 88.8069\n",
      "Epoch 64/149\n",
      "training Loss: 0.0001 Acc: 88.9437\n",
      "validation Loss: 0.0001 Acc: 88.8097\n",
      "Epoch 65/149\n",
      "training Loss: 0.0001 Acc: 88.9313\n",
      "validation Loss: 0.0001 Acc: 88.8925\n",
      "Epoch 66/149\n",
      "training Loss: 0.0001 Acc: 88.9497\n",
      "validation Loss: 0.0001 Acc: 88.8685\n",
      "Epoch 67/149\n",
      "training Loss: 0.0001 Acc: 88.9421\n",
      "validation Loss: 0.0001 Acc: 88.9017\n",
      "Epoch 68/149\n",
      "training Loss: 0.0001 Acc: 88.9472\n",
      "validation Loss: 0.0001 Acc: 88.8777\n",
      "Epoch 69/149\n",
      "training Loss: 0.0001 Acc: 88.9635\n",
      "validation Loss: 0.0001 Acc: 88.8226\n",
      "Epoch 70/149\n",
      "training Loss: 0.0001 Acc: 88.9338\n",
      "validation Loss: 0.0001 Acc: 88.8308\n",
      "Epoch 71/149\n",
      "training Loss: 0.0001 Acc: 88.9582\n",
      "validation Loss: 0.0001 Acc: 88.8032\n",
      "Early stopped.\n",
      "Best val acc: 89.262192\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 84.4230\n",
      "validation Loss: 0.0001 Acc: 86.2731\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0001 Acc: 86.0040\n",
      "validation Loss: 0.0001 Acc: 86.2326\n",
      "Epoch 2/149\n",
      "training Loss: 0.0001 Acc: 87.1362\n",
      "validation Loss: 0.0001 Acc: 87.6398\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0001 Acc: 87.5312\n",
      "validation Loss: 0.0001 Acc: 87.9111\n",
      "Saving..\n",
      "Epoch 4/149\n",
      "training Loss: 0.0001 Acc: 87.6664\n",
      "validation Loss: 0.0001 Acc: 87.6582\n",
      "Epoch 5/149\n",
      "training Loss: 0.0001 Acc: 87.7375\n",
      "validation Loss: 0.0001 Acc: 88.0601\n",
      "Saving..\n",
      "Epoch 6/149\n",
      "training Loss: 0.0001 Acc: 87.8991\n",
      "validation Loss: 0.0001 Acc: 88.0868\n",
      "Saving..\n",
      "Epoch 7/149\n",
      "training Loss: 0.0001 Acc: 87.8679\n",
      "validation Loss: 0.0001 Acc: 88.1401\n",
      "Saving..\n",
      "Epoch 8/149\n",
      "training Loss: 0.0001 Acc: 87.9640\n",
      "validation Loss: 0.0001 Acc: 88.4795\n",
      "Saving..\n",
      "Epoch 9/149\n",
      "training Loss: 0.0001 Acc: 88.0247\n",
      "validation Loss: 0.0001 Acc: 88.3259\n",
      "Epoch 10/149\n",
      "training Loss: 0.0001 Acc: 88.1125\n",
      "validation Loss: 0.0001 Acc: 88.7950\n",
      "Saving..\n",
      "Epoch 11/149\n",
      "training Loss: 0.0001 Acc: 88.0796\n",
      "validation Loss: 0.0001 Acc: 88.7232\n",
      "Epoch 12/149\n",
      "training Loss: 0.0001 Acc: 88.1698\n",
      "validation Loss: 0.0001 Acc: 88.2321\n",
      "Epoch 13/149\n",
      "training Loss: 0.0001 Acc: 88.2574\n",
      "validation Loss: 0.0001 Acc: 87.6085\n",
      "Epoch 14/149\n",
      "training Loss: 0.0001 Acc: 88.2084\n",
      "validation Loss: 0.0001 Acc: 88.4032\n",
      "Epoch 15/149\n",
      "training Loss: 0.0001 Acc: 88.2622\n",
      "validation Loss: 0.0001 Acc: 87.8431\n",
      "Epoch 16/149\n",
      "training Loss: 0.0001 Acc: 88.3153\n",
      "validation Loss: 0.0001 Acc: 88.7030\n",
      "Epoch 17/149\n",
      "training Loss: 0.0001 Acc: 88.4583\n",
      "validation Loss: 0.0001 Acc: 88.4160\n",
      "Epoch 18/149\n",
      "training Loss: 0.0001 Acc: 88.4949\n",
      "validation Loss: 0.0001 Acc: 88.4620\n",
      "Epoch 19/149\n",
      "training Loss: 0.0001 Acc: 88.5016\n",
      "validation Loss: 0.0001 Acc: 88.6469\n",
      "Epoch 20/149\n",
      "training Loss: 0.0001 Acc: 88.5211\n",
      "validation Loss: 0.0001 Acc: 87.6251\n",
      "Epoch 21/149\n",
      "training Loss: 0.0001 Acc: 88.5609\n",
      "validation Loss: 0.0001 Acc: 88.8143\n",
      "Saving..\n",
      "Epoch 22/149\n",
      "training Loss: 0.0001 Acc: 88.5609\n",
      "validation Loss: 0.0001 Acc: 88.8879\n",
      "Saving..\n",
      "Epoch 23/149\n",
      "training Loss: 0.0001 Acc: 88.5715\n",
      "validation Loss: 0.0001 Acc: 88.7453\n",
      "Epoch 24/149\n",
      "training Loss: 0.0001 Acc: 88.5239\n",
      "validation Loss: 0.0001 Acc: 88.2652\n",
      "Epoch 25/149\n",
      "training Loss: 0.0001 Acc: 88.5876\n",
      "validation Loss: 0.0001 Acc: 88.9596\n",
      "Saving..\n",
      "Epoch 26/149\n",
      "training Loss: 0.0001 Acc: 88.5742\n",
      "validation Loss: 0.0001 Acc: 89.1160\n",
      "Saving..\n",
      "Epoch 27/149\n",
      "training Loss: 0.0001 Acc: 88.5802\n",
      "validation Loss: 0.0001 Acc: 88.3075\n",
      "Epoch 28/149\n",
      "training Loss: 0.0001 Acc: 88.6784\n",
      "validation Loss: 0.0001 Acc: 88.6276\n",
      "Epoch 29/149\n",
      "training Loss: 0.0001 Acc: 88.7172\n",
      "validation Loss: 0.0001 Acc: 88.4924\n",
      "Epoch 30/149\n",
      "training Loss: 0.0001 Acc: 88.7529\n",
      "validation Loss: 0.0001 Acc: 88.4832\n",
      "Epoch 31/149\n",
      "training Loss: 0.0001 Acc: 88.7032\n",
      "validation Loss: 0.0001 Acc: 89.0047\n",
      "Epoch 32/149\n",
      "training Loss: 0.0001 Acc: 88.6883\n",
      "validation Loss: 0.0001 Acc: 88.3406\n",
      "Epoch 33/149\n",
      "training Loss: 0.0001 Acc: 88.7476\n",
      "validation Loss: 0.0001 Acc: 88.5936\n",
      "Epoch 34/149\n",
      "training Loss: 0.0001 Acc: 88.7248\n",
      "validation Loss: 0.0001 Acc: 88.7094\n",
      "Epoch 35/149\n",
      "training Loss: 0.0001 Acc: 88.6956\n",
      "validation Loss: 0.0001 Acc: 88.9909\n",
      "Epoch 36/149\n",
      "training Loss: 0.0001 Acc: 88.8265\n",
      "validation Loss: 0.0001 Acc: 89.0129\n",
      "Epoch 37/149\n",
      "training Loss: 0.0001 Acc: 88.8115\n",
      "validation Loss: 0.0001 Acc: 88.7058\n",
      "Epoch 38/149\n",
      "training Loss: 0.0001 Acc: 88.8577\n",
      "validation Loss: 0.0001 Acc: 88.9090\n",
      "Epoch 39/149\n",
      "training Loss: 0.0001 Acc: 88.8524\n",
      "validation Loss: 0.0001 Acc: 88.7692\n",
      "Epoch 40/149\n",
      "training Loss: 0.0001 Acc: 88.8278\n",
      "validation Loss: 0.0001 Acc: 88.9504\n",
      "Epoch 41/149\n",
      "training Loss: 0.0001 Acc: 88.8361\n",
      "validation Loss: 0.0001 Acc: 88.7508\n",
      "Epoch 42/149\n",
      "training Loss: 0.0001 Acc: 88.8356\n",
      "validation Loss: 0.0001 Acc: 88.6257\n",
      "Epoch 43/149\n",
      "training Loss: 0.0001 Acc: 88.8485\n",
      "validation Loss: 0.0001 Acc: 89.0663\n",
      "Epoch 44/149\n",
      "training Loss: 0.0001 Acc: 88.8582\n",
      "validation Loss: 0.0001 Acc: 88.6680\n",
      "Epoch 45/149\n",
      "training Loss: 0.0001 Acc: 88.8876\n",
      "validation Loss: 0.0001 Acc: 88.6956\n",
      "Epoch 46/149\n",
      "training Loss: 0.0001 Acc: 88.8665\n",
      "validation Loss: 0.0001 Acc: 88.6892\n",
      "Epoch 47/149\n",
      "training Loss: 0.0001 Acc: 88.9147\n",
      "validation Loss: 0.0001 Acc: 88.8060\n",
      "Epoch 48/149\n",
      "training Loss: 0.0001 Acc: 88.8897\n",
      "validation Loss: 0.0001 Acc: 88.5135\n",
      "Epoch 49/149\n",
      "training Loss: 0.0001 Acc: 88.9203\n",
      "validation Loss: 0.0001 Acc: 88.8842\n",
      "Epoch 50/149\n",
      "training Loss: 0.0001 Acc: 88.9177\n",
      "validation Loss: 0.0001 Acc: 88.9237\n",
      "Epoch 51/149\n",
      "training Loss: 0.0001 Acc: 88.9175\n",
      "validation Loss: 0.0001 Acc: 88.7839\n",
      "Epoch 52/149\n",
      "training Loss: 0.0001 Acc: 88.9085\n",
      "validation Loss: 0.0001 Acc: 88.8722\n",
      "Epoch 53/149\n",
      "training Loss: 0.0001 Acc: 88.9058\n",
      "validation Loss: 0.0001 Acc: 88.8382\n",
      "Epoch 54/149\n",
      "training Loss: 0.0001 Acc: 88.9380\n",
      "validation Loss: 0.0001 Acc: 88.8124\n",
      "Epoch 55/149\n",
      "training Loss: 0.0001 Acc: 88.9131\n",
      "validation Loss: 0.0001 Acc: 88.8869\n",
      "Epoch 56/149\n",
      "training Loss: 0.0001 Acc: 88.9515\n",
      "validation Loss: 0.0001 Acc: 88.8051\n",
      "Epoch 57/149\n",
      "training Loss: 0.0001 Acc: 88.9278\n",
      "validation Loss: 0.0001 Acc: 88.7444\n",
      "Epoch 58/149\n",
      "training Loss: 0.0001 Acc: 88.9299\n",
      "validation Loss: 0.0001 Acc: 88.9394\n",
      "Epoch 59/149\n",
      "training Loss: 0.0001 Acc: 88.9518\n",
      "validation Loss: 0.0001 Acc: 88.7407\n",
      "Epoch 60/149\n",
      "training Loss: 0.0001 Acc: 88.9421\n",
      "validation Loss: 0.0001 Acc: 88.7895\n",
      "Epoch 61/149\n",
      "training Loss: 0.0001 Acc: 88.9053\n",
      "validation Loss: 0.0001 Acc: 88.8143\n",
      "Epoch 62/149\n",
      "training Loss: 0.0001 Acc: 88.9308\n",
      "validation Loss: 0.0001 Acc: 88.8216\n",
      "Epoch 63/149\n",
      "training Loss: 0.0001 Acc: 88.9453\n",
      "validation Loss: 0.0001 Acc: 88.7628\n",
      "Epoch 64/149\n",
      "training Loss: 0.0001 Acc: 88.9262\n",
      "validation Loss: 0.0001 Acc: 88.8777\n",
      "Epoch 65/149\n",
      "training Loss: 0.0001 Acc: 88.9412\n",
      "validation Loss: 0.0001 Acc: 89.0102\n",
      "Epoch 66/149\n",
      "training Loss: 0.0001 Acc: 88.9667\n",
      "validation Loss: 0.0001 Acc: 88.8980\n",
      "Epoch 67/149\n",
      "training Loss: 0.0001 Acc: 88.9925\n",
      "validation Loss: 0.0001 Acc: 88.7793\n",
      "Epoch 68/149\n",
      "training Loss: 0.0001 Acc: 88.9658\n",
      "validation Loss: 0.0001 Acc: 88.9099\n",
      "Epoch 69/149\n",
      "training Loss: 0.0001 Acc: 88.9669\n",
      "validation Loss: 0.0001 Acc: 88.7720\n",
      "Epoch 70/149\n",
      "training Loss: 0.0001 Acc: 88.9426\n",
      "validation Loss: 0.0001 Acc: 88.8538\n",
      "Epoch 71/149\n",
      "training Loss: 0.0001 Acc: 88.9426\n",
      "validation Loss: 0.0001 Acc: 88.8308\n",
      "Early stopped.\n",
      "Best val acc: 89.115952\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 84.0924\n",
      "validation Loss: 0.0001 Acc: 86.7863\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0001 Acc: 86.5016\n",
      "validation Loss: 0.0001 Acc: 87.3115\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0001 Acc: 87.4418\n",
      "validation Loss: 0.0001 Acc: 87.9847\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0001 Acc: 87.5388\n",
      "validation Loss: 0.0001 Acc: 87.9617\n",
      "Epoch 4/149\n",
      "training Loss: 0.0001 Acc: 87.8389\n",
      "validation Loss: 0.0001 Acc: 88.4648\n",
      "Saving..\n",
      "Epoch 5/149\n",
      "training Loss: 0.0001 Acc: 87.9318\n",
      "validation Loss: 0.0001 Acc: 87.5055\n",
      "Epoch 6/149\n",
      "training Loss: 0.0001 Acc: 87.9431\n",
      "validation Loss: 0.0001 Acc: 88.1659\n",
      "Epoch 7/149\n",
      "training Loss: 0.0001 Acc: 88.0477\n",
      "validation Loss: 0.0001 Acc: 87.6168\n",
      "Epoch 8/149\n",
      "training Loss: 0.0001 Acc: 88.1040\n",
      "validation Loss: 0.0001 Acc: 88.3388\n",
      "Epoch 9/149\n",
      "training Loss: 0.0001 Acc: 88.1594\n",
      "validation Loss: 0.0001 Acc: 86.8424\n",
      "Epoch 10/149\n",
      "training Loss: 0.0001 Acc: 88.1491\n",
      "validation Loss: 0.0001 Acc: 87.1201\n",
      "Epoch 11/149\n",
      "training Loss: 0.0001 Acc: 88.1459\n",
      "validation Loss: 0.0001 Acc: 88.5779\n",
      "Saving..\n",
      "Epoch 12/149\n",
      "training Loss: 0.0001 Acc: 88.3018\n",
      "validation Loss: 0.0001 Acc: 87.4246\n",
      "Epoch 13/149\n",
      "training Loss: 0.0001 Acc: 88.2528\n",
      "validation Loss: 0.0001 Acc: 88.4703\n",
      "Epoch 14/149\n",
      "training Loss: 0.0001 Acc: 88.3259\n",
      "validation Loss: 0.0001 Acc: 87.3528\n",
      "Epoch 15/149\n",
      "training Loss: 0.0001 Acc: 88.3850\n",
      "validation Loss: 0.0001 Acc: 88.2275\n",
      "Epoch 16/149\n",
      "training Loss: 0.0001 Acc: 88.3689\n",
      "validation Loss: 0.0001 Acc: 88.2257\n",
      "Epoch 17/149\n",
      "training Loss: 0.0001 Acc: 88.4273\n",
      "validation Loss: 0.0001 Acc: 88.1319\n",
      "Epoch 18/149\n",
      "training Loss: 0.0001 Acc: 88.4641\n",
      "validation Loss: 0.0001 Acc: 88.1300\n",
      "Epoch 19/149\n",
      "training Loss: 0.0001 Acc: 88.3960\n",
      "validation Loss: 0.0001 Acc: 88.2542\n",
      "Epoch 20/149\n",
      "training Loss: 0.0001 Acc: 88.4202\n",
      "validation Loss: 0.0001 Acc: 87.6462\n",
      "Epoch 21/149\n",
      "training Loss: 0.0001 Acc: 88.5984\n",
      "validation Loss: 0.0001 Acc: 88.7021\n",
      "Saving..\n",
      "Epoch 22/149\n",
      "training Loss: 0.0001 Acc: 88.6145\n",
      "validation Loss: 0.0001 Acc: 88.1484\n",
      "Epoch 23/149\n",
      "training Loss: 0.0001 Acc: 88.6388\n",
      "validation Loss: 0.0001 Acc: 88.5779\n",
      "Epoch 24/149\n",
      "training Loss: 0.0001 Acc: 88.6411\n",
      "validation Loss: 0.0001 Acc: 88.2523\n",
      "Epoch 25/149\n",
      "training Loss: 0.0001 Acc: 88.6607\n",
      "validation Loss: 0.0001 Acc: 88.3369\n",
      "Epoch 26/149\n",
      "training Loss: 0.0001 Acc: 88.6660\n",
      "validation Loss: 0.0001 Acc: 88.3029\n",
      "Epoch 27/149\n",
      "training Loss: 0.0001 Acc: 88.7209\n",
      "validation Loss: 0.0001 Acc: 88.2155\n",
      "Epoch 28/149\n",
      "training Loss: 0.0001 Acc: 88.6683\n",
      "validation Loss: 0.0001 Acc: 87.7520\n",
      "Epoch 29/149\n",
      "training Loss: 0.0001 Acc: 88.6752\n",
      "validation Loss: 0.0001 Acc: 88.5945\n",
      "Epoch 30/149\n",
      "training Loss: 0.0001 Acc: 88.6609\n",
      "validation Loss: 0.0001 Acc: 88.5347\n",
      "Epoch 31/149\n",
      "training Loss: 0.0001 Acc: 88.7016\n",
      "validation Loss: 0.0001 Acc: 88.2937\n",
      "Epoch 32/149\n",
      "training Loss: 0.0001 Acc: 88.7046\n",
      "validation Loss: 0.0001 Acc: 88.0454\n",
      "Epoch 33/149\n",
      "training Loss: 0.0001 Acc: 88.7368\n",
      "validation Loss: 0.0001 Acc: 88.3461\n",
      "Epoch 34/149\n",
      "training Loss: 0.0001 Acc: 88.7499\n",
      "validation Loss: 0.0001 Acc: 88.2652\n",
      "Epoch 35/149\n",
      "training Loss: 0.0001 Acc: 88.8154\n",
      "validation Loss: 0.0001 Acc: 88.8207\n",
      "Saving..\n",
      "Epoch 36/149\n",
      "training Loss: 0.0001 Acc: 88.8717\n",
      "validation Loss: 0.0001 Acc: 88.4234\n",
      "Epoch 37/149\n",
      "training Loss: 0.0001 Acc: 88.8734\n",
      "validation Loss: 0.0001 Acc: 88.6009\n",
      "Epoch 38/149\n",
      "training Loss: 0.0001 Acc: 88.8580\n",
      "validation Loss: 0.0001 Acc: 88.6248\n",
      "Epoch 39/149\n",
      "training Loss: 0.0001 Acc: 88.8393\n",
      "validation Loss: 0.0001 Acc: 88.6772\n",
      "Epoch 40/149\n",
      "training Loss: 0.0001 Acc: 88.8425\n",
      "validation Loss: 0.0001 Acc: 88.2910\n",
      "Epoch 41/149\n",
      "training Loss: 0.0001 Acc: 88.8221\n",
      "validation Loss: 0.0001 Acc: 88.6791\n",
      "Epoch 42/149\n",
      "training Loss: 0.0001 Acc: 88.9173\n",
      "validation Loss: 0.0001 Acc: 89.0773\n",
      "Saving..\n",
      "Epoch 43/149\n",
      "training Loss: 0.0001 Acc: 88.9274\n",
      "validation Loss: 0.0001 Acc: 88.8851\n",
      "Epoch 44/149\n",
      "training Loss: 0.0001 Acc: 88.9393\n",
      "validation Loss: 0.0001 Acc: 88.5512\n",
      "Epoch 45/149\n",
      "training Loss: 0.0001 Acc: 88.9359\n",
      "validation Loss: 0.0001 Acc: 88.3811\n",
      "Epoch 46/149\n",
      "training Loss: 0.0001 Acc: 88.9097\n",
      "validation Loss: 0.0001 Acc: 88.6690\n",
      "Epoch 47/149\n",
      "training Loss: 0.0001 Acc: 88.9453\n",
      "validation Loss: 0.0001 Acc: 88.7113\n",
      "Epoch 48/149\n",
      "training Loss: 0.0001 Acc: 88.9725\n",
      "validation Loss: 0.0001 Acc: 88.6828\n",
      "Epoch 49/149\n",
      "training Loss: 0.0001 Acc: 88.9708\n",
      "validation Loss: 0.0001 Acc: 88.6984\n",
      "Epoch 50/149\n",
      "training Loss: 0.0001 Acc: 88.9285\n",
      "validation Loss: 0.0001 Acc: 88.6441\n",
      "Epoch 51/149\n",
      "training Loss: 0.0001 Acc: 88.9467\n",
      "validation Loss: 0.0001 Acc: 88.6285\n",
      "Epoch 52/149\n",
      "training Loss: 0.0001 Acc: 88.9446\n",
      "validation Loss: 0.0001 Acc: 88.6506\n",
      "Epoch 53/149\n",
      "training Loss: 0.0001 Acc: 88.9948\n",
      "validation Loss: 0.0001 Acc: 88.6073\n",
      "Epoch 54/149\n",
      "training Loss: 0.0001 Acc: 88.9708\n",
      "validation Loss: 0.0001 Acc: 88.7996\n",
      "Epoch 55/149\n",
      "training Loss: 0.0001 Acc: 88.9918\n",
      "validation Loss: 0.0001 Acc: 88.7591\n",
      "Epoch 56/149\n",
      "training Loss: 0.0001 Acc: 89.0017\n",
      "validation Loss: 0.0001 Acc: 88.6202\n",
      "Epoch 57/149\n",
      "training Loss: 0.0001 Acc: 88.9984\n",
      "validation Loss: 0.0001 Acc: 88.7775\n",
      "Epoch 58/149\n",
      "training Loss: 0.0001 Acc: 89.0014\n",
      "validation Loss: 0.0001 Acc: 88.4087\n",
      "Epoch 59/149\n",
      "training Loss: 0.0001 Acc: 89.0155\n",
      "validation Loss: 0.0001 Acc: 88.5954\n",
      "Epoch 60/149\n",
      "training Loss: 0.0001 Acc: 89.0136\n",
      "validation Loss: 0.0001 Acc: 88.7048\n",
      "Epoch 61/149\n",
      "training Loss: 0.0001 Acc: 89.0304\n",
      "validation Loss: 0.0001 Acc: 88.7711\n",
      "Epoch 62/149\n",
      "training Loss: 0.0001 Acc: 89.0005\n",
      "validation Loss: 0.0001 Acc: 88.8538\n",
      "Epoch 63/149\n",
      "training Loss: 0.0001 Acc: 89.0373\n",
      "validation Loss: 0.0001 Acc: 88.8014\n",
      "Epoch 64/149\n",
      "training Loss: 0.0001 Acc: 89.0647\n",
      "validation Loss: 0.0001 Acc: 88.7021\n",
      "Epoch 65/149\n",
      "training Loss: 0.0001 Acc: 89.0424\n",
      "validation Loss: 0.0001 Acc: 88.7251\n",
      "Epoch 66/149\n",
      "training Loss: 0.0001 Acc: 89.0302\n",
      "validation Loss: 0.0001 Acc: 88.7094\n",
      "Epoch 67/149\n",
      "training Loss: 0.0001 Acc: 89.0175\n",
      "validation Loss: 0.0001 Acc: 88.6883\n",
      "Epoch 68/149\n",
      "training Loss: 0.0001 Acc: 89.0486\n",
      "validation Loss: 0.0001 Acc: 88.7490\n",
      "Epoch 69/149\n",
      "training Loss: 0.0001 Acc: 89.0387\n",
      "validation Loss: 0.0001 Acc: 88.8042\n",
      "Early stopped.\n",
      "Best val acc: 89.077324\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 84.3223\n",
      "validation Loss: 0.0001 Acc: 86.8285\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0001 Acc: 86.3448\n",
      "validation Loss: 0.0001 Acc: 85.4158\n",
      "Epoch 2/149\n",
      "training Loss: 0.0001 Acc: 87.3126\n",
      "validation Loss: 0.0001 Acc: 87.5431\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0001 Acc: 87.6311\n",
      "validation Loss: 0.0001 Acc: 86.8772\n",
      "Epoch 4/149\n",
      "training Loss: 0.0001 Acc: 87.7989\n",
      "validation Loss: 0.0001 Acc: 85.8931\n",
      "Epoch 5/149\n",
      "training Loss: 0.0001 Acc: 87.8157\n",
      "validation Loss: 0.0001 Acc: 88.7112\n",
      "Saving..\n",
      "Epoch 6/149\n",
      "training Loss: 0.0001 Acc: 88.0114\n",
      "validation Loss: 0.0001 Acc: 88.4233\n",
      "Epoch 7/149\n",
      "training Loss: 0.0001 Acc: 88.0642\n",
      "validation Loss: 0.0001 Acc: 88.2789\n",
      "Epoch 8/149\n",
      "training Loss: 0.0001 Acc: 88.0948\n",
      "validation Loss: 0.0001 Acc: 88.1805\n",
      "Epoch 9/149\n",
      "training Loss: 0.0001 Acc: 88.1473\n",
      "validation Loss: 0.0001 Acc: 88.6808\n",
      "Epoch 10/149\n",
      "training Loss: 0.0001 Acc: 88.1820\n",
      "validation Loss: 0.0001 Acc: 88.6247\n",
      "Epoch 11/149\n",
      "training Loss: 0.0001 Acc: 88.2857\n",
      "validation Loss: 0.0001 Acc: 88.0720\n",
      "Epoch 12/149\n",
      "training Loss: 0.0001 Acc: 88.2436\n",
      "validation Loss: 0.0001 Acc: 88.1290\n",
      "Epoch 13/149\n",
      "training Loss: 0.0001 Acc: 88.3128\n",
      "validation Loss: 0.0001 Acc: 87.6047\n",
      "Epoch 14/149\n",
      "training Loss: 0.0001 Acc: 88.3457\n",
      "validation Loss: 0.0001 Acc: 88.7590\n",
      "Saving..\n",
      "Epoch 15/149\n",
      "training Loss: 0.0001 Acc: 88.3896\n",
      "validation Loss: 0.0001 Acc: 88.9098\n",
      "Saving..\n",
      "Epoch 16/149\n",
      "training Loss: 0.0001 Acc: 88.4218\n",
      "validation Loss: 0.0001 Acc: 88.6735\n",
      "Epoch 17/149\n",
      "training Loss: 0.0001 Acc: 88.4234\n",
      "validation Loss: 0.0001 Acc: 88.0793\n",
      "Epoch 18/149\n",
      "training Loss: 0.0001 Acc: 88.4025\n",
      "validation Loss: 0.0001 Acc: 88.2909\n",
      "Epoch 19/149\n",
      "training Loss: 0.0001 Acc: 88.4528\n",
      "validation Loss: 0.0001 Acc: 88.3148\n",
      "Epoch 20/149\n",
      "training Loss: 0.0001 Acc: 88.4519\n",
      "validation Loss: 0.0001 Acc: 88.8813\n",
      "Epoch 21/149\n",
      "training Loss: 0.0001 Acc: 88.4354\n",
      "validation Loss: 0.0001 Acc: 88.8206\n",
      "Epoch 22/149\n",
      "training Loss: 0.0001 Acc: 88.4839\n",
      "validation Loss: 0.0001 Acc: 88.5769\n",
      "Epoch 23/149\n",
      "training Loss: 0.0001 Acc: 88.4574\n",
      "validation Loss: 0.0001 Acc: 88.6247\n",
      "Epoch 24/149\n",
      "training Loss: 0.0001 Acc: 88.5101\n",
      "validation Loss: 0.0001 Acc: 89.0368\n",
      "Saving..\n",
      "Epoch 25/149\n",
      "training Loss: 0.0001 Acc: 88.5285\n",
      "validation Loss: 0.0001 Acc: 88.3405\n",
      "Epoch 26/149\n",
      "training Loss: 0.0001 Acc: 88.5089\n",
      "validation Loss: 0.0001 Acc: 88.8822\n",
      "Epoch 27/149\n",
      "training Loss: 0.0001 Acc: 88.6641\n",
      "validation Loss: 0.0001 Acc: 88.7149\n",
      "Epoch 28/149\n",
      "training Loss: 0.0001 Acc: 88.6724\n",
      "validation Loss: 0.0001 Acc: 88.9384\n",
      "Epoch 29/149\n",
      "training Loss: 0.0001 Acc: 88.6747\n",
      "validation Loss: 0.0001 Acc: 88.7001\n",
      "Epoch 30/149\n",
      "training Loss: 0.0001 Acc: 88.6904\n",
      "validation Loss: 0.0001 Acc: 88.7838\n",
      "Epoch 31/149\n",
      "training Loss: 0.0001 Acc: 88.6931\n",
      "validation Loss: 0.0001 Acc: 88.2614\n",
      "Epoch 32/149\n",
      "training Loss: 0.0001 Acc: 88.8311\n",
      "validation Loss: 0.0001 Acc: 88.7792\n",
      "Epoch 33/149\n",
      "training Loss: 0.0001 Acc: 88.8352\n",
      "validation Loss: 0.0001 Acc: 88.8271\n",
      "Epoch 34/149\n",
      "training Loss: 0.0001 Acc: 88.8182\n",
      "validation Loss: 0.0001 Acc: 88.5502\n",
      "Epoch 35/149\n",
      "training Loss: 0.0001 Acc: 88.8256\n",
      "validation Loss: 0.0001 Acc: 88.8510\n",
      "Epoch 36/149\n",
      "training Loss: 0.0001 Acc: 88.7904\n",
      "validation Loss: 0.0001 Acc: 88.8160\n",
      "Epoch 37/149\n",
      "training Loss: 0.0001 Acc: 88.8265\n",
      "validation Loss: 0.0001 Acc: 88.6992\n",
      "Epoch 38/149\n",
      "training Loss: 0.0001 Acc: 88.8237\n",
      "validation Loss: 0.0001 Acc: 88.8464\n",
      "Epoch 39/149\n",
      "training Loss: 0.0001 Acc: 88.8791\n",
      "validation Loss: 0.0001 Acc: 88.6486\n",
      "Epoch 40/149\n",
      "training Loss: 0.0001 Acc: 88.8720\n",
      "validation Loss: 0.0001 Acc: 88.9917\n",
      "Epoch 41/149\n",
      "training Loss: 0.0001 Acc: 88.9010\n",
      "validation Loss: 0.0001 Acc: 88.6100\n",
      "Epoch 42/149\n",
      "training Loss: 0.0001 Acc: 88.8971\n",
      "validation Loss: 0.0001 Acc: 88.6983\n",
      "Epoch 43/149\n",
      "training Loss: 0.0001 Acc: 88.8830\n",
      "validation Loss: 0.0001 Acc: 88.9338\n",
      "Epoch 44/149\n",
      "training Loss: 0.0001 Acc: 88.8805\n",
      "validation Loss: 0.0001 Acc: 88.8243\n",
      "Epoch 45/149\n",
      "training Loss: 0.0001 Acc: 88.8922\n",
      "validation Loss: 0.0001 Acc: 88.9052\n",
      "Epoch 46/149\n",
      "training Loss: 0.0001 Acc: 88.8902\n",
      "validation Loss: 0.0001 Acc: 88.7379\n",
      "Epoch 47/149\n",
      "training Loss: 0.0001 Acc: 88.9134\n",
      "validation Loss: 0.0001 Acc: 88.7608\n",
      "Epoch 48/149\n",
      "training Loss: 0.0001 Acc: 88.8968\n",
      "validation Loss: 0.0001 Acc: 88.8409\n",
      "Epoch 49/149\n",
      "training Loss: 0.0001 Acc: 88.9159\n",
      "validation Loss: 0.0001 Acc: 88.8455\n",
      "Epoch 50/149\n",
      "training Loss: 0.0001 Acc: 88.9672\n",
      "validation Loss: 0.0001 Acc: 88.7746\n",
      "Epoch 51/149\n",
      "training Loss: 0.0001 Acc: 88.9511\n",
      "validation Loss: 0.0001 Acc: 88.8004\n",
      "Epoch 52/149\n",
      "training Loss: 0.0001 Acc: 88.9421\n",
      "validation Loss: 0.0001 Acc: 88.7940\n",
      "Epoch 53/149\n",
      "training Loss: 0.0001 Acc: 88.9348\n",
      "validation Loss: 0.0001 Acc: 88.9025\n",
      "Epoch 54/149\n",
      "training Loss: 0.0001 Acc: 88.9511\n",
      "validation Loss: 0.0001 Acc: 88.9200\n",
      "Epoch 55/149\n",
      "training Loss: 0.0001 Acc: 88.9791\n",
      "validation Loss: 0.0001 Acc: 88.7425\n",
      "Epoch 56/149\n",
      "training Loss: 0.0001 Acc: 88.9396\n",
      "validation Loss: 0.0001 Acc: 88.7710\n",
      "Epoch 57/149\n",
      "training Loss: 0.0001 Acc: 88.9247\n",
      "validation Loss: 0.0001 Acc: 88.9255\n",
      "Epoch 58/149\n",
      "training Loss: 0.0001 Acc: 88.9635\n",
      "validation Loss: 0.0001 Acc: 88.8639\n",
      "Epoch 59/149\n",
      "training Loss: 0.0001 Acc: 88.9962\n",
      "validation Loss: 0.0001 Acc: 88.8353\n",
      "Epoch 60/149\n",
      "training Loss: 0.0001 Acc: 88.9835\n",
      "validation Loss: 0.0001 Acc: 88.8473\n",
      "Epoch 61/149\n",
      "training Loss: 0.0001 Acc: 88.9858\n",
      "validation Loss: 0.0001 Acc: 88.9761\n",
      "Epoch 62/149\n",
      "training Loss: 0.0001 Acc: 88.9817\n",
      "validation Loss: 0.0001 Acc: 88.8648\n",
      "Epoch 63/149\n",
      "training Loss: 0.0001 Acc: 88.9766\n",
      "validation Loss: 0.0001 Acc: 88.8749\n",
      "Epoch 64/149\n",
      "training Loss: 0.0001 Acc: 88.9849\n",
      "validation Loss: 0.0001 Acc: 88.7305\n",
      "Epoch 65/149\n",
      "training Loss: 0.0001 Acc: 88.9690\n",
      "validation Loss: 0.0001 Acc: 88.8151\n",
      "Epoch 66/149\n",
      "training Loss: 0.0001 Acc: 88.9897\n",
      "validation Loss: 0.0001 Acc: 88.8602\n",
      "Epoch 67/149\n",
      "training Loss: 0.0001 Acc: 88.9626\n",
      "validation Loss: 0.0001 Acc: 88.8639\n",
      "Epoch 68/149\n",
      "training Loss: 0.0001 Acc: 88.9796\n",
      "validation Loss: 0.0001 Acc: 88.8179\n",
      "Epoch 69/149\n",
      "training Loss: 0.0001 Acc: 88.9847\n",
      "validation Loss: 0.0001 Acc: 88.8832\n",
      "Epoch 70/149\n",
      "training Loss: 0.0001 Acc: 89.0309\n",
      "validation Loss: 0.0001 Acc: 88.8169\n",
      "Epoch 71/149\n",
      "training Loss: 0.0001 Acc: 88.9778\n",
      "validation Loss: 0.0001 Acc: 88.8850\n",
      "Epoch 72/149\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = OUTPUT_DIRPATH + CURRENT_TIME + 'BestConfigReallyTopclass.json'\n",
    "    best_conf = optimize_hyperparams(\n",
    "        skf, data_list, data_hlf, label, \n",
    "        config_file, len(input_hlf_vars), epochs=10\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + config_file + '.json') as f:\n",
    "    with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "        best_conf = json.load(f)\n",
    "        print(best_conf)\n",
    "\n",
    "fom = []\n",
    "train_weights = torch.FloatTensor([1.0, np.sum(data_aux.loc[label==1,'eventWeight'])]).cuda()\n",
    "# criterion = nn.BCELoss(weight=train_weights)\n",
    "criterion = nn.NLLLoss(weight=train_weights)\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx, (train_index, test_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    model_file = OUTPUT_DIRPATH + CURRENT_TIME +'_ReallyTopclassStyle_'+ f'{fold_idx}.torch'\n",
    "    state_file = OUTPUT_DIRPATH + CURRENT_TIME +'_BestPerfReallyTopclass_'+ f'{fold_idx}.torch'\n",
    "    \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], dnn_input=len(input_hlf_vars)\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), \n",
    "        batch_size=best_conf['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), \n",
    "        batch_size=best_conf['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        150, model, criterion, optimizer, scheduler, \n",
    "        state_file, model_file, data_loader=data_loader\n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC.\n"
     ]
    }
   ],
   "source": [
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + config_file + '.json') as f:\n",
    "with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    best_conf = json.load(f)\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, save=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, save=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.1952  |       0.9706      |    0.3034 +/- 0.0052     |\n",
      "|   0.3047  |       0.9500      |    0.2193 +/- 0.0040     |\n",
      "|   0.4443  |       0.9198      |    0.1498 +/- 0.0038     |\n",
      "|   0.8384  |       0.7538      |    0.0344 +/- 0.0013     |\n",
      "|   0.9524  |       0.5777      |    0.0089 +/- 0.0005     |\n",
      "|   0.9877  |       0.3839      |    0.0018 +/- 0.0001     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}IN_perf.json', 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "    NNtable.add_row(\n",
    "        [\n",
    "            IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "            \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "        ]\n",
    "    )\n",
    "print(NNtable)\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plot_train_val_losses(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='test_data',\n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    ")\n",
    "plot_roc(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='test_data', method='arr',\n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    ")\n",
    "plot_output_score(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='test_data', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train + val comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + config_file + '.json') as f:\n",
    "with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    best_conf = json.load(f)\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            data_list[train_index], data_hlf[train_index], label[train_index], \n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            data_list[val_index], data_hlf[val_index], label[val_index], \n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'r') as f:\n",
    "#     IN_perf_dict = json.load(f)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold '] * len(IN_perf_dict['train'])\n",
    "for fold_idx, (train_IN_dict, val_IN_dict) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'])):\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[fold_idx]+str(fold_idx), labels_arr[fold_idx+1]+str(fold_idx)]\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[fold_idx]+str(fold_idx), labels_arr[fold_idx+1]+str(fold_idx)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input Vars (pre-standardization and post-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/mplhep/utils.py:262: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  self.flat_scale(1 / np.sum(np.diff(self.edges) * self.values))\n",
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/mplhep/utils.py:197: RuntimeWarning: All sumw are zero!  Cannot compute meaningful error bars\n",
      "  return np.abs(method_fcn(self.values, variances) - self.values)\n",
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/mplhep/utils.py:242: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.values *= scale\n",
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/mplhep/utils.py:243: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.yerr_lo *= scale\n",
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/mplhep/utils.py:244: RuntimeWarning: invalid value encountered in multiply\n",
      "  self.yerr_hi *= scale\n"
     ]
    }
   ],
   "source": [
    "output_dir_pre_std = CURRENT_DIRPATH + \"/input_comparison/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    sig_mask = (label == 1)\n",
    "    sig_test_mask = (label_test == 1)\n",
    "    bkg_mask = (label == 0)\n",
    "    bkg_test_mask = (label_test == 0)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        \n",
    "        sig_train_mask = rectified_train_index & sig_mask\n",
    "        sig_val_mask = np.logical_not(rectified_train_index) & sig_mask\n",
    "        bkg_train_mask = rectified_train_index & bkg_mask\n",
    "        bkg_val_mask = np.logical_not(rectified_train_index) & bkg_mask\n",
    "\n",
    "        sig_train_np = copy.deepcopy(data_df.loc[sig_train_mask, var_name].to_numpy())\n",
    "        sig_val_np = copy.deepcopy(data_df.loc[sig_val_mask, var_name].to_numpy())\n",
    "        sig_test_np = copy.deepcopy(data_test_df.loc[sig_test_mask, var_name].to_numpy())\n",
    "        bkg_train_np = copy.deepcopy(data_df.loc[bkg_train_mask, var_name].to_numpy())\n",
    "        bkg_val_np = copy.deepcopy(data_df.loc[bkg_val_mask, var_name].to_numpy())\n",
    "        bkg_test_np = copy.deepcopy(data_test_df.loc[bkg_test_mask, var_name].to_numpy())\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=data_df.loc[sig_mask, var_name].to_numpy())\n",
    "    sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=data_test_df.loc[sig_test_mask, var_name].to_numpy())\n",
    "    bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=data_df.loc[bkg_mask, var_name].to_numpy())\n",
    "    bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=data_test_df.loc[bkg_test_mask, var_name].to_numpy())\n",
    "    pre_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_pre_std, var_name, pre_std_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_post_std = CURRENT_DIRPATH + \"/input_comparison/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "index_map = {\n",
    "    var_name: data_list_index_map(var_name) for var_name in (high_level_fields - set(input_hlf_vars))\n",
    "}\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = data_list, data_list_test\n",
    "    else:\n",
    "        data, data_test = data_hlf, data_hlf_test\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name, index_map, \n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np)\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np)\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name, index_map)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np)\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np)\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np)\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np)\n",
    "    post_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_post_std, var_name, post_std_hists[var_name], labels=label_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian smearing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    index2, index3 = data_list_index_map(var_name)\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear[:, index2, index3] *= rng.normal(size=len(particle_list_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear[:, index2, index3] += rng.normal(size=len(particle_list_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns[var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + config_file + '.json') as f:\n",
    "with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    best_conf = json.load(f)\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "    gauss_data_list, gauss_data_hlf = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        gauss_data_list = smear_particle_list(var_name, copy.deepcopy(data_list_test))\n",
    "        gauss_data_hlf = data_hlf_test\n",
    "    else:\n",
    "        gauss_data_list = data_list_test\n",
    "        gauss_data_hlf = smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test))\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list, gauss_data_hlf, label_test, OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear', \n",
    "    method='IN_arr', labels=[var_name for var_name in IN_perf_smear_dict.keys()], yscale='log', run2=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gauss_smear = CURRENT_DIRPATH + \"/input_comparison/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "index_map = {\n",
    "    var_name: data_list_index_map(var_name) for var_name in (high_level_fields - set(input_hlf_vars))\n",
    "}\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = smear_particle_list(var_name, data_list), smear_particle_list(var_name, data_list_test)\n",
    "    else:\n",
    "        data, data_test = smear_particle_hlf(var_name, data_hlf), smear_particle_hlf(var_name, data_hlf_test)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name, index_map, \n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np)\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np)\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_gauss_smear, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name, index_map)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np)\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np)\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np)\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np)\n",
    "    gauss_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_gauss_smear, var_name, gauss_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + config_file + '.json') as f:\n",
    "with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    best_conf = json.load(f)\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y in [('train', data_list, data_hlf, label), ('test', data_list_test, data_hlf_test, label_test)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'r') as f:\n",
    "#     IN_full_eval_dict = json.load(f)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {'mass': [], 'dijet_mass': []}\n",
    "for var_name in hist_dict.keys():\n",
    "    for i, score_cut in enumerate(score_cuts):\n",
    "        sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict)\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "        hist_dict[var_name].extend(\n",
    "            [\n",
    "                copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    make_input_plot(plot_destdir, var_name, hist_dict[var_name], labels=label_arr, density=True, plot_prefix=CURRENT_TIME+'_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna-hhbbgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
