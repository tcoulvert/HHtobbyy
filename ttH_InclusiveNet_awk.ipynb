{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu1.fnal.gov      Wed Aug 28 17:55:46 2024  555.42.02\n",
      "[0] Tesla P100-PCIE-12GB | 45Â°C,   0 % |  2748 / 12288 MB | ckapsiak(2746M)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "V2_MERGED = True\n",
    "\n",
    "SIGNAL_FILEPATHS = [\n",
    "    # Test sig files #\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE/GluGluToHH/nominal/*',\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE_merged_v2/GluGluToHH/nominal/*',\n",
    "    # ggF HH # \n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\",\n",
    "    # VBF HH #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [\n",
    "    # ttH (i.e. the main bkg to reduce) #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    # # Other single H samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\",\n",
    "    # # Prompt-Prompt samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\",\n",
    "    # # Prompt-Fake samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\",\n",
    "    # Fake-Fake samples #\n",
    "    # ADD HERE #\n",
    "]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v4'\n",
    "if VERSION == 'v1':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v2':\n",
    "    # CRITERION == \"BCELoss\"\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v3':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v4':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    # N_PARTICLES, N_PARTICLE_FIELDS = 6, 7\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v5':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "# VARS = 'base_vars'\n",
    "# CURRENT_TIME = '2024-08-10_10-29-50'\n",
    "# CURRENT_TIME = '2024-08-17_18-23-49'\n",
    "# VARS = 'extra_vars'\n",
    "# CURRENT_TIME = '2024-08-10_13-16-12'\n",
    "# CURRENT_TIME = '2024-08-17_11-45-34'\n",
    "# CURRENT_TIME = '2024-08-20_23-02-48'\n",
    "# VARS = 'extra_vars_no_dijet_mass'\n",
    "# CURRENT_TIME = '2024-08-21_15-28-02'\n",
    "VARS = 'no_bad_vars'\n",
    "# VARS = 'simplified_bad_vars'\n",
    "# VARS = 'extra_vars_and_bools'\n",
    "OUTPUT_DIRPATH = CURRENT_DIRPATH + f\"/model_outputs/{VERSION}/{VARS}/\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/model_outputs/v4/no_bad_vars/\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (258332, 4, 6)\n",
      "Data HLF: (258332, 8)\n",
      "Data list test: (258924, 4, 6)\n",
      "Data HLF test: (258924, 8)\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    data_df, data_test_df, \n",
    "    data_list, data_hlf, label, \n",
    "    data_list_test, data_hlf_test, label_test, \n",
    "    high_level_fields, input_hlf_vars, hlf_vars_columns,\n",
    "    data_aux, data_test_aux\n",
    ") = process_data(\n",
    "    4, 6, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED, return_pre_std=True\n",
    ")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "sig_samples_list = [ak.from_parquet(glob.glob(dir_path)) for dir_path in SIGNAL_FILEPATHS]\n",
    "sig_samples_pq = ak.concatenate(sig_samples_list)\n",
    "sig_df = pd.DataFrame(\n",
    "    {\n",
    "        field: ak.to_numpy(sig_samples_pq[field], allow_missing=False) for field in sig_samples_pq.fields\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num total electrons: 1332\n",
      "num total muons: 30895\n",
      "      puppiMET_sumEt  DeltaPhi_j1MET  DeltaPhi_j2MET  DeltaR_jg_min  n_jets  \\\n",
      "1952          431.75        2.558647         1.34063       1.912366       2   \n",
      "\n",
      "      chi_t0  chi_t1  CosThetaStar_CS  CosThetaStar_jj  dijet_mass  \\\n",
      "1952  -999.0  -999.0          -0.5148         0.640777  126.130989   \n",
      "\n",
      "      leadBjet_leadLepton  leadBjet_subleadLepton  subleadBjet_leadLepton  \\\n",
      "1952           999.397237              999.397237                998.3494   \n",
      "\n",
      "      subleadBjet_subleadLepton  \n",
      "1952                   998.3494  \n",
      "------------------------------------------------------------\n",
      "      lepton1_pt  lepton1_eta  lepton1_phi  lepton2_pt  lepton2_eta  \\\n",
      "1952      -999.0       -999.0       -999.0      -999.0       -999.0   \n",
      "\n",
      "      lepton2_phi  \n",
      "1952       -999.0  \n"
     ]
    }
   ],
   "source": [
    "# print(sig_df['event'][sig_df['event']<1000])\n",
    "num_total_electrons = np.sum(\n",
    "    [\n",
    "        np.where((sig_df['lepton1_pt'] != -999) & (sig_df['lepton1_generation'] == 1), 1, 0), \n",
    "        np.where((sig_df['lepton2_pt'] != -999) & (sig_df['lepton2_generation'] == 1), 1, 0),\n",
    "        np.where((sig_df['lepton3_pt'] != -999) & (sig_df['lepton3_generation'] == 1), 1, 0), \n",
    "        np.where((sig_df['lepton4_pt'] != -999) & (sig_df['lepton4_generation'] == 1), 1, 0)\n",
    "    ], axis=None\n",
    ")\n",
    "print(f\"num total electrons: {num_total_electrons}\")\n",
    "num_total_muons = np.sum(\n",
    "    [\n",
    "        np.where((sig_df['lepton1_pt'] != -999) & (sig_df['lepton1_generation'] == 2), 1, 0), \n",
    "        np.where((sig_df['lepton2_pt'] != -999) & (sig_df['lepton2_generation'] == 2), 1, 0),\n",
    "        np.where((sig_df['lepton3_pt'] != -999) & (sig_df['lepton3_generation'] == 2), 1, 0), \n",
    "        np.where((sig_df['lepton4_pt'] != -999) & (sig_df['lepton4_generation'] == 2), 1, 0)\n",
    "    ], axis=None\n",
    ")\n",
    "print(f\"num total muons: {num_total_muons}\")\n",
    "print(sig_df.loc[sig_df['event'] == 992, input_hlf_vars])\n",
    "print('-'*60)\n",
    "print(sig_df.loc[sig_df['event'] == 992, [\n",
    "    'lepton1_pt', 'lepton1_eta', 'lepton1_phi',\n",
    "    'lepton2_pt', 'lepton2_eta', 'lepton2_phi'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num total electrons: 22\n",
      "num total muons: 626\n",
      "   puppiMET_sumEt  DeltaR_jg_min  n_jets  CosThetaStar_CS  CosThetaStar_jj\n",
      "2          431.75       1.912366       2        -0.513182         0.640777\n",
      "------------------------------------------------------------\n",
      "   lepton1_pt  lepton1_eta  lepton1_phi  lepton2_pt  lepton2_eta  lepton2_phi\n",
      "2      -999.0       -999.0       -999.0      -999.0       -999.0       -999.0\n"
     ]
    }
   ],
   "source": [
    "# print(sig_df['event'])\n",
    "num_total_electrons = np.sum(\n",
    "    [\n",
    "        np.where((sig_df['lepton1_pt'] != -999) & (sig_df['lepton1_generation'] == 1), 1, 0), \n",
    "        np.where((sig_df['lepton2_pt'] != -999) & (sig_df['lepton2_generation'] == 1), 1, 0),\n",
    "        np.where((sig_df['lepton3_pt'] != -999) & (sig_df['lepton3_generation'] == 1), 1, 0), \n",
    "        np.where((sig_df['lepton4_pt'] != -999) & (sig_df['lepton4_generation'] == 1), 1, 0)\n",
    "    ], axis=None\n",
    ")\n",
    "print(f\"num total electrons: {num_total_electrons}\")\n",
    "num_total_muons = np.sum(\n",
    "    [\n",
    "        np.where((sig_df['lepton1_pt'] != -999) & (sig_df['lepton1_generation'] == 2), 1, 0), \n",
    "        np.where((sig_df['lepton2_pt'] != -999) & (sig_df['lepton2_generation'] == 2), 1, 0),\n",
    "        np.where((sig_df['lepton3_pt'] != -999) & (sig_df['lepton3_generation'] == 2), 1, 0), \n",
    "        np.where((sig_df['lepton4_pt'] != -999) & (sig_df['lepton4_generation'] == 2), 1, 0)\n",
    "    ], axis=None\n",
    ")\n",
    "print(f\"num total muons: {num_total_muons}\")\n",
    "print(sig_df.loc[sig_df['event'] == 992, [\n",
    "    'puppiMET_sumEt', 'DeltaR_jg_min', 'n_jets', 'CosThetaStar_CS', 'CosThetaStar_jj',\n",
    "]])\n",
    "print('-'*60)\n",
    "print(sig_df.loc[sig_df['event'] == 992, [\n",
    "    'lepton1_pt', 'lepton1_eta', 'lepton1_phi',\n",
    "    'lepton2_pt', 'lepton2_eta', 'lepton2_phi'\n",
    "]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_aux['event'][(data_aux['event'] < 800) & (data_aux['event'] > 700)])\n",
    "for i in range(0, 2):\n",
    "    event = 721\n",
    "    index = data_aux[data_aux['event']==event].index[i]\n",
    "    print(f\"index:, {index} event number: {event}, label: {label[index]}\")\n",
    "    print(data_df.loc[index, input_hlf_vars])\n",
    "    print('-'*60)\n",
    "    print(data_list[index,...])\n",
    "    print('-'*60)\n",
    "    print(data_hlf[index,...])\n",
    "    print('='*60)\n",
    "    print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:, 64025 event number: 721, label: 0.0\n",
      "puppiMET_sumEt               646.000000\n",
      "DeltaPhi_j1MET                 0.185547\n",
      "DeltaPhi_j2MET                 1.178205\n",
      "DeltaR_jg_min                  1.936699\n",
      "n_jets                         4.000000\n",
      "chi_t0                        20.037055\n",
      "chi_t1                      -999.000000\n",
      "CosThetaStar_CS                0.012029\n",
      "CosThetaStar_jj                0.936100\n",
      "dijet_mass                   160.309753\n",
      "leadBjet_leadLepton            0.009621\n",
      "leadBjet_subleadLepton      -999.000000\n",
      "subleadBjet_leadLepton         1.861168\n",
      "subleadBjet_subleadLepton   -999.000000\n",
      "Name: 64025, dtype: float64\n",
      "------------------------------------------------------------\n",
      "[[ 1.62836923  1.92114258  2.76855469  1.          0.          0.        ]\n",
      " [-5.7357468  -6.2434082  -6.57067871  1.          0.          0.        ]\n",
      " [ 0.71948176  1.71394169 -0.45006426  0.          1.          0.        ]\n",
      " [ 0.31649359  0.          2.59082031  0.          0.          1.        ]]\n",
      "------------------------------------------------------------\n",
      "[-0.16822058  0.18554688  1.17820484  0.70868943 -0.63577722 -0.54085647\n",
      " -7.31057135  0.0120293   0.93610018  1.337246   -1.20304011 -5.64068549\n",
      "  0.22412217 -5.64863515]\n",
      "============================================================\n",
      "============================================================\n",
      "index:, 121873 event number: 721, label: 0.0\n",
      "puppiMET_sumEt               745.000000\n",
      "DeltaPhi_j1MET                 0.447266\n",
      "DeltaPhi_j2MET                 1.925763\n",
      "DeltaR_jg_min                  2.060972\n",
      "n_jets                         5.000000\n",
      "chi_t0                         4.327038\n",
      "chi_t1                      -999.000000\n",
      "CosThetaStar_CS                0.078956\n",
      "CosThetaStar_jj               -0.938845\n",
      "dijet_mass                   107.080406\n",
      "leadBjet_leadLepton         -999.000000\n",
      "leadBjet_subleadLepton      -999.000000\n",
      "subleadBjet_leadLepton      -999.000000\n",
      "subleadBjet_subleadLepton   -999.000000\n",
      "Name: 121873, dtype: float64\n",
      "------------------------------------------------------------\n",
      "[[-5.88960879 -6.24853516 -6.57063293  1.          0.          0.        ]\n",
      " [-5.7357468  -6.2434082  -6.57067871  1.          0.          0.        ]\n",
      " [ 1.30511493  0.08091911 -1.1632282   0.          1.          0.        ]\n",
      " [ 2.11331428  0.          1.71875     0.          0.          1.        ]]\n",
      "------------------------------------------------------------\n",
      "[ 0.25457187  0.44726562  1.92576343  0.90284094 -0.12233609 -1.23256644\n",
      " -7.31057135  0.07895626 -0.93884456 -0.25351315 -5.6055626  -5.64068549\n",
      " -5.70818934 -5.64863515]\n",
      "============================================================\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# print(data_aux['event'][(data_aux['event'] < 800) & (data_aux['event'] > 700)])\n",
    "for i in range(0, 2):\n",
    "    event = 721\n",
    "    index = data_aux[data_aux['event']==event].index[i]\n",
    "    print(f\"index:, {index} event number: {event}, label: {label[index]}\")\n",
    "    print(data_df.loc[index, input_hlf_vars])\n",
    "    print('-'*60)\n",
    "    print(data_list[index,...])\n",
    "    print('-'*60)\n",
    "    print(data_hlf[index,...])\n",
    "    print('='*60)\n",
    "    print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dotted']\n",
    "        linestyles = linestyles * ((2*len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:2*len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx if fold_idx%2 == 0 else fold_idx+1]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx+1 if fold_idx%2 == 0 else fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='std', \n",
    "    labels=None, yscale='linear', run2=True, sort=None\n",
    "):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            IN_info['mean_fprs'], IN_info['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.plot(\n",
    "                IN_info['fprs'][fold_idx], IN_info['base_tpr'],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            IN_info['mean_fprs'], IN_info['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for i in index_arr:\n",
    "            plt.plot(\n",
    "                IN_info[i]['mean_fprs'], IN_info[i]['base_tpr'], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, n_bins=50\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist], w2=np.array([sig_hist.variances(), bkg_hist.variances()]),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist], w2=np.array([sig_hist.variances(), bkg_hist.variances()]),\n",
    "                alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist], w2=np.array([sig_hist.variances(), bkg_hist.variances()]),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info[i]['mean_label']) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info[i]['mean_label']) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'] if weights[i]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'] if weights[i]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist], w2=np.array([sig_hist.variances(), bkg_hist.variances()]),\n",
    "                alpha=0.7, density=(False if weights[i]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[i] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[i], linestyles[i]]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def s_over_root_b(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, lines_fold=None, lines=None, lines_labels=None, \n",
    "        lines_colors=None, only_fold=None, no_fold=False, n_bins=50\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/âb', alpha=0.8 \n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "        if only_fold is None:\n",
    "            sig_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info['mean_label']) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info['mean_label']) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/âb - avg. over folds', \n",
    "                alpha=0.5, \n",
    "            )\n",
    "            if lines is not None:\n",
    "                for i in range(len(lines)):\n",
    "                    plt.vlines(\n",
    "                        lines[i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/âb - avg.'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[i]\n",
    "                    )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['all_preds'][0]\n",
    "            )[\n",
    "                np.array(IN_info[i]['all_labels'][0]) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['all_preds'][0]\n",
    "            )[\n",
    "                np.array(IN_info[i]['all_labels'][0]) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'])\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "                label='s/âb'+(' - '+labels[i] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[i], \n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -10., 4., name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -10., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -10., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'jet1_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'jet2_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(40, -10., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'lepton1_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -10., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -10., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -10., 4., name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -10., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -10., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -10., 4., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -10., 4., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -10., 4., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -10., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, var_name, index_map, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label == 1\n",
    "    sig_test_mask = label_test == 1\n",
    "    bkg_mask = label == 0\n",
    "    bkg_test_mask = label_test == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns[var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "            # index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns[var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict):\n",
    "    sig_train_mask = (label == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux.loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux.loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir = output_dir + \"fold/\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n",
      "Epoch 0/149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha) (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706619781071/work/torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0001 Acc: 60.8913\n",
      "validation Loss: 0.0000 Acc: 71.2873\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 69.4104\n",
      "validation Loss: 0.0000 Acc: 70.9079\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 70.5267\n",
      "validation Loss: 0.0000 Acc: 70.9292\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 70.7038\n",
      "validation Loss: 0.0000 Acc: 75.0537\n",
      "Saving..\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 71.1165\n",
      "validation Loss: 0.0000 Acc: 72.0344\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 71.3812\n",
      "validation Loss: 0.0000 Acc: 74.3589\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 71.4485\n",
      "validation Loss: 0.0000 Acc: 72.2028\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 71.6019\n",
      "validation Loss: 0.0000 Acc: 72.6170\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 71.1707\n",
      "validation Loss: 0.0000 Acc: 70.1415\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 71.3333\n",
      "validation Loss: 0.0000 Acc: 72.9131\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 71.1011\n",
      "validation Loss: 0.0000 Acc: 71.8892\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 71.3522\n",
      "validation Loss: 0.0000 Acc: 71.1208\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 71.6174\n",
      "validation Loss: 0.0000 Acc: 72.2337\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 71.5085\n",
      "validation Loss: 0.0000 Acc: 75.2027\n",
      "Saving..\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 71.8525\n",
      "validation Loss: 0.0000 Acc: 73.7840\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 71.6004\n",
      "validation Loss: 0.0000 Acc: 70.5479\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 71.6875\n",
      "validation Loss: 0.0000 Acc: 72.6170\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 71.7112\n",
      "validation Loss: 0.0000 Acc: 73.5944\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 71.7151\n",
      "validation Loss: 0.0000 Acc: 76.0408\n",
      "Saving..\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 71.6193\n",
      "validation Loss: 0.0000 Acc: 74.8602\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 71.8380\n",
      "validation Loss: 0.0000 Acc: 70.5673\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 71.6004\n",
      "validation Loss: 0.0000 Acc: 74.4344\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 71.8670\n",
      "validation Loss: 0.0000 Acc: 75.7118\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 72.0315\n",
      "validation Loss: 0.0000 Acc: 73.6137\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 71.7867\n",
      "validation Loss: 0.0000 Acc: 73.4318\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 71.7185\n",
      "validation Loss: 0.0000 Acc: 71.7634\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 71.7766\n",
      "validation Loss: 0.0000 Acc: 70.8247\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 72.0896\n",
      "validation Loss: 0.0000 Acc: 72.5918\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 71.6415\n",
      "validation Loss: 0.0000 Acc: 72.1524\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 72.3587\n",
      "validation Loss: 0.0000 Acc: 74.2079\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 72.1104\n",
      "validation Loss: 0.0000 Acc: 74.7615\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 72.5445\n",
      "validation Loss: 0.0000 Acc: 73.8073\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 72.4211\n",
      "validation Loss: 0.0000 Acc: 69.7099\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 72.0843\n",
      "validation Loss: 0.0000 Acc: 74.8814\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 72.1767\n",
      "validation Loss: 0.0000 Acc: 71.2370\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 72.4303\n",
      "validation Loss: 0.0000 Acc: 73.7202\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 72.5459\n",
      "validation Loss: 0.0000 Acc: 72.0944\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 72.2483\n",
      "validation Loss: 0.0000 Acc: 72.8763\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 72.4007\n",
      "validation Loss: 0.0000 Acc: 71.4111\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 72.2517\n",
      "validation Loss: 0.0000 Acc: 71.7092\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 72.4927\n",
      "validation Loss: 0.0000 Acc: 73.9892\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 72.7443\n",
      "validation Loss: 0.0000 Acc: 73.8769\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 72.6451\n",
      "validation Loss: 0.0000 Acc: 73.2886\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 72.3838\n",
      "validation Loss: 0.0000 Acc: 73.2866\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 72.6485\n",
      "validation Loss: 0.0000 Acc: 73.6311\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 72.5082\n",
      "validation Loss: 0.0000 Acc: 72.3247\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 72.6320\n",
      "validation Loss: 0.0000 Acc: 73.1763\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 72.4719\n",
      "validation Loss: 0.0000 Acc: 74.8602\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 72.7675\n",
      "validation Loss: 0.0000 Acc: 72.9557\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 72.7893\n",
      "validation Loss: 0.0000 Acc: 73.7105\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 72.8541\n",
      "validation Loss: 0.0000 Acc: 73.5673\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 72.9988\n",
      "validation Loss: 0.0000 Acc: 71.6124\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 72.6122\n",
      "validation Loss: 0.0000 Acc: 74.2176\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 72.7666\n",
      "validation Loss: 0.0000 Acc: 73.8982\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 72.7070\n",
      "validation Loss: 0.0000 Acc: 75.2414\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 73.0046\n",
      "validation Loss: 0.0000 Acc: 74.1015\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 72.9737\n",
      "validation Loss: 0.0000 Acc: 73.4724\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 72.8493\n",
      "validation Loss: 0.0000 Acc: 74.0434\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 72.9441\n",
      "validation Loss: 0.0000 Acc: 73.9718\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 72.8503\n",
      "validation Loss: 0.0000 Acc: 73.9292\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 72.9707\n",
      "validation Loss: 0.0000 Acc: 74.1402\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 73.1096\n",
      "validation Loss: 0.0000 Acc: 74.0550\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 72.9170\n",
      "validation Loss: 0.0000 Acc: 74.0415\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 72.9006\n",
      "validation Loss: 0.0000 Acc: 73.5924\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 72.8672\n",
      "validation Loss: 0.0000 Acc: 74.3027\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 73.1130\n",
      "validation Loss: 0.0000 Acc: 73.9698\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 72.9746\n",
      "validation Loss: 0.0000 Acc: 74.1769\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 73.1120\n",
      "validation Loss: 0.0000 Acc: 74.0318\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 73.0453\n",
      "validation Loss: 0.0000 Acc: 73.7627\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 72.9267\n",
      "validation Loss: 0.0000 Acc: 74.1363\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 73.1295\n",
      "validation Loss: 0.0000 Acc: 73.4918\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 72.8682\n",
      "validation Loss: 0.0000 Acc: 74.0550\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 73.0037\n",
      "validation Loss: 0.0000 Acc: 74.1034\n",
      "Epoch 73/149\n",
      "training Loss: 0.0000 Acc: 73.0762\n",
      "validation Loss: 0.0000 Acc: 73.9718\n",
      "Epoch 74/149\n",
      "training Loss: 0.0000 Acc: 72.7530\n",
      "validation Loss: 0.0000 Acc: 73.9737\n",
      "Epoch 75/149\n",
      "training Loss: 0.0000 Acc: 73.0912\n",
      "validation Loss: 0.0000 Acc: 74.1905\n",
      "Epoch 76/149\n",
      "training Loss: 0.0000 Acc: 73.0738\n",
      "validation Loss: 0.0000 Acc: 73.9698\n",
      "Epoch 77/149\n",
      "training Loss: 0.0000 Acc: 73.0177\n",
      "validation Loss: 0.0000 Acc: 73.9447\n",
      "Epoch 78/149\n",
      "training Loss: 0.0000 Acc: 72.9180\n",
      "validation Loss: 0.0000 Acc: 74.0473\n",
      "Early stopped.\n",
      "Best val acc: 76.040794\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 61.8915\n",
      "validation Loss: 0.0000 Acc: 68.1750\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 69.6267\n",
      "validation Loss: 0.0000 Acc: 69.3363\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 70.0269\n",
      "validation Loss: 0.0000 Acc: 73.3621\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 70.5562\n",
      "validation Loss: 0.0000 Acc: 72.8995\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 71.0328\n",
      "validation Loss: 0.0000 Acc: 72.2744\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 70.8252\n",
      "validation Loss: 0.0000 Acc: 72.0402\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 70.7817\n",
      "validation Loss: 0.0000 Acc: 70.1066\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 70.9588\n",
      "validation Loss: 0.0000 Acc: 71.8234\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 70.8591\n",
      "validation Loss: 0.0000 Acc: 70.9486\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 70.9128\n",
      "validation Loss: 0.0000 Acc: 71.3434\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 70.8393\n",
      "validation Loss: 0.0000 Acc: 71.4886\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 70.9603\n",
      "validation Loss: 0.0000 Acc: 67.2944\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 71.0473\n",
      "validation Loss: 0.0000 Acc: 71.4634\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 71.1253\n",
      "validation Loss: 0.0000 Acc: 72.2860\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 71.5448\n",
      "validation Loss: 0.0000 Acc: 69.2086\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 71.2022\n",
      "validation Loss: 0.0000 Acc: 74.0298\n",
      "Saving..\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 71.5603\n",
      "validation Loss: 0.0000 Acc: 69.4795\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 71.7543\n",
      "validation Loss: 0.0000 Acc: 70.6892\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 71.7553\n",
      "validation Loss: 0.0000 Acc: 73.2731\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 71.7345\n",
      "validation Loss: 0.0000 Acc: 71.8582\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 71.8941\n",
      "validation Loss: 0.0000 Acc: 71.7770\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 71.9082\n",
      "validation Loss: 0.0000 Acc: 72.8550\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 71.5946\n",
      "validation Loss: 0.0000 Acc: 73.9234\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 71.9275\n",
      "validation Loss: 0.0000 Acc: 71.6628\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 71.8733\n",
      "validation Loss: 0.0000 Acc: 69.4873\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 72.1777\n",
      "validation Loss: 0.0000 Acc: 72.4757\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 72.1583\n",
      "validation Loss: 0.0000 Acc: 72.4021\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 72.4182\n",
      "validation Loss: 0.0000 Acc: 73.7298\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 72.2067\n",
      "validation Loss: 0.0000 Acc: 72.3518\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 72.2546\n",
      "validation Loss: 0.0000 Acc: 72.6634\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 72.2082\n",
      "validation Loss: 0.0000 Acc: 71.4421\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 72.2832\n",
      "validation Loss: 0.0000 Acc: 73.2615\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 72.3316\n",
      "validation Loss: 0.0000 Acc: 73.8169\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 72.4109\n",
      "validation Loss: 0.0000 Acc: 73.0137\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 72.3025\n",
      "validation Loss: 0.0000 Acc: 73.6776\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 72.5658\n",
      "validation Loss: 0.0000 Acc: 71.9995\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 72.2740\n",
      "validation Loss: 0.0000 Acc: 71.3570\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 72.5406\n",
      "validation Loss: 0.0000 Acc: 72.2008\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 72.3678\n",
      "validation Loss: 0.0000 Acc: 72.9827\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 72.4380\n",
      "validation Loss: 0.0000 Acc: 72.9537\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 72.4622\n",
      "validation Loss: 0.0000 Acc: 72.6692\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 72.4486\n",
      "validation Loss: 0.0000 Acc: 73.9156\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 72.3741\n",
      "validation Loss: 0.0000 Acc: 72.9208\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 72.5406\n",
      "validation Loss: 0.0000 Acc: 73.4395\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 72.5812\n",
      "validation Loss: 0.0000 Acc: 72.1157\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 72.4259\n",
      "validation Loss: 0.0000 Acc: 72.6769\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 72.4366\n",
      "validation Loss: 0.0000 Acc: 73.7802\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 72.4854\n",
      "validation Loss: 0.0000 Acc: 72.8492\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 72.3983\n",
      "validation Loss: 0.0000 Acc: 74.3124\n",
      "Saving..\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 72.5938\n",
      "validation Loss: 0.0000 Acc: 72.4505\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 72.3935\n",
      "validation Loss: 0.0000 Acc: 73.1569\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 72.9582\n",
      "validation Loss: 0.0000 Acc: 72.3053\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 72.5077\n",
      "validation Loss: 0.0000 Acc: 73.8518\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 72.5411\n",
      "validation Loss: 0.0000 Acc: 73.0253\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 72.8048\n",
      "validation Loss: 0.0000 Acc: 72.5802\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 72.6538\n",
      "validation Loss: 0.0000 Acc: 73.1511\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 72.4157\n",
      "validation Loss: 0.0000 Acc: 73.4357\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 72.7893\n",
      "validation Loss: 0.0000 Acc: 72.4737\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 72.5430\n",
      "validation Loss: 0.0000 Acc: 72.7021\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 72.7182\n",
      "validation Loss: 0.0000 Acc: 73.0079\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 72.6983\n",
      "validation Loss: 0.0000 Acc: 73.0640\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 72.7366\n",
      "validation Loss: 0.0000 Acc: 72.8260\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 72.6572\n",
      "validation Loss: 0.0000 Acc: 73.2111\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 72.7017\n",
      "validation Loss: 0.0000 Acc: 73.6060\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 72.8720\n",
      "validation Loss: 0.0000 Acc: 72.5473\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 72.5159\n",
      "validation Loss: 0.0000 Acc: 73.3660\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 72.8145\n",
      "validation Loss: 0.0000 Acc: 73.0931\n",
      "Early stopped.\n",
      "Best val acc: 74.312424\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 61.3241\n",
      "validation Loss: 0.0000 Acc: 69.6996\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 70.0933\n",
      "validation Loss: 0.0000 Acc: 69.7712\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 70.6110\n",
      "validation Loss: 0.0000 Acc: 70.7196\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 70.9889\n",
      "validation Loss: 0.0000 Acc: 73.7816\n",
      "Saving..\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 71.2614\n",
      "validation Loss: 0.0000 Acc: 70.4583\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 71.1941\n",
      "validation Loss: 0.0000 Acc: 72.5835\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 71.3320\n",
      "validation Loss: 0.0000 Acc: 73.2532\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 71.4607\n",
      "validation Loss: 0.0000 Acc: 74.2306\n",
      "Saving..\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 71.7646\n",
      "validation Loss: 0.0000 Acc: 73.8803\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 71.7075\n",
      "validation Loss: 0.0000 Acc: 71.6235\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 71.5812\n",
      "validation Loss: 0.0000 Acc: 69.5757\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 71.9886\n",
      "validation Loss: 0.0000 Acc: 74.5190\n",
      "Saving..\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 72.0385\n",
      "validation Loss: 0.0000 Acc: 75.2952\n",
      "Saving..\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 72.0685\n",
      "validation Loss: 0.0000 Acc: 70.7332\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 72.2315\n",
      "validation Loss: 0.0000 Acc: 73.9229\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 72.1812\n",
      "validation Loss: 0.0000 Acc: 69.4461\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 72.0467\n",
      "validation Loss: 0.0000 Acc: 73.1913\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 72.0389\n",
      "validation Loss: 0.0000 Acc: 73.1177\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 72.0336\n",
      "validation Loss: 0.0000 Acc: 72.3435\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 72.1643\n",
      "validation Loss: 0.0000 Acc: 73.6229\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 72.2131\n",
      "validation Loss: 0.0000 Acc: 72.3938\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 72.3389\n",
      "validation Loss: 0.0000 Acc: 72.9532\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 72.0975\n",
      "validation Loss: 0.0000 Acc: 73.1003\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 72.0772\n",
      "validation Loss: 0.0000 Acc: 73.7545\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 72.4551\n",
      "validation Loss: 0.0000 Acc: 72.6242\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 72.3138\n",
      "validation Loss: 0.0000 Acc: 73.7738\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 72.5446\n",
      "validation Loss: 0.0000 Acc: 73.4448\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 72.5446\n",
      "validation Loss: 0.0000 Acc: 72.9648\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 72.6651\n",
      "validation Loss: 0.0000 Acc: 73.0209\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 72.5354\n",
      "validation Loss: 0.0000 Acc: 71.4241\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 72.5693\n",
      "validation Loss: 0.0000 Acc: 72.3629\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 72.5867\n",
      "validation Loss: 0.0000 Acc: 71.9332\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 72.5828\n",
      "validation Loss: 0.0000 Acc: 73.2938\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 73.0294\n",
      "validation Loss: 0.0000 Acc: 72.8584\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 72.7280\n",
      "validation Loss: 0.0000 Acc: 73.3190\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 72.9181\n",
      "validation Loss: 0.0000 Acc: 72.4848\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 72.9685\n",
      "validation Loss: 0.0000 Acc: 73.4971\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 72.8668\n",
      "validation Loss: 0.0000 Acc: 72.7209\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 72.7217\n",
      "validation Loss: 0.0000 Acc: 73.7235\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 72.8572\n",
      "validation Loss: 0.0000 Acc: 73.7661\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 73.0406\n",
      "validation Loss: 0.0000 Acc: 72.9764\n",
      "Early stopped.\n",
      "Best val acc: 75.295166\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 62.5294\n",
      "validation Loss: 0.0000 Acc: 71.2132\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 70.3599\n",
      "validation Loss: 0.0000 Acc: 64.4002\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 70.3294\n",
      "validation Loss: 0.0000 Acc: 69.8390\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 70.6304\n",
      "validation Loss: 0.0000 Acc: 69.9880\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 71.0485\n",
      "validation Loss: 0.0000 Acc: 73.2803\n",
      "Saving..\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 70.9014\n",
      "validation Loss: 0.0000 Acc: 71.4203\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 70.9430\n",
      "validation Loss: 0.0000 Acc: 65.9428\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 70.7736\n",
      "validation Loss: 0.0000 Acc: 70.1854\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 70.9657\n",
      "validation Loss: 0.0000 Acc: 70.4506\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 71.1128\n",
      "validation Loss: 0.0000 Acc: 71.1358\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 71.6959\n",
      "validation Loss: 0.0000 Acc: 69.7151\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 70.7122\n",
      "validation Loss: 0.0000 Acc: 68.8093\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 71.2783\n",
      "validation Loss: 0.0000 Acc: 71.1938\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 71.1036\n",
      "validation Loss: 0.0000 Acc: 70.5609\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 70.7223\n",
      "validation Loss: 0.0000 Acc: 72.0009\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 71.3064\n",
      "validation Loss: 0.0000 Acc: 71.7532\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 71.4941\n",
      "validation Loss: 0.0000 Acc: 72.1867\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 71.1955\n",
      "validation Loss: 0.0000 Acc: 72.5022\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 71.2570\n",
      "validation Loss: 0.0000 Acc: 71.9545\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 71.0847\n",
      "validation Loss: 0.0000 Acc: 75.0377\n",
      "Saving..\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 72.0859\n",
      "validation Loss: 0.0000 Acc: 71.5790\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 71.8023\n",
      "validation Loss: 0.0000 Acc: 72.5467\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 71.6872\n",
      "validation Loss: 0.0000 Acc: 71.8345\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 71.8338\n",
      "validation Loss: 0.0000 Acc: 72.2293\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 71.4389\n",
      "validation Loss: 0.0000 Acc: 74.7745\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 71.8473\n",
      "validation Loss: 0.0000 Acc: 71.3158\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 71.9252\n",
      "validation Loss: 0.0000 Acc: 72.4132\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 71.7022\n",
      "validation Loss: 0.0000 Acc: 71.4009\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 71.6828\n",
      "validation Loss: 0.0000 Acc: 71.3370\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 71.5991\n",
      "validation Loss: 0.0000 Acc: 72.0725\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 71.9702\n",
      "validation Loss: 0.0000 Acc: 71.0816\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 72.0839\n",
      "validation Loss: 0.0000 Acc: 72.0183\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 72.3941\n",
      "validation Loss: 0.0000 Acc: 72.2777\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 72.2543\n",
      "validation Loss: 0.0000 Acc: 73.1177\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 72.3946\n",
      "validation Loss: 0.0000 Acc: 72.2642\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 72.0989\n",
      "validation Loss: 0.0000 Acc: 71.9758\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 72.1894\n",
      "validation Loss: 0.0000 Acc: 72.1209\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 72.3249\n",
      "validation Loss: 0.0000 Acc: 73.3151\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 72.2262\n",
      "validation Loss: 0.0000 Acc: 73.0964\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 72.1207\n",
      "validation Loss: 0.0000 Acc: 72.0667\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 72.2354\n",
      "validation Loss: 0.0000 Acc: 73.5822\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 72.4647\n",
      "validation Loss: 0.0000 Acc: 72.5332\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 72.0254\n",
      "validation Loss: 0.0000 Acc: 73.3693\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 72.4067\n",
      "validation Loss: 0.0000 Acc: 71.1106\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 72.1889\n",
      "validation Loss: 0.0000 Acc: 73.2145\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 72.1720\n",
      "validation Loss: 0.0000 Acc: 72.7442\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 72.2451\n",
      "validation Loss: 0.0000 Acc: 71.0990\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 72.1812\n",
      "validation Loss: 0.0000 Acc: 73.3538\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 72.5006\n",
      "validation Loss: 0.0000 Acc: 73.6926\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 72.3849\n",
      "validation Loss: 0.0000 Acc: 73.5184\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 72.3389\n",
      "validation Loss: 0.0000 Acc: 71.9274\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 72.3428\n",
      "validation Loss: 0.0000 Acc: 72.4171\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 72.4106\n",
      "validation Loss: 0.0000 Acc: 73.0713\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 72.6123\n",
      "validation Loss: 0.0000 Acc: 72.5796\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 72.4749\n",
      "validation Loss: 0.0000 Acc: 72.0396\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 72.6167\n",
      "validation Loss: 0.0000 Acc: 72.9977\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 72.4115\n",
      "validation Loss: 0.0000 Acc: 72.8932\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 72.2988\n",
      "validation Loss: 0.0000 Acc: 73.7429\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 72.5518\n",
      "validation Loss: 0.0000 Acc: 73.2067\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 72.6418\n",
      "validation Loss: 0.0000 Acc: 72.3745\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 72.3801\n",
      "validation Loss: 0.0000 Acc: 73.6384\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 72.7130\n",
      "validation Loss: 0.0000 Acc: 72.7751\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 72.6462\n",
      "validation Loss: 0.0000 Acc: 73.0190\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 72.6138\n",
      "validation Loss: 0.0000 Acc: 72.8119\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 72.6239\n",
      "validation Loss: 0.0000 Acc: 72.5661\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 72.5688\n",
      "validation Loss: 0.0000 Acc: 72.6977\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 72.4710\n",
      "validation Loss: 0.0000 Acc: 72.7771\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 72.6539\n",
      "validation Loss: 0.0000 Acc: 72.3512\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 72.6341\n",
      "validation Loss: 0.0000 Acc: 72.6125\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 72.5523\n",
      "validation Loss: 0.0000 Acc: 73.0461\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 72.6128\n",
      "validation Loss: 0.0000 Acc: 72.7093\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 72.7314\n",
      "validation Loss: 0.0000 Acc: 72.8119\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 72.7125\n",
      "validation Loss: 0.0000 Acc: 72.9842\n",
      "Epoch 73/149\n",
      "training Loss: 0.0000 Acc: 72.7580\n",
      "validation Loss: 0.0000 Acc: 72.8409\n",
      "Epoch 74/149\n",
      "training Loss: 0.0000 Acc: 72.6801\n",
      "validation Loss: 0.0000 Acc: 72.9687\n",
      "Epoch 75/149\n",
      "training Loss: 0.0000 Acc: 72.7372\n",
      "validation Loss: 0.0000 Acc: 72.9319\n",
      "Epoch 76/149\n",
      "training Loss: 0.0000 Acc: 72.5267\n",
      "validation Loss: 0.0000 Acc: 72.9532\n",
      "Epoch 77/149\n",
      "training Loss: 0.0000 Acc: 72.8528\n",
      "validation Loss: 0.0000 Acc: 72.9571\n",
      "Epoch 78/149\n",
      "training Loss: 0.0000 Acc: 72.5499\n",
      "validation Loss: 0.0000 Acc: 72.6900\n",
      "Epoch 79/149\n",
      "training Loss: 0.0000 Acc: 72.7062\n",
      "validation Loss: 0.0000 Acc: 72.8061\n",
      "Epoch 80/149\n",
      "training Loss: 0.0000 Acc: 72.4391\n",
      "validation Loss: 0.0000 Acc: 73.2571\n",
      "Epoch 81/149\n",
      "training Loss: 0.0000 Acc: 72.7594\n",
      "validation Loss: 0.0000 Acc: 72.7945\n",
      "Epoch 82/149\n",
      "training Loss: 0.0000 Acc: 72.5296\n",
      "validation Loss: 0.0000 Acc: 73.1855\n",
      "Epoch 83/149\n",
      "training Loss: 0.0000 Acc: 72.7449\n",
      "validation Loss: 0.0000 Acc: 73.0461\n",
      "Epoch 84/149\n",
      "training Loss: 0.0000 Acc: 72.6056\n",
      "validation Loss: 0.0000 Acc: 73.0306\n",
      "Epoch 85/149\n",
      "training Loss: 0.0000 Acc: 72.6636\n",
      "validation Loss: 0.0000 Acc: 73.3113\n",
      "Epoch 86/149\n",
      "training Loss: 0.0000 Acc: 72.8557\n",
      "validation Loss: 0.0000 Acc: 72.9474\n",
      "Epoch 87/149\n",
      "training Loss: 0.0000 Acc: 72.7260\n",
      "validation Loss: 0.0000 Acc: 73.0558\n",
      "Epoch 88/149\n",
      "training Loss: 0.0000 Acc: 72.7623\n",
      "validation Loss: 0.0000 Acc: 72.9067\n",
      "Epoch 89/149\n",
      "training Loss: 0.0000 Acc: 72.5262\n",
      "validation Loss: 0.0000 Acc: 72.9609\n",
      "Epoch 90/149\n",
      "training Loss: 0.0000 Acc: 72.6733\n",
      "validation Loss: 0.0000 Acc: 73.0984\n",
      "Epoch 91/149\n",
      "training Loss: 0.0000 Acc: 72.6806\n",
      "validation Loss: 0.0000 Acc: 72.9919\n",
      "Early stopped.\n",
      "Best val acc: 75.037743\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 62.5091\n",
      "validation Loss: 0.0000 Acc: 69.6803\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 69.7473\n",
      "validation Loss: 0.0000 Acc: 70.7119\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 70.2578\n",
      "validation Loss: 0.0000 Acc: 70.0209\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 70.1847\n",
      "validation Loss: 0.0000 Acc: 72.1209\n",
      "Saving..\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 70.7678\n",
      "validation Loss: 0.0000 Acc: 70.9616\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 70.5984\n",
      "validation Loss: 0.0000 Acc: 73.8474\n",
      "Saving..\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 70.7460\n",
      "validation Loss: 0.0000 Acc: 71.2132\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 70.9207\n",
      "validation Loss: 0.0000 Acc: 66.7847\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 70.8481\n",
      "validation Loss: 0.0000 Acc: 72.1364\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 71.4235\n",
      "validation Loss: 0.0000 Acc: 71.0216\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 71.4714\n",
      "validation Loss: 0.0000 Acc: 72.2893\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 71.0368\n",
      "validation Loss: 0.0000 Acc: 70.7583\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 71.2870\n",
      "validation Loss: 0.0000 Acc: 71.2287\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 71.2589\n",
      "validation Loss: 0.0000 Acc: 73.6326\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 71.1172\n",
      "validation Loss: 0.0000 Acc: 70.0286\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 71.3185\n",
      "validation Loss: 0.0000 Acc: 71.6796\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 71.6799\n",
      "validation Loss: 0.0000 Acc: 70.5745\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 71.8207\n",
      "validation Loss: 0.0000 Acc: 72.4403\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 71.6567\n",
      "validation Loss: 0.0000 Acc: 73.1119\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 71.9794\n",
      "validation Loss: 0.0000 Acc: 72.2719\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 71.7026\n",
      "validation Loss: 0.0000 Acc: 72.8022\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 71.9605\n",
      "validation Loss: 0.0000 Acc: 71.5693\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 71.8454\n",
      "validation Loss: 0.0000 Acc: 71.5151\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 71.7501\n",
      "validation Loss: 0.0000 Acc: 72.7384\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 71.7022\n",
      "validation Loss: 0.0000 Acc: 73.3964\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 71.8014\n",
      "validation Loss: 0.0000 Acc: 72.3938\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 71.7989\n",
      "validation Loss: 0.0000 Acc: 72.8603\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 72.0810\n",
      "validation Loss: 0.0000 Acc: 70.6887\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 71.9944\n",
      "validation Loss: 0.0000 Acc: 73.3984\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 72.1164\n",
      "validation Loss: 0.0000 Acc: 72.5758\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 72.2364\n",
      "validation Loss: 0.0000 Acc: 72.1558\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 72.1130\n",
      "validation Loss: 0.0000 Acc: 71.4570\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 72.1749\n",
      "validation Loss: 0.0000 Acc: 72.8255\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 72.3839\n",
      "validation Loss: 0.0000 Acc: 72.6261\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 72.3278\n",
      "validation Loss: 0.0000 Acc: 71.2190\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 72.3728\n",
      "validation Loss: 0.0000 Acc: 72.0861\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 72.2436\n",
      "validation Loss: 0.0000 Acc: 72.1829\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 72.2867\n",
      "validation Loss: 0.0000 Acc: 74.1590\n",
      "Saving..\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 72.4923\n",
      "validation Loss: 0.0000 Acc: 72.5409\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 72.3283\n",
      "validation Loss: 0.0000 Acc: 72.3067\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 72.2964\n",
      "validation Loss: 0.0000 Acc: 73.2842\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 72.6138\n",
      "validation Loss: 0.0000 Acc: 72.1906\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 72.3264\n",
      "validation Loss: 0.0000 Acc: 73.1719\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 72.3646\n",
      "validation Loss: 0.0000 Acc: 72.3222\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 72.4231\n",
      "validation Loss: 0.0000 Acc: 73.0577\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 72.5035\n",
      "validation Loss: 0.0000 Acc: 72.8680\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 72.4986\n",
      "validation Loss: 0.0000 Acc: 72.6358\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 72.4468\n",
      "validation Loss: 0.0000 Acc: 72.7500\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 72.4802\n",
      "validation Loss: 0.0000 Acc: 72.3029\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 72.4115\n",
      "validation Loss: 0.0000 Acc: 72.1171\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 72.2915\n",
      "validation Loss: 0.0000 Acc: 73.1487\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 72.8470\n",
      "validation Loss: 0.0000 Acc: 72.7654\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 72.4265\n",
      "validation Loss: 0.0000 Acc: 72.6435\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 72.4178\n",
      "validation Loss: 0.0000 Acc: 72.8584\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 72.4851\n",
      "validation Loss: 0.0000 Acc: 72.6455\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 72.7110\n",
      "validation Loss: 0.0000 Acc: 73.1932\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 72.6196\n",
      "validation Loss: 0.0000 Acc: 72.6319\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 72.3501\n",
      "validation Loss: 0.0000 Acc: 72.9938\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 72.6297\n",
      "validation Loss: 0.0000 Acc: 73.2242\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 72.5243\n",
      "validation Loss: 0.0000 Acc: 72.7500\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 72.5528\n",
      "validation Loss: 0.0000 Acc: 72.9416\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 72.6133\n",
      "validation Loss: 0.0000 Acc: 72.6455\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 72.5785\n",
      "validation Loss: 0.0000 Acc: 72.7964\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 72.3602\n",
      "validation Loss: 0.0000 Acc: 73.2319\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 72.6273\n",
      "validation Loss: 0.0000 Acc: 72.5351\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 72.4720\n",
      "validation Loss: 0.0000 Acc: 72.7384\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 72.6689\n",
      "validation Loss: 0.0000 Acc: 72.5913\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 72.4855\n",
      "validation Loss: 0.0000 Acc: 72.4925\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 72.5175\n",
      "validation Loss: 0.0000 Acc: 72.8448\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 72.4981\n",
      "validation Loss: 0.0000 Acc: 72.7887\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 72.5639\n",
      "validation Loss: 0.0000 Acc: 72.5313\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 72.5504\n",
      "validation Loss: 0.0000 Acc: 72.7326\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 72.6331\n",
      "validation Loss: 0.0000 Acc: 72.5138\n",
      "Epoch 73/149\n",
      "training Loss: 0.0000 Acc: 72.4880\n",
      "validation Loss: 0.0000 Acc: 73.0480\n",
      "Epoch 74/149\n",
      "training Loss: 0.0000 Acc: 72.5847\n",
      "validation Loss: 0.0000 Acc: 72.7558\n",
      "Epoch 75/149\n",
      "training Loss: 0.0000 Acc: 72.5364\n",
      "validation Loss: 0.0000 Acc: 72.6280\n",
      "Epoch 76/149\n",
      "training Loss: 0.0000 Acc: 72.4952\n",
      "validation Loss: 0.0000 Acc: 72.7151\n",
      "Epoch 77/149\n",
      "training Loss: 0.0000 Acc: 72.4918\n",
      "validation Loss: 0.0000 Acc: 72.7887\n",
      "Early stopped.\n",
      "Best val acc: 74.159019\n",
      "----------\n",
      "Average best_acc across k-fold: 74.96903228759766\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "if CRITERION == \"NLLLoss\":\n",
    "    train_weights = torch.FloatTensor(\n",
    "        [1.0, np.sum(data_aux.loc[label==0,'eventWeight']) / np.sum(data_aux.loc[label==1,'eventWeight'])]\n",
    "    ).cuda()\n",
    "    criterion = nn.NLLLoss(weight=train_weights)\n",
    "elif CRITERION == \"BCELoss\":\n",
    "    train_weights = torch.FloatTensor(data_aux.loc[:, \"eventWeight\"]).cuda()\n",
    "    criterion = nn.BCELoss(weight=train_weights)\n",
    "else:\n",
    "    raise Exception(f\"CRITERION must be either 'NLLLoss' or 'BCELoss'. You provided {CRITERION}.\")\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json'\n",
    "    best_conf = optimize_hyperparams(\n",
    "        skf, data_list, data_hlf, label, \n",
    "        config_file, epochs=10,\n",
    "        criterion=criterion\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "    # with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "        best_conf = json.load(f)\n",
    "        print(best_conf)\n",
    "\n",
    "NUM_EPOCHS = 150\n",
    "fom = []\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    model_file = OUTPUT_DIRPATH + CURRENT_TIME +'_ReallyTopclassStyle_'+ f'{fold_idx}.torch'\n",
    "    state_file = OUTPUT_DIRPATH + CURRENT_TIME +'_BestPerfReallyTopclass_'+ f'{fold_idx}.torch'\n",
    "\n",
    "    if CRITERION == \"NLLLoss\":\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        sig_train_mask = rectified_train_index & (label == 1)\n",
    "        bkg_train_mask = rectified_train_index & (label == 0)\n",
    "        train_weights = torch.FloatTensor(\n",
    "            [1.0, np.sum(data_aux.loc[bkg_train_mask,'eventWeight']) / np.sum(data_aux.loc[sig_train_mask,'eventWeight'])]\n",
    "        ).cuda()\n",
    "        criterion = nn.NLLLoss(weight=train_weights)\n",
    "    elif CRITERION == \"BCELoss\":\n",
    "        train_weights = torch.FloatTensor((data_aux.iloc[train_index]).loc[:, \"eventWeight\"]).cuda()\n",
    "        criterion = nn.BCELoss(weight=train_weights)\n",
    "        \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], \n",
    "        dnn_input=np.shape(data_hlf)[-1], rnn_input=np.shape(data_list)[-1]\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(data_list[val_index], data_hlf[val_index], label[val_index]), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        NUM_EPOCHS, model, criterion, optimizer, scheduler, \n",
    "        state_file, model_file, data_loader=data_loader\n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC.\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, \n",
    "        save=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, \n",
    "        save=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.5033  |       0.9706      |    0.3543 +/- 0.0094     |\n",
      "|   0.6465  |       0.9500      |    0.2612 +/- 0.0070     |\n",
      "|   0.7618  |       0.9198      |    0.1840 +/- 0.0045     |\n",
      "|   0.9516  |       0.7538      |    0.0457 +/- 0.0015     |\n",
      "|   0.9857  |       0.5777      |    0.0133 +/- 0.0006     |\n",
      "|   0.9955  |       0.3839      |    0.0032 +/- 0.0003     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "    NNtable.add_row(\n",
    "        [\n",
    "            IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "            \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "        ]\n",
    "    )\n",
    "print(NNtable)\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots_all_samples'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plot_train_val_losses(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data',\n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    ")\n",
    "plot_roc(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='arr',\n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    ")\n",
    "plot_output_score(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }\n",
    ")\n",
    "plot_output_score(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    ")\n",
    "s_over_root_b(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimized cut-boundaries for ttH score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(IN_perf, weights, bins=50):\n",
    "    hist_list_fold = []\n",
    "    cut_boundaries_fold = []\n",
    "    cut_s_over_root_bs_fold = []\n",
    "    sig_weights_fold = []\n",
    "    bkg_weights_fold = []\n",
    "    for fold_idx in range(skf.get_n_splits()):\n",
    "        sig_np = np.exp(\n",
    "            IN_perf['all_preds'][fold_idx]\n",
    "        )[\n",
    "            np.array(IN_perf['all_labels'][fold_idx]) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_perf['all_preds'][fold_idx]\n",
    "        )[\n",
    "            np.array(IN_perf['all_labels'][fold_idx]) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "        fold_idx_cuts_bins_inclusive = []\n",
    "        fold_idx_sig_weights = []\n",
    "        fold_idx_bkg_weights = []\n",
    "        fold_idx_prev_s_over_root_b = []\n",
    "        prev_s_over_root_b = 0\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                fold_idx_sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        fold_idx_sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_cuts_bins_inclusive.append(0)\n",
    "        fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "        fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "        cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "        cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "        sig_weights_fold.append(fold_idx_sig_weights)\n",
    "        bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "                \n",
    "    sig_np = np.exp(\n",
    "        IN_perf['mean_pred']\n",
    "    )[\n",
    "        np.array(IN_perf['mean_label']) == 1,1\n",
    "    ]\n",
    "    bkg_np = np.exp(\n",
    "        IN_perf['mean_pred']\n",
    "    )[\n",
    "        np.array(IN_perf['mean_label']) == 0,1\n",
    "    ]\n",
    "    hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "    sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "    bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "\n",
    "    cut_boundaries = []\n",
    "    cut_s_over_root_bs = []\n",
    "    prev_s_over_root_b = 0\n",
    "    sig_weights = []\n",
    "    bkg_weights = []\n",
    "    for i in range(bins):\n",
    "        s = np.sum(sig_hist.values().flatten()[\n",
    "            (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "        ])\n",
    "        sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "            (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "        ]))\n",
    "        if prev_s_over_root_b < (s / sqrt_b):\n",
    "            prev_s_over_root_b = s / sqrt_b\n",
    "            continue\n",
    "        else:\n",
    "            sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            cut_boundaries.append(bins - i)\n",
    "            cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "            prev_s_over_root_b = 0\n",
    "    sig_weights.append(\n",
    "        {\n",
    "            'value': np.sum(sig_hist.values().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]),\n",
    "            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])),\n",
    "        }\n",
    "    )\n",
    "    bkg_weights.append(\n",
    "        {\n",
    "            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]),\n",
    "            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])),\n",
    "        }\n",
    "    )\n",
    "    cut_boundaries.append(0)\n",
    "    cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "    cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "    return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold, cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "\n",
    "(\n",
    "    cut_boundaries_fold, cut_s_over_root_bs_fold, \n",
    "    sig_weights_fold, bkg_weights_fold, \n",
    "    cut_boundaries, cut_s_over_root_bs, \n",
    "    sig_weights, bkg_weights \n",
    ") = optimize_cut_boundaries(\n",
    "    IN_perf, {\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "fold_labels = [\n",
    "    [\n",
    "        f\"s/âb={cut_s_over_root_bs_fold[fold_idx][cut_idx]:.04f}, s={sig_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{sig_weights_fold[fold_idx][cut_idx]['w2']:.04f}, b={bkg_weights_fold[fold_idx][cut_idx]['value']:.04f}Â±{bkg_weights_fold[fold_idx][cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[fold_idx]))\n",
    "    ] for fold_idx in range(skf.get_n_splits())\n",
    "]\n",
    "fold_colors = [copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries_fold[i]) // len(cmap_petroff10)) + 1)) for i in range(skf.get_n_splits())]\n",
    "for fold_idx in range(skf.get_n_splits()):\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_fold{fold_idx}', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "            'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "        }, lines_fold=cut_boundaries_fold, lines_labels=fold_labels, only_fold=fold_idx, lines_colors=fold_colors\n",
    "    )\n",
    "labels = [\n",
    "    f\"s/âb={cut_s_over_root_bs[cut_idx]:.04f}, s={sig_weights[cut_idx]['value']:.04f}Â±{sig_weights[cut_idx]['w2']:.04f}, b={bkg_weights[cut_idx]['value']:.04f}Â±{bkg_weights[cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs))\n",
    "]\n",
    "colors = copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries) // len(cmap_petroff10)) + 1))\n",
    "s_over_root_b(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_foldAvg', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }, lines=cut_boundaries, lines_labels=labels, no_fold=True, lines_colors=colors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train + val comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            data_list[train_index], data_hlf[train_index], label[train_index], \n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            data_list[val_index], data_hlf[val_index], label[val_index], \n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m     val_weights_arr\u001b[38;5;241m.\u001b[39mappend(copy\u001b[38;5;241m.\u001b[39mdeepcopy(weights[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     25\u001b[0m     plot_output_score(\n\u001b[1;32m     26\u001b[0m         [train_IN_dict, val_IN_dict], plot_destdir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mCURRENT_TIME, plot_postfix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_train_val_weighted_comparison\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     27\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIN_arr\u001b[39m\u001b[38;5;124m'\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[labels_arr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_idx), labels_arr[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_idx)], weights\u001b[38;5;241m=\u001b[39mweights\n\u001b[1;32m     28\u001b[0m     )\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mplot_output_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_IN_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_IN_dict\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_destdir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mCURRENT_TIME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_postfix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_train_val_density_comparison\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mIN_arr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlabels_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_arr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbkg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     s_over_root_b(\n\u001b[1;32m     34\u001b[0m         [train_IN_dict, val_IN_dict], plot_destdir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mCURRENT_TIME, plot_postfix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_train_val_comparison\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     35\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIN_arr\u001b[39m\u001b[38;5;124m'\u001b[39m, labels\u001b[38;5;241m=\u001b[39m[labels_arr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_idx), labels_arr[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(fold_idx)], weights\u001b[38;5;241m=\u001b[39mweights\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m labels_arr \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval - fold 0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval - fold 1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval - fold 2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval - fold 3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval - fold 4\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[12], line 182\u001b[0m, in \u001b[0;36mplot_output_score\u001b[0;34m(IN_info, plot_prefix, plot_postfix, method, labels, weights, n_bins)\u001b[0m\n\u001b[1;32m    176\u001b[0m bkg_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\n\u001b[1;32m    177\u001b[0m     IN_info[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_pred\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    178\u001b[0m )[\n\u001b[1;32m    179\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(IN_info[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_label\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    180\u001b[0m ]\n\u001b[1;32m    181\u001b[0m hist_axis \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39maxis\u001b[38;5;241m.\u001b[39mRegular(n_bins, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m1.\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvar\u001b[39m\u001b[38;5;124m'\u001b[39m, growth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, underflow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overflow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 182\u001b[0m sig_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(hist_axis, storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39msig_np, weight\u001b[38;5;241m=\u001b[39mweights[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msig\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msig\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mones_like(sig_np))\n\u001b[1;32m    183\u001b[0m bkg_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(hist_axis, storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39mbkg_np, weight\u001b[38;5;241m=\u001b[39mweights[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbkg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m weights[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msig\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mones_like(bkg_np))\n\u001b[1;32m    184\u001b[0m hep\u001b[38;5;241m.\u001b[39mhistplot(\n\u001b[1;32m    185\u001b[0m     [sig_hist, bkg_hist], w2\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([sig_hist\u001b[38;5;241m.\u001b[39mvariances(), bkg_hist\u001b[38;5;241m.\u001b[39mvariances()]),\n\u001b[1;32m    186\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, density\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m weights[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msig\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m), histtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m     ], linestyle\u001b[38;5;241m=\u001b[39m[linestyles[i], linestyles[i]]\n\u001b[1;32m    191\u001b[0m )\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'r') as f:\n",
    "    IN_perf_dict = json.load(f)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold ']\n",
    "val_weights_arr = []\n",
    "for fold_idx, (train_IN_dict, val_IN_dict, (train_index, val_index)) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'], skf.split(data_hlf, label))):\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )\n",
    "    rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "    rectified_train_index[val_index] = False\n",
    "    sig_train_mask = rectified_train_index & (label == 1)\n",
    "    sig_val_mask = np.logical_not(rectified_train_index) & (label == 1)\n",
    "    bkg_train_mask = rectified_train_index & (label == 0)\n",
    "    bkg_val_mask = np.logical_not(rectified_train_index) & (label == 0)\n",
    "    weights = [\n",
    "        {'sig': data_aux.loc[sig_train_mask, \"eventWeight\"], 'bkg': data_aux.loc[bkg_train_mask, \"eventWeight\"]},\n",
    "        {'sig': data_aux.loc[sig_val_mask, \"eventWeight\"], 'bkg': data_aux.loc[bkg_val_mask, \"eventWeight\"]}\n",
    "    ]\n",
    "    val_weights_arr.append(copy.deepcopy(weights[1]))\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_weighted_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_density_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=[{'sig': None, 'bkg': None}]*len(labels_arr)\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights\n",
    "    )\n",
    "labels_arr = ['val - fold 0', 'val - fold 1', 'val - fold 2', 'val - fold 3', 'val - fold 4']\n",
    "s_over_root_b(\n",
    "    IN_perf_dict['val'], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_val_comparison', \n",
    "    method='IN_arr', labels=labels_arr, weights=val_weights_arr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input Vars (pre-standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison/{VERSION}/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    sig_mask = (label == 1)\n",
    "    sig_test_mask = (label_test == 1)\n",
    "    bkg_mask = (label == 0)\n",
    "    bkg_test_mask = (label_test == 0)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        \n",
    "        sig_train_mask = rectified_train_index & sig_mask\n",
    "        sig_val_mask = np.logical_not(rectified_train_index) & sig_mask\n",
    "        bkg_train_mask = rectified_train_index & bkg_mask\n",
    "        bkg_val_mask = np.logical_not(rectified_train_index) & bkg_mask\n",
    "\n",
    "        sig_train_np = data_df.loc[sig_train_mask, var_name].to_numpy()\n",
    "        sig_val_np = data_df.loc[sig_val_mask, var_name].to_numpy()\n",
    "        sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "        bkg_train_np = data_df.loc[bkg_train_mask, var_name].to_numpy()\n",
    "        bkg_val_np = data_df.loc[bkg_val_mask, var_name].to_numpy()\n",
    "        bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np = data_df.loc[sig_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_df.loc[bkg_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "    sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    pre_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_pre_std, var_name, pre_std_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input Vars (post-standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison/{VERSION}/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = data_list, data_list_test\n",
    "    else:\n",
    "        data, data_test = data_hlf, data_hlf_test\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name,\n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name, index_map)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    post_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_post_std, var_name, post_std_hists[var_name], labels=label_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian smearing on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    index2, index3 = data_list_index_map(var_name)\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear[:, index2, index3] *= rng.normal(size=len(particle_list_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear[:, index2, index3] += rng.normal(size=len(particle_list_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns[var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate smeared variable test-data on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "    gauss_data_list, gauss_data_hlf = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        gauss_data_list = smear_particle_list(var_name, copy.deepcopy(data_list_test))\n",
    "        gauss_data_hlf = data_hlf_test\n",
    "    else:\n",
    "        gauss_data_list = data_list_test\n",
    "        gauss_data_hlf = smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test))\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list, gauss_data_hlf, label_test, OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "IN_perf_smear_list = []\n",
    "label_arr = []\n",
    "for var_name, IN_perf_smear in IN_perf_smear_dict.items():\n",
    "    IN_perf_smear_list.append(IN_perf_smear)\n",
    "    label_arr.append(var_name)\n",
    "sort = np.argsort([IN_perf_smear['mean_area'] for IN_perf_smear in IN_perf_smear_list])\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear', \n",
    "    method='IN_arr', labels=label_arr, yscale='log', run2=False, sort=sort\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison/{VERSION}/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = smear_particle_list(var_name, data_list), smear_particle_list(var_name, data_list_test)\n",
    "    else:\n",
    "        data, data_test = smear_particle_hlf(var_name, data_hlf), smear_particle_hlf(var_name, data_hlf_test)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name, \n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_gauss_smear, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name, index_map)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    gauss_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_gauss_smear, var_name, gauss_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y in [('train', data_list, data_hlf, label), ('test', data_list_test, data_hlf_test, label_test)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'r') as f:\n",
    "    IN_full_eval_dict = json.load(f)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {'mass': [], 'dijet_mass': []}\n",
    "for var_name in hist_dict.keys():\n",
    "    for i, score_cut in enumerate(score_cuts):\n",
    "        sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict)\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "        hist_dict[var_name].extend(\n",
    "            [\n",
    "                copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "            ]\n",
    "        )\n",
    "    for mod_factor, label_mod in enumerate(['sig_train', 'sig_test', 'bkg_train', 'bkg_test']):\n",
    "        plot_list = []\n",
    "        label_list = []\n",
    "        for i in range(len(hist_dict[var_name])):\n",
    "            if (i - mod_factor) % 4 == 0:\n",
    "                plot_list.append(hist_dict[var_name][i])\n",
    "                label_list.append(label_arr[i])\n",
    "        make_input_plot(\n",
    "            plot_destdir, var_name, plot_list, labels=label_list, density=True, \n",
    "            plot_prefix=CURRENT_TIME+'_', plot_postfix='_'+label_mod, alpha=0.5,\n",
    "            linestyle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna-hhbbgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
