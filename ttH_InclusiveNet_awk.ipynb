{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu1.fnal.gov      Wed Aug 21 22:15:40 2024  555.42.02\n",
      "[0] Tesla P100-PCIE-12GB | 42Â°C,   0 % |     0 / 12288 MB |\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "SIGNAL_FILEPATHS = [lpc_fileprefix+\"/Run3_2022preEE_merged/GluGluToHH/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/GluGluToHH/nominal/*\"]\n",
    "SIGNAL_FILEPATHS = [\n",
    "    lpc_fileprefix+\"/Run3_2022preEE_merged/GluGluToHH/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/GluGluToHH/nominal/*\",\n",
    "    # lpc_fileprefix+\"/Run3_2022preEE_merged/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [lpc_fileprefix+\"/Run3_2022preEE_merged/ttHToGG/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/ttHToGG/nominal/*\"]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v4'\n",
    "if VERSION == 'v1':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v2':\n",
    "    # CRITERION == \"BCELoss\"\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v3':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v4':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    # N_PARTICLES, N_PARTICLE_FIELDS = 6, 7\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v5':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "# VARS = 'base_vars'\n",
    "# CURRENT_TIME = '2024-08-10_10-29-50'\n",
    "# CURRENT_TIME = '2024-08-17_18-23-49'\n",
    "VARS = 'extra_vars'\n",
    "# CURRENT_TIME = '2024-08-10_13-16-12'\n",
    "# CURRENT_TIME = '2024-08-17_11-45-34'\n",
    "CURRENT_TIME = '2024-08-20_23-02-48'\n",
    "# VARS = 'extra_vars_no_dijet_mass'\n",
    "# CURRENT_TIME = '2024-08-21_15-28-02'\n",
    "OUTPUT_DIRPATH = CURRENT_DIRPATH + f\"/model_outputs/{VERSION}/{VARS}/\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False\n",
    "PRE_STD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (258332, 4, 6)\n",
      "Data HLF: (258332, 14)\n",
      "Data list test: (258924, 4, 6)\n",
      "Data HLF test: (258924, 14)\n"
     ]
    }
   ],
   "source": [
    "if PRE_STD:\n",
    "    (\n",
    "        data_df, data_test_df, \n",
    "        data_list, data_hlf, label, \n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        high_level_fields, input_hlf_vars, hlf_vars_columns,\n",
    "        data_aux, data_test_aux\n",
    "    ) = process_data(\n",
    "        4, 6, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED, return_pre_std=True\n",
    "    )\n",
    "else:\n",
    "    (\n",
    "        data_list, data_hlf, label, \n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        high_level_fields, input_hlf_vars, hlf_vars_columns,\n",
    "        data_aux, data_test_aux\n",
    "    ) = process_data(\n",
    "        4, 6, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED\n",
    "    )\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dotted']\n",
    "        linestyles = linestyles * ((2*len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:2*len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx if fold_idx%2 == 0 else fold_idx+1]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx+1 if fold_idx%2 == 0 else fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='std', \n",
    "    labels=None, yscale='linear', run2=True, sort=None\n",
    "):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            IN_info['mean_fprs'], IN_info['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.plot(\n",
    "                IN_info['fprs'][fold_idx], IN_info['base_tpr'],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            IN_info['mean_fprs'], IN_info['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for i in index_arr:\n",
    "            plt.plot(\n",
    "                IN_info[i]['mean_fprs'], IN_info[i]['base_tpr'], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, weights=None):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.hist(np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['all_labels']) == 0,1\n",
    "        ], bins=60, label='ttH background', histtype='step', alpha=0.5, density=True)\n",
    "        plt.hist(np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['all_labels']) == 1,1\n",
    "        ], bins=60, label='HH signal', histtype='step', alpha=0.5, density=True)\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.hist(np.exp(\n",
    "                    IN_info['all_preds'][fold_idx]\n",
    "                )[\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 0,1\n",
    "                ], bins=60, label='ttH background'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                histtype='step', alpha=0.5, density=True if weights is None else False, weights=weights['bkg'],\n",
    "                linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            plt.hist(\n",
    "                np.exp(\n",
    "                    IN_info['all_preds'][fold_idx]\n",
    "                )[\n",
    "                    np.array(IN_info['all_labels'][fold_idx]) == 1,1\n",
    "                ], bins=60, label='HH signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                histtype='step', alpha=0.5, density=True if weights is None else False, weights=weights['sig'], \n",
    "                linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "        plt.hist(np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ], bins=60, label='ttH background - avg. over folds', histtype='step', alpha=0.8, \n",
    "            density=True if weights is None else False, weights=weights['bkg']\n",
    "        )\n",
    "        plt.hist(np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ], bins=60, label='HH signal - avg. over folds', histtype='step', alpha=0.8, \n",
    "            density=True if weights is None else False, weights=weights['sig']\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            plt.hist(\n",
    "                np.exp(\n",
    "                    IN_info[i]['all_preds'][0]\n",
    "                )[\n",
    "                    np.array(IN_info[i]['all_labels'][0]) == 0, 1\n",
    "                ], bins=60, \n",
    "                label='ttH background'+(' - '+labels[i] if labels is not None else ''), \n",
    "                histtype='step', linestyle=linestyles[i], alpha=0.75, density=True\n",
    "            )\n",
    "            plt.hist(\n",
    "                np.exp(\n",
    "                    IN_info[i]['all_preds'][0]\n",
    "                )[\n",
    "                    np.array(IN_info[i]['all_labels'][0]) == 1, 1\n",
    "                ], bins=60, \n",
    "                label='HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                histtype='step', linestyle=linestyles[i], alpha=0.75, density=True\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def s_over_root_b(IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, weights=None):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        plt.hist(sig_np / np.sqrt(bkg_np), bins=60, label='$\\frac{s}{\\sqrt{b}}', histtype='step', alpha=0.5, density=True)\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(50, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            plt.bar(\n",
    "                np.arange(0., 1., 0.02), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "                label='s / âb'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                histtype='step', alpha=0.5, density=True if weights is None else False,\n",
    "                linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(50, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        plt.bar(\n",
    "            np.arange(0., 1., 0.02), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "            label='s / âb'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "            histtype='step', alpha=0.5, density=True if weights is None else False,\n",
    "            linestyle=linestyles[fold_idx], \n",
    "        )\n",
    "\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            plt.hist(\n",
    "                np.exp(\n",
    "                    IN_info[i]['all_preds'][0]\n",
    "                )[\n",
    "                    np.array(IN_info[i]['all_labels'][0]) == 0, 1\n",
    "                ], bins=60, \n",
    "                label='ttH background'+(' - '+labels[i] if labels is not None else ''), \n",
    "                histtype='step', linestyle=linestyles[i], alpha=0.75, density=True\n",
    "            )\n",
    "            plt.hist(\n",
    "                np.exp(\n",
    "                    IN_info[i]['all_preds'][0]\n",
    "                )[\n",
    "                    np.array(IN_info[i]['all_labels'][0]) == 1, 1\n",
    "                ], bins=60, \n",
    "                label='HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                histtype='step', linestyle=linestyles[i], alpha=0.75, density=True\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -4., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -4., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'jet1_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'jet2_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(40, -4., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'lepton1_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -10., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -10., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -4., 4., name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -4., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -4., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -4., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, var_name, index_map, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label == 1\n",
    "    sig_test_mask = label_test == 1\n",
    "    bkg_mask = label == 0\n",
    "    bkg_test_mask = label_test == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "            index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2, index3]\n",
    "            sig_val_np = data[sig_val_mask, index2, index3]\n",
    "            sig_test_np = data_test[sig_test_mask, index2, index3]\n",
    "            bkg_train_np = data[bkg_train_mask, index2, index3]\n",
    "            bkg_val_np = data[bkg_val_mask, index2, index3]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2, index3]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns[var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "            index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[sig_mask, index2, index3]\n",
    "            sig_test_np = data_test[sig_test_mask, index2, index3]\n",
    "            bkg_train_np = data[bkg_mask, index2, index3]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2, index3]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns[var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict):\n",
    "    sig_train_mask = (label == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux.loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux.loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir = output_dir + \"fold/\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n",
      "Epoch 0/149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha) (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706619781071/work/torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0001 Acc: 55.8116\n",
      "validation Loss: 0.0000 Acc: 64.3622\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 68.1697\n",
      "validation Loss: 0.0000 Acc: 73.4008\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 70.2054\n",
      "validation Loss: 0.0000 Acc: 74.0356\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 70.3027\n",
      "validation Loss: 0.0000 Acc: 70.4395\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 70.7822\n",
      "validation Loss: 0.0000 Acc: 71.8118\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 71.2462\n",
      "validation Loss: 0.0000 Acc: 72.7389\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 71.7504\n",
      "validation Loss: 0.0000 Acc: 73.9950\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 71.5220\n",
      "validation Loss: 0.0000 Acc: 69.6208\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 72.1167\n",
      "validation Loss: 0.0000 Acc: 73.4840\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 71.8525\n",
      "validation Loss: 0.0000 Acc: 70.1376\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 71.5569\n",
      "validation Loss: 0.0000 Acc: 72.1253\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 72.3349\n",
      "validation Loss: 0.0000 Acc: 76.0582\n",
      "Saving..\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 72.7109\n",
      "validation Loss: 0.0000 Acc: 75.6634\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 72.8299\n",
      "validation Loss: 0.0000 Acc: 74.7789\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 72.6964\n",
      "validation Loss: 0.0000 Acc: 73.6331\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 72.9456\n",
      "validation Loss: 0.0000 Acc: 74.1247\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 73.3109\n",
      "validation Loss: 0.0000 Acc: 71.5292\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 73.0196\n",
      "validation Loss: 0.0000 Acc: 75.3131\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 73.1396\n",
      "validation Loss: 0.0000 Acc: 72.9595\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 72.9112\n",
      "validation Loss: 0.0000 Acc: 75.2918\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 72.8309\n",
      "validation Loss: 0.0000 Acc: 75.4447\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 72.9441\n",
      "validation Loss: 0.0000 Acc: 71.4479\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 73.0216\n",
      "validation Loss: 0.0000 Acc: 73.9486\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 73.3041\n",
      "validation Loss: 0.0000 Acc: 75.0595\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 73.3622\n",
      "validation Loss: 0.0000 Acc: 71.3841\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 73.1532\n",
      "validation Loss: 0.0000 Acc: 74.2621\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 73.2103\n",
      "validation Loss: 0.0000 Acc: 75.9189\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 73.2974\n",
      "validation Loss: 0.0000 Acc: 76.2266\n",
      "Saving..\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 73.6041\n",
      "validation Loss: 0.0000 Acc: 72.7021\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 73.2896\n",
      "validation Loss: 0.0000 Acc: 74.5621\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 73.5282\n",
      "validation Loss: 0.0000 Acc: 71.7982\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 73.5548\n",
      "validation Loss: 0.0000 Acc: 74.9434\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 73.5877\n",
      "validation Loss: 0.0000 Acc: 73.6137\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 73.8756\n",
      "validation Loss: 0.0000 Acc: 74.6705\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 73.7691\n",
      "validation Loss: 0.0000 Acc: 75.3460\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 74.0169\n",
      "validation Loss: 0.0000 Acc: 72.9886\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 73.6816\n",
      "validation Loss: 0.0000 Acc: 74.3627\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 73.8049\n",
      "validation Loss: 0.0000 Acc: 75.3731\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 73.5732\n",
      "validation Loss: 0.0000 Acc: 73.6737\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 73.6153\n",
      "validation Loss: 0.0000 Acc: 74.0511\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 73.9187\n",
      "validation Loss: 0.0000 Acc: 75.8627\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 73.8921\n",
      "validation Loss: 0.0000 Acc: 75.2937\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 73.7174\n",
      "validation Loss: 0.0000 Acc: 75.1989\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 73.6501\n",
      "validation Loss: 0.0000 Acc: 74.9840\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 73.8974\n",
      "validation Loss: 0.0000 Acc: 73.5479\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 73.7991\n",
      "validation Loss: 0.0000 Acc: 72.1002\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 73.8045\n",
      "validation Loss: 0.0000 Acc: 73.4318\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 73.7735\n",
      "validation Loss: 0.0000 Acc: 75.0479\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 73.6429\n",
      "validation Loss: 0.0000 Acc: 75.2918\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 73.7977\n",
      "validation Loss: 0.0000 Acc: 75.1447\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 74.1790\n",
      "validation Loss: 0.0000 Acc: 74.4692\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 73.9109\n",
      "validation Loss: 0.0000 Acc: 74.8253\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 73.6583\n",
      "validation Loss: 0.0000 Acc: 75.1737\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 74.0222\n",
      "validation Loss: 0.0000 Acc: 74.6492\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 73.7982\n",
      "validation Loss: 0.0000 Acc: 73.7802\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 74.0237\n",
      "validation Loss: 0.0000 Acc: 75.0131\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 73.9961\n",
      "validation Loss: 0.0000 Acc: 74.9105\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 74.1417\n",
      "validation Loss: 0.0000 Acc: 75.4040\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 73.9637\n",
      "validation Loss: 0.0000 Acc: 74.0240\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 73.9366\n",
      "validation Loss: 0.0000 Acc: 74.5524\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 73.9772\n",
      "validation Loss: 0.0000 Acc: 74.8408\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 74.0140\n",
      "validation Loss: 0.0000 Acc: 74.3047\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 74.0987\n",
      "validation Loss: 0.0000 Acc: 74.7227\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 74.1906\n",
      "validation Loss: 0.0000 Acc: 74.5737\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 74.0943\n",
      "validation Loss: 0.0000 Acc: 75.2550\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 74.1253\n",
      "validation Loss: 0.0000 Acc: 75.1156\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 74.1533\n",
      "validation Loss: 0.0000 Acc: 74.7150\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 73.9046\n",
      "validation Loss: 0.0000 Acc: 75.6673\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 74.4137\n",
      "validation Loss: 0.0000 Acc: 75.2202\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 74.0338\n",
      "validation Loss: 0.0000 Acc: 75.2860\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 74.2138\n",
      "validation Loss: 0.0000 Acc: 75.0731\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 74.0943\n",
      "validation Loss: 0.0000 Acc: 74.9279\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 74.1204\n",
      "validation Loss: 0.0000 Acc: 75.1040\n",
      "Early stopped.\n",
      "Best val acc: 76.226601\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 55.5813\n",
      "validation Loss: 0.0000 Acc: 68.2583\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 69.2352\n",
      "validation Loss: 0.0000 Acc: 69.1796\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 70.2780\n",
      "validation Loss: 0.0000 Acc: 68.3938\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 70.6593\n",
      "validation Loss: 0.0000 Acc: 68.9434\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 71.2912\n",
      "validation Loss: 0.0000 Acc: 71.4208\n",
      "Saving..\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 71.9324\n",
      "validation Loss: 0.0000 Acc: 71.5002\n",
      "Saving..\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 71.9333\n",
      "validation Loss: 0.0000 Acc: 70.4686\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 71.8632\n",
      "validation Loss: 0.0000 Acc: 70.9892\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 72.2517\n",
      "validation Loss: 0.0000 Acc: 75.8879\n",
      "Saving..\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 72.2106\n",
      "validation Loss: 0.0000 Acc: 70.0002\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 72.5508\n",
      "validation Loss: 0.0000 Acc: 70.2770\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 72.4743\n",
      "validation Loss: 0.0000 Acc: 75.0556\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 72.7211\n",
      "validation Loss: 0.0000 Acc: 72.7989\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 72.4811\n",
      "validation Loss: 0.0000 Acc: 72.6711\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 72.5928\n",
      "validation Loss: 0.0000 Acc: 76.7763\n",
      "Saving..\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 73.0308\n",
      "validation Loss: 0.0000 Acc: 69.8841\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 72.9083\n",
      "validation Loss: 0.0000 Acc: 74.9202\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 73.0274\n",
      "validation Loss: 0.0000 Acc: 71.2331\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 72.9432\n",
      "validation Loss: 0.0000 Acc: 73.8169\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 73.0022\n",
      "validation Loss: 0.0000 Acc: 69.9479\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 72.8551\n",
      "validation Loss: 0.0000 Acc: 74.3821\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 72.9529\n",
      "validation Loss: 0.0000 Acc: 73.4976\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 72.6287\n",
      "validation Loss: 0.0000 Acc: 75.5763\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 73.4653\n",
      "validation Loss: 0.0000 Acc: 72.6982\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 73.1299\n",
      "validation Loss: 0.0000 Acc: 74.6995\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 73.5659\n",
      "validation Loss: 0.0000 Acc: 74.8485\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 73.6816\n",
      "validation Loss: 0.0000 Acc: 73.8944\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 73.4909\n",
      "validation Loss: 0.0000 Acc: 74.7769\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 73.5538\n",
      "validation Loss: 0.0000 Acc: 76.1298\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 73.6070\n",
      "validation Loss: 0.0000 Acc: 73.9718\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 74.1345\n",
      "validation Loss: 0.0000 Acc: 73.4744\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 73.7556\n",
      "validation Loss: 0.0000 Acc: 74.4614\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 73.9574\n",
      "validation Loss: 0.0000 Acc: 73.9524\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 73.7188\n",
      "validation Loss: 0.0000 Acc: 75.5124\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 73.8025\n",
      "validation Loss: 0.0000 Acc: 72.8163\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 73.9912\n",
      "validation Loss: 0.0000 Acc: 73.9563\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 74.0232\n",
      "validation Loss: 0.0000 Acc: 75.2453\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 73.8379\n",
      "validation Loss: 0.0000 Acc: 74.5176\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 74.0382\n",
      "validation Loss: 0.0000 Acc: 73.7802\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 74.1137\n",
      "validation Loss: 0.0000 Acc: 74.6414\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 74.0459\n",
      "validation Loss: 0.0000 Acc: 73.8944\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 74.0580\n",
      "validation Loss: 0.0000 Acc: 74.4363\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 74.2791\n",
      "validation Loss: 0.0000 Acc: 73.0002\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 74.0362\n",
      "validation Loss: 0.0000 Acc: 74.1576\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 73.9961\n",
      "validation Loss: 0.0000 Acc: 74.9898\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 74.0638\n",
      "validation Loss: 0.0000 Acc: 74.2215\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 73.8921\n",
      "validation Loss: 0.0000 Acc: 74.4266\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 74.1983\n",
      "validation Loss: 0.0000 Acc: 74.2389\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 74.0445\n",
      "validation Loss: 0.0000 Acc: 73.8169\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 74.0866\n",
      "validation Loss: 0.0000 Acc: 74.7440\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 74.2816\n",
      "validation Loss: 0.0000 Acc: 74.8311\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 74.1916\n",
      "validation Loss: 0.0000 Acc: 74.3879\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 74.2472\n",
      "validation Loss: 0.0000 Acc: 73.6815\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 74.0774\n",
      "validation Loss: 0.0000 Acc: 75.2937\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 74.2133\n",
      "validation Loss: 0.0000 Acc: 75.4389\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 74.2632\n",
      "validation Loss: 0.0000 Acc: 74.5195\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 74.2608\n",
      "validation Loss: 0.0000 Acc: 74.8911\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 74.2245\n",
      "validation Loss: 0.0000 Acc: 73.6505\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 74.2341\n",
      "validation Loss: 0.0000 Acc: 74.1479\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 74.0706\n",
      "validation Loss: 0.0000 Acc: 74.3821\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 74.1979\n",
      "validation Loss: 0.0000 Acc: 74.8795\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 74.3082\n",
      "validation Loss: 0.0000 Acc: 73.2963\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 74.0648\n",
      "validation Loss: 0.0000 Acc: 74.4769\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 74.2003\n",
      "validation Loss: 0.0000 Acc: 75.0363\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 74.2433\n",
      "validation Loss: 0.0000 Acc: 75.1814\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 74.3019\n",
      "validation Loss: 0.0000 Acc: 74.3879\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 74.3551\n",
      "validation Loss: 0.0000 Acc: 74.9705\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 74.0730\n",
      "validation Loss: 0.0000 Acc: 75.2318\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 74.3256\n",
      "validation Loss: 0.0000 Acc: 74.5427\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 74.2583\n",
      "validation Loss: 0.0000 Acc: 73.7569\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 74.1645\n",
      "validation Loss: 0.0000 Acc: 74.7924\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 74.4509\n",
      "validation Loss: 0.0000 Acc: 74.5563\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 74.3440\n",
      "validation Loss: 0.0000 Acc: 74.6473\n",
      "Epoch 73/149\n",
      "training Loss: 0.0000 Acc: 74.2608\n",
      "validation Loss: 0.0000 Acc: 74.7344\n",
      "Epoch 74/149\n",
      "training Loss: 0.0000 Acc: 74.3391\n",
      "validation Loss: 0.0000 Acc: 75.0653\n",
      "Epoch 75/149\n",
      "training Loss: 0.0000 Acc: 74.2821\n",
      "validation Loss: 0.0000 Acc: 74.9937\n",
      "Epoch 76/149\n",
      "training Loss: 0.0000 Acc: 74.4625\n",
      "validation Loss: 0.0000 Acc: 74.7324\n",
      "Epoch 77/149\n",
      "training Loss: 0.0000 Acc: 74.2840\n",
      "validation Loss: 0.0000 Acc: 74.5931\n",
      "Epoch 78/149\n",
      "training Loss: 0.0000 Acc: 74.2767\n",
      "validation Loss: 0.0000 Acc: 74.4789\n",
      "Epoch 79/149\n",
      "training Loss: 0.0000 Acc: 74.2922\n",
      "validation Loss: 0.0000 Acc: 74.5776\n",
      "Epoch 80/149\n",
      "training Loss: 0.0000 Acc: 74.3125\n",
      "validation Loss: 0.0000 Acc: 75.0092\n",
      "Epoch 81/149\n",
      "training Loss: 0.0000 Acc: 74.3837\n",
      "validation Loss: 0.0000 Acc: 74.9492\n",
      "Epoch 82/149\n",
      "training Loss: 0.0000 Acc: 74.2298\n",
      "validation Loss: 0.0000 Acc: 74.8253\n",
      "Epoch 83/149\n",
      "training Loss: 0.0000 Acc: 74.3648\n",
      "validation Loss: 0.0000 Acc: 74.8098\n",
      "Epoch 84/149\n",
      "training Loss: 0.0000 Acc: 74.4698\n",
      "validation Loss: 0.0000 Acc: 74.3956\n",
      "Epoch 85/149\n",
      "training Loss: 0.0000 Acc: 74.2579\n",
      "validation Loss: 0.0000 Acc: 74.4518\n",
      "Epoch 86/149\n",
      "training Loss: 0.0000 Acc: 74.1867\n",
      "validation Loss: 0.0000 Acc: 74.8331\n",
      "Epoch 87/149\n",
      "training Loss: 0.0000 Acc: 74.3682\n",
      "validation Loss: 0.0000 Acc: 74.6589\n",
      "Epoch 88/149\n",
      "training Loss: 0.0000 Acc: 74.3498\n",
      "validation Loss: 0.0000 Acc: 74.6434\n",
      "Epoch 89/149\n",
      "training Loss: 0.0000 Acc: 74.2501\n",
      "validation Loss: 0.0000 Acc: 75.0092\n",
      "Epoch 90/149\n",
      "training Loss: 0.0000 Acc: 74.4093\n",
      "validation Loss: 0.0000 Acc: 74.7382\n",
      "Epoch 91/149\n",
      "training Loss: 0.0000 Acc: 74.2995\n",
      "validation Loss: 0.0000 Acc: 74.7963\n",
      "Epoch 92/149\n",
      "training Loss: 0.0000 Acc: 74.3319\n",
      "validation Loss: 0.0000 Acc: 74.8214\n",
      "Epoch 93/149\n",
      "training Loss: 0.0000 Acc: 74.4040\n",
      "validation Loss: 0.0000 Acc: 74.6666\n",
      "Epoch 94/149\n",
      "training Loss: 0.0000 Acc: 74.3730\n",
      "validation Loss: 0.0000 Acc: 74.7053\n",
      "Epoch 95/149\n",
      "training Loss: 0.0000 Acc: 74.3169\n",
      "validation Loss: 0.0000 Acc: 74.7634\n",
      "Epoch 96/149\n",
      "training Loss: 0.0000 Acc: 74.3919\n",
      "validation Loss: 0.0000 Acc: 74.7073\n",
      "Epoch 97/149\n",
      "training Loss: 0.0000 Acc: 74.3092\n",
      "validation Loss: 0.0000 Acc: 74.6763\n",
      "Epoch 98/149\n",
      "training Loss: 0.0000 Acc: 74.3483\n",
      "validation Loss: 0.0000 Acc: 74.7150\n",
      "Early stopped.\n",
      "Best val acc: 76.776276\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 47.5821\n",
      "validation Loss: 0.0001 Acc: 52.1736\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 67.0744\n",
      "validation Loss: 0.0000 Acc: 70.3364\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 70.9889\n",
      "validation Loss: 0.0000 Acc: 69.8603\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 71.1070\n",
      "validation Loss: 0.0000 Acc: 73.4390\n",
      "Saving..\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 71.6020\n",
      "validation Loss: 0.0000 Acc: 71.9796\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 71.5730\n",
      "validation Loss: 0.0000 Acc: 76.9713\n",
      "Saving..\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 72.1439\n",
      "validation Loss: 0.0000 Acc: 71.2499\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 72.1831\n",
      "validation Loss: 0.0000 Acc: 71.5654\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 72.2867\n",
      "validation Loss: 0.0000 Acc: 67.3751\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 72.3718\n",
      "validation Loss: 0.0000 Acc: 74.1939\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 72.6685\n",
      "validation Loss: 0.0000 Acc: 73.8029\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 72.6385\n",
      "validation Loss: 0.0000 Acc: 74.8926\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 72.6249\n",
      "validation Loss: 0.0000 Acc: 72.3745\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 72.5136\n",
      "validation Loss: 0.0000 Acc: 73.8939\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 72.8925\n",
      "validation Loss: 0.0000 Acc: 74.8790\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 72.9191\n",
      "validation Loss: 0.0000 Acc: 72.3319\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 73.1364\n",
      "validation Loss: 0.0000 Acc: 75.3203\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 73.5801\n",
      "validation Loss: 0.0000 Acc: 74.1106\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 73.3159\n",
      "validation Loss: 0.0000 Acc: 73.7468\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 73.0381\n",
      "validation Loss: 0.0000 Acc: 73.5145\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 73.1243\n",
      "validation Loss: 0.0000 Acc: 74.6177\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 73.2559\n",
      "validation Loss: 0.0000 Acc: 76.1158\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 73.3807\n",
      "validation Loss: 0.0000 Acc: 70.4564\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 73.3483\n",
      "validation Loss: 0.0000 Acc: 73.9868\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 73.8259\n",
      "validation Loss: 0.0000 Acc: 75.1558\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 73.7364\n",
      "validation Loss: 0.0000 Acc: 72.1422\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 73.7209\n",
      "validation Loss: 0.0000 Acc: 71.9448\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 73.5839\n",
      "validation Loss: 0.0000 Acc: 72.4325\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 73.6730\n",
      "validation Loss: 0.0000 Acc: 76.3403\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 73.6435\n",
      "validation Loss: 0.0000 Acc: 72.7867\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 73.8133\n",
      "validation Loss: 0.0000 Acc: 75.7694\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 74.1191\n",
      "validation Loss: 0.0000 Acc: 73.5126\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 73.8685\n",
      "validation Loss: 0.0000 Acc: 73.2222\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 73.7393\n",
      "validation Loss: 0.0000 Acc: 72.8022\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 73.8230\n",
      "validation Loss: 0.0000 Acc: 75.0068\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 73.9977\n",
      "validation Loss: 0.0000 Acc: 73.0558\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 73.8414\n",
      "validation Loss: 0.0000 Acc: 73.6558\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 73.7601\n",
      "validation Loss: 0.0000 Acc: 75.2777\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 74.0572\n",
      "validation Loss: 0.0000 Acc: 74.2016\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 73.9919\n",
      "validation Loss: 0.0000 Acc: 74.3545\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 74.0272\n",
      "validation Loss: 0.0000 Acc: 74.8055\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 74.0818\n",
      "validation Loss: 0.0000 Acc: 73.8493\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 74.1917\n",
      "validation Loss: 0.0000 Acc: 74.7668\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 73.9662\n",
      "validation Loss: 0.0000 Acc: 74.4184\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 74.1351\n",
      "validation Loss: 0.0000 Acc: 73.9964\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 74.2773\n",
      "validation Loss: 0.0000 Acc: 73.7022\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 74.2081\n",
      "validation Loss: 0.0000 Acc: 74.8848\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 74.3272\n",
      "validation Loss: 0.0000 Acc: 75.4171\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 74.1888\n",
      "validation Loss: 0.0000 Acc: 74.1861\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 74.3969\n",
      "validation Loss: 0.0000 Acc: 74.1068\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 74.1186\n",
      "validation Loss: 0.0000 Acc: 75.5874\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 74.1433\n",
      "validation Loss: 0.0000 Acc: 74.5035\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 74.4143\n",
      "validation Loss: 0.0000 Acc: 73.8203\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 74.2328\n",
      "validation Loss: 0.0000 Acc: 74.1222\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 74.3819\n",
      "validation Loss: 0.0000 Acc: 74.2093\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 74.2865\n",
      "validation Loss: 0.0000 Acc: 74.0506\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 74.3025\n",
      "validation Loss: 0.0000 Acc: 74.4048\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 74.3678\n",
      "validation Loss: 0.0000 Acc: 73.8280\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 74.3910\n",
      "validation Loss: 0.0000 Acc: 73.6422\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 74.3712\n",
      "validation Loss: 0.0000 Acc: 73.8087\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 74.2556\n",
      "validation Loss: 0.0000 Acc: 74.5171\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 74.3988\n",
      "validation Loss: 0.0000 Acc: 74.5635\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 74.2730\n",
      "validation Loss: 0.0000 Acc: 74.4397\n",
      "Early stopped.\n",
      "Best val acc: 76.971313\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 49.6811\n",
      "validation Loss: 0.0000 Acc: 59.8750\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 68.0083\n",
      "validation Loss: 0.0000 Acc: 67.3015\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 70.0575\n",
      "validation Loss: 0.0000 Acc: 70.1641\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 70.5070\n",
      "validation Loss: 0.0000 Acc: 70.6403\n",
      "Saving..\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 70.9168\n",
      "validation Loss: 0.0000 Acc: 68.1473\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 71.5909\n",
      "validation Loss: 0.0000 Acc: 69.3880\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 71.3151\n",
      "validation Loss: 0.0000 Acc: 73.9635\n",
      "Saving..\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 71.5831\n",
      "validation Loss: 0.0000 Acc: 73.2919\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 71.6194\n",
      "validation Loss: 0.0000 Acc: 72.7538\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 72.3089\n",
      "validation Loss: 0.0000 Acc: 73.6558\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 72.2852\n",
      "validation Loss: 0.0000 Acc: 74.0429\n",
      "Saving..\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 72.2296\n",
      "validation Loss: 0.0000 Acc: 67.4564\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 72.2291\n",
      "validation Loss: 0.0000 Acc: 72.3880\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 72.1807\n",
      "validation Loss: 0.0000 Acc: 74.5190\n",
      "Saving..\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 72.4265\n",
      "validation Loss: 0.0000 Acc: 70.8048\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 72.8891\n",
      "validation Loss: 0.0000 Acc: 74.6216\n",
      "Saving..\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 72.9322\n",
      "validation Loss: 0.0000 Acc: 72.2448\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 72.9312\n",
      "validation Loss: 0.0000 Acc: 74.6100\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 72.9181\n",
      "validation Loss: 0.0000 Acc: 74.2190\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 72.8180\n",
      "validation Loss: 0.0000 Acc: 72.3396\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 72.5635\n",
      "validation Loss: 0.0000 Acc: 75.7036\n",
      "Saving..\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 73.3043\n",
      "validation Loss: 0.0000 Acc: 75.0571\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 73.0793\n",
      "validation Loss: 0.0000 Acc: 73.3713\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 72.8649\n",
      "validation Loss: 0.0000 Acc: 76.3287\n",
      "Saving..\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 73.1930\n",
      "validation Loss: 0.0000 Acc: 73.1332\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 73.1189\n",
      "validation Loss: 0.0000 Acc: 74.1242\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 73.2975\n",
      "validation Loss: 0.0000 Acc: 75.1577\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 73.0885\n",
      "validation Loss: 0.0000 Acc: 72.4771\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 73.2462\n",
      "validation Loss: 0.0000 Acc: 74.3564\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 72.9893\n",
      "validation Loss: 0.0000 Acc: 69.5080\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 73.0362\n",
      "validation Loss: 0.0000 Acc: 74.7629\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 72.8736\n",
      "validation Loss: 0.0000 Acc: 73.6442\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 73.0594\n",
      "validation Loss: 0.0000 Acc: 72.2854\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 73.1760\n",
      "validation Loss: 0.0000 Acc: 73.0364\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 73.0101\n",
      "validation Loss: 0.0000 Acc: 74.6990\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 73.1141\n",
      "validation Loss: 0.0000 Acc: 75.0668\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 73.5138\n",
      "validation Loss: 0.0000 Acc: 73.6945\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 73.4746\n",
      "validation Loss: 0.0000 Acc: 74.2287\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 73.6231\n",
      "validation Loss: 0.0000 Acc: 74.3177\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 73.7843\n",
      "validation Loss: 0.0000 Acc: 74.6777\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 73.3967\n",
      "validation Loss: 0.0000 Acc: 74.1339\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 73.4436\n",
      "validation Loss: 0.0000 Acc: 74.8635\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 73.6817\n",
      "validation Loss: 0.0000 Acc: 74.1300\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 73.3914\n",
      "validation Loss: 0.0000 Acc: 75.9997\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 73.7877\n",
      "validation Loss: 0.0000 Acc: 74.5693\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 73.6715\n",
      "validation Loss: 0.0000 Acc: 73.7816\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 73.8602\n",
      "validation Loss: 0.0000 Acc: 73.8029\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 73.7020\n",
      "validation Loss: 0.0000 Acc: 72.9126\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 73.6212\n",
      "validation Loss: 0.0000 Acc: 74.4590\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 73.8385\n",
      "validation Loss: 0.0000 Acc: 74.7745\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 73.5399\n",
      "validation Loss: 0.0000 Acc: 75.4771\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 73.9227\n",
      "validation Loss: 0.0000 Acc: 73.9055\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 73.9130\n",
      "validation Loss: 0.0000 Acc: 73.7971\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 73.8467\n",
      "validation Loss: 0.0000 Acc: 75.6048\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 73.8496\n",
      "validation Loss: 0.0000 Acc: 75.5894\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 74.0422\n",
      "validation Loss: 0.0000 Acc: 75.1558\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 74.1017\n",
      "validation Loss: 0.0000 Acc: 73.5513\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 73.7664\n",
      "validation Loss: 0.0000 Acc: 74.3081\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 74.0756\n",
      "validation Loss: 0.0000 Acc: 73.9422\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 74.1477\n",
      "validation Loss: 0.0000 Acc: 72.0667\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 73.9101\n",
      "validation Loss: 0.0000 Acc: 73.3480\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 73.9328\n",
      "validation Loss: 0.0000 Acc: 73.2067\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 73.8128\n",
      "validation Loss: 0.0000 Acc: 75.6532\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 74.1757\n",
      "validation Loss: 0.0000 Acc: 73.9887\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 73.9836\n",
      "validation Loss: 0.0000 Acc: 74.4648\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 74.1893\n",
      "validation Loss: 0.0000 Acc: 73.8145\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 74.0543\n",
      "validation Loss: 0.0000 Acc: 74.4648\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 73.9798\n",
      "validation Loss: 0.0000 Acc: 74.9816\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 74.1080\n",
      "validation Loss: 0.0000 Acc: 74.0526\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 73.9696\n",
      "validation Loss: 0.0000 Acc: 74.5984\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 74.1322\n",
      "validation Loss: 0.0000 Acc: 74.7203\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 74.1912\n",
      "validation Loss: 0.0000 Acc: 74.7281\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 74.0427\n",
      "validation Loss: 0.0000 Acc: 74.3661\n",
      "Epoch 73/149\n",
      "training Loss: 0.0000 Acc: 73.9062\n",
      "validation Loss: 0.0000 Acc: 74.3235\n",
      "Epoch 74/149\n",
      "training Loss: 0.0000 Acc: 74.0954\n",
      "validation Loss: 0.0000 Acc: 74.3874\n",
      "Epoch 75/149\n",
      "training Loss: 0.0000 Acc: 74.2227\n",
      "validation Loss: 0.0000 Acc: 74.3835\n",
      "Epoch 76/149\n",
      "training Loss: 0.0000 Acc: 74.0247\n",
      "validation Loss: 0.0000 Acc: 74.2074\n",
      "Epoch 77/149\n",
      "training Loss: 0.0000 Acc: 73.9096\n",
      "validation Loss: 0.0000 Acc: 74.2055\n",
      "Epoch 78/149\n",
      "training Loss: 0.0000 Acc: 74.2135\n",
      "validation Loss: 0.0000 Acc: 74.1977\n",
      "Epoch 79/149\n",
      "training Loss: 0.0000 Acc: 74.1162\n",
      "validation Loss: 0.0000 Acc: 74.3352\n",
      "Epoch 80/149\n",
      "training Loss: 0.0000 Acc: 74.0310\n",
      "validation Loss: 0.0000 Acc: 74.5016\n",
      "Epoch 81/149\n",
      "training Loss: 0.0000 Acc: 74.2759\n",
      "validation Loss: 0.0000 Acc: 74.5732\n",
      "Epoch 82/149\n",
      "training Loss: 0.0000 Acc: 74.1728\n",
      "validation Loss: 0.0000 Acc: 74.4997\n",
      "Epoch 83/149\n",
      "training Loss: 0.0000 Acc: 74.0920\n",
      "validation Loss: 0.0000 Acc: 74.2093\n",
      "Epoch 84/149\n",
      "training Loss: 0.0000 Acc: 74.1341\n",
      "validation Loss: 0.0000 Acc: 74.1184\n",
      "Epoch 85/149\n",
      "training Loss: 0.0000 Acc: 74.0504\n",
      "validation Loss: 0.0000 Acc: 74.3526\n",
      "Epoch 86/149\n",
      "training Loss: 0.0000 Acc: 74.0581\n",
      "validation Loss: 0.0000 Acc: 74.1745\n",
      "Epoch 87/149\n",
      "training Loss: 0.0000 Acc: 74.0649\n",
      "validation Loss: 0.0000 Acc: 74.2790\n",
      "Early stopped.\n",
      "Best val acc: 76.328728\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 52.1044\n",
      "validation Loss: 0.0000 Acc: 70.2938\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 68.7699\n",
      "validation Loss: 0.0000 Acc: 66.4441\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 69.6994\n",
      "validation Loss: 0.0000 Acc: 67.3325\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 70.7876\n",
      "validation Loss: 0.0000 Acc: 69.3570\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 70.8109\n",
      "validation Loss: 0.0000 Acc: 73.6519\n",
      "Saving..\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 71.6102\n",
      "validation Loss: 0.0000 Acc: 74.1532\n",
      "Saving..\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 71.4046\n",
      "validation Loss: 0.0000 Acc: 68.8906\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 71.7888\n",
      "validation Loss: 0.0000 Acc: 70.5667\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 71.7375\n",
      "validation Loss: 0.0000 Acc: 76.0326\n",
      "Saving..\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 72.3230\n",
      "validation Loss: 0.0000 Acc: 65.9660\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 71.9872\n",
      "validation Loss: 0.0000 Acc: 73.9771\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 72.1817\n",
      "validation Loss: 0.0000 Acc: 78.1307\n",
      "Saving..\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 72.3254\n",
      "validation Loss: 0.0000 Acc: 71.2732\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 72.1255\n",
      "validation Loss: 0.0000 Acc: 72.3029\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 72.3360\n",
      "validation Loss: 0.0000 Acc: 73.0209\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 72.4918\n",
      "validation Loss: 0.0000 Acc: 74.9913\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 72.7962\n",
      "validation Loss: 0.0000 Acc: 73.9093\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 72.9196\n",
      "validation Loss: 0.0000 Acc: 72.6706\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 72.7618\n",
      "validation Loss: 0.0000 Acc: 75.7248\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 72.7691\n",
      "validation Loss: 0.0000 Acc: 74.4087\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 73.0856\n",
      "validation Loss: 0.0000 Acc: 71.4938\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 72.8635\n",
      "validation Loss: 0.0000 Acc: 74.6952\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 73.4185\n",
      "validation Loss: 0.0000 Acc: 71.9448\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 73.3551\n",
      "validation Loss: 0.0000 Acc: 74.0932\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 73.4315\n",
      "validation Loss: 0.0000 Acc: 73.9984\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 73.2685\n",
      "validation Loss: 0.0000 Acc: 73.9113\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 73.5147\n",
      "validation Loss: 0.0000 Acc: 73.2338\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 73.4678\n",
      "validation Loss: 0.0000 Acc: 75.3939\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 73.4891\n",
      "validation Loss: 0.0000 Acc: 74.0700\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 73.4620\n",
      "validation Loss: 0.0000 Acc: 73.7545\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 73.4639\n",
      "validation Loss: 0.0000 Acc: 72.6087\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 73.4964\n",
      "validation Loss: 0.0000 Acc: 74.9777\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 73.7664\n",
      "validation Loss: 0.0000 Acc: 72.3725\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 73.5235\n",
      "validation Loss: 0.0000 Acc: 73.6500\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 73.8210\n",
      "validation Loss: 0.0000 Acc: 73.5300\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 73.6560\n",
      "validation Loss: 0.0000 Acc: 74.2113\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 73.7107\n",
      "validation Loss: 0.0000 Acc: 73.3306\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 73.6623\n",
      "validation Loss: 0.0000 Acc: 74.1300\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 73.8201\n",
      "validation Loss: 0.0000 Acc: 74.8558\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 73.9004\n",
      "validation Loss: 0.0000 Acc: 75.0803\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 74.0339\n",
      "validation Loss: 0.0000 Acc: 74.2771\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 74.0044\n",
      "validation Loss: 0.0000 Acc: 74.5945\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 73.9309\n",
      "validation Loss: 0.0000 Acc: 73.6577\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 73.8864\n",
      "validation Loss: 0.0000 Acc: 73.4932\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 73.8099\n",
      "validation Loss: 0.0000 Acc: 74.3081\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 73.9018\n",
      "validation Loss: 0.0000 Acc: 73.9906\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 73.8864\n",
      "validation Loss: 0.0000 Acc: 74.8248\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 73.9270\n",
      "validation Loss: 0.0000 Acc: 74.4087\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 73.8525\n",
      "validation Loss: 0.0000 Acc: 73.8803\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 74.1114\n",
      "validation Loss: 0.0000 Acc: 73.4313\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 73.9260\n",
      "validation Loss: 0.0000 Acc: 74.7010\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 74.2067\n",
      "validation Loss: 0.0000 Acc: 73.9422\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 74.0238\n",
      "validation Loss: 0.0000 Acc: 73.7874\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 73.8883\n",
      "validation Loss: 0.0000 Acc: 73.9964\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 74.0504\n",
      "validation Loss: 0.0000 Acc: 74.8055\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 73.9856\n",
      "validation Loss: 0.0000 Acc: 74.1493\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 74.0122\n",
      "validation Loss: 0.0000 Acc: 74.1939\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 73.9038\n",
      "validation Loss: 0.0000 Acc: 74.3332\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 74.0877\n",
      "validation Loss: 0.0000 Acc: 74.2829\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 73.9139\n",
      "validation Loss: 0.0000 Acc: 74.5074\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 74.0281\n",
      "validation Loss: 0.0000 Acc: 74.6855\n",
      "Early stopped.\n",
      "Best val acc: 78.130684\n",
      "----------\n",
      "Average best_acc across k-fold: 76.88671875\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "if CRITERION == \"NLLLoss\":\n",
    "    train_weights = torch.FloatTensor(\n",
    "        [1.0, np.sum(data_aux.loc[label==0,'eventWeight']) / np.sum(data_aux.loc[label==1,'eventWeight'])]\n",
    "    ).cuda()\n",
    "    criterion = nn.NLLLoss(weight=train_weights)\n",
    "elif CRITERION == \"BCELoss\":\n",
    "    train_weights = torch.FloatTensor(data_aux.loc[:, \"eventWeight\"]).cuda()\n",
    "    criterion = nn.BCELoss(weight=train_weights)\n",
    "else:\n",
    "    raise Exception(f\"CRITERION must be either 'NLLLoss' or 'BCELoss'. You provided {CRITERION}.\")\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json'\n",
    "    best_conf = optimize_hyperparams(\n",
    "        skf, data_list, data_hlf, label, \n",
    "        config_file, epochs=10,\n",
    "        criterion=criterion\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "    # with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "        best_conf = json.load(f)\n",
    "        print(best_conf)\n",
    "\n",
    "NUM_EPOCHS = 150\n",
    "fom = []\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    model_file = OUTPUT_DIRPATH + CURRENT_TIME +'_ReallyTopclassStyle_'+ f'{fold_idx}.torch'\n",
    "    state_file = OUTPUT_DIRPATH + CURRENT_TIME +'_BestPerfReallyTopclass_'+ f'{fold_idx}.torch'\n",
    "\n",
    "    if CRITERION == \"NLLLoss\":\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        sig_train_mask = rectified_train_index & (label == 1)\n",
    "        bkg_train_mask = rectified_train_index & (label == 0)\n",
    "        train_weights = torch.FloatTensor(\n",
    "            [1.0, np.sum(data_aux.loc[bkg_train_mask,'eventWeight']) / np.sum(data_aux.loc[sig_train_mask,'eventWeight'])]\n",
    "        ).cuda()\n",
    "        criterion = nn.NLLLoss(weight=train_weights)\n",
    "    elif CRITERION == \"BCELoss\":\n",
    "        train_weights = torch.FloatTensor((data_aux.iloc[train_index]).loc[:, \"eventWeight\"]).cuda()\n",
    "        criterion = nn.BCELoss(weight=train_weights)\n",
    "        \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], \n",
    "        dnn_input=np.shape(data_hlf)[-1], rnn_input=np.shape(data_list)[-1]\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(data_list[val_index], data_hlf[val_index], label[val_index]), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        NUM_EPOCHS, model, criterion, optimizer, scheduler, \n",
    "        state_file, model_file, data_loader=data_loader\n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC.\n"
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, save=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, save=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Rectangle.set() got an unexpected keyword argument 'histtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 36\u001b[0m\n\u001b[1;32m     20\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(plot_destdir)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# plot_train_val_losses(\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#     IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data',\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#     labels=[str(i) for i in range(len(IN_perf['all_preds']))]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43ms_over_root_b\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mIN_perf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_destdir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mCURRENT_TIME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_postfix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_test_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mIN_perf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall_preds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbkg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test_aux\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_test\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meventWeight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test_aux\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_test\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meventWeight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 211\u001b[0m, in \u001b[0;36ms_over_root_b\u001b[0;34m(IN_info, plot_prefix, plot_postfix, method, labels, weights)\u001b[0m\n\u001b[1;32m    209\u001b[0m     sig_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(hist_axis, storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39msig_np, weight\u001b[38;5;241m=\u001b[39mweights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msig\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    210\u001b[0m     bkg_hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mHist(hist_axis, storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfill(var\u001b[38;5;241m=\u001b[39mbkg_np, weight\u001b[38;5;241m=\u001b[39mweights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbkg\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 211\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msig_hist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbkg_hist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms / âb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m - \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhisttype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlinestyle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinestyles\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfold_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m sig_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\n\u001b[1;32m    218\u001b[0m     IN_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_pred\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    219\u001b[0m )[\n\u001b[1;32m    220\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(IN_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_label\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    221\u001b[0m ]\n\u001b[1;32m    222\u001b[0m bkg_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\n\u001b[1;32m    223\u001b[0m     IN_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_pred\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    224\u001b[0m )[\n\u001b[1;32m    225\u001b[0m     np\u001b[38;5;241m.\u001b[39marray(IN_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_label\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    226\u001b[0m ]\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/matplotlib/pyplot.py:2739\u001b[0m, in \u001b[0;36mbar\u001b[0;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[1;32m   2728\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mbar)\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbar\u001b[39m(\n\u001b[1;32m   2730\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2737\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2738\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BarContainer:\n\u001b[0;32m-> 2739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2743\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbottom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2747\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/matplotlib/axes/_axes.py:2528\u001b[0m, in \u001b[0;36mAxes.bar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l, b, w, h, c, e, lw, htch, lbl \u001b[38;5;129;01min\u001b[39;00m args:\n\u001b[1;32m   2520\u001b[0m     r \u001b[38;5;241m=\u001b[39m mpatches\u001b[38;5;241m.\u001b[39mRectangle(\n\u001b[1;32m   2521\u001b[0m         xy\u001b[38;5;241m=\u001b[39m(l, b), width\u001b[38;5;241m=\u001b[39mw, height\u001b[38;5;241m=\u001b[39mh,\n\u001b[1;32m   2522\u001b[0m         facecolor\u001b[38;5;241m=\u001b[39mc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2526\u001b[0m         hatch\u001b[38;5;241m=\u001b[39mhtch,\n\u001b[1;32m   2527\u001b[0m         )\n\u001b[0;32m-> 2528\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2529\u001b[0m     r\u001b[38;5;241m.\u001b[39mget_path()\u001b[38;5;241m.\u001b[39m_interpolation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m   2530\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m orientation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/matplotlib/artist.py:1219\u001b[0m, in \u001b[0;36mArtist._internal_update\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, kwargs):\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m    Update artist properties without prenormalizing them, but generating\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;124;03m    errors as if calling `set`.\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \n\u001b[1;32m   1217\u001b[0m \u001b[38;5;124;03m    The lack of prenormalization is to maintain backcompatibility.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_props\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{cls.__name__}\u001b[39;49;00m\u001b[38;5;124;43m.set() got an unexpected keyword argument \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{prop_name!r}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/matplotlib/artist.py:1193\u001b[0m, in \u001b[0;36mArtist._update_props\u001b[0;34m(self, props, errfmt)\u001b[0m\n\u001b[1;32m   1191\u001b[0m             func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1192\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[0;32m-> 1193\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1194\u001b[0m                     errfmt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m), prop_name\u001b[38;5;241m=\u001b[39mk))\n\u001b[1;32m   1195\u001b[0m             ret\u001b[38;5;241m.\u001b[39mappend(func(v))\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n",
      "\u001b[0;31mAttributeError\u001b[0m: Rectangle.set() got an unexpected keyword argument 'histtype'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwIAAAJXCAYAAAAgiONIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA54ElEQVR4nO39f5BV9Z0n/j8bGgFxELSNwG38rQjOxOhkR9EkRpcf2UxZkqSiW1bGrFHJOF3ZGZ3KGmvNKGjGGndna0yqzc5qdiemdtesY7KsmSlkrTGaGVlF0WQDKoIrpLvligJNBKQbON8//HZ/0tJA3+5Ld+t5PKq6pO77nPd5ndtv7z3Pfp8fDUVRFAEAAEplzEgXAAAADD9BAAAASkgQAACAEhIEAACghAQBAAAoIUEAAABKSBAAAIASEgQAAKCEBAEAACihIQeBu+++Ow0NDdm3b1/N63Z1deXOO+/M2WefnYkTJ6ZSqeSGG25IR0fHUMsCAAAOoaEoimKwKxdFkfPPPz8vvvhi9u7dm7Fjxw543e7u7syfPz9PPvlkkmTKlCnZvn17kmTatGl55plnctJJJw22NAAA4BAGPSOwb9++LF26NC+++OKg1r/nnnvy5JNPprm5Oc8//3y2bduWjRs3Zv78+dm8eXOuu+66wZYGAAAcRs0zAj/5yU/yyCOP5Kc//Wlef/313tdrmRHo7u7OjBkz8tZbb2XlypW58MILe9u2bduWOXPmZPPmzXnxxRdz7rnn1lIeAAAwADXPCDzyyCP5m7/5mz4hoFYrV67MW2+9lbPPPrtPCEiSqVOn5oorrkjyXugAAADqr+YgcNddd+WXv/xl789grFy5MkmycOHCftt7Xu9ZDgAAqK/GWleoVCqpVCpD2uj69euTJGeccUa/7aeffnqSZMOGDUPaDgAA0L8ReY7Ali1bkrx3p6D+HHfccUmSarU6XCUBAECp1DwjUA+7du1K8t71AP3peb1nuf5MmjQp7777bsaOHZsTTjhh0LU0NDQMel0AAHi/IdydP1u2bMm+ffsyYcKE7Ny5s45VHWhEgkDPm3OwN6nn7kOHekjZu+++m/3792f//v0eQAYAwIfKu+++e8S3MSJBYNKkSUneu1Vof3pmAnqW68/YsWOzf//+jBkzJtOmTRt0LfWaEahWqznxxBPr0tdQqaWvoijS0dGRGTNmjIoZoNHwnvRQy4GMl4NTS1/GysGp5UDGy8F9GGsZyozA5s2bs3///poe1DtYIxIEmpqakqT3ScLv9+abb/ZZrj8nnHBCOjo6Mm3atLS3t9e9xlrNmTMna9euHekykqjl/Xbs2JFjjz02L730UiZPnjyitSSj4z3poZYDGS8Hp5a+jJWDU8uBjJeDU0tflUolHR0dQzr1faBG5GLhM888M0mybt26ftvXrFnTZzkAAKC+RiQIzJ07N0myYsWKftsfe+yxJDngYWMAAEB9jFgQaGpqyrp16/L000/3adu6dWseffTRJOl9wjAAAFBfRzQIdHR0ZPbs2Zk9e3ZWrVrV+/q4ceNy0003JUmuuuqqrF69OkmycePGXHXVVdm8eXMWLFiQj33sY0eyPAAAKK0jerFwd3d3Xn755SQHPhPg61//elasWJEnn3wyv/u7v5upU6f23kVo+vTpeeCBB45kaQAAUGojcmpQ8t6swIoVK7JkyZKceeaZ2bVrV6ZPn54bbrghq1evzsyZM0eqtEFpaWkZ6RJ6qWV0G03viVpGv9H0vqhldBtN74laRr/R9L6oZeQ0FEO50ekI6rm10owZM0bF7UMZvXpu2dbZ2TkqbtnG6Ga8MFDGCrUwXhio4TzGHbEZAQAAYOQIAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACU0Ac2CDQ0NPT5LxzM+PHjc/vtt2f8+PEjXQofAMYLA2WsUAvjhYEazmPcD+wDxZqbm9Pe3p5KpZK2traRLgcAAIZsOI9xP7AzAgAAwOAJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAl1DjSBQxVtVrNnDlz+m1raWlJS0vLMFcEAAAH19ramtbW1n7bqtXqsNXhycIAADBKeLIwAABwRAkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACTWOdAFDVa1WM2fOnH7bWlpa0tLSMswVAQDAwbW2tqa1tbXftmq1Omx1NBRFUQzb1uqoubk57e3tqVQqaWtrG+lyAABgyIbzGNepQQAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKNI13AUFWr1cyZM6fftpaWlrS0tAxzRQAAcHCtra1pbW3tt61arQ5bHQ1FURTDtrU6am5uTnt7eyqVStra2ka6HAAAGLLhPMZ1ahAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKDCgJvvPFGFi9enObm5kycODGzZs3K0qVL09XVVXNfe/bsyZIlS3LhhRdm8uTJOeecc3L99dfnjTfeGExpAADAADQURVHUssKmTZtywQUXZPPmzUmSKVOmZPv27UmST33qU3n88cczbty4AfXV2dmZiy++OGvWrEmSfOQjH8nbb7+dffv2ZerUqVm+fHl+7/d+r991m5ub097enkqlkra2tlp2AQAARqXhPMateUbguuuuy+bNm7NgwYJs2rQp27Zty6pVq1KpVPLUU0/lnnvuGXBf/+bf/JusWbMmn/jEJ/L666+nWq2ms7MzN954Y7Zt25brrrsu3d3dtZYIAAAcRk0zAi+88ELOP//8TJs2LWvXrs3UqVN721auXJmLLrooJ5xwQjo6OtLY2HjIvrq7uzNp0qQ0NDTktddeS6VS6W3bv39/zj333Pzyl7/Mk08+mU996lMHrG9GAACAD5tROyPw6KOPJkkWLVrUJwQkydy5czNr1qxs2bIlzzzzzGH7evnll9Pd3Z1Zs2b1CQFJMmbMmHz6059OkvziF7+opUQAAGAAagoCK1euTJIsXLiw3/ae13uWO5SdO3cmSfbt29dv+969e5Mku3btqqVEAABgAGoKAuvXr0+SnHHGGf22n3766UmSDRs2HLav2bNnZ/z48XnllVfyyiuv9Gnbs2dPVqxYkSQ599xzaykRAAAYgEOfyP8+W7ZsSfLenYL6c9xxxyVJqtXqYfs69thj86d/+qf58z//8yxatCj33Xdf/tk/+2fZuHFjbrnllrz22mv5xCc+kXnz5h2yn6IosmPHjlp2o4/x48dn/Pjxg14fAAB67NmzJ3v27Bn0+jXe0HNIagoCPafpvP/6gB49rw/0dJ4777wz77zzTr797W/nsssu69N2ySWX5Mc//nHGjh17yD46Ojpy7LHHDmh7/bn99ttzxx13DHp9AADocffdd2fJkiUjXcaA1BQEehwsqfQctB/svP/3e+6557J8+fIkSUNDQ0488cS8/fbb6e7uzs9//vM8+uijueaaaw7Zx4wZM/LSSy/VUH1fZgMAAKiXW2+9NTfffPOg1589e3Y6OjrqWNHB1RQEjj766HR2dmbbtm055phjDmjvmQmYNGnSYftat25dFixYkF//+tdZunRpbrrpphxzzDHZu3dvHnnkkbS0tOTLX/5yjjrqqPzLf/kvD9pPQ0NDJk+eXMtuAADAETHU084bGhrqWM2h1XSxcFNTU5L0Pkn4/d58880+yx3KX/zFX6SzszN//Md/nG9+85u9waKxsTFXXXVVvve97yVJbrvttlpKBAAABqCmIHDmmWcmee+v+f1Zs2ZNn+UO5bnnnkuSfP7zn++3/fd///czfvz4bNiw4aDBAwAAGJyagsDcuXOTpPfWnu/32GOPJUkuvPDCw/bVczrP4aY/GhsbM2HChFrKBAAADqOmIHD55ZcnSZYtW5atW7f2aXv66afz6quvpqmpKRdddNFh+zrvvPOSJI888ki/7X/3d3+XPXv25JxzzhEEAACgzmoKAuedd17mz5+farWaq6++Om1tbSmKIs8//3yuvPLKJMnNN9+ccePG9a7T0dGR2bNnZ/bs2Vm1alXv6zfeeGMmTpyYe++9N9/61rd6nzS8d+/ePPTQQ7nuuut6+wMAAOqroajxqQWbNm3KBRdckM2bNyd57+FiPefwX3rppVmxYkUaG/+/mxFt3Lgxp5xySpLkpz/9aS655JLetgcffDCLFy/Onj170tDQkGnTpuWtt95Kd3d3kvfCwn333ddvHc3NzWlvb0+lUklbW1stuwAAAKPScB7j1jQjkCQnnXRSVq9eneuvvz7Tp0/P7t27c9ZZZ2Xp0qVZvnx5nxBwONdcc03Wrl2ba6+9Nr/zO7+Tzs7OnHzyybniiivyxBNPHDQEAAAAQ1PzjMBoYUYAAIAPm1E9IwAAAHzwCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACXUONIFDFW1Ws2cOXP6bWtpaUlLS8swVwQAAAfX2tqa1tbWftuq1eqw1dFQFEUxbFuro+bm5rS3t6dSqaStrW2kywEAgCEbzmNcpwYBAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJNY50AUNVrVYzZ86cfttaWlrS0tIyzBUBAMDBtba2prW1td+2arU6bHU0FEVRDNvW6qi5uTnt7e2pVCppa2sb6XIAAGDIhvMY16lBAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQo0jXcBQVavVzJkzp9+2lpaWtLS0DHNFAABwcK2trWltbe23rVqtDlsdDUVRFMO2tTpqbm5Oe3t7KpVK2traRrocAAAYsuE8xnVqEAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACU0KCCwBtvvJHFixenubk5EydOzKxZs7J06dJ0dXUNqoh/+Id/yGc/+9mccMIJOf744zNv3rw8+eSTg+oLAAA4vIaiKIpaVti0aVMuuOCCbN68OUkyZcqUbN++PUnyqU99Ko8//njGjRs34P6+/e1v50/+5E9SFEUmTpyYsWPH5p133klDQ0Puv//+XHfddf2u19zcnPb29lQqlbS1tdWyCwAAMCoN5zFuzTMC1113XTZv3pwFCxZk06ZN2bZtW1atWpVKpZKnnnoq99xzz4D7+j//5//k5ptvTmNjY37wgx/k7bffTmdnZ7773e8mSf7kT/7EQT4AABwBNQWBF154IY8//nimTZuWhx56KDNnzkySfPzjH8/DDz+cJLn33nuzd+/eAfV3xx13ZN++ffnOd76TL33pS5k4cWLGjBmTP/zDP8w111yTd955J//jf/yPGncJAAA4nJqCwKOPPpokWbRoUaZOndqnbe7cuZk1a1a2bNmSZ5555rB9vfnmm1mxYkWmTJmSr3zlKwe0L168OJ/+9KezdevWWkoEAAAGoLGWhVeuXJkkWbhwYb/tCxcuzCuvvJKVK1fm4osvPmRfjz/+eIqiyOWXX97vNQUXXXRRnnjiiVrKAwAABqimGYH169cnSc4444x+208//fQkyYYNGw7b19q1a5MkH/3oR2spAQAAqIOaZgS2bNmS5L07BfXnuOOOS5JUq9XD9vXaa68lSU444YQ8++yz+au/+qusXLkyO3fuzHnnnZevfOUrueqqqw7bT1EU2bFjxwD34EDjx4/P+PHjB70+AAD02LNnT/bs2TPo9Wu8oeeQ1BQEdu3alSQHXB/Qo+f1nuUOpefg/Wc/+1luvPHG7N69O8cdd1x2796dFStWZMWKFfn7v//7fP/73z9kPx0dHTn22GNr2Y0+br/99txxxx2DXh8AAHrcfffdWbJkyUiXMSA1BYEeB0sqY8eOTZLs27fvsH28++67SZLvfe97mTdvXlpbW3PWWWdl3759+V//63/l+uuvz4MPPpjPfvazh5wZmDFjRl566aVB7MV7zAYAAFAvt956a26++eZBrz979ux0dHTUsaKDqykIHH300ens7My2bdtyzDHHHNDeMxMwadKkw/bVM3tw2mmnZdmyZTn66KOTvBcmPve5z6WzszPXXntt7rnnnkMGgYaGhkyePLmW3QAAgCNiqKedNzQ01LGaQ6vpYuGmpqYk6X2S8Pu9+eabfZY7lGnTpiVJrrrqqt4Q8JuuvPLKNDQ0ZO3atQOaYQAAAAaupiBw5plnJknWrVvXb/uaNWv6LHcoJ554YpKkUqn023700UdnypQpeffddw8aPAAAgMGpKQjMnTs3SbJixYp+2x977LEkyYUXXnjYvmbNmpXk4KGi5xSkpqamHH/88bWUCQAAHEZNQeDyyy9PkixbtuyAJ/4+/fTTefXVV9PU1JSLLrrosH3Nnz8/Rx11VP7bf/tv/T49+IEHHkiSfPzjH6+lRAAAYABqCgLnnXde5s+fn2q1mquvvjptbW0piiLPP/98rrzyyiTJzTff3OdJwR0dHZk9e3Zmz56dVatW9b4+ZcqU3HDDDXnrrbfyL/7Fv+h9wFhXV1fuv//+fPOb38zYsWPzrW99qx77CQAA/Iaabx/6wAMP5IILLshjjz2WmTNnZsqUKb3n8F966aX5+te/3mf57u7uvPzyy0kOfL7A3XffnX/8x3/Ms88+m3POOSfHH398fv3rX6erqyuNjY35d//u3+X8888f5K4BAAAHU9OMQJKcdNJJWb16da6//vpMnz49u3fvzllnnZWlS5dm+fLlaWwceLb4rd/6rTz99NO57bbbctZZZ2Xnzp2ZOXNmvvjFL2blypW56aabai0PAAAYgIZiOJ9jXEfNzc1pb29PpVJJW1vbSJcDAABDNpzHuDXPCAAAAB98ggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAk1jnQBQ1WtVjNnzpx+21paWtLS0jLMFQEAwMG1tramtbW137ZqtTpsdTQURVEM29bqqLm5Oe3t7alUKmlraxvpcgAAYMiG8xjXqUEAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCjSNdwFBVq9XMmTOn37aWlpa0tLQMc0UAAHBwra2taW1t7betWq0OWx0NRVEUw7a1Ompubk57e3sqlUra2tpGuhwAABiy4TzGdWoQAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUUONIFzBU1Wo1c+bM6betpaUlLS0tw1wRAAAcXGtra1pbW/ttq1arw1ZHQ1EUxbBtrY6am5vT3t6eSqWStra2kS4HAACGbDiPcZ0aBAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACU0KCCwBtvvJHFixenubk5EydOzKxZs7J06dJ0dXUNuaCdO3fm1FNPzcyZM4fcFwAA0L+ag8CmTZty/vnn5/777097e3smTJiQdevW5fbbb8/8+fPT3d09pIK++c1v5vXXXx9SHwAAwKHVHASuu+66bN68OQsWLMimTZuybdu2rFq1KpVKJU899VTuueeeQRezatWqfPvb3x70+gAAwMDUFAReeOGFPP7445k2bVoeeuih3tN3Pv7xj+fhhx9Oktx7773Zu3dvzYV0d3fn+uuvz759+2peFwAAqE1NQeDRRx9NkixatChTp07t0zZ37tzMmjUrW7ZsyTPPPFNzIffcc09+8Ytf5Nprr615XQAAoDY1BYGVK1cmSRYuXNhve8/rPcsN1CuvvJI777wzc+bMyTe+8Y2a1gUAAGpXUxBYv359kuSMM87ot/30009PkmzYsGHAfRZFkRtuuCFdXV25//77M378+FpKAgAABqGmILBly5YkyZQpU/ptP+6445Ik1Wp1wH3+9V//dX72s5/lxhtvzEUXXVRLOQAAwCA11rLwrl27kuSA6wN69Lzes9zhtLe355ZbbkmlUsndd99dSym9iqLIjh07BrVukowfP94sBAAAdbFnz57s2bNn0OsXRVHHag6tpiDQ42AFjh07NkkGfOeflpaW7NixIw8++GAmT548mFLS0dGRY489dlDrJsntt9+eO+64Y9DrAwBAj7vvvjtLliwZ6TIGpKYgcPTRR6ezszPbtm3LMcccc0B7z0zApEmTDtvX3/7t32bZsmX5whe+kCuuuKKWMvqYMWNGXnrppUGvbzYAAIB6ufXWW3PzzTcPev3Zs2eno6OjjhUdXE1BoKmpKZ2dndm+fXvvMwR+05tvvtm73KF0dXXla1/7Wo499th85zvfqaWEAzQ0NAx6NgEAAOppqKedNzQ01LGaQ6vpYuEzzzwzSbJu3bp+29esWdNnuYPZvXt3Nm/enM7OzsyYMSMNDQ29P6ecckqSpK2trfe1ZcuW1VImAABwGDXNCMydOzfLly/PihUr8oUvfOGA9sceeyxJcuGFFx6ynzFjxhz0FqTd3d3ZuHFjxo4dm1NPPTXJwE41AgAABq6hqOHS5BdeeCHnn39+TjzxxKxdu7b3dqFJ8vTTT+fiiy9OU1NTOjo6Mm7cuEEVtHHjxpxyyilpbm7Or371q4Mu19zcnPb29lQqlbS1tQ1qWwAAMJoM5zFuTacGnXfeeZk/f36q1WquvvrqtLW1pSiKPP/887nyyiuTJDfffHOfENDR0ZHZs2dn9uzZWbVqVX2rBwAABqXm24c+8MADueCCC/LYY49l5syZmTJlSrZv354kufTSS/P1r3+9z/Ld3d15+eWXkwz8+QIAAMCRVdOMQJKcdNJJWb16da6//vpMnz49u3fvzllnnZWlS5dm+fLlaWwc1KMJAACAYTSoo/bp06fn/vvvH9CyJ598ck1PSKt1eQAAoHY1zwgAAAAffIIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCjSNdwFBVq9XMmTOn37aWlpa0tLQMc0UAAHBwra2taW1t7betWq0OWx0NRVEUw7a1Ompubk57e3sqlUra2tpGuhwAABiy4TzGdWoQAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUUONIFzBU1Wo1c+bM6betpaUlLS0tw1wRAAAcXGtra1pbW/ttq1arw1ZHQ1EUxbBtrY6am5vT3t6eSqWStra2kS4HAACGbDiPcZ0aBAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJdQ40gUMVbVazZw5c/pta2lpSUtLyzBXBAAAB9fa2prW1tZ+26rV6rDV0VAURTFsW6uj5ubmtLe3p1KppK2tbaTLAQCAIRvOY1ynBgEAQAkJAgAAUEKCAAAAlJAgAAAAJSQIAABACQkCAABQQoIAAACUkCAAAAAlJAgAAEAJCQIAAFBCggAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJTSoIPDGG29k8eLFaW5uzsSJEzNr1qwsXbo0XV1dNfe1c+fOfOMb38jcuXMzZcqUnHrqqfnc5z6XJ598cjClAQAAA9BQFEVRywqbNm3KBRdckM2bNydJpkyZku3btydJPvWpT+Xxxx/PuHHjBtTXxo0bc9lll+W1115LkjQ1NaWzszPd3d1paGjIv/23/zZ33nlnv+s2Nzenvb09lUolbW1ttewCAACMSsN5jFvzjMB1112XzZs3Z8GCBdm0aVO2bduWVatWpVKp5Kmnnso999wz4L5uu+22vPbaa5k7d242bNiQLVu25J133sn999+fo48+OnfddVcef/zxWksEAAAOo6YZgRdeeCHnn39+pk2blrVr12bq1Km9bStXrsxFF12UE044IR0dHWlsbDxkX5s2bcqpp56asWPH5v/9v/+XSqXSp/273/1u/uiP/igXX3xx/vEf//GA9c0IAADwYTNqZwQeffTRJMmiRYv6hIAkmTt3bmbNmpUtW7bkmWeeOWxfL7/8cvbv35/LLrvsgBCQJNdcc03GjBmTF198MTWevQQAABxGTUFg5cqVSZKFCxf2297zes9yh/L6668nSU455ZR+2ydNmpTJkydn586deeutt2opEwAAOIxDn7/zPuvXr0+SnHHGGf22n3766UmSDRs2HLavefPmZfny5Tn11FMPuq3t27dnwoQJaWpqqqVMAADgMGoKAlu2bEny3p2C+nPcccclSarV6mH7Ou2003Laaaf121YURW655ZYk780yNDQ01FImAABwGDUFgV27diXJAdcH9Oh5vWe5wdi5c2e++tWv5kc/+lEaGxtz6623HnL5oiiyY8eOQW9v/PjxGT9+/KDXBwCAHnv27MmePXsGvf5wXhtbUxDocbACx44dmyTZt2/foIpZtmxZ/vW//tfZtGlTkuTee+/NBRdccMh1Ojo6cuyxxw5qe0ly++2354477hj0+gAA0OPuu+/OkiVLRrqMAakpCBx99NHp7OzMtm3bcswxxxzQ3jMTMGnSpJqK2L59exYvXpyHH344yXszC//5P//nLFq06LDrzpgxIy+99FJN2/tNZgMAAKiXW2+9NTfffPOg1589e3Y6OjrqWNHB1RQEep78u3379sycOfOA9jfffLN3uYFatWpVvvjFL2bjxo1Jkj/4gz/IPffck2nTpg1o/YaGhkyePHnA2wMAgCNlqKedD+e1sTXdPvTMM89Mkqxbt67f9jVr1vRZ7nDWr1+fz3zmM9m4cWNOOeWUPPXUU3nwwQcHHAIAAIDBqSkIzJ07N0myYsWKftsfe+yxJMmFF1542L6KosjnP//5bN26NZ/85Cfz4osv5pOf/GQt5QAAAINUUxC4/PLLk7x3Ue/WrVv7tD399NN59dVX09TUlIsuuuiwfT355JP5v//3/2bGjBn5yU9+MqQLfgEAgNrUFATOO++8zJ8/P9VqNVdffXXa2tpSFEWef/75XHnllUmSm2++OePGjetdp6OjI7Nnz87s2bOzatWq3td/+MMfJkm++tWvOscfAACGWc23D33ggQdywQUX5LHHHsvMmTMzZcqUbN++PUly6aWX5utf/3qf5bu7u/Pyyy8n6ft8gVdffTVJct999+UHP/jBIbf58ssv996aFAAAGLqag8BJJ52U1atX58/+7M/yd3/3d9m6dWvOOuusfOlLX8ott9ySxsaBdfn6668nee8pxAN5EjEAAFA/g3qg2PTp03P//fcPaNmTTz653weQrV+/fjCbBgAA6qCmawQAAIAPB0EAAABKSBAAAIASEgQAAKCEBAEAACghQQAAAEpIEAAAgBISBAAAoIQEAQAAKCFBAAAASkgQAACAEhIEAACghAQBAAAoIUEAAABKSBAAAIASEgQAAKCEBAEAACihxpEuYKiq1WrmzJnTb1tLS0taWlqGuSIAADi41tbWtLa29ttWrVaHrY6GoiiKYdtaHTU3N6e9vT2VSiVtbW0jXQ4AAAzZcB7jOjUIAABKSBAAAIASEgQAAKCEBAEAACghQQAAAEpIEAAAgBISBAAAoIQEAQAAKCFBAAAASkgQAACAEhIEAACghAQBAAAoIUEAAABKSBAAAIASEgQAAKCEBAEAACghQQAAAEpIEAAAgBISBAAAoIQEAQAAKCFBAAAASkgQAACAEhIEAACghAQBAAAoIUEAAABKqHGkCxiqarWaOXPm9NvW0tKSlpaWYa4IAAAOrrW1Na2trf22VavVYaujoSiKYti2VkfNzc1pb29PpVJJW1vbSJcDAABDNpzHuE4NAgCAEhIEAACghAQBAAAoIUEAAABKSBAAAIASEgQAAKCEBAEAACghQQAAAEpIEAAAgBISBAAAoIQEAQAAKCFBAAAASkgQAACAEhIEAACghAQBAAAoIUEAAABKSBAAAIASEgQAAKCEBAEAACghQQAAAEpIEAAAgBISBAAAoIQEAQAAKCFBAAAASkgQAACAEmoc6QKGqlqtZs6cOf22tbS0pKWlZZgrAgCAg2ttbU1ra2u/bdVqddjqaCiKohi2rdVRc3Nz2tvbU6lU0tbWNtLlAADAkA3nMa5TgwAAoIQEAQAAKCFBAAAASkgQAACAEhIEAACghAQBAAAoIUEAAABKSBAAAIASEgT40NuzZ0/uuOOO7NmzZ6RL4QPAeGGgjBVqYbwwGn1gnyxcqVTS0dGRGTNmpL29faTLYRTbsWNHjj322HR2dmby5MkjXQ6jnPHCQBkr1MJ4YaCG8xjXjAAAAJSQIAAAACUkCAAAQAkJAgAAUEKCAAAAlJAgAAAAJTSoIPDGG29k8eLFaW5uzsSJEzNr1qwsXbo0XV1dNffV1dWVO++8M2effXYmTpyYSqWSG264IR0dHYMpDQAAGICag8CmTZty/vnn5/777097e3smTJiQdevW5fbbb8/8+fPT3d094L66u7uzYMGC/Nmf/VleeeWVTJgwIR0dHXnggQfyu7/7u9m0aVOt5Y2Y1tbWkS6hl1pGt9H0nqhl9BtN74taRrfR9J6oZfQbTe+LWkZQUaN58+YVSYoFCxYUmzZtKoqiKFatWlVUKpUiSXHXXXcNuK+77rqrSFI0NzcXzz//fFEURbFx48Zi/vz5RZJi3rx5B113xowZRZJixowZte7CETF79uyRLqGXWvrq7OwskhSdnZ0jXUpRFKPjPemhlgMZLwenlr6MlYNTy4GMl4NTS1/DeYxb04zACy+8kMcffzzTpk3LQw89lJkzZyZJPv7xj+fhhx9Oktx7773Zu3fvYfvq7u7OX/3VXyVJHn744Zx//vlJkpNOOik//OEPM23atDz++OP5+c9/XkuJAADAANQUBB599NEkyaJFizJ16tQ+bXPnzs2sWbOyZcuWPPPMM4fta+XKlXnrrbdy9tln58ILL+zTNnXq1FxxxRVJkp/85Ce1lAgAAAxATUFg5cqVSZKFCxf2297zes9yw9UXAABQm5qCwPr165MkZ5xxRr/tp59+epJkw4YNw9oXAABQm5qCwJYtW5IkU6ZM6bf9uOOOS5JUq9Vh7QsAAKhNYy0L79q1K0kOuD6gR8/rPcsdyb56gsTmzZtTqVQOu72DaWhoGPS6v6laraa5ubkufQ2VWvoqiiJJMnv27Lr9vodiNLwnPdRyIOPl4NTSl7FycGo5kPFycB/GWnp+34OxefPmJP/fse6RVFMQ6HGwnRs7dmySZN++fQPuY7B99by+f//+UfPwsfb29pEuoZdaDjRaxkkyet6TRC0HY7z0Ty0HMlb6p5b+GS/9U8uBBnI8PVQ1BYGjjz46nZ2d2bZtW4455pgD2nv+ej9p0qTD9tWzzLZt2/ptP1xfEyZMyLvvvpuxY8fmhBNOGFD9/RkNqRwAgA+PocwIbNmyJfv27cuECRPqWFH/agoCTU1N6ezszPbt23ufIfCb3nzzzd7lBtJXkmzfvr3f9sP1tXPnzoGUDAAA9KOmi4XPPPPMJMm6dev6bV+zZk2f5YarLwAAoDY1BYG5c+cmSVasWNFv+2OPPZYkBzwg7Ej3BQAA1KamIHD55ZcnSZYtW5atW7f2aXv66afz6quvpqmpKRdddNFh+5o7d26ampqybt26PP30033atm7d2vsU454nDAMAAPVTUxA477zzMn/+/FSr1Vx99dVpa2tLURR5/vnnc+WVVyZJbr755owbN653nY6OjsyePTuzZ8/OqlWrel8fN25cbrrppiTJVVddldWrVydJNm7cmKuuuiqbN2/OggUL8rGPfWyo+wgAALxPQ1HjZc2bNm3KBRdc0HuP0ylTpvRe8HvppZdmxYoVaWz8/65B3rhxY0455ZQkyU9/+tNccsklvW3d3d2ZP39+nnzyySTvPTug5y5C06dPzzPPPNPvRckAAMDQ1DQjkCQnnXRSVq9eneuvvz7Tp0/P7t27c9ZZZ2Xp0qVZvnx5nxBwOOPGjcuKFSuyZMmSnHbaadmxY0fGjBmTsWPHZuLEifkv/+W/pKurq9YS09XVlTvvvDNnn312Jk6cmEqlkhtuuGFU3buXwXvjjTeyePHiNDc3Z+LEiZk1a1aWLl06qLGyc+fOfOMb38jcuXMzZcqUnHrqqfnc5z7XG0754KvneHm/nTt35tRTT/UHiw+Jeo+Vf/iHf8hnP/vZnHDCCTn++OMzb948ny0fIvUcL3v27MmSJUty4YUXZvLkyTnnnHNy/fXX54033jgClTPS7r777jQ0NAzqOQF1P8YtRoGNGzcW06ZNK5IUSYopU6b0/vtTn/pU0dXVNeC+urq6iksuuaTfvqZNm1Zs3LjxCO4JR1o9x8rrr79enHbaab3rNzU1FePGjSuSFA0NDcVtt912BPeE4VDP8dKfm266qUhSNDc316liRkq9x8q9995bNDQ0FEmKiRMnFsccc0zvZ8sDDzxwhPaC4VLP8bJ9+/binHPO6V3/Ix/5SDF27NgiSTF16tTimWeeOYJ7wnDbv39/8bGPfaxIUuzdu7emdY/EMe6oCALz5s0rkhQLFiwoNm3aVBRFUaxataqoVCpFkuKuu+4acF933XVX7xfz888/XxTFe//Dzp8/v0hSzJs374jsA8OjnmPlS1/6UpGkmDt3brFhw4aiKIpiz549xf33319MmjSpSFL87//9v4/IfjA86jle3u/ZZ5/t/bIWBD746jlWVq5cWYwdO7YYN25c8YMf/KDYtWtXsW/fvuK73/1u0dDQUBxzzDHFr371qyO1KwyDeo6XxYsXF0mKT3ziE8Xrr79eFEVRvPPOO8WNN95YJCl++7d/e8h/tGB02Lt3b3HHHXf0HrzXGgSOxDHuiAeB1atX9yaZrVu39ml7+umniyTFCSecUHR3dx+2r66urqKpqalIUqxcubJP29atW3vT+4svvljXfWB41HOsbNy4sRgzZkwxbty4oq2t7YD2++67r0hSXHzxxXWrn+FVz/Hyfl1dXcVHP/rR3g9zQeCDrd5jZeHChUWS4j/+x/94QNuXv/zlIknxl3/5l3WpneFX7+OWcePGFUcdddQB30X79u0rfvu3f7tIUjz55JN13QeG16OPPlr8q3/1r4pTTjml93uj1iBwpI5xa75GoN56bhO6aNGiTJ06tU/b3LlzM2vWrGzZsiXPPPPMYftauXJl3nrrrZx99tkHPH9g6tSpvbci/clPflKn6hlO9RwrL7/8cvbv35/LLrsslUrlgPZrrrkmY8aMyYsvvjikx4Qzcuo5Xt7vnnvuyS9+8Ytce+21damVkVXPsfLmm29mxYoVmTJlSr7yla8c0L548eJ8+tOfPuAW3Hxw1Pu7qLu7O7NmzTrgu2jMmDH59Kc/nST5xS9+UZ/iGRGPPPJI/uZv/iavv/76oPs4Use4Ix4EVq5cmSRZuHBhv+09r/csN1x9MfrU8/fb8z9jzx2t3m/SpEmZPHlydu7cmbfeeqv2YhlxR+rz4JVXXsmdd96ZOXPm5Bvf+MbQimRUqOdYefzxx1MURS6//PI+t9LucdFFF+WJJ57IXXfdNYSKGUn1HC87d+5MkoNeNLp3794kya5du2quk9Hjrrvuyi9/+cven8E4Ut9pA7/FzxGyfv36JMkZZ5zRb/vpp5+eJNmwYcOw9sXoU8/f77x587J8+fKceuqpB93W9u3bM2HChDQ1NQ2yYkbSkfg8KIoiN9xwQ7q6unL//fdn/PjxQy+UEVfPsbJ27dokyUc/+tE6VcdoU8/xMnv27IwfPz6vvPJKXnnllcyaNau3bc+ePVmxYkWS5Nxzzx1q2YygSqXS79kHtThSx7gjPiOwZcuWJO89j6A/xx13XJKkWq0Oa1+MPvX8/Z522mlZuHBhzjrrrAPaiqLILbfckuS9hN3Q0DDIihlJR+Lz4K//+q/zs5/9LDfeeOOAnqDOB0M9x8prr72WJDnhhBPy7LPP5uqrr86pp56aj3zkI1m4cGF++MMf1qdoRkw9x8uxxx6bP/3TP82+ffuyaNGiPPHEE3nnnXeyZs2afOELX8hrr72WT3ziE5k3b17d6ueD6Ugd4474jEDPdNf7z7Pr0fP6QKbF6tkXo89w/H537tyZr371q/nRj36UxsbG3HrrrYPui5FV7/HS3t6eW265JZVKJXfffXd9imRUqOdY2bFjR5L0Bsbdu3fnuOOOy+7du7NixYqsWLEif//3f5/vf//7daqe4Vbvz5Y777wz77zzTr797W/nsssu69N2ySWX5Mc//nHGjh07hIr5MDhSx0AjPiPQ42AXZPYM/oE8dKGnj3r0xeh1pH6/y5Yty5w5c/Jf/+t/TZLce++9ueCCCwZXJKNGvcZLS0tLduzYkdbW1kyePLlu9TF61GOsvPvuu0mS733ve7n44ovzyiuv5O23386vf/3r/OhHP8pxxx2XBx980MzAh0C9Pluee+65LF++PEnS0NCQadOm9V5f8vOf/7z34mTK7Ugd4454EDj66KOTJNu2beu3vSfZTJo06bB99SxTj74Yfeo5Vn7T9u3bc+WVV2bRokXZtGlTpk6dmh//+Mf5oz/6o6EVzIiq53j527/92yxbtixf+MIXeu/MwIdHPcdKz1/lTjvttCxbtqz39MOxY8fmc5/7XP7yL/8yyXt3nuKDqZ7jZd26dVmwYEHWr1+fpUuXZseOHXnjjTeya9euPPTQQxk7dmy+/OUv56GHHqrfDvCBdKSOcUc8CPRciLl9+/Z+2998880+yw1XX4w+R+L3u2rVqnzsYx/Lww8/nCT5gz/4g6xduzaLFi0aUq2MvHqNl66urnzta1/Lsccem+985zt1rZHRoZ6fLdOmTUuSXHXVVb0HjL/pyiuvTENDQ9auXWt2+gOqnuPlL/7iL9LZ2Zk//uM/zje/+c0cc8wxSZLGxsZcddVV+d73vpckue222+pQOR9kR+oYd8SDwJlnnpnkvVTcnzVr1vRZbrj6YvSp9+93/fr1+cxnPpONGzfmlFNOyVNPPZUHH3yw94ucD7Z6jZfdu3dn8+bN6ezszIwZM9LQ0ND703P72ba2tt7Xli1bVr+dYFjU87PlxBNPTJKD3iHk6KOPzpQpU/Luu+8e9Aud0a2e4+W5555Lknz+85/vt/33f//3M378+GzYsMF4KbkjdYw74kFg7ty5SdJ7i6z3e+yxx5LkgIcnHOm+GH3q+fstiiKf//zns3Xr1nzyk5/Miy++mE9+8pP1K5YRV6/xMmbMmJxxxhn9/px88slJ3jvto+c1px5+8NTzs6Xn9o8H+7Lu7OzMtm3b0tTUlOOPP34w5TLC6jleeq43Otzd6RobGzNhwoRayuRD5ogd49b0HOIjoOdR3SeeeGLx9ttv92n7p3/6pyJJ0dTUVHR1dR22r998/PI//dM/9Wl7++23ex+//MILL9RzFxgm9RwrTzzxRJGkmDFjRtHZ2XmkSmYE1XO8HMzrr79eJCmam5uHWi4jqJ5jZdu2bcVRRx1VNDU1HdBXURTFv//3/75IUnzmM5+pW/0Mr3qOl6997WtFkuKmm27qt/1//s//WSQpzj333HqUziiRpEhS7N27d8DrHKlj3BEPAkVRFPPnzy+SFAsXLix+9atfFfv37y+ee+65olKpFEmKP//zP++zfHt7e3H22WcXZ599dvHss8/2afvWt77V+8X8/PPPF0Xx3pf1vHnziiTFggULhm2/qL96jZU//MM/LJIUS5YsGe5dYBjV87OlP4LAh0c9x0pLS0uRpPi93/u9Ys2aNUVRFMWePXuK//Sf/lMxceLEYuzYsb3fT3ww1Wu8rF27tpg4cWIxZsyY4q677ireeeedoiiKoru7u/jv//2/F8cff3yRpPj+978/rPvHkXWoIDDcx7ijIghs3LixN8kkKaZMmdL770svvbTo7u7us3zPl2+S4qc//Wmftq6uruKSSy7pbZ86dWrvv6dPn15s2rRpOHeNOqvXWPnn//yf9/5F54wzzjjkTy2JndGlnp8t/REEPjzqOVZ27NhRnHvuub3txx9/fHHUUUcVSYrGxsbiP/yH/zCcu8YRUM/x8v3vf78YP358kaRoaGgopk+fXowbN653+RtvvHE4d41hcKggMNzHuCN+jUCSnHTSSVm9enWuv/76TJ8+Pbt3785ZZ52VpUuXZvny5WlsHPhzz8aNG5cVK1ZkyZIlOfPMM7Nr165Mnz49N9xwQ1avXp2ZM2cewT3hSKvXWHn99deTvPcEvvXr1x/yhw+uen628OFWz7HyW7/1W3n66adz22235ayzzsrOnTszc+bMfPGLX8zKlStz0003HcE9YTjUc7xcc801Wbt2ba699tr8zu/8Tjo7O3PyySfniiuuyBNPPJH77rvvCO4JHyRH4hi34f+fTAAAgBIZFTMCAADA8BIEAACghAQBAAAoIUEAAABKSBAAAIASEgQAAKCEBAEAACghQQAAAEpIEAAAgBISBAAAoIQEAQAAKCFBAAAASkgQAACAEhIEAACghP5/m5L4Hy9riNkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 900x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "\n",
    "# TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "# print(\"Neural network performance\")\n",
    "# NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "# NNtable.float_format = \".4\"\n",
    "# for TPR_threshold in TPR_thresholds:\n",
    "#     thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "#     NNtable.add_row(\n",
    "#         [\n",
    "#             IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "#             \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "#         ]\n",
    "#     )\n",
    "# print(NNtable)\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "# plot_train_val_losses(\n",
    "#     IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data',\n",
    "#     labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    "# )\n",
    "# plot_roc(\n",
    "#     IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='arr',\n",
    "#     labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    "# )\n",
    "# plot_output_score(\n",
    "#     IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', \n",
    "#     labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "#         'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "#     }\n",
    "# )\n",
    "s_over_root_b(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train + val comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            data_list[train_index], data_hlf[train_index], label[train_index], \n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            data_list[val_index], data_hlf[val_index], label[val_index], \n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'r') as f:\n",
    "    IN_perf_dict = json.load(f)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold ']\n",
    "for fold_idx, (train_IN_dict, val_IN_dict) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'])):\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input Vars (pre-standardization and post-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison/{VERSION}/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    sig_mask = (label == 1)\n",
    "    sig_test_mask = (label_test == 1)\n",
    "    bkg_mask = (label == 0)\n",
    "    bkg_test_mask = (label_test == 0)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        \n",
    "        sig_train_mask = rectified_train_index & sig_mask\n",
    "        sig_val_mask = np.logical_not(rectified_train_index) & sig_mask\n",
    "        bkg_train_mask = rectified_train_index & bkg_mask\n",
    "        bkg_val_mask = np.logical_not(rectified_train_index) & bkg_mask\n",
    "\n",
    "        sig_train_np = data_df.loc[sig_train_mask, var_name].to_numpy()\n",
    "        sig_val_np = data_df.loc[sig_val_mask, var_name].to_numpy()\n",
    "        sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "        bkg_train_np = data_df.loc[bkg_train_mask, var_name].to_numpy()\n",
    "        bkg_val_np = data_df.loc[bkg_val_mask, var_name].to_numpy()\n",
    "        bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np = data_df.loc[sig_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_df.loc[bkg_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "    sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    pre_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_pre_std, var_name, pre_std_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison/{VERSION}/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "index_map = {\n",
    "    var_name: data_list_index_map(var_name) for var_name in (high_level_fields - set(input_hlf_vars))\n",
    "}\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = data_list, data_list_test\n",
    "    else:\n",
    "        data, data_test = data_hlf, data_hlf_test\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name, index_map, \n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name, index_map)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    post_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_post_std, var_name, post_std_hists[var_name], labels=label_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian smearing on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    index2, index3 = data_list_index_map(var_name)\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear[:, index2, index3] *= rng.normal(size=len(particle_list_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear[:, index2, index3] += rng.normal(size=len(particle_list_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns[var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "    gauss_data_list, gauss_data_hlf = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        gauss_data_list = smear_particle_list(var_name, copy.deepcopy(data_list_test))\n",
    "        gauss_data_hlf = data_hlf_test\n",
    "    else:\n",
    "        gauss_data_list = data_list_test\n",
    "        gauss_data_hlf = smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test))\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list, gauss_data_hlf, label_test, OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "IN_perf_smear_list = []\n",
    "label_arr = []\n",
    "for var_name, IN_perf_smear in IN_perf_smear_dict.items():\n",
    "    IN_perf_smear_list.append(IN_perf_smear)\n",
    "    label_arr.append(var_name)\n",
    "sort = np.argsort([IN_perf_smear['mean_area'] for IN_perf_smear in IN_perf_smear_list])\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear', \n",
    "    method='IN_arr', labels=label_arr, yscale='log', run2=False, sort=sort\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison/{VERSION}/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "index_map = {\n",
    "    var_name: data_list_index_map(var_name) for var_name in (high_level_fields - set(input_hlf_vars))\n",
    "}\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = smear_particle_list(var_name, data_list), smear_particle_list(var_name, data_list_test)\n",
    "    else:\n",
    "        data, data_test = smear_particle_hlf(var_name, data_hlf), smear_particle_hlf(var_name, data_hlf_test)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name, index_map, \n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_gauss_smear, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name, index_map)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    gauss_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_gauss_smear, var_name, gauss_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y in [('train', data_list, data_hlf, label), ('test', data_list_test, data_hlf_test, label_test)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'r') as f:\n",
    "    IN_full_eval_dict = json.load(f)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {'mass': [], 'dijet_mass': []}\n",
    "for var_name in hist_dict.keys():\n",
    "    for i, score_cut in enumerate(score_cuts):\n",
    "        sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict)\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "        hist_dict[var_name].extend(\n",
    "            [\n",
    "                copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "            ]\n",
    "        )\n",
    "    for mod_factor, label_mod in enumerate(['sig_train', 'sig_test', 'bkg_train', 'bkg_test']):\n",
    "        plot_list = []\n",
    "        label_list = []\n",
    "        for i in range(len(hist_dict[var_name])):\n",
    "            if (i - mod_factor) % 4 == 0:\n",
    "                plot_list.append(hist_dict[var_name][i])\n",
    "                label_list.append(label_arr[i])\n",
    "        make_input_plot(\n",
    "            plot_destdir, var_name, plot_list, labels=label_list, density=True, \n",
    "            plot_prefix=CURRENT_TIME+'_', plot_postfix='_'+label_mod, alpha=0.5,\n",
    "            linestyle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna-hhbbgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
