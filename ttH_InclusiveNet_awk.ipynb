{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu1.fnal.gov      Fri Aug  9 22:17:07 2024  555.42.02\n",
      "[0] Tesla P100-PCIE-12GB | 41Â°C,   0 % |     0 / 12288 MB |\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# import ROOT as rt\n",
    "# from root_numpy import root2array, tree2array\n",
    "\n",
    "import awkward as ak\n",
    "import h5py\n",
    "import numpy as np\n",
    "import numpy.lib.recfunctions as nlr\n",
    "import pandas as pd\n",
    "import os, sys, datetime\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import json\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "import torch.utils.data.distributed\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import gpustat\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "SIGNAL_FILEPATHS = [lpc_fileprefix+\"/Run3_2022preEE_merged/GluGluToHH/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/GluGluToHH/nominal/*\"]\n",
    "SIGNAL_FILEPATHS = [\n",
    "    lpc_fileprefix+\"/Run3_2022preEE_merged/GluGluToHH/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/GluGluToHH/nominal/*\",\n",
    "    lpc_fileprefix+\"/Run3_2022preEE_merged/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", \n",
    "    # lpc_fileprefix+\"/Run3_2022preEE_merged/ZHH_HHto2B2G_CV-1p0_C2V-1p0_C3-1p0_TuneCP5_13p6TeV/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/ZHH_HHto2B2G_CV-1p0_C2V-1p0_C3-1p0_TuneCP5_13p6TeV/nominal/*\",\n",
    "    # lpc_fileprefix+\"/Run3_2022preEE_merged/WHH_HHto2B2G_CV-1p0_C2V-1p0_C3-1p0_TuneCP5_13p6TeV/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/WHH_HHto2B2G_CV-1p0_C2V-1p0_C3-1p0_TuneCP5_13p6TeV/nominal/*\"\n",
    "]\n",
    "BKG_FILEPATHS = [lpc_fileprefix+\"/Run3_2022preEE_merged/ttHToGG/nominal/*\", lpc_fileprefix+\"/Run3_2022postEE_merged/ttHToGG/nominal/*\"]\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "OUTPUT_DIRPATH = CURRENT_DIRPATH + \"/model_outputs/v1/base_vars/\"\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "# SIGNAL_FILEPATHS = [\"/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_parquet/v1/GluGluToHH/nominal/*\", \"/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_parquet/v1_preEE/GluGluToHH/nominal/*\"]\n",
    "# BKG_FILEPATHS = [\"/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_parquet/v1/ttHToGG/nominal/*\", \"/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_parquet/v1_preEE/ttHToGG/nominal/*\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMSGrad(optim.Optimizer):\n",
    "    \"\"\"Implements AMSGrad algorithm.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "    .. _Adam\\: A Method for Stochastic Optimization:\n",
    "        https://arxiv.org/abs/1412.6980\n",
    "    .. _On the Convergence of Adam and Beyond:\n",
    "        https://openreview.net/forum?id=ryQu7f-RZ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=True):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(AMSGrad, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(AMSGrad, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', True)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "destdir = 'v1_merged_plots'\n",
    "sig_samples_list = [ak.from_parquet(glob.glob(dir_path)) for dir_path in SIGNAL_FILEPATHS]\n",
    "sig_samples_pq = ak.concatenate(sig_samples_list)\n",
    "bkg_samples_list = [ak.from_parquet(glob.glob(dir_path)) for dir_path in BKG_FILEPATHS]\n",
    "bkg_samples_pq = ak.concatenate(bkg_samples_list)\n",
    "samples = {\n",
    "    'sig': sig_samples_pq,\n",
    "    'bkg': bkg_samples_pq,\n",
    "}\n",
    "\n",
    "# print(sig_samples_pq.fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len sig: 742869\n",
      "len bkg: 346502\n"
     ]
    }
   ],
   "source": [
    "pandas_samples = {}\n",
    "high_level_fields = {\n",
    "    'event', # event number\n",
    "    'puppiMET_sumEt', 'puppiMET_pt', 'puppiMET_eta', 'puppiMET_phi', # MET variables\n",
    "    'DeltaPhi_j1MET', 'DeltaPhi_j2MET', # jet-MET variables\n",
    "    'DeltaR_jg_min', 'n_jets', 'chi_t0', 'chi_t1', # jet variables\n",
    "    'lepton1_pt' ,'lepton2_pt', 'pt', # lepton and diphoton pt\n",
    "    'lepton1_eta', 'lepton2_eta', 'eta', # lepton and diphoton eta\n",
    "    'lepton1_phi', 'lepton2_phi', 'phi', # lepton and diphoton phi\n",
    "    'abs_CosThetaStar_CS', 'abs_CosThetaStar_jj', # angular variables\n",
    "    # 'dijet_mass', # mass of b-dijet (resonance for H->bb)\n",
    "    # 'leadBjet_leadLepton', 'leadBjet_subleadLepton', # deltaR btwn bjets and leptons (b/c b often decays to muons)\n",
    "    # 'subleadBjet_leadLepton', 'subleadBjet_subleadLepton'\n",
    "}\n",
    "\n",
    "pandas_aux_samples = {}\n",
    "high_level_aux_fields = {\n",
    "    'mass', 'dijet_mass' # diphoton and bb-dijet mass\n",
    "} # https://stackoverflow.com/questions/67003141/how-to-remove-a-field-from-a-collection-of-records-created-by-awkward-zip\n",
    "\n",
    "for sample_name, sample in samples.items():\n",
    "    pandas_samples[sample_name] = {\n",
    "        field: ak.to_numpy(sample[field], allow_missing=False) for field in high_level_fields\n",
    "    }\n",
    "    pandas_aux_samples[sample_name] = {\n",
    "        field: ak.to_numpy(sample[field], allow_missing=False) for field in high_level_aux_fields\n",
    "    }\n",
    "\n",
    "# del samples\n",
    "\n",
    "# sig_frame = ak.to_dataframe(pandas_samples['sig'])\n",
    "# sig_aux_frame = ak.to_dataframe(pandas_aux_samples['sig'])\n",
    "# bkg_frame = ak.to_dataframe(pandas_samples['bkg'])\n",
    "# bkg_aux_frame = ak.to_dataframe(pandas_aux_samples['bkg'])\n",
    "sig_frame = pd.DataFrame(pandas_samples['sig'])\n",
    "sig_aux_frame = pd.DataFrame(pandas_aux_samples['sig'])\n",
    "bkg_frame = pd.DataFrame(pandas_samples['bkg'])\n",
    "bkg_aux_frame = pd.DataFrame(pandas_aux_samples['bkg'])\n",
    "\n",
    "# print(f\"len sig: {len(sig_frame)}\")\n",
    "# print(f\"len bkg: {len(bkg_frame)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 21\n",
    "\n",
    "sig_frame = sig_frame.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "bkg_frame = bkg_frame.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def train_test_split_df(sig_df, bkg_df, method='modulus'):\n",
    "    if method == 'modulus':\n",
    "        # Train/Val events are those with odd event #s, test events have even event #s\n",
    "        sig_train_frame = sig_df.loc[(sig_df['event'] % 2).ne(0)].reset_index(drop=True)\n",
    "        sig_test_frame = sig_df.loc[(sig_df['event'] % 2).ne(1)].reset_index(drop=True)\n",
    "        bkg_train_frame = bkg_df.loc[(bkg_df['event'] % 2).ne(0)].reset_index(drop=True)\n",
    "        bkg_test_frame = bkg_df.loc[(bkg_df['event'] % 2).ne(1)].reset_index(drop=True)\n",
    "    elif method == 'sample':\n",
    "        sig_train_frame = sig_df.sample(frac=0.75, random_state=random_state).reset_index(drop=True)\n",
    "        sig_test_frame = sig_df.drop(sig_train_frame.index).reset_index(drop=True)\n",
    "        bkg_train_frame = bkg_df.sample(frac=0.75, random_state=random_state).reset_index(drop=True)\n",
    "        bkg_test_frame = bkg_df.drop(bkg_train_frame.index).reset_index(drop=True)\n",
    "    else:\n",
    "        raise Exception(f\"Only 2 accepted methods: 'sample' and 'modulus'. You input {method}\")\n",
    "    return sig_train_frame, sig_test_frame, bkg_train_frame, bkg_test_frame\n",
    "\n",
    "sig_train_frame, sig_test_frame, bkg_train_frame, bkg_test_frame = train_test_split_df(sig_frame, bkg_frame)\n",
    "\n",
    "# print(f\"len sig train: {len(sig_train_frame)}\")\n",
    "# print(f\"len sig test: {len(sig_test_frame)}\")\n",
    "# print(f\"len bkg train: {len(bkg_train_frame)}\")\n",
    "# print(f\"len bkg test: {len(bkg_test_frame)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and std calculated.\n",
      "ACTUAL num_non4 / (total) = 0.0\n",
      "Mean and std calculated for particle list.\n",
      "signal train number: (370348, 4, 6)\n",
      "background train number: (173291, 4, 6)\n",
      "Data list: (543639, 4, 6)\n",
      "Data HLF: (543639, 9)\n",
      "Data list test: (545732, 4, 6)\n",
      "Data HLF test: (545732, 9)\n",
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
      "num_non4 / (total) = 0.0\n"
     ]
    }
   ],
   "source": [
    "FILL_VALUE = -999\n",
    "# Because of zero-padding, standardization needs special treatment\n",
    "# Masked out zero\n",
    "# zero_entries = bkg_frame == 0 \n",
    "# zero_entries = (bkg_train_frame == FILL_VALUE) # now pad with -999 instead of 0\n",
    "masked_x_sample = np.ma.array(bkg_train_frame, mask=(bkg_train_frame == FILL_VALUE))\n",
    "x_mean = masked_x_sample.mean(axis=0)\n",
    "x_std = masked_x_sample.std(axis=0)\n",
    "print(\"Mean and std calculated.\")\n",
    "\n",
    "# Standardize background\n",
    "normed_bkg_train = (masked_x_sample - x_mean)/x_std\n",
    "normed_bkg_test = (np.ma.array(bkg_test_frame, mask=(bkg_test_frame == FILL_VALUE)) - x_mean)/x_std\n",
    "\n",
    "# Standardize signal\n",
    "# zero_entries = sig_frame == 0 \n",
    "# zero_entries = (sig_frame == FILL_VALUE) # now pad with -999 instead of 0\n",
    "masked_x_sample = np.ma.array(sig_train_frame, mask=(sig_train_frame == FILL_VALUE))\n",
    "normed_sig_train = (masked_x_sample - x_mean)/x_std\n",
    "normed_sig_test = (np.ma.array(sig_test_frame, mask=(sig_test_frame == FILL_VALUE)) - x_mean)/x_std\n",
    "\n",
    "normed_bkg_train_frame = pd.DataFrame(normed_bkg_train.filled(0), columns=list(bkg_train_frame))\n",
    "normed_bkg_train_frame.head()\n",
    "normed_bkg_test_frame = pd.DataFrame(normed_bkg_test.filled(0), columns=list(bkg_test_frame))\n",
    "normed_bkg_test_frame.head()\n",
    "\n",
    "normed_sig_train_frame = pd.DataFrame(normed_sig_train.filled(0), columns=list(sig_train_frame))\n",
    "normed_sig_train_frame.head()\n",
    "normed_sig_test_frame = pd.DataFrame(normed_sig_test.filled(0), columns=list(sig_test_frame))\n",
    "normed_sig_test_frame.head()\n",
    "\n",
    "num_non4_ACTUAL, num_4_ACTUAL = 0, 0\n",
    "def to_p_list(data_frame):\n",
    "    # Inputs: Pandas data frame\n",
    "    # Outputs: Numpy array of dimension (Event, Particle, Attributes)\n",
    "    \n",
    "    \n",
    "    particle_list_sig = np.zeros(shape=(len(data_frame),4,6))\n",
    "    sorted_particle_list = np.zeros(shape=(len(data_frame),4,6))\n",
    "    # 4: max particles: l1, l2, dipho, MET\n",
    "    # 6: pt, eta, phi, isLep, isDipho, isMET\n",
    "   \n",
    "    for i in range(len(data_frame)): # loop through the list of events\n",
    "        ptl1 = data_frame['lepton1_pt'][i]\n",
    "        ptl2 = data_frame['lepton2_pt'][i]\n",
    "        ptdipho = data_frame['pt'][i]\n",
    "        ptMET = data_frame['puppiMET_pt'][i]\n",
    "\n",
    "        etal1 = data_frame['lepton1_eta'][i]\n",
    "        etal2 = data_frame['lepton2_eta'][i]\n",
    "        etadipho = data_frame['eta'][i]\n",
    "        etaMET = data_frame['puppiMET_eta'][i]\n",
    "\n",
    "        phil1 = data_frame['lepton1_phi'][i]\n",
    "        phil2 = data_frame['lepton2_phi'][i]\n",
    "        phidipho = data_frame['phi'][i]\n",
    "        phiMET = data_frame['puppiMET_phi'][i]\n",
    "\n",
    "        # list through list of particles: l1, l2, diphoton, MET\n",
    "        # 0: leading lep\n",
    "        particle_list_sig[i,0, 0] = ptl1\n",
    "        particle_list_sig[i,0, 1] = etal1\n",
    "        particle_list_sig[i,0, 2] = phil1\n",
    "        particle_list_sig[i,0, 3] = 1 if ptl1 != 0 else 0 # isLep\n",
    "        particle_list_sig[i,0, 4] = 0 # isDiPho\n",
    "        particle_list_sig[i,0, 5] = 0 # isMET\n",
    "\n",
    "        # 1: subleading lep\n",
    "        particle_list_sig[i,1, 0] = ptl2\n",
    "        particle_list_sig[i,1, 1] = etal2\n",
    "        particle_list_sig[i,1, 2] = phil2\n",
    "        particle_list_sig[i,1, 3] = 1 if ptl2 != 0 else 0 # isLep\n",
    "        particle_list_sig[i,1, 4] = 0 # isDiPho\n",
    "        particle_list_sig[i,1, 5] = 0 # isMET\n",
    "\n",
    "        # 2: dipho\n",
    "        particle_list_sig[i,2, 0] = ptdipho\n",
    "        particle_list_sig[i,2, 1] = etadipho\n",
    "        particle_list_sig[i,2, 2] = phidipho\n",
    "        particle_list_sig[i,2, 3] = 0 # isLep\n",
    "        particle_list_sig[i,2, 4] = 1 if ptdipho != 0 else 0 # isDiPho\n",
    "        particle_list_sig[i,2, 5] = 0 # isMET\n",
    "\n",
    "        # 3: MET\n",
    "        particle_list_sig[i,3, 0] = ptMET\n",
    "        particle_list_sig[i,3, 1] = etaMET\n",
    "        particle_list_sig[i,3, 2] = phiMET\n",
    "        particle_list_sig[i,3, 3] = 0 #isLep\n",
    "        particle_list_sig[i,3, 4] = 0 # isDiPho\n",
    "        particle_list_sig[i,3, 5] = 1 if ptMET != 0 else 0 # isMET\n",
    "    \n",
    "        # Sort by descending pT. \n",
    "        # This was implemented when standardization was done before sorting. Thus zero entry needs to be excluded\n",
    "        # Redesigned the code with standardization done after sorting. Same code still works.\n",
    "        nonzero_indices = np.nonzero(particle_list_sig[i,:,0])[0]\n",
    "        # nonzero_indices = np.where(particle_list_sig[i,:,0] != FILL_VALUE)[0]\n",
    "        sorted_indices = particle_list_sig[i,nonzero_indices,0].argsort()[::-1] # sort by first column, which is the pT\n",
    "        global_sorted_indices = nonzero_indices[sorted_indices]\n",
    "        sorted_particle_list[i,:len(nonzero_indices),:] = particle_list_sig[i,global_sorted_indices,:]\n",
    "\n",
    "        global num_non4_ACTUAL, num_4_ACTUAL\n",
    "        if len(nonzero_indices) != 4:\n",
    "            num_non4_ACTUAL += 1\n",
    "        else:\n",
    "            num_4_ACTUAL += 1\n",
    "        \n",
    "    return sorted_particle_list\n",
    "\n",
    "sig_train_list = to_p_list(sig_train_frame)\n",
    "sig_test_list = to_p_list(sig_test_frame)\n",
    "bkg_train_list = to_p_list(bkg_train_frame)\n",
    "bkg_test_list = to_p_list(bkg_test_frame)\n",
    "\n",
    "print(f\"ACTUAL num_non4 / (total) = {num_non4_ACTUAL / (num_non4_ACTUAL + num_4_ACTUAL)}\")\n",
    "\n",
    "# Standardize the particle list\n",
    "x_sample = bkg_train_list[:,:,:3] # don't standardize boolean flags\n",
    "# Flatten out\n",
    "x_flat = x_sample.reshape((x_sample.shape[0]*x_sample.shape[1], x_sample.shape[2]))\n",
    "# Masked out zero\n",
    "zero_entries = (x_flat == 0)\n",
    "masked_x_sample = np.ma.array(x_flat, mask=zero_entries)\n",
    "x_list_mean = masked_x_sample.mean(axis=0)\n",
    "x_list_std = masked_x_sample.std(axis=0)\n",
    "print(\"Mean and std calculated for particle list.\")\n",
    "del x_sample, x_flat, zero_entries, masked_x_sample # release the memory\n",
    "\n",
    "def standardize_p_list(inputs):\n",
    "    global x_list_mean, x_list_std\n",
    "    to_norm = inputs[:,:,:3]\n",
    "    zero_entries = (to_norm == 0)\n",
    "    masked_to_norm = np.ma.array(to_norm, mask=zero_entries)\n",
    "    normed_x = (masked_to_norm - x_list_mean)/x_list_std\n",
    "    return np.concatenate((normed_x.filled(0), inputs[:,:,3:]), axis=2)\n",
    "    \n",
    "normed_sig_list = standardize_p_list(sig_train_list)\n",
    "normed_sig_test_list = standardize_p_list(sig_test_list)\n",
    "normed_bkg_list = standardize_p_list(bkg_train_list)\n",
    "normed_bkg_test_list = standardize_p_list(bkg_test_list)\n",
    "\n",
    "input_hlf_vars = ['puppiMET_sumEt','DeltaPhi_j1MET','DeltaPhi_j2MET','DeltaR_jg_min','n_jets','chi_t0',\n",
    "                                   'chi_t1','abs_CosThetaStar_CS','abs_CosThetaStar_jj']\n",
    "# input_hlf_vars = ['puppiMET_sumEt','DeltaPhi_j1MET','DeltaPhi_j2MET','DeltaR_jg_min','n_jets','chi_t0',\n",
    "#                   'chi_t1','abs_CosThetaStar_CS','abs_CosThetaStar_jj','dijet_mass', 'leadBjet_leadLepton', \n",
    "#                   'leadBjet_subleadLepton', 'subleadBjet_leadLepton', 'subleadBjet_subleadLepton']\n",
    "\n",
    "normed_sig_hlf = normed_sig_train_frame[input_hlf_vars].values\n",
    "normed_sig_test_hlf = normed_sig_test_frame[input_hlf_vars].values\n",
    "\n",
    "normed_bkg_hlf = normed_bkg_train_frame[input_hlf_vars].values\n",
    "normed_bkg_test_hlf = normed_bkg_test_frame[input_hlf_vars].values\n",
    "    \n",
    "# # Shuffle before splitting into train-val\n",
    "# randix = np.arange(len(normed_bkg_list))\n",
    "# np.random.shuffle(randix)\n",
    "\n",
    "# background_list = normed_bkg_list[randix]\n",
    "# background_list = background_list[:len(normed_sig_list)] # downsampling\n",
    "background_list = normed_bkg_list[:len(normed_bkg_list)] # downsampling\n",
    "background_test_list = normed_bkg_test_list[:len(normed_sig_test_list)] # downsampling\n",
    "print(f'signal train number: {normed_sig_list.shape}')\n",
    "print(f'background train number: {background_list.shape}')\n",
    "\n",
    "# background_hlf = normed_bkg_hlf[randix]\n",
    "# background_hlf = background_hlf[:len(normed_sig_hlf)]\n",
    "background_hlf = normed_bkg_hlf[:len(normed_bkg_hlf)]\n",
    "background_test_hlf = normed_bkg_test_hlf[:len(normed_sig_test_hlf)]\n",
    "\n",
    "sig_label = np.ones(len(normed_sig_hlf))\n",
    "bkg_label = np.zeros(len(background_hlf))\n",
    "\n",
    "sig_test_label = np.ones(len(normed_sig_test_hlf))\n",
    "bkg_test_label = np.zeros(len(background_test_hlf))\n",
    "\n",
    "# data_list_full = np.concatenate((normed_sig_list, background_list))\n",
    "# data_hlf_full = np.concatenate((normed_sig_hlf, background_hlf))\n",
    "# label_full = np.concatenate((sig_label, bkg_label))\n",
    "# print(\"Data list: {}\".format(data_list_full.shape))\n",
    "# print(\"Data HLF: {}\".format(data_hlf_full.shape))\n",
    "data_list = np.concatenate((normed_sig_list, background_list))\n",
    "data_hlf = np.concatenate((normed_sig_hlf, background_hlf))\n",
    "label = np.concatenate((sig_label, bkg_label))\n",
    "print(\"Data list: {}\".format(data_list.shape))\n",
    "print(\"Data HLF: {}\".format(data_hlf.shape))\n",
    "\n",
    "data_list_test = np.concatenate((normed_sig_test_list, background_test_list))\n",
    "data_hlf_test = np.concatenate((normed_sig_test_hlf, background_test_hlf))\n",
    "label_test = np.concatenate((sig_test_label, bkg_test_label))\n",
    "print(\"Data list test: {}\".format(data_list_test.shape))\n",
    "print(\"Data HLF test: {}\".format(data_hlf_test.shape))\n",
    "\n",
    "# data_list, data_list_test, data_hlf, data_hlf_test, label, label_test = train_test_split(\n",
    "#     data_list_full, data_hlf_full, label_full, test_size=0.25, random_state=None)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# skf.get_n_splits(data_hlf, label)\n",
    "print(skf)\n",
    "\n",
    "num_non4_list = 0\n",
    "num_4_list = 0\n",
    "for event in data_list:\n",
    "    if len(event) != 4:\n",
    "        num_non4_list += 1\n",
    "    else:\n",
    "        num_4_list += 1\n",
    "for event in data_list_test:\n",
    "    if len(event) != 4:\n",
    "        num_non4_list += 1\n",
    "    else:\n",
    "        num_4_list += 1\n",
    "print(f\"num_non4 / (total) = {num_non4_list / (num_non4_list + num_4_list)}\")\n",
    "\n",
    "## NOT DOING PROPER HANDLING FOR AUX SAMPLES ##\n",
    "# inspo:\n",
    "#    https://github.com/Sara-mibo/LRP_EncoderDecoder_GRU/blob/main/LRP/utils_lrp.py\n",
    "#    https://github.com/pytorch/captum/blob/master/captum/attr/_utils/lrp_rules.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=5\n",
    "model_file = 'ReallyTopclassStyle'\n",
    "state_file = 'BestPerfReallyTopclass'\n",
    "config_file = 'BestConfigReallyTopclass'\n",
    "retrain=True\n",
    "\n",
    "class ParticleHLF(Dataset):\n",
    "    def __init__(self, data_particles, data_hlf, data_y):\n",
    "        self.len = data_y.shape[0]\n",
    "        self.data_particles = torch.from_numpy(data_particles).float()\n",
    "        self.data_hlf = torch.from_numpy(data_hlf).float()\n",
    "        self.data_y = torch.from_numpy(data_y).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.data_particles[idx], self.data_hlf[idx], self.data_y[idx])\n",
    "\n",
    "class InclusiveNetwork(nn.Module):\n",
    "    def __init__(self, num_hiddens=2, initial_node=500, dropout=0.5, gru_layers=2, gru_size=50, dropout_g=0.1, rnn_input=6, dnn_input=len(input_hlf_vars)):\n",
    "        super(InclusiveNetwork, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.dropout_g = dropout_g\n",
    "        self.hiddens = nn.ModuleList()\n",
    "        nodes = [initial_node]\n",
    "        for i in range(num_hiddens):\n",
    "            nodes.append(int(nodes[i]/2))\n",
    "            self.hiddens.append(nn.Linear(nodes[i],nodes[i+1]))\n",
    "        # self.gru = nn.GRU(input_size=7, hidden_size=gru_size, num_layers=gru_layers, batch_first=True, dropout=self.dropout_g)\n",
    "        self.gru = nn.GRU(input_size=rnn_input, hidden_size=gru_size, num_layers=gru_layers, batch_first=True, dropout=self.dropout_g)\n",
    "        self.merge = nn.Linear(dnn_input+gru_size,initial_node)\n",
    "        self.out = nn.Linear(nodes[-1],2)\n",
    "\n",
    "    def forward(self, particles, hlf):\n",
    "        _, hgru = self.gru(particles)\n",
    "        hgru = hgru[-1] # Get the last hidden layer\n",
    "        x = torch.cat((hlf,hgru), dim=1)\n",
    "        x = F.dropout(self.merge(x), training=self.training, p=self.dropout)\n",
    "        for i in range(len(self.hiddens)):\n",
    "            x = F.relu(self.hiddens[i](x))\n",
    "            x = F.dropout(x, training=self.training, p=self.dropout)\n",
    "        x = self.out(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early Stopping to terminate training early under certain conditions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 monitor='val_loss',\n",
    "                 min_delta=0,\n",
    "                 patience=10):\n",
    "        self.monitor = monitor\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "        self.stopped_epoch = 0\n",
    "        self.stop_training= False\n",
    "        #print(\"This is my patience {}\".format(patience))\n",
    "    \n",
    "    def on_train_begin(self):\n",
    "        self.wait = 0\n",
    "        self.best_loss = 1e15\n",
    "    \n",
    "    def on_epoch_end(self, epoch, current_loss):\n",
    "        if current_loss is None:\n",
    "            pass\n",
    "        else:\n",
    "            if (current_loss - self.best_loss) < -self.min_delta:\n",
    "                self.best_loss = current_loss\n",
    "                self.wait = 1\n",
    "            else:\n",
    "                if self.wait >= self.patience:\n",
    "                    self.stopped_epoch = epoch + 1\n",
    "                    self.stop_training = True\n",
    "                self.wait += 1\n",
    "            return  self.stop_training\n",
    "        \n",
    "    def on_train_end(self):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print('\\nTerminated training for early stopping at epoch %04i' % \n",
    "                (self.stopped_epoch))\n",
    "\n",
    "def train(num_epochs, model, criterion, optimizer,scheduler,volatile=False, data_loader=None, time='None', fold_idx=0):\n",
    "    best_model = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    train_losses ,val_losses = [],[]\n",
    "    callback = EarlyStopping(patience=10)\n",
    "    callback.on_train_begin()\n",
    "    breakdown = False\n",
    "    for epoch in range(num_epochs):\n",
    "        if breakdown:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['training', 'validation']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            if phase == 'training':\n",
    "                model.train() # Set model to training mode\n",
    "                volatile=False\n",
    "            else:\n",
    "                model.eval() # Set model to evaluate mode\n",
    "                volatile=True\n",
    "            \n",
    "\n",
    "            # Iterate over data.\n",
    "            for batch_idx, (particles_data, hlf_data, y_data) in enumerate(data_loader[phase]):\n",
    "                particles_data = particles_data.numpy()\n",
    "                arr = np.sum(particles_data!=0, axis=1)[:,0] # the number of particles in each batch\n",
    "                arr = [1 if x==0 else x for x in arr]\n",
    "                arr = np.array(arr)\n",
    "                sorted_indices_la = np.argsort(-arr)\n",
    "                particles_data = torch.from_numpy(particles_data[sorted_indices_la]).float()\n",
    "                hlf_data = hlf_data[sorted_indices_la]\n",
    "                y_data = y_data[sorted_indices_la]\n",
    "                particles_data = Variable(particles_data, requires_grad=not volatile).cuda() \n",
    "                # particles_data = Variable(particles_data, requires_grad=not volatile)\n",
    "                \n",
    "                hlf_data = Variable(hlf_data, requires_grad=not volatile).cuda()\n",
    "                # hlf_data = Variable(hlf_data, requires_grad=not volatile)\n",
    "                y_data = Variable(y_data, requires_grad=False).cuda()\n",
    "                # y_data = Variable(y_data, requires_grad=not volatile)\n",
    "                t_seq_length = [arr[i] for i in sorted_indices_la]\n",
    "                particles_data = torch.nn.utils.rnn.pack_padded_sequence(particles_data, t_seq_length, batch_first=True)\n",
    "                \n",
    "                if phase == 'training':\n",
    "                    optimizer.zero_grad()\n",
    "                # forward pass\n",
    "                outputs = model(particles_data, hlf_data)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, y_data)\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'training':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # statistics\n",
    "                # running_loss += loss.data[0]\n",
    "                running_loss += loss.data.item()\n",
    "                running_corrects += torch.sum(preds == y_data.data)\n",
    "                #print(\"I finished %d batch\" % batch_idx)\n",
    "            \n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = 100. * running_corrects / len(data_loader[phase].dataset)\n",
    "            if phase == 'training':\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                scheduler.step(epoch_loss)\n",
    "                val_losses.append(epoch_loss)\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'validation' and epoch_acc > best_acc:\n",
    "                print('Saving..')\n",
    "                state = {\n",
    "                        'net': model, #.module if use_cuda else net,\n",
    "                        'epoch': epoch,\n",
    "                        'best_acc':epoch_acc,\n",
    "                        'train_loss':train_losses,\n",
    "                        'val_loss':val_losses,\n",
    "                        }\n",
    "                if time is not None and fold_idx is not None:\n",
    "                    torch.save(state, OUTPUT_DIRPATH + time +'_' + state_file + '_{fold_idx}.torch')\n",
    "                    torch.save(model.state_dict(), OUTPUT_DIRPATH + time +'_' + model_file + '_{fold_idx}.torch')\n",
    "                # else:\n",
    "                #     torch.save(state, state_file + '.torch')\n",
    "                #     torch.save(model.state_dict(), model_file + '.torch'\n",
    "                best_acc = epoch_acc\n",
    "                best_model = model.state_dict()\n",
    "            if phase == 'validation':\n",
    "                # breakdown = callback.on_epoch_end(epoch, -epoch_acc)\n",
    "                breakdown = callback.on_epoch_end(epoch, epoch_loss)\n",
    "                \n",
    "         \n",
    "    print('Best val acc: {:4f}'.format(best_acc))\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    print('-' * 10)\n",
    "    return best_acc, train_losses, val_losses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.01, 'gru_layers': 2, 'gru_size': 442, 'dropout_g': 0.01, 'learning_rate': 0.003297552560160522, 'batch_size': 4000, 'L2_reg': 4.783663281646104e-05}\n",
      "Loaded best configuration from BestConfigReallyTopclass\n",
      "==================================================\n",
      "Fold 0\n",
      "==================================================\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 84.8817\n",
      "validation Loss: 0.0001 Acc: 86.9675\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0001 Acc: 86.8201\n",
      "validation Loss: 0.0001 Acc: 87.8164\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0001 Acc: 87.4278\n",
      "validation Loss: 0.0001 Acc: 88.3232\n",
      "Saving..\n",
      "Epoch 3/149\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "space  = [\n",
    "    Integer(1, 3, name='hidden_layers'),\n",
    "    Integer(10, 500, name='initial_nodes'),\n",
    "    Real(0.01,0.9,name='dropout'),\n",
    "    Integer(2, 3, name='gru_layers'),\n",
    "    Integer(10, 500, name='gru_size'),\n",
    "    Real(0.01,0.9,name='dropout_g'),\n",
    "    Real(10**-5, 10**-1, \"log-uniform\", name='learning_rate'),\n",
    "    Integer(4000,4001,name='batch_size'),\n",
    "    # Integer(32,512,name='batch_size'),\n",
    "    Real(10**-5, 10**-4, \"log-uniform\", name='L2_reg')\n",
    "]\n",
    "# L1 reg: https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch\n",
    "# batch_size = 4000\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**X):\n",
    "    print(\"New configuration: {}\".format(X))\n",
    "    fom = []\n",
    "    for train_index, test_index in skf.split(data_hlf, label):\n",
    "        train_loader = DataLoader(\n",
    "            ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), \n",
    "            batch_size=int(X['batch_size']), \n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), \n",
    "            batch_size=int(X['batch_size']), \n",
    "            shuffle=True\n",
    "        )\n",
    "        data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "        # print(train_loader)\n",
    "\n",
    "        model = InclusiveNetwork(\n",
    "            int(X['hidden_layers']), \n",
    "            int(X['initial_nodes']), \n",
    "            float(X['dropout']), \n",
    "            int(X['gru_layers']), \n",
    "            int(X['gru_size']), \n",
    "            float(X['dropout_g'])\n",
    "        ).cuda()\n",
    "        # model = InclusiveNetwork(X['hidden_layers'], X['initial_nodes'], X['dropout'], X['gru_layers'], X['gru_size'], X['dropout_g'])\n",
    "\n",
    "        optimizer = AMSGrad(model.parameters(), lr=X['learning_rate'], weight_decay=X['L2_reg'])\n",
    "        criterion= nn.NLLLoss()\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=4)\n",
    "        best_acc, train_losses, val_losses = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader, time=CURRENT_TIME, fold_idx=fold_idx)\n",
    "        fom.append(best_acc)\n",
    "    Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "    print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "    return -Y\n",
    "\n",
    "# res_gp = gp_minimize(objective, space, n_calls=30, random_state=0)\n",
    "\n",
    "# print(\"Best parameters: {}\".format(res_gp.x))\n",
    "# best_hidden_layers = int(res_gp.x[0])\n",
    "# best_initial_nodes = int(res_gp.x[1])\n",
    "# best_dropout = float(res_gp.x[2])\n",
    "# best_gru_layers = int(res_gp.x[3])\n",
    "# best_gru_size = int(res_gp.x[4])\n",
    "# best_dropout_g = float(res_gp.x[5])\n",
    "# best_learning_rate = float(res_gp.x[6])\n",
    "# best_batch_size = int(res_gp.x[7])\n",
    "# best_L2_reg = float(res_gp.x[8])\n",
    "\n",
    "# best_conf = {\"hidden_layers\": best_hidden_layers,\n",
    "#           \"initial_nodes\": best_initial_nodes,\n",
    "#           \"dropout\": best_dropout,\n",
    "#           \"gru_layers\": best_gru_layers,\n",
    "#           \"gru_size\": best_gru_size,\n",
    "#           \"dropout_g\": best_dropout_g,\n",
    "#           \"learning_rate\": best_learning_rate,\n",
    "#           \"batch_size\": best_batch_size,\n",
    "#           \"L2_reg\": best_L2_reg}\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + config_file + '.json', 'w') as config:\n",
    "#     json.dump(best_conf, config)\n",
    "#     print(\"Save best configuration to {}\".format(config_file))\n",
    "\n",
    "# best_conf = {\n",
    "#     'hidden_layers': 1, 'initial_nodes': 257, 'dropout': 0.01, \n",
    "#     'gru_layers': 2, 'gru_size': 497, 'dropout_g': 0.23179373811904597, \n",
    "#     'learning_rate': 0.002616690292572499, 'batch_size': 4001\n",
    "# }\n",
    "\n",
    "\n",
    "# train_index, test_index = skf.split(data_hlf, label).next()\n",
    "# train_index, test_index = next(skf.split(data_hlf, label))\n",
    "# model = InclusiveNetwork(best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "#                         best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']).cuda()\n",
    "# # model = InclusiveNetwork(best_hidden_layers, best_initial_nodes, best_dropout, best_gru_layers, best_gru_size, best_dropout_g).cuda()\n",
    "# # model = InclusiveNetwork(best_hidden_layers, best_initial_nodes, best_dropout, best_gru_layers, best_gru_size, best_dropout_g)\n",
    "# train_loader = DataLoader(ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), batch_size = best_conf['batch_size'], shuffle=True)\n",
    "# val_loader = DataLoader(ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), batch_size = best_conf['batch_size'], shuffle=True)\n",
    "# # train_loader = DataLoader(ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), batch_size = best_batch_size, shuffle=True)\n",
    "# # val_loader = DataLoader(ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), batch_size = best_batch_size, shuffle=True)\n",
    "# data_loader = {\"training\": train_loader, \"validation\": val_loader}\n",
    "# optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'])\n",
    "# # optimizer = AMSGrad(model.parameters(), lr=best_learning_rate)\n",
    "# criterion= nn.NLLLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=3)\n",
    "# EPOCHS = 70\n",
    "# best_acc, train_losses, val_losses = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader)\n",
    "# torch.save(model.state_dict(), model_file)\n",
    "\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + config_file + '.json') as f:\n",
    "with open('model_outputs/v0/' + config_file + '.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "    print(best_conf)\n",
    "print(\"Loaded best configuration from {}\".format(config_file))\n",
    "# model = InclusiveNetwork(best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']).cuda()\n",
    "\n",
    "\n",
    "fom = []\n",
    "# model = InclusiveNetwork(best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "                        # best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']).cuda()\n",
    "# optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "criterion= nn.NLLLoss()\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=3)\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "EPOCHS = 150\n",
    "for fold_idx, (train_index, test_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    print('='*50)\n",
    "    print(f'Fold {fold_idx}')\n",
    "    print('='*50)\n",
    "    model = InclusiveNetwork(best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode ='min',factor=0.5,patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), \n",
    "        batch_size=best_conf['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), \n",
    "        batch_size=best_conf['batch_size'], \n",
    "        shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(EPOCHS, model, criterion, optimizer, scheduler, data_loader=data_loader, time=CURRENT_TIME, fold_idx=fold_idx)\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    # model_i_file = OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'\n",
    "    # torch.save(model.state_dict(), model_i_file)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.01, 'gru_layers': 2, 'gru_size': 442, 'dropout_g': 0.01, 'learning_rate': 0.003297552560160522, 'batch_size': 4000, 'L2_reg': 4.783663281646104e-05}\n",
      "Loaded best configuration from model_outputs/v0/BestConfigReallyTopclass.json\n"
     ]
    }
   ],
   "source": [
    "with open('model_outputs/v0/' + config_file + '.json') as f:\n",
    "    bestconf = json.load(f)\n",
    "    print(bestconf)\n",
    "print(\"Loaded best configuration from {}\".format('model_outputs/v0/' + config_file))\n",
    "model = InclusiveNetwork(bestconf['hidden_layers'], bestconf['initial_nodes'], bestconf['dropout'], bestconf['gru_layers'], bestconf['gru_size'], bestconf['dropout_g']).cuda()\n",
    "# model = InclusiveNetwork(bestconf['hidden_layers'], bestconf['initial_nodes'], bestconf['dropout'], bestconf['gru_layers'], bestconf['gru_size'], bestconf['dropout_g'])\n",
    "# model.load_state_dict(torch.load(model_file))\n",
    "# print(\"Loaded best model parameter from {}\".format(model_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size=1, shuffle=False, req_grad=True):\n",
    "    # train_index, test_index = next(skf.split(data_hlf, label))\n",
    "    # full_test_data = DataLoader(ParticleHLF(data_list[test_index], data_hlf[test_index], label[test_index]), batch_size=batch_size, shuffle=shuffle)\n",
    "    full_test_data = DataLoader(ParticleHLF(data_list_test, data_hlf_test, label_test), batch_size=bestconf['batch_size'], shuffle=False)\n",
    "\n",
    "    for (particles_data, hlf_data, y_data) in full_test_data:\n",
    "\n",
    "        particles_data = particles_data.numpy()\n",
    "        # print(particles_data)\n",
    "        arr = np.sum(particles_data!=0, axis=1)[:,0] # the number of particles in each batch\n",
    "        arr = [1 if x==0 else x for x in arr]\n",
    "        arr = np.array(arr)\n",
    "        sorted_indices_la= np.argsort(-arr)\n",
    "        particles_data = torch.from_numpy(particles_data[sorted_indices_la]).float()\n",
    "        hlf_data = hlf_data[sorted_indices_la]\n",
    "        y_data = y_data[sorted_indices_la]\n",
    "        particles_data.requires_grad = req_grad\n",
    "        particles_data = particles_data.cuda()\n",
    "        # t_seq_length= [arr[i] for i in sorted_indices_la]\n",
    "        # particles_data = torch.nn.utils.rnn.pack_padded_sequence(particles_data, t_seq_length, batch_first=True)\n",
    "\n",
    "        hlf_data.requires_grad = req_grad\n",
    "        hlf_data = hlf_data.cuda()\n",
    "\n",
    "        y_data = Variable(y_data, requires_grad=False).cuda()\n",
    "\n",
    "        trial_input = (\n",
    "            particles_data, \n",
    "            hlf_data\n",
    "        )\n",
    "    return trial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Module of type <class 'torch.nn.modules.rnn.GRU'> has no rule defined and nodefault rule exists for this module type. Please, set a ruleexplicitly for this module and assure that it is appropriatefor this type of layer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m trial_input_lrp \u001b[38;5;241m=\u001b[39m get_data()\n\u001b[1;32m      5\u001b[0m lrp \u001b[38;5;241m=\u001b[39m LRP(model)\n\u001b[0;32m----> 6\u001b[0m attribution \u001b[38;5;241m=\u001b[39m \u001b[43mlrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial_input_lrp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(attribution)\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/captum/attr/_core/lrp.py:193\u001b[0m, in \u001b[0;36mLRP.attribute\u001b[0;34m(self, inputs, target, additional_forward_args, return_convergence_delta, verbose)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: List[Module] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_layers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_and_attach_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_handles: List[RemovableHandle] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_handles: List[RemovableHandle] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/captum/attr/_core/lrp.py:294\u001b[0m, in \u001b[0;36mLRP._check_and_attach_rules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m     layer\u001b[38;5;241m.\u001b[39mrule \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    295\u001b[0m         (\n\u001b[1;32m    296\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no rule defined and no\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault rule exists for this module type. Please, set a rule\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplicitly for this module and assure that it is appropriate\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this type of layer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m         )\n\u001b[1;32m    301\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Module of type <class 'torch.nn.modules.rnn.GRU'> has no rule defined and nodefault rule exists for this module type. Please, set a ruleexplicitly for this module and assure that it is appropriatefor this type of layer."
     ]
    }
   ],
   "source": [
    "from captum.attr import LRP\n",
    "\n",
    "trial_input_lrp = get_data()\n",
    "\n",
    "lrp = LRP(model)\n",
    "attribution = lrp.attribute(trial_input_lrp, target=1)\n",
    "print(attribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x Gradient (captum.attr.InputXGradient)\n",
      "--------------------RNN particles--------------------\n",
      "4 arrays = 4 max particles: l1, l2, dipho, MET\n",
      "6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET\n",
      "tensor([[ 1.0862e+00, -4.7879e-01, -6.9271e-01, -8.6824e-02, -4.6987e-02,\n",
      "          1.4419e-01],\n",
      "        [-1.1658e+00, -3.4647e-01,  5.9264e-01, -1.3464e-01, -1.1869e-01,\n",
      "          2.2870e-01],\n",
      "        [-1.3662e+00,  3.0545e-01,  1.0882e+00, -6.1627e-02, -1.1924e-02,\n",
      "          1.7489e-01],\n",
      "        [ 5.1425e-01,  2.5074e-03, -3.1560e-01,  1.6140e-02, -3.0173e-04,\n",
      "          1.1100e-03]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "--------------------DNN High Level Features--------------------\n",
      "9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \n",
      "    n_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj\n",
      "tensor([ 0.0799, -0.2580, -0.1419, -0.0764,  0.0240, -0.1035, -0.2513,  0.0443,\n",
      "        -0.0560], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import InputXGradient\n",
    "\n",
    "trial_input_ixg = get_data(batch_size=bestconf['batch_size'])\n",
    "# print('input')\n",
    "# print('-'*50)\n",
    "# print('-'*20 + 'RNN particles' + '-'*20)\n",
    "# print('4 arrays = 4 max particles: l1, l2, dipho, MET')\n",
    "# print('6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET')\n",
    "# print(trial_input_ixg[0])\n",
    "# print('-'*20 + 'DNN High Level Features' + '-'*20)\n",
    "# print('9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \\nn_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj')\n",
    "# print(trial_input_ixg[1])\n",
    "# print('='*50)\n",
    "\n",
    "model.train()\n",
    "output_rnn_ixg, output_dnn_ixg = None, None\n",
    "for fold_idx in range(skf.get_n_splits()):\n",
    "    model.load_state_dict(torch.load(f'ReallyTopclassStyle_{fold_idx}.torch'))\n",
    "    input_x_grad = InputXGradient(model)\n",
    "    attribution_ixg = input_x_grad.attribute(trial_input_ixg, target=1)\n",
    "    \n",
    "    if fold_idx == 0:\n",
    "        output_rnn_ixg = torch.mean(attribution_ixg[0], 0)\n",
    "        output_dnn_ixg = torch.mean(attribution_ixg[1], 0)\n",
    "    else:\n",
    "        output_rnn_ixg += torch.mean(attribution_ixg[0], 0)\n",
    "        output_dnn_ixg += torch.mean(attribution_ixg[1], 0)\n",
    "    \n",
    "\n",
    "print('Input x Gradient (captum.attr.InputXGradient)')\n",
    "print('-'*20 + 'RNN particles' + '-'*20)\n",
    "print('4 arrays = 4 max particles: l1, l2, dipho, MET')\n",
    "print('6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET')\n",
    "print(output_rnn_ixg / skf.get_n_splits())\n",
    "print('-'*20 + 'DNN High Level Features' + '-'*20)\n",
    "print('9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \\n    n_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj')\n",
    "print(output_dnn_ixg / skf.get_n_splits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Permutation (captum.attr.FeaturePermutation)\n",
      "--------------------RNN particles--------------------\n",
      "4 arrays = 4 max particles: l1, l2, dipho, MET\n",
      "6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET\n",
      "tensor([[ 1.0803e+00, -4.7619e-01, -6.9067e-01, -8.7585e-02, -4.8117e-02,\n",
      "          1.4357e-01],\n",
      "        [-1.1782e+00, -3.5606e-01,  5.9954e-01, -1.3248e-01, -1.1878e-01,\n",
      "          2.3600e-01],\n",
      "        [-1.3530e+00,  3.0737e-01,  1.0784e+00, -6.2604e-02, -1.1893e-02,\n",
      "          1.7370e-01],\n",
      "        [ 5.1794e-01,  3.0706e-03, -3.1958e-01,  1.5590e-02, -3.1101e-04,\n",
      "          1.1548e-03]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "--------------------DNN High Level Features--------------------\n",
      "9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \n",
      "    n_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj\n",
      "tensor([ 0.0812, -0.2633, -0.1487, -0.0765,  0.0245, -0.1023, -0.2515,  0.0450,\n",
      "        -0.0577], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from captum.attr import FeaturePermutation\n",
    "\n",
    "trial_input_fp = get_data(batch_size=bestconf['batch_size'], req_grad=False)\n",
    "\n",
    "# feat_perm = FeaturePermutation(model)\n",
    "# attribution_fp = feat_perm.attribute(trial_input_fp, target=1)\n",
    "# print('Feature Permutation (captum.attr.FeaturePermutation)')\n",
    "# print('-'*20 + 'RNN particles' + '-'*20)\n",
    "# print('4 arrays = 4 max particles: l1, l2, dipho, MET')\n",
    "# print('6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET')\n",
    "# # print(attribution_fp[0])\n",
    "# print(torch.mean(attribution_fp[0], 0))\n",
    "# print('-'*20 + 'DNN High Level Features' + '-'*20)\n",
    "# print('9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \\n    n_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj')\n",
    "# # print(attribution_fp[1])\n",
    "# print(torch.mean(attribution_fp[1], 0))\n",
    "\n",
    "model.eval()\n",
    "output_rnn_fp, output_dnn_fp = None, None\n",
    "for fold_idx in range(skf.get_n_splits()):\n",
    "    model.load_state_dict(torch.load(f'ReallyTopclassStyle_{fold_idx}.torch'))\n",
    "    feat_perm = FeaturePermutation(model)\n",
    "    attribution_fp = feat_perm.attribute(trial_input_fp, target=1)\n",
    "    \n",
    "    if fold_idx == 0:\n",
    "        output_rnn_fp = torch.mean(attribution_ixg[0], 0)\n",
    "        output_dnn_fp = torch.mean(attribution_ixg[1], 0)\n",
    "    else:\n",
    "        output_rnn_fp += torch.mean(attribution_ixg[0], 0)\n",
    "        output_dnn_fp += torch.mean(attribution_ixg[1], 0)\n",
    "\n",
    "print('Feature Permutation (captum.attr.FeaturePermutation)')\n",
    "print('-'*20 + 'RNN particles' + '-'*20)\n",
    "print('4 arrays = 4 max particles: l1, l2, dipho, MET')\n",
    "print('6 elements = 6 particle variables: pt, eta, phi, isLep, isDipho, isMET')\n",
    "print(output_rnn_fp / skf.get_n_splits())\n",
    "print('-'*20 + 'DNN High Level Features' + '-'*20)\n",
    "print('9 HLFs: puppiMET_sumEt, DeltaPhi_j1MET, DeltaPhi_j2MET, DeltaR_jg_min, \\n    n_jets, chi_t0, chi_t1, abs_CosThetaStar_CS, abs_CosThetaStar_jj')\n",
    "print(output_dnn_fp / skf.get_n_splits())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for InclusiveNetwork:\n\tMissing key(s) in state_dict: \"hiddens.0.weight\", \"hiddens.0.bias\", \"hiddens.1.weight\", \"hiddens.1.bias\", \"gru.weight_ih_l0\", \"gru.weight_hh_l0\", \"gru.bias_ih_l0\", \"gru.bias_hh_l0\", \"gru.weight_ih_l1\", \"gru.weight_hh_l1\", \"gru.bias_ih_l1\", \"gru.bias_hh_l1\", \"merge.weight\", \"merge.bias\", \"out.weight\", \"out.bias\". \n\tUnexpected key(s) in state_dict: \"net\", \"epoch\", \"best_acc\", \"train_loss\", \"val_loss\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# CURRENT_TIME = '2024-08-08_17-12-14'\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(skf\u001b[38;5;241m.\u001b[39mget_n_splits()):\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIRPATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCURRENT_TIME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ReallyTopclassStyle_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.torch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for InclusiveNetwork:\n\tMissing key(s) in state_dict: \"hiddens.0.weight\", \"hiddens.0.bias\", \"hiddens.1.weight\", \"hiddens.1.bias\", \"gru.weight_ih_l0\", \"gru.weight_hh_l0\", \"gru.bias_ih_l0\", \"gru.bias_hh_l0\", \"gru.weight_ih_l1\", \"gru.weight_hh_l1\", \"gru.bias_ih_l1\", \"gru.bias_hh_l1\", \"merge.weight\", \"merge.bias\", \"out.weight\", \"out.bias\". \n\tUnexpected key(s) in state_dict: \"net\", \"epoch\", \"best_acc\", \"train_loss\", \"val_loss\". "
     ]
    }
   ],
   "source": [
    "def fill_array(array_to_fill, value, index, batch_size):\n",
    "    array_to_fill[index*batch_size:min((index+1)*batch_size, array_to_fill.shape[0])] = value  \n",
    "\n",
    "TPR_thresholds = [0.96, 0.935, 0.9, 0.7, 0.5, 0.3]\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "\n",
    "fprs = []\n",
    "base_tpr = np.linspace(0, 1, 5000)\n",
    "thresholds = []\n",
    "# volatile=True\n",
    "best_batch_size = bestconf['batch_size']\n",
    "# for train_index, test_index in skf.split(data_hlf, label):\n",
    "val_loader = DataLoader(\n",
    "    ParticleHLF(data_list_test, data_hlf_test, label_test), \n",
    "    batch_size=bestconf['batch_size'],\n",
    "    shuffle=False\n",
    ")\n",
    "all_pred = np.zeros(shape=(len(data_hlf_test),2))\n",
    "all_label = np.zeros(shape=(len(data_hlf_test)))\n",
    "criterion= nn.NLLLoss()\n",
    "\n",
    "# CURRENT_TIME = '2024-08-08_17-12-14'\n",
    "for fold_idx in range(skf.get_n_splits()):\n",
    "    model.load_state_dict(torch.load(OUTPUT_DIRPATH + f'{CURRENT_TIME}_ReallyTopclassStyle_{fold_idx}.torch'))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (particles_data, hlf_data, y_data) in enumerate(val_loader):\n",
    "            # print(f\"val_loader: {batch_idx}\")\n",
    "            particles_data = particles_data.numpy()\n",
    "            arr = np.sum(particles_data!=0, axis=1)[:,0] # the number of particles in the whole batch\n",
    "            arr = [1 if x==0 else x for x in arr]\n",
    "            arr = np.array(arr)\n",
    "            sorted_indices_la= np.argsort(-arr)\n",
    "            particles_data = torch.from_numpy(particles_data[sorted_indices_la]).float()\n",
    "            hlf_data = hlf_data[sorted_indices_la]\n",
    "            particles_data = Variable(particles_data).cuda()\n",
    "            hlf_data = Variable(hlf_data).cuda()\n",
    "            # particles_data = Variable(particles_data)\n",
    "            # hlf_data = Variable(hlf_data)\n",
    "            t_seq_length= [arr[i] for i in sorted_indices_la]\n",
    "            particles_data = torch.nn.utils.rnn.pack_padded_sequence(particles_data, t_seq_length, batch_first=True)\n",
    "\n",
    "            outputs = model(particles_data, hlf_data)\n",
    "\n",
    "            # Unsort the predictions (to match the original data order)\n",
    "            # https://stackoverflow.com/questions/34159608/how-to-unsort-a-np-array-given-the-argsort\n",
    "            b = np.argsort(sorted_indices_la)\n",
    "            unsorted_pred = outputs[b].data.cpu().numpy()\n",
    "\n",
    "            fill_array(all_pred, unsorted_pred, batch_idx, best_batch_size)\n",
    "            fill_array(all_label, y_data.numpy(), batch_idx, best_batch_size)\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(all_label, np.exp(all_pred)[:,1])\n",
    "\n",
    "    fpr = np.interp(base_tpr, tpr, fpr)\n",
    "    threshold = np.interp(base_tpr, tpr, threshold)\n",
    "    fpr[0] = 0.0\n",
    "    fprs.append(fpr)\n",
    "    thresholds.append(threshold)\n",
    "\n",
    "thresholds = np.array(thresholds)\n",
    "mean_thresholds = thresholds.mean(axis=0)\n",
    "\n",
    "fprs = np.array(fprs)\n",
    "mean_fprs = fprs.mean(axis=0)\n",
    "std_fprs = fprs.std(axis=0)\n",
    "fprs_right = np.minimum(mean_fprs + std_fprs, 1)\n",
    "fprs_left = np.maximum(mean_fprs - std_fprs,0)\n",
    "\n",
    "mean_area = auc(mean_fprs, base_tpr)\n",
    "\n",
    "# [tl.tolist() for tl in train_losses_arr]\n",
    "\n",
    "\n",
    "IN_perf = {\n",
    "    'train losses': train_losses_arr,\n",
    "    'val losses': val_losses_arr,\n",
    "    'fprs': fprs.tolist(),\n",
    "    'thresholds': thresholds.tolist(),\n",
    "    'mean_fprs': mean_fprs.tolist(),\n",
    "    'mean_thresholds': mean_thresholds.tolist(),\n",
    "    'base_tpr': base_tpr.tolist(),\n",
    "    'mean_area': float(mean_area),\n",
    "    'all_pred': all_pred.tolist(),\n",
    "    'all_label': all_label.tolist()\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_{fold_idx}.json', 'w') as f:\n",
    "    json.dump(IN_perf, f)\n",
    "\n",
    "# with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}IN_perf_{fold_idx}.json', 'r') as f:\n",
    "#     IN_perf = json.load(f)\n",
    "\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(base_tpr>TPR_threshold)\n",
    "    NNtable.add_row([mean_thresholds[thres_idx], base_tpr[thres_idx], \"{:.4f} +/- {:.4f}\".format(mean_fprs[thres_idx], std_fprs[thres_idx])])\n",
    "print(NNtable)\n",
    "\n",
    "# plt.figure(figsize=(9,7))\n",
    "# for fold_idx in range(skf.get_n_splits()):\n",
    "#     plt.plot(range(len(IN_perf['train_losses_arr'][fold_idx])), IN_perf['train_losses_arr'][fold_idx], label=f\"Train data losses - fold {fold_idx}\", alpha=0.7)\n",
    "#     plt.plot(range(len(IN_perf['train_losses_arr'][fold_idx])), IN_perf['val_losses_arr'][fold_idx], label=f\"Validation data losses - fold {fold_idx}\", alpha=0.7)\n",
    "# # plt.plot(range(len(train_losses_arr[0])), train_losses_arr[0], label=\"Train data losses\")\n",
    "# # plt.plot(range(len(train_losses_arr[0])), val_losses_arr[0], label=\"Validation data losses\")\n",
    "# # plt.fill_betweenx(base_tpr, fprs_left, fprs_right, color='grey', alpha=0.4)\n",
    "# plt.legend(loc='best')\n",
    "# plt.xlabel('EPOCH')\n",
    "# plt.ylabel('Data Loss')\n",
    "\n",
    "# plt.figure(figsize=(9,7))\n",
    "# plt.plot(range(EPOCHS), val_losses, label=\"val losses vs. epoch\")\n",
    "# plt.fill_betweenx(base_tpr, fprs_left, fprs_right, color='grey', alpha=0.4)\n",
    "# plt.legend(loc='best')\n",
    "# plt.xlabel('EPOCH')\n",
    "# plt.ylabel('Validation data Loss')\n",
    "\n",
    "run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.plot(IN_perf['mean_fprs'], IN_perf['base_tpr'],label=\"Run3 NN AUC (test data) = %.4f\" % mean_area)\n",
    "plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "# plt.fill_betweenx(base_tpr, fprs_left, fprs_right, color='grey', alpha=0.4)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Background contamination')\n",
    "plt.ylabel('Signal efficiency')\n",
    "#plt.axhline(tpr[thres_idx],ls='--',color='tab:gray')\n",
    "#plt.axvline(fpr[thres_idx],ls='--',color='tab:gray')\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.hist(np.exp(all_pred)[all_label==0,1], bins=60, label='ttH background',alpha=0.5, density=True)\n",
    "plt.hist(np.exp(all_pred)[all_label==1,1], bins=60, label='HH signal', alpha=0.5, density=True)\n",
    "#plt.axvline(thresholds[thres_idx], ls='--',color='tab:gray')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"Threshold\", fontsize=18)\n",
    "\n",
    "with h5py.File(\"ReallyInclusive_ROC.h5\",\"w\") as out:\n",
    "    out['FPR'] = mean_fprs\n",
    "    out['dFPR'] = std_fprs\n",
    "    out['TPR'] = base_tpr\n",
    "    out['Thresholds'] = mean_thresholds\n",
    "    print(\"Saved ROC.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna-hhbbgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
