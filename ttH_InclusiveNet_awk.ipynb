{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmslpcgpu1.fnal.gov      Thu Aug 29 19:25:43 2024  555.42.02\n",
      "[0] Tesla P100-PCIE-12GB | 45°C,   0 % |  2748 / 12288 MB | ckapsiak(2746M)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Stdlib packages\n",
    "import copy\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Common Py packages\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "# HEP packages\n",
    "import gpustat\n",
    "import hist\n",
    "import mplhep as hep\n",
    "from cycler import cycler\n",
    "\n",
    "# ML packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Module packages\n",
    "from AMSGrad import AMSGrad\n",
    "from data_processing import process_data, data_list_index_map\n",
    "from evaluate import evaluate\n",
    "from InclusiveNetwork import InclusiveNetwork\n",
    "from ParticleHLF import ParticleHLF\n",
    "from space_optimization import optimize_hyperparams\n",
    "from train import train\n",
    "\n",
    "gpustat.print_gpustat()\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "plt.style.use(hep.style.CMS)\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "cmap_petroff10 = [\"#3f90da\", \"#ffa90e\", \"#bd1f01\", \"#94a4a2\", \"#832db6\", \"#a96b59\", \"#e76300\", \"#b9ac70\", \"#717581\", \"#92dadd\"]\n",
    "plt.rcParams.update({\"axes.prop_cycle\": cycler(\"color\", cmap_petroff10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpc_fileprefix = \"/eos/uscms/store/group/lpcdihiggsboost/tsievert/HiggsDNA_parquet/v1\"\n",
    "\n",
    "V2_MERGED = True\n",
    "\n",
    "SIGNAL_FILEPATHS = [\n",
    "    # Test sig files #\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE/GluGluToHH/nominal/*',\n",
    "    # '/uscms/home/tsievert/nobackup/XHYbbgg/HiggsDNA_official/output_test_HH/Run3_2022preEE_merged_v2/GluGluToHH/nominal/*',\n",
    "    # ggF HH # \n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluToHH/nominal/*\",\n",
    "    # VBF HH #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHHto2B2G_CV_1_C2V_1_C3_1/nominal/*\",\n",
    "]\n",
    "BKG_FILEPATHS = [\n",
    "    # ttH (i.e. the main bkg to reduce) #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/ttHToGG/nominal/*\",\n",
    "    # # Other single H samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GluGluHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VBFHToGG/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/VHToGG/nominal/*\",\n",
    "    # # Prompt-Prompt samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GGJets/nominal/*\",\n",
    "    # # Prompt-Fake samples #\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt20To40/nominal/*\",\n",
    "    # lpc_fileprefix+f\"/Run3_2022preEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\", lpc_fileprefix+f\"/Run3_2022postEE_merged{'_v2' if V2_MERGED else ''}/GJetPt40/nominal/*\",\n",
    "    # Fake-Fake samples #\n",
    "    # ADD HERE #\n",
    "]\n",
    "\n",
    "\n",
    "CURRENT_DIRPATH = str(Path().absolute())\n",
    "VERSION = 'v4'\n",
    "if VERSION == 'v1':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v2':\n",
    "    # CRITERION == \"BCELoss\"\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v3':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v4':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    # N_PARTICLES, N_PARTICLE_FIELDS = 6, 7\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "elif VERSION == 'v5':\n",
    "    CRITERION = \"NLLLoss\"\n",
    "    N_PARTICLES, N_PARTICLE_FIELDS = 4, 6\n",
    "# VARS = 'base_vars'\n",
    "# CURRENT_TIME = '2024-08-10_10-29-50'\n",
    "# CURRENT_TIME = '2024-08-17_18-23-49'\n",
    "# VARS = 'extra_vars'\n",
    "# CURRENT_TIME = '2024-08-10_13-16-12'\n",
    "# CURRENT_TIME = '2024-08-17_11-45-34'\n",
    "# CURRENT_TIME = '2024-08-20_23-02-48'\n",
    "# VARS = 'extra_vars_no_dijet_mass'\n",
    "# CURRENT_TIME = '2024-08-21_15-28-02'\n",
    "# VARS = 'no_bad_vars'\n",
    "# CURRENT_TIME = '2024-08-28_17-57-36'\n",
    "# VARS = 'simplified_bad_vars'\n",
    "# CURRENT_TIME = '2024-08-29_14-39-08'\n",
    "# VARS = 'extra_vars_and_bools'\n",
    "# CURRENT_TIME = '2024-08-29_16-33-23'\n",
    "VARS = 'extra_vars_in_RNN'\n",
    "OUTPUT_DIRPATH = CURRENT_DIRPATH + f\"/model_outputs/{VERSION}/{VARS}/\"\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIRPATH):\n",
    "    os.makedirs(OUTPUT_DIRPATH)\n",
    "\n",
    "SEED = 21\n",
    "OPTIMIZE_SPACE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/tsievert/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data list: (258332, 10, 12)\n",
      "Data HLF: (258332, 8)\n",
      "Data list test: (258924, 10, 12)\n",
      "Data HLF test: (258924, 8)\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    data_df, data_test_df, \n",
    "    data_list, data_hlf, label, \n",
    "    data_list_test, data_hlf_test, label_test, \n",
    "    high_level_fields, input_hlf_vars, hlf_vars_columns,\n",
    "    data_aux, data_test_aux\n",
    ") = process_data(\n",
    "    4, 6, SIGNAL_FILEPATHS, BKG_FILEPATHS, OUTPUT_DIRPATH, seed=SEED, return_pre_std=True\n",
    ")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_val_losses(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, sort=None\n",
    "):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['train_losses_arr'], label=f\"Train data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.plot(\n",
    "            range(len(IN_info['train_losses_arr'])), \n",
    "            IN_info['val_losses_arr'], label=f\"Validation data losses\", \n",
    "            alpha=0.7\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['solid', 'dotted']\n",
    "        linestyles = linestyles * ((2*len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:2*len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['train_losses_arr'][fold_idx], \n",
    "                label=f\"Train data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx if fold_idx%2 == 0 else fold_idx+1]\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(len(IN_info['train_losses_arr'][fold_idx])), \n",
    "                IN_info['val_losses_arr'][fold_idx], \n",
    "                label=f\"Validation data losses - fold {fold_idx}\", alpha=0.5,\n",
    "                linestyle=linestyles[fold_idx+1 if fold_idx%2 == 0 else fold_idx]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std' or 'arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('EPOCH')\n",
    "    plt.ylabel('Data Loss')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_train_val_losses{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc(\n",
    "    IN_info, plot_prefix, plot_postfix='', method='std', \n",
    "    labels=None, yscale='linear', run2=True, sort=None\n",
    "):\n",
    "    run2_sigEff = [.9704, .9498, .9196, .7536, .5777, .3837]\n",
    "    run2_bkgCont = [.2831, .2114, .1539, .0442, .0158, .0041]\n",
    "    run2_bkgCont_err = [.0077, .0036, .0011, .0032, .0006, .0001]\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        plt.plot(\n",
    "            IN_info['mean_fprs'], IN_info['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area'])\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            plt.plot(\n",
    "                IN_info['fprs'][fold_idx], IN_info['base_tpr'],\n",
    "                label=\"Run3 NN - fold %d\" % (fold_idx), linestyle=linestyles[fold_idx],\n",
    "                alpha=0.5\n",
    "            )\n",
    "        plt.plot(\n",
    "            IN_info['mean_fprs'], IN_info['base_tpr'], \n",
    "            label=\"Run3 NN AUC = %.4f\" % (IN_info['mean_area']),\n",
    "            alpha=0.8\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        if sort is not None:\n",
    "            index_arr = sort\n",
    "        else:\n",
    "            index_arr = range(len(IN_info))\n",
    "        for i in index_arr:\n",
    "            plt.plot(\n",
    "                IN_info[i]['mean_fprs'], IN_info[i]['base_tpr'], \n",
    "                label=(labels[i]+', ' if labels is not None else '') + \"AUC = %.4f\" % (IN_info[i]['mean_area']), \n",
    "                linestyle=linestyles[i]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    if run2:\n",
    "        plt.errorbar(run2_bkgCont, run2_sigEff, xerr=run2_bkgCont_err, label=\"Run2 NN AUC (val data) = {}\".format(0.9469))\n",
    "    if yscale is not None:\n",
    "        plt.yscale(yscale)\n",
    "    plt.ylim(0.1, 1.1)\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('Background contamination')\n",
    "    plt.ylabel('Signal efficiency')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_roc_curve{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_output_score(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, n_bins=50\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist], w2=np.array([sig_hist.variances(), bkg_hist.variances()]),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist], w2=np.array([sig_hist.variances(), bkg_hist.variances()]),\n",
    "                alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[fold_idx] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[fold_idx] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[fold_idx], linestyles[fold_idx]]\n",
    "            )\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'] if weights['sig'] is not None else np.ones_like(sig_np))\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'] if weights['sig'] is not None else np.ones_like(bkg_np))\n",
    "        hep.histplot(\n",
    "            [sig_hist, bkg_hist], w2=np.array([sig_hist.variances(), bkg_hist.variances()]),\n",
    "            alpha=0.7, density=(False if weights['sig'] is not None else True), histtype='step',\n",
    "            label=['HH signal', 'ttH background']\n",
    "        )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info[i]['mean_label']) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info[i]['mean_label']) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'] if weights[i]['sig'] is not None else np.ones_like(sig_np))\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'] if weights[i]['sig'] is not None else np.ones_like(bkg_np))\n",
    "            hep.histplot(\n",
    "                [sig_hist, bkg_hist], w2=np.array([sig_hist.variances(), bkg_hist.variances()]),\n",
    "                alpha=0.7, density=(False if weights[i]['sig'] is not None else True), histtype='step',\n",
    "                label=[\n",
    "                    'HH signal'+(' - '+labels[i] if labels is not None else ''), \n",
    "                    'ttH background'+(' - '+labels[i] if labels is not None else '')\n",
    "                ], linestyle=[linestyles[i], linestyles[i]]\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_score_dist{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def s_over_root_b(\n",
    "        IN_info, plot_prefix, plot_postfix='', method='arr', labels=None, \n",
    "        weights={'sig': None, 'bkg': None}, lines_fold=None, lines=None, lines_labels=None, \n",
    "        lines_colors=None, only_fold=None, no_fold=False, n_bins=50\n",
    "    ):\n",
    "    plt.figure(figsize=(9,7))\n",
    "    if method == 'std':\n",
    "        sig_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_info['mean_pred']\n",
    "        )[\n",
    "            np.array(IN_info['mean_label']) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "        plt.plot(\n",
    "            np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "            label='s/√b', alpha=0.8 \n",
    "        )\n",
    "    elif method == 'arr':\n",
    "        linestyles = ['dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info['all_preds']) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info['all_preds'])]\n",
    "        for fold_idx in range(skf.get_n_splits()):\n",
    "            if (only_fold is not None and fold_idx != only_fold) or no_fold == True:\n",
    "                continue\n",
    "            sig_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['all_preds'][fold_idx]\n",
    "            )[\n",
    "                np.array(IN_info['all_labels'][fold_idx]) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/√b'+(' - fold '+labels[fold_idx] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[fold_idx], \n",
    "            )\n",
    "            if lines_fold is not None:\n",
    "                for i in range(len(lines_fold[fold_idx])):\n",
    "                    plt.vlines(\n",
    "                        lines_fold[fold_idx][i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/√b'+(' - '+lines_labels[fold_idx][i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[fold_idx][i]\n",
    "                    )\n",
    "        if only_fold is None:\n",
    "            sig_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info['mean_label']) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info['mean_pred']\n",
    "            )[\n",
    "                np.array(IN_info['mean_label']) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "            s_over_root_b_points = sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten())\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), s_over_root_b_points, \n",
    "                label='s/√b - avg. over folds', \n",
    "                alpha=0.5, \n",
    "            )\n",
    "            if lines is not None:\n",
    "                for i in range(len(lines)):\n",
    "                    plt.vlines(\n",
    "                        lines[i], 0, np.max(s_over_root_b_points), \n",
    "                        label='s/√b - avg.'+(' - '+lines_labels[i] if lines_labels is not None else ''), \n",
    "                        alpha=0.5, colors=lines_colors[i]\n",
    "                    )\n",
    "    elif method == 'IN_arr':\n",
    "        linestyles = ['solid', 'dashed', 'dotted']\n",
    "        linestyles = linestyles * ((len(IN_info) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(IN_info)]\n",
    "        for i in range(len(IN_info)):\n",
    "            sig_np = np.exp(\n",
    "                IN_info[i]['all_preds'][0]\n",
    "            )[\n",
    "                np.array(IN_info[i]['all_labels'][0]) == 1,1\n",
    "            ]\n",
    "            bkg_np = np.exp(\n",
    "                IN_info[i]['all_preds'][0]\n",
    "            )[\n",
    "                np.array(IN_info[i]['all_labels'][0]) == 0,1\n",
    "            ]\n",
    "            hist_axis = hist.axis.Regular(n_bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "            sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights[i]['sig'])\n",
    "            bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights[i]['bkg'])\n",
    "            plt.plot(\n",
    "                np.arange(0., 1., 1/n_bins), sig_hist.values().flatten() / np.sqrt(bkg_hist.values().flatten()), \n",
    "                label='s/√b'+(' - '+labels[i] if labels is not None else ''), \n",
    "                alpha=0.5, linestyle=linestyles[i], \n",
    "            )\n",
    "    else:\n",
    "        raise Exception(f\"Must used methods 'std', 'arr', or 'IN_arr'. You used {method}.\")\n",
    "    plt.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel(\"Counts\", fontsize=18)\n",
    "    plt.xlabel(\"Threshold\", fontsize=18)\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.pdf', bbox_inches='tight')\n",
    "    plt.savefig(f'{plot_prefix}_model_s_over_b{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_NAMES_PRETTY = {\n",
    "    \"GGJets\": r\"$\\gamma\\gamma+3j$\",\n",
    "    \"GJetPt20To40\": r\"$\\gamma+j$, 20<$p_T$<40GeV\",\n",
    "    \"GJetPt40\": r\"$\\gamma+j$, 40GeV<$p_T$\",\n",
    "    \"GluGluHToGG\": r\"ggF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VBFHToGG\": r\"VBF $H\\rightarrow \\gamma\\gamma$\",\n",
    "    \"VHToGG\": r\"V$H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"ttHToGG\": r\"$t\\bar{t}H\\rightarrow\\gamma\\gamma$\",\n",
    "    \"GluGluToHH\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    \"signal\": r\"ggF $HH\\rightarrow bb\\gamma\\gamma$ + VBF $HH\\rightarrow bb\\gamma\\gamma$\"\n",
    "    # \"VBFHHto2B2G_CV_1_C2V_1_C3_1\": r\"VBF $HH\\rightarrow bb\\gamma\\gamma$\",\n",
    "    # Need to fill in pretty print for BSM samples #\n",
    "}\n",
    "LUMINOSITIES = {\n",
    "    '2022preEE': 7.9804, \n",
    "    '2022postEE': 26.6717,\n",
    "    # Need to fill in lumis for other eras #\n",
    "}\n",
    "LUMINOSITIES['total_lumi'] = sum(LUMINOSITIES.values())\n",
    "\n",
    "# Dictionary of variables\n",
    "VARIABLES = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(20,-3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(30, 0, 5, name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'jet1_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'jet2_pt': hist.axis.Regular(40, 20., 250, name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Integer(0, 10, name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, 0., 150, name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(30, 0., 500, name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'lepton1_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, 0., 200., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(30, -5., 5., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, 20., 2000, name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(20, -5., 5., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(20, -3.2, 3.2, name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(20, 0, 1, name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False),\n",
    "    'CosThetaStar_CS': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(20, -1, 1, name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(30, 0, 5, name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(50, 25., 180., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "# Dictionary of variables to do MC/Data comparison\n",
    "VARIABLES_STD = {\n",
    "    # key: hist.axis axes for plotting #\n",
    "    # MET variables\n",
    "    'puppiMET_sumEt': hist.axis.Regular(40, -10., 4., name='var', label=r'puppiMET $\\Sigma E_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'puppiMET $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'puppiMET_phi': hist.axis.Regular(40, -10., 4., name='var', label=r'puppiMET $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-MET variables\n",
    "    'DeltaPhi_j1MET': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta\\phi (j_1,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    'DeltaPhi_j2MET': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta\\phi (j_2,E_T^{miss})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-photon variables\n",
    "    'DeltaR_jg_min': hist.axis.Regular(40, -10., 4., name='var', label=r'min$(\\Delta R(jet, \\gamma))$', growth=False, underflow=False, overflow=False), \n",
    "    # jet variables\n",
    "    'jet1_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'lead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'jet2_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead jet $p_T$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'n_jets': hist.axis.Regular(40, -10., 4., name='var', label=r'$n_{jets}$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t0': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\chi_{t0}^2$', growth=False, underflow=False, overflow=False), \n",
    "    'chi_t1': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\chi_{t1}^2$', growth=False, underflow=False, overflow=False), \n",
    "    # lepton variables\n",
    "    'lepton1_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'lead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_pt': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead lepton $p_T$ [GeV]', growth=False, underflow=False, overflow=False), \n",
    "    'lepton1_eta': hist.axis.Regular(40, -10., 4., name='var', label=r'lead lepton $\\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_eta': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead lepton $\\eta$', growth=False, underflow=False, overflow=False),\n",
    "    'lepton1_phi': hist.axis.Regular(40, -10., 4., name='var', label=r'lead lepton $\\phi$', growth=False, underflow=False, overflow=False), \n",
    "    'lepton2_phi': hist.axis.Regular(40, -10., 4., name='var', label=r'sublead lepton $\\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables\n",
    "    'pt': hist.axis.Regular(40, -10., 4., name='var', label=r' $\\gamma\\gamma p_{T}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    'eta': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\gamma\\gamma \\eta$', growth=False, underflow=False, overflow=False), \n",
    "    'phi': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\gamma \\gamma \\phi$', growth=False, underflow=False, overflow=False),\n",
    "    # angular (cos) variables\n",
    "    'abs_CosThetaStar_CS': hist.axis.Regular(40, -10., 4., name='var', label=r'|cos$(\\theta_{CS})$|', growth=False, underflow=False, overflow=False), \n",
    "    'abs_CosThetaStar_jj': hist.axis.Regular(40, -10., 4., name='var', label=r'|cos$(\\theta_{jj})$|', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_CS': hist.axis.Regular(40, -10., 4., name='var', label=r'cos$(\\theta_{CS})$', growth=False, underflow=False, overflow=False), \n",
    "    'CosThetaStar_jj': hist.axis.Regular(40, -10., 4., name='var', label=r'cos$(\\theta_{jj})$', growth=False, underflow=False, overflow=False), \n",
    "    # jet-lepton variables\n",
    "    'leadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'leadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{lead}, l_{sublead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_leadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{lead})$', growth=False, underflow=False, overflow=False), \n",
    "    'subleadBjet_subleadLepton': hist.axis.Regular(40, -10., 4., name='var', label=r'$\\Delta R(bjet_{sublead}, l_{sublead})$', growth=False, underflow=False, overflow=False),\n",
    "    # dijet variables (must be blinded on data)\n",
    "    'dijet_mass': hist.axis.Regular(40, -10., 4., name='var', label=r'$M_{jj}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "    # diphoton variables (must be blinded on data)\n",
    "    'mass': hist.axis.Regular(40, -10., 4., name='var', label=r'$M_{\\gamma\\gamma}$ [GeV]', growth=False, underflow=False, overflow=False),\n",
    "}\n",
    "\n",
    "def post_std_np_arrays(\n",
    "        data, data_test, var_name, train_index=None, val_index=None\n",
    "):\n",
    "    sig_mask = label == 1\n",
    "    sig_test_mask = label_test == 1\n",
    "    bkg_mask = label == 0\n",
    "    bkg_test_mask = label_test == 0\n",
    "    if train_index is not None and val_index is not None:\n",
    "        sig_train_mask = sig_mask & train_index \n",
    "        sig_val_mask = sig_mask & val_index\n",
    "        bkg_train_mask = bkg_mask & train_index\n",
    "        bkg_val_mask = bkg_mask & val_index\n",
    "        if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            sig_val_np = data[data_list_index_map(var_name, data, sig_val_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, sig_train_mask)]\n",
    "            bkg_val_np = data[data_list_index_map(var_name, data, bkg_val_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns[var_name]\n",
    "            sig_train_np = data[sig_train_mask, index2]\n",
    "            sig_val_np = data[sig_val_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_train_mask, index2]\n",
    "            bkg_val_np = data[bkg_val_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "\n",
    "        return (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np\n",
    "        )\n",
    "    elif train_index is None and val_index is None:\n",
    "        if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "            # index2, index3 = index_map[var_name]\n",
    "            sig_train_np = data[data_list_index_map(var_name, data, sig_mask)]\n",
    "            sig_test_np = data_test[data_list_index_map(var_name, data_test, sig_test_mask)]\n",
    "            bkg_train_np = data[data_list_index_map(var_name, data, bkg_mask)]\n",
    "            bkg_test_np = data_test[data_list_index_map(var_name, data_test, bkg_test_mask)]\n",
    "        else:\n",
    "            index2 = hlf_vars_columns[var_name]\n",
    "            sig_train_np = data[sig_mask, index2]\n",
    "            sig_test_np = data_test[sig_test_mask, index2]\n",
    "            bkg_train_np = data[bkg_mask, index2]\n",
    "            bkg_test_np = data_test[bkg_test_mask, index2]\n",
    "        return (\n",
    "            copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "            copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(\"Either both train_index and val_index must be 'None', or both should not be 'None'. You cannot mix and match.\")\n",
    "\n",
    "def aux_np_arrays(var_name, score_cut, IN_full_eval_dict):\n",
    "    sig_train_mask = (label == 1) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    sig_test_mask = (label_test == 1) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_train_mask = (label == 0) & (\n",
    "        np.exp(IN_full_eval_dict['train']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "    bkg_test_mask = (label_test == 0) & (\n",
    "        np.exp(IN_full_eval_dict['test']['mean_pred'])[:, 1] > score_cut\n",
    "    )\n",
    "\n",
    "    sig_train_np = data_aux.loc[sig_train_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_aux.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_aux.loc[bkg_train_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_aux.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "    return (\n",
    "        copy.deepcopy(sig_train_np), copy.deepcopy(sig_test_np), \n",
    "        copy.deepcopy(bkg_train_np), copy.deepcopy(bkg_test_np)\n",
    "    )\n",
    "\n",
    "def make_input_plot(output_dir, var_name, hist_list, fold_idx=None, labels=None, density=True, plot_prefix='', plot_postfix='', alpha=0.8, linestyle=True):\n",
    "    fig, ax = plt.subplots()\n",
    "    if linestyle:\n",
    "        if fold_idx is not None:\n",
    "            linestyles = [\"solid\", \"dashed\", \"dotted\", \"solid\", \"dashed\", \"dotted\"]\n",
    "        else:\n",
    "            linestyles = [\"solid\", \"dotted\", \"solid\", \"dotted\"]\n",
    "        linestyles = linestyles * ((len(hist_list) // len(linestyles)) + 1)\n",
    "        linestyles = linestyles[:len(hist_list)]\n",
    "    else:\n",
    "        linestyles = None\n",
    "    hep.histplot(\n",
    "        hist_list, ax=ax, linewidth=3, histtype=\"step\", yerr=True, density=density,\n",
    "        linestyle=linestyles, label=labels, alpha=alpha\n",
    "    )\n",
    "    # Plotting niceties #\n",
    "    hep.cms.lumitext(f\"{LUMINOSITIES['total_lumi']:.2f}\" + r\"fb$^{-1}$ (13.6 TeV)\", ax=ax)\n",
    "    hep.cms.text(\"Work in Progress\", ax=ax)\n",
    "    # Plot legend properly\n",
    "    ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "    # Make angular and chi^2 plots linear, otherwise log\n",
    "    if re.match('chi_t', var_name) is None and re.match('DeltaPhi', var_name) is None and re.match('mass', var_name) is None:\n",
    "        ax.set_yscale('log')\n",
    "    else:\n",
    "        ax.set_yscale('linear')\n",
    "    ax.set_yscale('linear')\n",
    "    # Save out the plot\n",
    "    if fold_idx is not None:\n",
    "        output_dir = output_dir + \"fold/\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}_fold{fold_idx}.png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.pdf', bbox_inches='tight')\n",
    "        plt.savefig(f'{output_dir}/{plot_prefix}1dhist_{var_name}{plot_postfix}.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layers': 2, 'initial_nodes': 500, 'dropout': 0.27817607062770483, 'gru_layers': 2, 'gru_size': 500, 'dropout_g': 0.6184468141076988, 'learning_rate': 0.004851812500501461, 'batch_size': 4000, 'L2_reg': 0.0001}\n",
      "Epoch 0/149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/AMSGrad.py:86: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha) (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1706619781071/work/torch/csrc/utils/python_arg_parser.cpp:1519.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0001 Acc: 60.1776\n",
      "validation Loss: 0.0000 Acc: 72.1505\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 70.8896\n",
      "validation Loss: 0.0000 Acc: 70.8189\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 71.7911\n",
      "validation Loss: 0.0000 Acc: 74.3840\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 71.6478\n",
      "validation Loss: 0.0000 Acc: 74.9802\n",
      "Saving..\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 72.5280\n",
      "validation Loss: 0.0000 Acc: 75.4195\n",
      "Saving..\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 72.9591\n",
      "validation Loss: 0.0000 Acc: 72.7137\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 73.1043\n",
      "validation Loss: 0.0000 Acc: 69.6112\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 72.6712\n",
      "validation Loss: 0.0000 Acc: 72.3499\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 73.0099\n",
      "validation Loss: 0.0000 Acc: 73.9273\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 73.5533\n",
      "validation Loss: 0.0000 Acc: 77.0279\n",
      "Saving..\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 73.5635\n",
      "validation Loss: 0.0000 Acc: 71.0008\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 73.2398\n",
      "validation Loss: 0.0000 Acc: 76.7724\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 73.6685\n",
      "validation Loss: 0.0000 Acc: 74.3724\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 73.4561\n",
      "validation Loss: 0.0000 Acc: 72.0189\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 73.6090\n",
      "validation Loss: 0.0000 Acc: 73.5595\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 73.7546\n",
      "validation Loss: 0.0000 Acc: 74.9511\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 73.6061\n",
      "validation Loss: 0.0000 Acc: 71.6628\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 73.6487\n",
      "validation Loss: 0.0000 Acc: 75.4756\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 73.8011\n",
      "validation Loss: 0.0000 Acc: 70.8479\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 73.9249\n",
      "validation Loss: 0.0000 Acc: 75.3073\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 74.2859\n",
      "validation Loss: 0.0000 Acc: 75.0034\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 74.0188\n",
      "validation Loss: 0.0000 Acc: 74.8814\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 74.2879\n",
      "validation Loss: 0.0000 Acc: 73.7608\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 74.1146\n",
      "validation Loss: 0.0000 Acc: 74.6879\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 74.0788\n",
      "validation Loss: 0.0000 Acc: 74.1324\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 73.9690\n",
      "validation Loss: 0.0000 Acc: 72.5511\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 74.1538\n",
      "validation Loss: 0.0000 Acc: 76.0272\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 74.1800\n",
      "validation Loss: 0.0000 Acc: 76.7221\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 74.4456\n",
      "validation Loss: 0.0000 Acc: 75.3750\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 74.4069\n",
      "validation Loss: 0.0000 Acc: 74.8389\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 74.4480\n",
      "validation Loss: 0.0000 Acc: 74.7344\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 74.2898\n",
      "validation Loss: 0.0000 Acc: 75.2840\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 74.6687\n",
      "validation Loss: 0.0000 Acc: 74.2853\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 74.2414\n",
      "validation Loss: 0.0000 Acc: 74.6356\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 74.1577\n",
      "validation Loss: 0.0000 Acc: 75.3518\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 74.6222\n",
      "validation Loss: 0.0000 Acc: 73.9195\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 74.3508\n",
      "validation Loss: 0.0000 Acc: 74.2737\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 74.4732\n",
      "validation Loss: 0.0000 Acc: 74.5640\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 74.3662\n",
      "validation Loss: 0.0000 Acc: 75.8414\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 74.2288\n",
      "validation Loss: 0.0000 Acc: 73.9698\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 74.5138\n",
      "validation Loss: 0.0000 Acc: 73.4376\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 74.2772\n",
      "validation Loss: 0.0000 Acc: 77.0685\n",
      "Saving..\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 74.6150\n",
      "validation Loss: 0.0000 Acc: 74.6221\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 74.4577\n",
      "validation Loss: 0.0000 Acc: 74.1731\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 74.6309\n",
      "validation Loss: 0.0000 Acc: 74.8002\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 74.5777\n",
      "validation Loss: 0.0000 Acc: 74.7943\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 74.4693\n",
      "validation Loss: 0.0000 Acc: 75.0460\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 74.6058\n",
      "validation Loss: 0.0000 Acc: 74.8621\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 74.6508\n",
      "validation Loss: 0.0000 Acc: 76.6176\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 74.4717\n",
      "validation Loss: 0.0000 Acc: 76.1279\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 74.6212\n",
      "validation Loss: 0.0000 Acc: 74.3124\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 74.6382\n",
      "validation Loss: 0.0000 Acc: 75.8356\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 74.7011\n",
      "validation Loss: 0.0000 Acc: 75.3866\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 74.6019\n",
      "validation Loss: 0.0000 Acc: 75.3402\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 74.6871\n",
      "validation Loss: 0.0000 Acc: 75.2705\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 74.6633\n",
      "validation Loss: 0.0000 Acc: 75.3479\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 74.4650\n",
      "validation Loss: 0.0000 Acc: 75.9169\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 74.6953\n",
      "validation Loss: 0.0000 Acc: 75.3750\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 74.7050\n",
      "validation Loss: 0.0000 Acc: 76.1027\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 74.7567\n",
      "validation Loss: 0.0000 Acc: 76.1492\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 74.5395\n",
      "validation Loss: 0.0000 Acc: 75.7234\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 74.7475\n",
      "validation Loss: 0.0000 Acc: 74.8524\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 74.6382\n",
      "validation Loss: 0.0000 Acc: 75.7466\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 74.8816\n",
      "validation Loss: 0.0000 Acc: 75.0905\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 74.6745\n",
      "validation Loss: 0.0000 Acc: 74.7634\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 74.7756\n",
      "validation Loss: 0.0000 Acc: 75.0634\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 74.7925\n",
      "validation Loss: 0.0000 Acc: 75.1273\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 74.5748\n",
      "validation Loss: 0.0000 Acc: 75.3595\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 74.7519\n",
      "validation Loss: 0.0000 Acc: 75.8705\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 74.7650\n",
      "validation Loss: 0.0000 Acc: 75.3034\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 74.5859\n",
      "validation Loss: 0.0000 Acc: 75.3518\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 74.6696\n",
      "validation Loss: 0.0000 Acc: 75.8066\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 74.7838\n",
      "validation Loss: 0.0000 Acc: 75.2840\n",
      "Epoch 73/149\n",
      "training Loss: 0.0000 Acc: 74.8148\n",
      "validation Loss: 0.0000 Acc: 75.5666\n",
      "Early stopped.\n",
      "Best val acc: 77.068535\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 59.2156\n",
      "validation Loss: 0.0000 Acc: 71.2776\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 70.7227\n",
      "validation Loss: 0.0000 Acc: 74.1537\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 71.4775\n",
      "validation Loss: 0.0000 Acc: 72.4698\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 72.1351\n",
      "validation Loss: 0.0000 Acc: 72.4118\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 72.1235\n",
      "validation Loss: 0.0000 Acc: 71.6124\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 72.7419\n",
      "validation Loss: 0.0000 Acc: 72.6924\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 72.3470\n",
      "validation Loss: 0.0000 Acc: 74.9647\n",
      "Saving..\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 73.1396\n",
      "validation Loss: 0.0000 Acc: 74.5466\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 73.2408\n",
      "validation Loss: 0.0000 Acc: 71.9550\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 73.0956\n",
      "validation Loss: 0.0000 Acc: 70.8557\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 73.0816\n",
      "validation Loss: 0.0000 Acc: 75.0014\n",
      "Saving..\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 73.1808\n",
      "validation Loss: 0.0000 Acc: 77.2021\n",
      "Saving..\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 73.3187\n",
      "validation Loss: 0.0000 Acc: 76.9195\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 73.5645\n",
      "validation Loss: 0.0000 Acc: 71.2118\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 73.6569\n",
      "validation Loss: 0.0000 Acc: 72.6479\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 73.2199\n",
      "validation Loss: 0.0000 Acc: 73.4086\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 73.4827\n",
      "validation Loss: 0.0000 Acc: 72.4931\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 73.3816\n",
      "validation Loss: 0.0000 Acc: 76.1589\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 73.2291\n",
      "validation Loss: 0.0000 Acc: 75.3363\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 73.5340\n",
      "validation Loss: 0.0000 Acc: 73.7647\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 73.4599\n",
      "validation Loss: 0.0000 Acc: 72.6518\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 73.5620\n",
      "validation Loss: 0.0000 Acc: 75.2260\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 73.6225\n",
      "validation Loss: 0.0000 Acc: 75.6421\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 73.7512\n",
      "validation Loss: 0.0000 Acc: 76.3679\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 73.8616\n",
      "validation Loss: 0.0000 Acc: 70.7066\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 73.3400\n",
      "validation Loss: 0.0000 Acc: 74.0086\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 74.0508\n",
      "validation Loss: 0.0000 Acc: 74.1653\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 73.7299\n",
      "validation Loss: 0.0000 Acc: 73.0021\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 74.1383\n",
      "validation Loss: 0.0000 Acc: 73.4918\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 74.1330\n",
      "validation Loss: 0.0000 Acc: 76.4008\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 74.2254\n",
      "validation Loss: 0.0000 Acc: 74.5795\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 73.9801\n",
      "validation Loss: 0.0000 Acc: 74.0182\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 74.1287\n",
      "validation Loss: 0.0000 Acc: 75.1350\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 74.2637\n",
      "validation Loss: 0.0000 Acc: 75.8898\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 74.3338\n",
      "validation Loss: 0.0000 Acc: 73.9331\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 73.8921\n",
      "validation Loss: 0.0000 Acc: 73.2498\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 74.1843\n",
      "validation Loss: 0.0000 Acc: 75.6227\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 74.3338\n",
      "validation Loss: 0.0000 Acc: 75.5453\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 74.2782\n",
      "validation Loss: 0.0000 Acc: 72.1679\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 74.2351\n",
      "validation Loss: 0.0000 Acc: 74.7537\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 74.4654\n",
      "validation Loss: 0.0000 Acc: 75.1640\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 74.2540\n",
      "validation Loss: 0.0000 Acc: 74.9627\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 74.2143\n",
      "validation Loss: 0.0000 Acc: 75.0847\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 74.4761\n",
      "validation Loss: 0.0000 Acc: 73.2653\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 74.2501\n",
      "validation Loss: 0.0000 Acc: 75.6092\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 74.3996\n",
      "validation Loss: 0.0000 Acc: 74.2660\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 74.5883\n",
      "validation Loss: 0.0000 Acc: 73.7956\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 74.3943\n",
      "validation Loss: 0.0000 Acc: 73.5944\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 74.4437\n",
      "validation Loss: 0.0000 Acc: 75.4389\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 74.3706\n",
      "validation Loss: 0.0000 Acc: 75.1602\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 74.4930\n",
      "validation Loss: 0.0000 Acc: 75.7485\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 74.4529\n",
      "validation Loss: 0.0000 Acc: 74.5524\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 74.6793\n",
      "validation Loss: 0.0000 Acc: 74.8640\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 74.5269\n",
      "validation Loss: 0.0000 Acc: 74.4266\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 74.5472\n",
      "validation Loss: 0.0000 Acc: 74.3144\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 74.3454\n",
      "validation Loss: 0.0000 Acc: 75.6808\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 74.6106\n",
      "validation Loss: 0.0000 Acc: 74.1208\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 74.6169\n",
      "validation Loss: 0.0000 Acc: 74.5602\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 74.4330\n",
      "validation Loss: 0.0000 Acc: 75.0344\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 74.4456\n",
      "validation Loss: 0.0000 Acc: 75.1853\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 74.6425\n",
      "validation Loss: 0.0000 Acc: 74.5427\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 74.6058\n",
      "validation Loss: 0.0000 Acc: 75.2763\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 74.3246\n",
      "validation Loss: 0.0000 Acc: 75.2473\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 74.6212\n",
      "validation Loss: 0.0000 Acc: 75.1834\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 74.6929\n",
      "validation Loss: 0.0000 Acc: 74.5892\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 74.6392\n",
      "validation Loss: 0.0000 Acc: 74.8079\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 74.4906\n",
      "validation Loss: 0.0000 Acc: 75.0866\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 74.5462\n",
      "validation Loss: 0.0000 Acc: 75.1931\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 74.5400\n",
      "validation Loss: 0.0000 Acc: 75.0208\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 74.6982\n",
      "validation Loss: 0.0000 Acc: 74.5485\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 74.4882\n",
      "validation Loss: 0.0000 Acc: 75.5163\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 74.7974\n",
      "validation Loss: 0.0000 Acc: 74.9279\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 74.5932\n",
      "validation Loss: 0.0000 Acc: 75.1253\n",
      "Epoch 73/149\n",
      "training Loss: 0.0000 Acc: 74.7441\n",
      "validation Loss: 0.0000 Acc: 74.8776\n",
      "Epoch 74/149\n",
      "training Loss: 0.0000 Acc: 74.5496\n",
      "validation Loss: 0.0000 Acc: 74.6569\n",
      "Epoch 75/149\n",
      "training Loss: 0.0000 Acc: 74.6058\n",
      "validation Loss: 0.0000 Acc: 74.6066\n",
      "Epoch 76/149\n",
      "training Loss: 0.0000 Acc: 74.5806\n",
      "validation Loss: 0.0000 Acc: 75.0537\n",
      "Epoch 77/149\n",
      "training Loss: 0.0000 Acc: 74.5941\n",
      "validation Loss: 0.0000 Acc: 75.0479\n",
      "Epoch 78/149\n",
      "training Loss: 0.0000 Acc: 74.6808\n",
      "validation Loss: 0.0000 Acc: 74.9550\n",
      "Early stopped.\n",
      "Best val acc: 77.202080\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 60.1434\n",
      "validation Loss: 0.0000 Acc: 72.4171\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 70.9009\n",
      "validation Loss: 0.0000 Acc: 69.8835\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 71.9625\n",
      "validation Loss: 0.0000 Acc: 76.4100\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 72.1609\n",
      "validation Loss: 0.0000 Acc: 73.8958\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 72.6825\n",
      "validation Loss: 0.0000 Acc: 72.6803\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 73.1847\n",
      "validation Loss: 0.0000 Acc: 76.6597\n",
      "Saving..\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 72.9989\n",
      "validation Loss: 0.0000 Acc: 74.7377\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 73.1552\n",
      "validation Loss: 0.0000 Acc: 74.7416\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 73.1310\n",
      "validation Loss: 0.0000 Acc: 74.4842\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 73.1867\n",
      "validation Loss: 0.0000 Acc: 71.3738\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 73.2027\n",
      "validation Loss: 0.0000 Acc: 72.2816\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 73.4785\n",
      "validation Loss: 0.0000 Acc: 77.8578\n",
      "Saving..\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 73.8873\n",
      "validation Loss: 0.0000 Acc: 75.4074\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 73.8844\n",
      "validation Loss: 0.0000 Acc: 72.7287\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 73.6227\n",
      "validation Loss: 0.0000 Acc: 73.9132\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 74.1931\n",
      "validation Loss: 0.0000 Acc: 74.1822\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 73.8622\n",
      "validation Loss: 0.0000 Acc: 73.3829\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 74.1515\n",
      "validation Loss: 0.0000 Acc: 73.1642\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 74.0920\n",
      "validation Loss: 0.0000 Acc: 71.4319\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 74.0823\n",
      "validation Loss: 0.0000 Acc: 74.9816\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 74.2125\n",
      "validation Loss: 0.0000 Acc: 75.3842\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 73.9856\n",
      "validation Loss: 0.0000 Acc: 74.1958\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 73.8215\n",
      "validation Loss: 0.0000 Acc: 73.7487\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 74.0470\n",
      "validation Loss: 0.0000 Acc: 74.3061\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 74.2280\n",
      "validation Loss: 0.0000 Acc: 72.1267\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 74.2236\n",
      "validation Loss: 0.0000 Acc: 74.2403\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 74.4859\n",
      "validation Loss: 0.0000 Acc: 71.2306\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 73.8980\n",
      "validation Loss: 0.0000 Acc: 74.0526\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 74.2725\n",
      "validation Loss: 0.0000 Acc: 74.2210\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 74.2894\n",
      "validation Loss: 0.0000 Acc: 74.3313\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 74.3896\n",
      "validation Loss: 0.0000 Acc: 73.7603\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 74.2299\n",
      "validation Loss: 0.0000 Acc: 74.8906\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 74.4099\n",
      "validation Loss: 0.0000 Acc: 74.9313\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 74.2735\n",
      "validation Loss: 0.0000 Acc: 74.9294\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 74.4893\n",
      "validation Loss: 0.0000 Acc: 74.3951\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 74.4549\n",
      "validation Loss: 0.0000 Acc: 75.2158\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 74.4685\n",
      "validation Loss: 0.0000 Acc: 72.0842\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 74.5546\n",
      "validation Loss: 0.0000 Acc: 74.5287\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 74.2227\n",
      "validation Loss: 0.0000 Acc: 74.1958\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 74.5473\n",
      "validation Loss: 0.0000 Acc: 74.8558\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 74.5725\n",
      "validation Loss: 0.0000 Acc: 75.6416\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 74.6601\n",
      "validation Loss: 0.0000 Acc: 73.1564\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 74.3480\n",
      "validation Loss: 0.0000 Acc: 74.8500\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 74.6494\n",
      "validation Loss: 0.0000 Acc: 73.6693\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 74.8628\n",
      "validation Loss: 0.0000 Acc: 74.1474\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 74.4564\n",
      "validation Loss: 0.0000 Acc: 73.4584\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 74.3460\n",
      "validation Loss: 0.0000 Acc: 74.4842\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 74.6020\n",
      "validation Loss: 0.0000 Acc: 75.4016\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 74.5294\n",
      "validation Loss: 0.0000 Acc: 74.9990\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 74.6306\n",
      "validation Loss: 0.0000 Acc: 74.8364\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 74.6296\n",
      "validation Loss: 0.0000 Acc: 75.0939\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 74.6683\n",
      "validation Loss: 0.0000 Acc: 74.7300\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 74.8086\n",
      "validation Loss: 0.0000 Acc: 74.6158\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 74.7022\n",
      "validation Loss: 0.0000 Acc: 74.8732\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 74.5560\n",
      "validation Loss: 0.0000 Acc: 74.7281\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 74.6775\n",
      "validation Loss: 0.0000 Acc: 75.2410\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 74.9170\n",
      "validation Loss: 0.0000 Acc: 74.7900\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 74.5952\n",
      "validation Loss: 0.0000 Acc: 74.1571\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 74.8604\n",
      "validation Loss: 0.0000 Acc: 73.8590\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 74.5889\n",
      "validation Loss: 0.0000 Acc: 74.6042\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 74.9475\n",
      "validation Loss: 0.0000 Acc: 74.4610\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 74.7022\n",
      "validation Loss: 0.0000 Acc: 75.0532\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 74.7104\n",
      "validation Loss: 0.0000 Acc: 75.0203\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 74.7960\n",
      "validation Loss: 0.0000 Acc: 75.5100\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 74.8739\n",
      "validation Loss: 0.0000 Acc: 74.7339\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 74.8914\n",
      "validation Loss: 0.0000 Acc: 74.6642\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 74.7868\n",
      "validation Loss: 0.0000 Acc: 74.7126\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 74.9920\n",
      "validation Loss: 0.0000 Acc: 74.2964\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 74.7380\n",
      "validation Loss: 0.0000 Acc: 74.7939\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 74.7428\n",
      "validation Loss: 0.0000 Acc: 74.8868\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 74.7781\n",
      "validation Loss: 0.0000 Acc: 75.0861\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 74.8507\n",
      "validation Loss: 0.0000 Acc: 75.1752\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 74.8014\n",
      "validation Loss: 0.0000 Acc: 75.0203\n",
      "Epoch 73/149\n",
      "training Loss: 0.0000 Acc: 74.8773\n",
      "validation Loss: 0.0000 Acc: 74.8190\n",
      "Epoch 74/149\n",
      "training Loss: 0.0000 Acc: 74.7840\n",
      "validation Loss: 0.0000 Acc: 74.8674\n",
      "Early stopped.\n",
      "Best val acc: 77.857773\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 60.2049\n",
      "validation Loss: 0.0000 Acc: 69.6803\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 70.3860\n",
      "validation Loss: 0.0000 Acc: 74.5384\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 71.6460\n",
      "validation Loss: 0.0000 Acc: 70.5125\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 72.2906\n",
      "validation Loss: 0.0000 Acc: 76.7545\n",
      "Saving..\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 72.6989\n",
      "validation Loss: 0.0000 Acc: 72.4538\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 72.9738\n",
      "validation Loss: 0.0000 Acc: 71.7551\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 72.5098\n",
      "validation Loss: 0.0000 Acc: 75.8448\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 72.8920\n",
      "validation Loss: 0.0000 Acc: 70.2164\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 73.3618\n",
      "validation Loss: 0.0000 Acc: 71.2267\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 72.9477\n",
      "validation Loss: 0.0000 Acc: 71.5325\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 73.1596\n",
      "validation Loss: 0.0000 Acc: 77.3100\n",
      "Saving..\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 73.4339\n",
      "validation Loss: 0.0000 Acc: 76.1739\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 73.2433\n",
      "validation Loss: 0.0000 Acc: 72.9242\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 73.6139\n",
      "validation Loss: 0.0000 Acc: 71.8674\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 73.3710\n",
      "validation Loss: 0.0000 Acc: 73.2629\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 73.4238\n",
      "validation Loss: 0.0000 Acc: 74.4126\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 73.5549\n",
      "validation Loss: 0.0000 Acc: 73.4584\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 73.7039\n",
      "validation Loss: 0.0000 Acc: 75.8719\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 73.2046\n",
      "validation Loss: 0.0000 Acc: 75.6300\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 73.3812\n",
      "validation Loss: 0.0000 Acc: 72.9203\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 73.6788\n",
      "validation Loss: 0.0000 Acc: 75.2835\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 74.0809\n",
      "validation Loss: 0.0000 Acc: 73.7758\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 73.9614\n",
      "validation Loss: 0.0000 Acc: 73.2358\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 73.8728\n",
      "validation Loss: 0.0000 Acc: 75.1616\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 73.9691\n",
      "validation Loss: 0.0000 Acc: 74.8771\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 73.7451\n",
      "validation Loss: 0.0000 Acc: 76.1081\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 74.0727\n",
      "validation Loss: 0.0000 Acc: 74.2964\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 73.7606\n",
      "validation Loss: 0.0000 Acc: 74.7145\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 73.9246\n",
      "validation Loss: 0.0000 Acc: 73.6113\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 74.0639\n",
      "validation Loss: 0.0000 Acc: 73.7661\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 73.8283\n",
      "validation Loss: 0.0000 Acc: 74.3235\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 74.0920\n",
      "validation Loss: 0.0000 Acc: 74.1532\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 73.9018\n",
      "validation Loss: 0.0000 Acc: 72.9513\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 73.9991\n",
      "validation Loss: 0.0000 Acc: 74.6564\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 74.1080\n",
      "validation Loss: 0.0000 Acc: 73.1371\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 73.7828\n",
      "validation Loss: 0.0000 Acc: 76.1623\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 74.0509\n",
      "validation Loss: 0.0000 Acc: 74.6545\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 74.1515\n",
      "validation Loss: 0.0000 Acc: 74.1977\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 73.9120\n",
      "validation Loss: 0.0000 Acc: 73.8184\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 73.7877\n",
      "validation Loss: 0.0000 Acc: 73.3848\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 73.8535\n",
      "validation Loss: 0.0000 Acc: 75.1152\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 74.3852\n",
      "validation Loss: 0.0000 Acc: 73.6055\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 73.8864\n",
      "validation Loss: 0.0000 Acc: 75.1558\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 74.2498\n",
      "validation Loss: 0.0000 Acc: 73.4022\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 74.0727\n",
      "validation Loss: 0.0000 Acc: 73.8803\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 74.1138\n",
      "validation Loss: 0.0000 Acc: 75.6087\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 74.1859\n",
      "validation Loss: 0.0000 Acc: 73.3538\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 74.0712\n",
      "validation Loss: 0.0000 Acc: 74.3797\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 74.4360\n",
      "validation Loss: 0.0000 Acc: 74.6081\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 74.1506\n",
      "validation Loss: 0.0000 Acc: 74.4164\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 74.3809\n",
      "validation Loss: 0.0000 Acc: 75.5390\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 74.4443\n",
      "validation Loss: 0.0000 Acc: 76.1642\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 74.2285\n",
      "validation Loss: 0.0000 Acc: 74.6952\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 74.5222\n",
      "validation Loss: 0.0000 Acc: 74.1242\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 74.3896\n",
      "validation Loss: 0.0000 Acc: 75.2777\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 74.4975\n",
      "validation Loss: 0.0000 Acc: 76.2107\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 74.4235\n",
      "validation Loss: 0.0000 Acc: 73.5919\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 74.3789\n",
      "validation Loss: 0.0000 Acc: 73.7700\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 74.5231\n",
      "validation Loss: 0.0000 Acc: 74.4087\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 74.7598\n",
      "validation Loss: 0.0000 Acc: 74.2693\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 74.8556\n",
      "validation Loss: 0.0000 Acc: 74.9255\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 74.6615\n",
      "validation Loss: 0.0000 Acc: 75.9281\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 74.6896\n",
      "validation Loss: 0.0000 Acc: 74.2093\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 74.4143\n",
      "validation Loss: 0.0000 Acc: 75.2971\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 74.7810\n",
      "validation Loss: 0.0000 Acc: 74.6971\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 74.6857\n",
      "validation Loss: 0.0000 Acc: 75.1519\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 74.7583\n",
      "validation Loss: 0.0000 Acc: 75.6765\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 74.8198\n",
      "validation Loss: 0.0000 Acc: 74.3506\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 74.8227\n",
      "validation Loss: 0.0000 Acc: 74.4377\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 74.8188\n",
      "validation Loss: 0.0000 Acc: 75.2487\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 74.9175\n",
      "validation Loss: 0.0000 Acc: 74.0061\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 74.5236\n",
      "validation Loss: 0.0000 Acc: 74.9390\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 74.7951\n",
      "validation Loss: 0.0000 Acc: 75.0939\n",
      "Epoch 73/149\n",
      "training Loss: 0.0000 Acc: 74.6373\n",
      "validation Loss: 0.0000 Acc: 75.2971\n",
      "Epoch 74/149\n",
      "training Loss: 0.0000 Acc: 74.9993\n",
      "validation Loss: 0.0000 Acc: 74.8771\n",
      "Epoch 75/149\n",
      "training Loss: 0.0000 Acc: 74.8168\n",
      "validation Loss: 0.0000 Acc: 75.0590\n",
      "Epoch 76/149\n",
      "training Loss: 0.0000 Acc: 74.8667\n",
      "validation Loss: 0.0000 Acc: 74.9816\n",
      "Epoch 77/149\n",
      "training Loss: 0.0000 Acc: 74.8130\n",
      "validation Loss: 0.0000 Acc: 74.8190\n",
      "Epoch 78/149\n",
      "training Loss: 0.0000 Acc: 74.8154\n",
      "validation Loss: 0.0000 Acc: 75.3474\n",
      "Epoch 79/149\n",
      "training Loss: 0.0000 Acc: 75.0627\n",
      "validation Loss: 0.0000 Acc: 74.3293\n",
      "Epoch 80/149\n",
      "training Loss: 0.0000 Acc: 74.8052\n",
      "validation Loss: 0.0000 Acc: 74.5152\n",
      "Epoch 81/149\n",
      "training Loss: 0.0000 Acc: 74.7815\n",
      "validation Loss: 0.0000 Acc: 74.8926\n",
      "Epoch 82/149\n",
      "training Loss: 0.0000 Acc: 74.7554\n",
      "validation Loss: 0.0000 Acc: 75.2313\n",
      "Epoch 83/149\n",
      "training Loss: 0.0000 Acc: 74.8744\n",
      "validation Loss: 0.0000 Acc: 75.2739\n",
      "Epoch 84/149\n",
      "training Loss: 0.0000 Acc: 74.9833\n",
      "validation Loss: 0.0000 Acc: 75.1906\n",
      "Epoch 85/149\n",
      "training Loss: 0.0000 Acc: 74.7685\n",
      "validation Loss: 0.0000 Acc: 75.1539\n",
      "Epoch 86/149\n",
      "training Loss: 0.0000 Acc: 74.8788\n",
      "validation Loss: 0.0000 Acc: 74.9719\n",
      "Epoch 87/149\n",
      "training Loss: 0.0000 Acc: 74.9054\n",
      "validation Loss: 0.0000 Acc: 74.7648\n",
      "Epoch 88/149\n",
      "training Loss: 0.0000 Acc: 74.8299\n",
      "validation Loss: 0.0000 Acc: 74.9623\n",
      "Epoch 89/149\n",
      "training Loss: 0.0000 Acc: 74.7985\n",
      "validation Loss: 0.0000 Acc: 75.0513\n",
      "Epoch 90/149\n",
      "training Loss: 0.0000 Acc: 74.9538\n",
      "validation Loss: 0.0000 Acc: 74.9274\n",
      "Epoch 91/149\n",
      "training Loss: 0.0000 Acc: 74.7714\n",
      "validation Loss: 0.0000 Acc: 74.9758\n",
      "Epoch 92/149\n",
      "training Loss: 0.0000 Acc: 74.9209\n",
      "validation Loss: 0.0000 Acc: 74.9816\n",
      "Epoch 93/149\n",
      "training Loss: 0.0000 Acc: 74.9170\n",
      "validation Loss: 0.0000 Acc: 75.0726\n",
      "Epoch 94/149\n",
      "training Loss: 0.0000 Acc: 74.9485\n",
      "validation Loss: 0.0000 Acc: 74.9468\n",
      "Epoch 95/149\n",
      "training Loss: 0.0000 Acc: 74.8343\n",
      "validation Loss: 0.0000 Acc: 75.0048\n",
      "Early stopped.\n",
      "Best val acc: 77.310028\n",
      "----------\n",
      "Epoch 0/149\n",
      "training Loss: 0.0001 Acc: 56.6494\n",
      "validation Loss: 0.0000 Acc: 70.0422\n",
      "Saving..\n",
      "Epoch 1/149\n",
      "training Loss: 0.0000 Acc: 69.7473\n",
      "validation Loss: 0.0000 Acc: 71.8132\n",
      "Saving..\n",
      "Epoch 2/149\n",
      "training Loss: 0.0000 Acc: 71.3189\n",
      "validation Loss: 0.0000 Acc: 72.7538\n",
      "Saving..\n",
      "Epoch 3/149\n",
      "training Loss: 0.0000 Acc: 71.7951\n",
      "validation Loss: 0.0000 Acc: 77.2423\n",
      "Saving..\n",
      "Epoch 4/149\n",
      "training Loss: 0.0000 Acc: 72.1314\n",
      "validation Loss: 0.0000 Acc: 74.4184\n",
      "Epoch 5/149\n",
      "training Loss: 0.0000 Acc: 72.5020\n",
      "validation Loss: 0.0000 Acc: 73.2184\n",
      "Epoch 6/149\n",
      "training Loss: 0.0000 Acc: 72.5397\n",
      "validation Loss: 0.0000 Acc: 72.4384\n",
      "Epoch 7/149\n",
      "training Loss: 0.0000 Acc: 72.9273\n",
      "validation Loss: 0.0000 Acc: 68.4841\n",
      "Epoch 8/149\n",
      "training Loss: 0.0000 Acc: 72.8407\n",
      "validation Loss: 0.0000 Acc: 72.8642\n",
      "Epoch 9/149\n",
      "training Loss: 0.0000 Acc: 72.6777\n",
      "validation Loss: 0.0000 Acc: 75.8836\n",
      "Epoch 10/149\n",
      "training Loss: 0.0000 Acc: 73.2288\n",
      "validation Loss: 0.0000 Acc: 73.5145\n",
      "Epoch 11/149\n",
      "training Loss: 0.0000 Acc: 73.0052\n",
      "validation Loss: 0.0000 Acc: 74.5074\n",
      "Epoch 12/149\n",
      "training Loss: 0.0000 Acc: 73.5709\n",
      "validation Loss: 0.0000 Acc: 71.5790\n",
      "Epoch 13/149\n",
      "training Loss: 0.0000 Acc: 73.2148\n",
      "validation Loss: 0.0000 Acc: 73.4874\n",
      "Epoch 14/149\n",
      "training Loss: 0.0000 Acc: 73.2259\n",
      "validation Loss: 0.0000 Acc: 74.0990\n",
      "Epoch 15/149\n",
      "training Loss: 0.0000 Acc: 73.1727\n",
      "validation Loss: 0.0000 Acc: 71.7880\n",
      "Epoch 16/149\n",
      "training Loss: 0.0000 Acc: 73.2215\n",
      "validation Loss: 0.0000 Acc: 72.4171\n",
      "Epoch 17/149\n",
      "training Loss: 0.0000 Acc: 73.2293\n",
      "validation Loss: 0.0000 Acc: 72.7132\n",
      "Epoch 18/149\n",
      "training Loss: 0.0000 Acc: 73.7310\n",
      "validation Loss: 0.0000 Acc: 72.6261\n",
      "Epoch 19/149\n",
      "training Loss: 0.0000 Acc: 73.5191\n",
      "validation Loss: 0.0000 Acc: 72.8642\n",
      "Epoch 20/149\n",
      "training Loss: 0.0000 Acc: 73.7780\n",
      "validation Loss: 0.0000 Acc: 72.8138\n",
      "Epoch 21/149\n",
      "training Loss: 0.0000 Acc: 73.9439\n",
      "validation Loss: 0.0000 Acc: 74.4842\n",
      "Epoch 22/149\n",
      "training Loss: 0.0000 Acc: 73.6599\n",
      "validation Loss: 0.0000 Acc: 72.3300\n",
      "Epoch 23/149\n",
      "training Loss: 0.0000 Acc: 73.8186\n",
      "validation Loss: 0.0000 Acc: 74.2287\n",
      "Epoch 24/149\n",
      "training Loss: 0.0000 Acc: 73.7494\n",
      "validation Loss: 0.0000 Acc: 74.5306\n",
      "Epoch 25/149\n",
      "training Loss: 0.0000 Acc: 73.7848\n",
      "validation Loss: 0.0000 Acc: 74.5248\n",
      "Epoch 26/149\n",
      "training Loss: 0.0000 Acc: 73.7857\n",
      "validation Loss: 0.0000 Acc: 74.4455\n",
      "Epoch 27/149\n",
      "training Loss: 0.0000 Acc: 74.1893\n",
      "validation Loss: 0.0000 Acc: 75.8584\n",
      "Epoch 28/149\n",
      "training Loss: 0.0000 Acc: 74.1917\n",
      "validation Loss: 0.0000 Acc: 75.8294\n",
      "Epoch 29/149\n",
      "training Loss: 0.0000 Acc: 74.0978\n",
      "validation Loss: 0.0000 Acc: 72.6938\n",
      "Epoch 30/149\n",
      "training Loss: 0.0000 Acc: 74.0606\n",
      "validation Loss: 0.0000 Acc: 73.5377\n",
      "Epoch 31/149\n",
      "training Loss: 0.0000 Acc: 74.1404\n",
      "validation Loss: 0.0000 Acc: 73.1913\n",
      "Epoch 32/149\n",
      "training Loss: 0.0000 Acc: 73.9986\n",
      "validation Loss: 0.0000 Acc: 74.9177\n",
      "Epoch 33/149\n",
      "training Loss: 0.0000 Acc: 74.2120\n",
      "validation Loss: 0.0000 Acc: 74.8500\n",
      "Epoch 34/149\n",
      "training Loss: 0.0000 Acc: 74.2057\n",
      "validation Loss: 0.0000 Acc: 75.0648\n",
      "Epoch 35/149\n",
      "training Loss: 0.0000 Acc: 74.3436\n",
      "validation Loss: 0.0000 Acc: 73.7816\n",
      "Epoch 36/149\n",
      "training Loss: 0.0000 Acc: 74.2338\n",
      "validation Loss: 0.0000 Acc: 73.4990\n",
      "Epoch 37/149\n",
      "training Loss: 0.0000 Acc: 74.1022\n",
      "validation Loss: 0.0000 Acc: 73.2009\n",
      "Epoch 38/149\n",
      "training Loss: 0.0000 Acc: 74.1249\n",
      "validation Loss: 0.0000 Acc: 75.2642\n",
      "Epoch 39/149\n",
      "training Loss: 0.0000 Acc: 74.2899\n",
      "validation Loss: 0.0000 Acc: 74.2674\n",
      "Epoch 40/149\n",
      "training Loss: 0.0000 Acc: 74.1873\n",
      "validation Loss: 0.0000 Acc: 73.3132\n",
      "Epoch 41/149\n",
      "training Loss: 0.0000 Acc: 74.1752\n",
      "validation Loss: 0.0000 Acc: 74.7126\n",
      "Epoch 42/149\n",
      "training Loss: 0.0000 Acc: 74.5333\n",
      "validation Loss: 0.0000 Acc: 73.4313\n",
      "Epoch 43/149\n",
      "training Loss: 0.0000 Acc: 74.3741\n",
      "validation Loss: 0.0000 Acc: 73.9848\n",
      "Epoch 44/149\n",
      "training Loss: 0.0000 Acc: 74.2981\n",
      "validation Loss: 0.0000 Acc: 74.1610\n",
      "Epoch 45/149\n",
      "training Loss: 0.0000 Acc: 74.4017\n",
      "validation Loss: 0.0000 Acc: 73.7506\n",
      "Epoch 46/149\n",
      "training Loss: 0.0000 Acc: 74.3465\n",
      "validation Loss: 0.0000 Acc: 73.4216\n",
      "Epoch 47/149\n",
      "training Loss: 0.0000 Acc: 74.3688\n",
      "validation Loss: 0.0000 Acc: 74.0584\n",
      "Epoch 48/149\n",
      "training Loss: 0.0000 Acc: 74.3910\n",
      "validation Loss: 0.0000 Acc: 74.2558\n",
      "Epoch 49/149\n",
      "training Loss: 0.0000 Acc: 74.4036\n",
      "validation Loss: 0.0000 Acc: 74.9390\n",
      "Epoch 50/149\n",
      "training Loss: 0.0000 Acc: 74.5038\n",
      "validation Loss: 0.0000 Acc: 73.7642\n",
      "Epoch 51/149\n",
      "training Loss: 0.0000 Acc: 74.6199\n",
      "validation Loss: 0.0000 Acc: 74.5403\n",
      "Epoch 52/149\n",
      "training Loss: 0.0000 Acc: 74.5807\n",
      "validation Loss: 0.0000 Acc: 74.2693\n",
      "Epoch 53/149\n",
      "training Loss: 0.0000 Acc: 74.4728\n",
      "validation Loss: 0.0000 Acc: 74.0351\n",
      "Epoch 54/149\n",
      "training Loss: 0.0000 Acc: 74.4873\n",
      "validation Loss: 0.0000 Acc: 74.8964\n",
      "Epoch 55/149\n",
      "training Loss: 0.0000 Acc: 74.6233\n",
      "validation Loss: 0.0000 Acc: 74.7281\n",
      "Epoch 56/149\n",
      "training Loss: 0.0000 Acc: 74.4443\n",
      "validation Loss: 0.0000 Acc: 75.0687\n",
      "Epoch 57/149\n",
      "training Loss: 0.0000 Acc: 74.4312\n",
      "validation Loss: 0.0000 Acc: 74.7958\n",
      "Epoch 58/149\n",
      "training Loss: 0.0000 Acc: 74.5512\n",
      "validation Loss: 0.0000 Acc: 74.2926\n",
      "Epoch 59/149\n",
      "training Loss: 0.0000 Acc: 74.4307\n",
      "validation Loss: 0.0000 Acc: 75.2197\n",
      "Epoch 60/149\n",
      "training Loss: 0.0000 Acc: 74.7027\n",
      "validation Loss: 0.0000 Acc: 74.7261\n",
      "Epoch 61/149\n",
      "training Loss: 0.0000 Acc: 74.6102\n",
      "validation Loss: 0.0000 Acc: 73.8571\n",
      "Epoch 62/149\n",
      "training Loss: 0.0000 Acc: 74.5952\n",
      "validation Loss: 0.0000 Acc: 73.9461\n",
      "Epoch 63/149\n",
      "training Loss: 0.0000 Acc: 74.4709\n",
      "validation Loss: 0.0000 Acc: 74.3797\n",
      "Epoch 64/149\n",
      "training Loss: 0.0000 Acc: 74.6509\n",
      "validation Loss: 0.0000 Acc: 73.6113\n",
      "Epoch 65/149\n",
      "training Loss: 0.0000 Acc: 74.4752\n",
      "validation Loss: 0.0000 Acc: 74.8287\n",
      "Epoch 66/149\n",
      "training Loss: 0.0000 Acc: 74.5260\n",
      "validation Loss: 0.0000 Acc: 74.9216\n",
      "Epoch 67/149\n",
      "training Loss: 0.0000 Acc: 74.7075\n",
      "validation Loss: 0.0000 Acc: 74.2829\n",
      "Epoch 68/149\n",
      "training Loss: 0.0000 Acc: 74.5048\n",
      "validation Loss: 0.0000 Acc: 75.1655\n",
      "Epoch 69/149\n",
      "training Loss: 0.0000 Acc: 74.6867\n",
      "validation Loss: 0.0000 Acc: 74.5132\n",
      "Epoch 70/149\n",
      "training Loss: 0.0000 Acc: 74.4583\n",
      "validation Loss: 0.0000 Acc: 74.7571\n",
      "Epoch 71/149\n",
      "training Loss: 0.0000 Acc: 74.7012\n",
      "validation Loss: 0.0000 Acc: 75.2990\n",
      "Epoch 72/149\n",
      "training Loss: 0.0000 Acc: 74.7506\n",
      "validation Loss: 0.0000 Acc: 74.6197\n",
      "Epoch 73/149\n",
      "training Loss: 0.0000 Acc: 74.6877\n",
      "validation Loss: 0.0000 Acc: 74.7281\n",
      "Epoch 74/149\n",
      "training Loss: 0.0000 Acc: 74.6920\n",
      "validation Loss: 0.0000 Acc: 74.7919\n",
      "Epoch 75/149\n",
      "training Loss: 0.0000 Acc: 74.6388\n",
      "validation Loss: 0.0000 Acc: 74.6390\n",
      "Epoch 76/149\n",
      "training Loss: 0.0000 Acc: 74.6485\n",
      "validation Loss: 0.0000 Acc: 75.1345\n",
      "Early stopped.\n",
      "Best val acc: 77.242287\n",
      "----------\n",
      "Average best_acc across k-fold: 77.33613586425781\n"
     ]
    }
   ],
   "source": [
    "CURRENT_TIME = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "if CRITERION == \"NLLLoss\":\n",
    "    train_weights = torch.FloatTensor(\n",
    "        [1.0, np.sum(data_aux.loc[label==0,'eventWeight']) / np.sum(data_aux.loc[label==1,'eventWeight'])]\n",
    "    ).cuda()\n",
    "    criterion = nn.NLLLoss(weight=train_weights)\n",
    "elif CRITERION == \"BCELoss\":\n",
    "    train_weights = torch.FloatTensor(data_aux.loc[:, \"eventWeight\"]).cuda()\n",
    "    criterion = nn.BCELoss(weight=train_weights)\n",
    "else:\n",
    "    raise Exception(f\"CRITERION must be either 'NLLLoss' or 'BCELoss'. You provided {CRITERION}.\")\n",
    "\n",
    "if OPTIMIZE_SPACE:\n",
    "    config_file = OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json'\n",
    "    best_conf = optimize_hyperparams(\n",
    "        skf, data_list, data_hlf, label, \n",
    "        config_file, epochs=10,\n",
    "        criterion=criterion\n",
    "    )\n",
    "    print(best_conf)\n",
    "else:\n",
    "    # with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "    # with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "    with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "        best_conf = json.load(f)\n",
    "        print(best_conf)\n",
    "\n",
    "NUM_EPOCHS = 150\n",
    "fom = []\n",
    "train_losses_arr, val_losses_arr = [], []\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    model_file = OUTPUT_DIRPATH + CURRENT_TIME +'_ReallyTopclassStyle_'+ f'{fold_idx}.torch'\n",
    "    state_file = OUTPUT_DIRPATH + CURRENT_TIME +'_BestPerfReallyTopclass_'+ f'{fold_idx}.torch'\n",
    "\n",
    "    if CRITERION == \"NLLLoss\":\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        sig_train_mask = rectified_train_index & (label == 1)\n",
    "        bkg_train_mask = rectified_train_index & (label == 0)\n",
    "        train_weights = torch.FloatTensor(\n",
    "            [1.0, np.sum(data_aux.loc[bkg_train_mask,'eventWeight']) / np.sum(data_aux.loc[sig_train_mask,'eventWeight'])]\n",
    "        ).cuda()\n",
    "        criterion = nn.NLLLoss(weight=train_weights)\n",
    "    elif CRITERION == \"BCELoss\":\n",
    "        train_weights = torch.FloatTensor((data_aux.iloc[train_index]).loc[:, \"eventWeight\"]).cuda()\n",
    "        criterion = nn.BCELoss(weight=train_weights)\n",
    "        \n",
    "    model = InclusiveNetwork(\n",
    "        best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "        best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g'], \n",
    "        dnn_input=np.shape(data_hlf)[-1], rnn_input=np.shape(data_list)[-1]\n",
    "    ).cuda()\n",
    "    optimizer = AMSGrad(model.parameters(), lr=best_conf['learning_rate'], weight_decay=best_conf['L2_reg'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        ParticleHLF(data_list[train_index], data_hlf[train_index], label[train_index]), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ParticleHLF(data_list[val_index], data_hlf[val_index], label[val_index]), \n",
    "        batch_size=best_conf['batch_size'], shuffle=True\n",
    "    )\n",
    "    data_loader = {\"training\": train_loader, \"validation\": val_loader} \n",
    "\n",
    "    best_acc, train_losses, val_losses = train(\n",
    "        NUM_EPOCHS, model, criterion, optimizer, scheduler, \n",
    "        state_file, model_file, data_loader=data_loader\n",
    "    )\n",
    "    train_losses_arr.append(train_losses)\n",
    "    val_losses_arr.append(val_losses)\n",
    "\n",
    "    fom.append(best_acc)\n",
    "\n",
    "Y = np.mean(np.asarray([acc.cpu() for acc in fom]))\n",
    "print(\"Average best_acc across k-fold: {}\".format(Y))\n",
    "model = InclusiveNetwork(\n",
    "    best_conf['hidden_layers'], best_conf['initial_nodes'], best_conf['dropout'], \n",
    "    best_conf['gru_layers'], best_conf['gru_size'], best_conf['dropout_g']\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for InclusiveNetwork:\n\tsize mismatch for gru.weight_ih_l0: copying a param with shape torch.Size([1500, 12]) from checkpoint, the shape in current model is torch.Size([1500, 6]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     IN_perf \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_hlf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOUTPUT_DIRPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCURRENT_TIME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_losses_arr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_losses_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_losses_arr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_losses_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:53\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(p_list, hlf, label, OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, train_losses_arr, val_losses_arr, save, only_fold_idx)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIRPATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCURRENT_TIME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ReallyTopclassStyle_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.torch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for InclusiveNetwork:\n\tsize mismatch for gru.weight_ih_l0: copying a param with shape torch.Size([1500, 12]) from checkpoint, the shape in current model is torch.Size([1500, 6]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      6\u001b[0m     IN_perf \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m      7\u001b[0m         data_list_test, data_hlf_test, label_test, \n\u001b[1;32m      8\u001b[0m         OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, \n\u001b[1;32m      9\u001b[0m         train_losses_arr\u001b[38;5;241m=\u001b[39mtrain_losses_arr, val_losses_arr\u001b[38;5;241m=\u001b[39mval_losses_arr, \n\u001b[1;32m     10\u001b[0m         save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     IN_perf \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_hlf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mOUTPUT_DIRPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCURRENT_TIME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/uscms_data/d3/tsievert/XHYbbgg/HHtobbyy/evaluate.py:53\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(p_list, hlf, label, OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, train_losses_arr, val_losses_arr, save, only_fold_idx)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m only_fold_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m fold_idx \u001b[38;5;241m!=\u001b[39m only_fold_idx:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIRPATH\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCURRENT_TIME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ReallyTopclassStyle_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.torch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/nobackup/miniconda3/envs/higgs-dna-hhbbgg/lib/python3.10/site-packages/torch/nn/modules/module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for InclusiveNetwork:\n\tsize mismatch for gru.weight_ih_l0: copying a param with shape torch.Size([1500, 12]) from checkpoint, the shape in current model is torch.Size([1500, 6])."
     ]
    }
   ],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "try:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, \n",
    "        train_losses_arr=train_losses_arr, val_losses_arr=val_losses_arr, \n",
    "        save=True\n",
    "    )\n",
    "except:\n",
    "    IN_perf = evaluate(\n",
    "        data_list_test, data_hlf_test, label_test, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, \n",
    "        save=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train/Val Loss curves, ROC curves, and Output Score Dist for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network performance\n",
      "+-----------+-------------------+--------------------------+\n",
      "| Threshold | Signal Efficiency | Background Contamination |\n",
      "+-----------+-------------------+--------------------------+\n",
      "|   0.4922  |       0.9706      |    0.3361 +/- 0.0089     |\n",
      "|   0.6374  |       0.9500      |    0.2460 +/- 0.0083     |\n",
      "|   0.7479  |       0.9198      |    0.1729 +/- 0.0057     |\n",
      "|   0.9463  |       0.7538      |    0.0432 +/- 0.0019     |\n",
      "|   0.9844  |       0.5777      |    0.0130 +/- 0.0008     |\n",
      "|   0.9955  |       0.3839      |    0.0032 +/- 0.0002     |\n",
      "+-----------+-------------------+--------------------------+\n"
     ]
    }
   ],
   "source": [
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "\n",
    "TPR_thresholds = [0.9704, 0.9498, 0.9196, 0.7536, 0.5777, 0.3837]\n",
    "print(\"Neural network performance\")\n",
    "NNtable = PrettyTable(['Threshold','Signal Efficiency','Background Contamination'])\n",
    "NNtable.float_format = \".4\"\n",
    "for TPR_threshold in TPR_thresholds:\n",
    "    thres_idx = np.argmax(np.array(IN_perf['base_tpr'])>TPR_threshold)\n",
    "    NNtable.add_row(\n",
    "        [\n",
    "            IN_perf['mean_thresholds'][thres_idx], IN_perf['base_tpr'][thres_idx], \n",
    "            \"{:.4f} +/- {:.4f}\".format(IN_perf['mean_fprs'][thres_idx], IN_perf['std_fprs'][thres_idx])\n",
    "        ]\n",
    "    )\n",
    "print(NNtable)\n",
    "\n",
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "\n",
    "plot_train_val_losses(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data',\n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    ")\n",
    "plot_roc(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', method='arr',\n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))]\n",
    ")\n",
    "plot_output_score(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_weighted', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }, n_bins=25\n",
    ")\n",
    "plot_output_score(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data_density', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], n_bins=25\n",
    ")\n",
    "s_over_root_b(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix='_test_data', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }, n_bins=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimized cut-boundaries for ttH score output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_cut_boundaries(IN_perf, weights, bins=50):\n",
    "    hist_list_fold = []\n",
    "    cut_boundaries_fold = []\n",
    "    cut_s_over_root_bs_fold = []\n",
    "    sig_weights_fold = []\n",
    "    bkg_weights_fold = []\n",
    "    for fold_idx in range(skf.get_n_splits()):\n",
    "        sig_np = np.exp(\n",
    "            IN_perf['all_preds'][fold_idx]\n",
    "        )[\n",
    "            np.array(IN_perf['all_labels'][fold_idx]) == 1,1\n",
    "        ]\n",
    "        bkg_np = np.exp(\n",
    "            IN_perf['all_preds'][fold_idx]\n",
    "        )[\n",
    "            np.array(IN_perf['all_labels'][fold_idx]) == 0,1\n",
    "        ]\n",
    "        hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "        sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "        bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "        hist_list_fold.append({'sig': copy.deepcopy(sig_hist), 'bkg': copy.deepcopy(bkg_hist)})\n",
    "\n",
    "        fold_idx_cuts_bins_inclusive = []\n",
    "        fold_idx_sig_weights = []\n",
    "        fold_idx_bkg_weights = []\n",
    "        fold_idx_prev_s_over_root_b = []\n",
    "        prev_s_over_root_b = 0\n",
    "        for i in range(bins):\n",
    "            s = np.sum(sig_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ])\n",
    "            sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                (bins-1) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "            ]))\n",
    "            if prev_s_over_root_b < (s / sqrt_b):\n",
    "                prev_s_over_root_b = s / sqrt_b\n",
    "                continue\n",
    "            else:\n",
    "                fold_idx_sig_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(sig_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_bkg_weights.append(\n",
    "                    {\n",
    "                        'value': np.sum(bkg_hist.values().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ]),\n",
    "                        'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                            (bins) - i : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                        ])),\n",
    "                    }\n",
    "                )\n",
    "                fold_idx_cuts_bins_inclusive.append(bins - i)\n",
    "                fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "                prev_s_over_root_b = 0\n",
    "        fold_idx_sig_weights.append(\n",
    "            {\n",
    "                'value': np.sum(sig_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_bkg_weights.append(\n",
    "            {\n",
    "                'value': np.sum(bkg_hist.values().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ]),\n",
    "                'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                    0 : bins if len(fold_idx_cuts_bins_inclusive) == 0 else fold_idx_cuts_bins_inclusive[-1]\n",
    "                ])),\n",
    "            }\n",
    "        )\n",
    "        fold_idx_cuts_bins_inclusive.append(0)\n",
    "        fold_idx_prev_s_over_root_b.append(prev_s_over_root_b)\n",
    "        fold_idx_score_cuts = [bin_i / bins for bin_i in fold_idx_cuts_bins_inclusive]\n",
    "        cut_boundaries_fold.append(fold_idx_score_cuts)\n",
    "        cut_s_over_root_bs_fold.append(fold_idx_prev_s_over_root_b)\n",
    "        sig_weights_fold.append(fold_idx_sig_weights)\n",
    "        bkg_weights_fold.append(fold_idx_bkg_weights)\n",
    "                \n",
    "    sig_np = np.exp(\n",
    "        IN_perf['mean_pred']\n",
    "    )[\n",
    "        np.array(IN_perf['mean_label']) == 1,1\n",
    "    ]\n",
    "    bkg_np = np.exp(\n",
    "        IN_perf['mean_pred']\n",
    "    )[\n",
    "        np.array(IN_perf['mean_label']) == 0,1\n",
    "    ]\n",
    "    hist_axis = hist.axis.Regular(bins, 0., 1., name='var', growth=False, underflow=False, overflow=False)\n",
    "    sig_hist = hist.Hist(hist_axis, storage='weight').fill(var=sig_np, weight=weights['sig'])\n",
    "    bkg_hist = hist.Hist(hist_axis, storage='weight').fill(var=bkg_np, weight=weights['bkg'])\n",
    "\n",
    "    cut_boundaries = []\n",
    "    cut_s_over_root_bs = []\n",
    "    prev_s_over_root_b = 0\n",
    "    sig_weights = []\n",
    "    bkg_weights = []\n",
    "    for i in range(bins):\n",
    "        s = np.sum(sig_hist.values().flatten()[\n",
    "            (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "        ])\n",
    "        sqrt_b = np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "            (bins-1) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "        ]))\n",
    "        if prev_s_over_root_b < (s / sqrt_b):\n",
    "            prev_s_over_root_b = s / sqrt_b\n",
    "            continue\n",
    "        else:\n",
    "            sig_weights.append(\n",
    "                {\n",
    "                    'value': np.sqrt(np.sum(sig_hist.values().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                    'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            bkg_weights.append(\n",
    "                {\n",
    "                    'value': np.sqrt(np.sum(bkg_hist.values().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                    'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                        (bins) - i : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "                    ])),\n",
    "                }\n",
    "            )\n",
    "            cut_boundaries.append(bins - i)\n",
    "            cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "            prev_s_over_root_b = 0\n",
    "    sig_weights.append(\n",
    "        {\n",
    "            'value': np.sum(sig_hist.values().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]),\n",
    "            'w2': np.sqrt(np.sum(sig_hist.variances().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])),\n",
    "        }\n",
    "    )\n",
    "    bkg_weights.append(\n",
    "        {\n",
    "            'value': np.sum(bkg_hist.values().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ]),\n",
    "            'w2': np.sqrt(np.sum(bkg_hist.variances().flatten()[\n",
    "                0 : bins if len(cut_boundaries) == 0 else cut_boundaries[-1]\n",
    "            ])),\n",
    "        }\n",
    "    )\n",
    "    cut_boundaries.append(0)\n",
    "    cut_s_over_root_bs.append(prev_s_over_root_b)\n",
    "    cut_boundaries = [bin_i / bins for bin_i in cut_boundaries]\n",
    "    return cut_boundaries_fold, cut_s_over_root_bs_fold, sig_weights_fold, bkg_weights_fold, cut_boundaries, cut_s_over_root_bs, sig_weights, bkg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf.json', 'r') as f:\n",
    "    IN_perf = json.load(f)\n",
    "\n",
    "(\n",
    "    cut_boundaries_fold, cut_s_over_root_bs_fold, \n",
    "    sig_weights_fold, bkg_weights_fold, \n",
    "    cut_boundaries, cut_s_over_root_bs, \n",
    "    sig_weights, bkg_weights \n",
    ") = optimize_cut_boundaries(\n",
    "    IN_perf, {\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "fold_labels = [\n",
    "    [\n",
    "        f\"s/√b={cut_s_over_root_bs_fold[fold_idx][cut_idx]:.04f}, s={sig_weights_fold[fold_idx][cut_idx]['value']:.04f}±{sig_weights_fold[fold_idx][cut_idx]['w2']:.04f}, b={bkg_weights_fold[fold_idx][cut_idx]['value']:.04f}±{bkg_weights_fold[fold_idx][cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs_fold[fold_idx]))\n",
    "    ] for fold_idx in range(skf.get_n_splits())\n",
    "]\n",
    "fold_colors = [copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries_fold[i]) // len(cmap_petroff10)) + 1)) for i in range(skf.get_n_splits())]\n",
    "for fold_idx in range(skf.get_n_splits()):\n",
    "    s_over_root_b(\n",
    "        IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_fold{fold_idx}', \n",
    "        labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "            'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "        }, lines_fold=cut_boundaries_fold, lines_labels=fold_labels, only_fold=fold_idx, lines_colors=fold_colors\n",
    "    )\n",
    "labels = [\n",
    "    f\"s/√b={cut_s_over_root_bs[cut_idx]:.04f}, s={sig_weights[cut_idx]['value']:.04f}±{sig_weights[cut_idx]['w2']:.04f}, b={bkg_weights[cut_idx]['value']:.04f}±{bkg_weights[cut_idx]['w2']:.04f}\" for cut_idx in range(len(cut_s_over_root_bs))\n",
    "]\n",
    "colors = copy.deepcopy(cmap_petroff10 * ((len(cut_boundaries) // len(cmap_petroff10)) + 1))\n",
    "s_over_root_b(\n",
    "    IN_perf, plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_test_data_foldAvg', \n",
    "    labels=[str(i) for i in range(len(IN_perf['all_preds']))], weights={\n",
    "        'bkg': data_test_aux.loc[label_test==0, \"eventWeight\"], 'sig': data_test_aux.loc[label_test==1, \"eventWeight\"]\n",
    "    }, lines=cut_boundaries, lines_labels=labels, no_fold=True, lines_colors=colors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train + val comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_dict = {'train': [], 'val': []}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "    IN_perf_dict['train'].append(\n",
    "        evaluate(\n",
    "            data_list[train_index], data_hlf[train_index], label[train_index], \n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "    IN_perf_dict['val'].append(\n",
    "        evaluate(\n",
    "            data_list[val_index], data_hlf[val_index], label[val_index], \n",
    "            OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf, only_fold_idx=fold_idx,\n",
    "        )\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'w') as f:\n",
    "    json.dump(IN_perf_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC and Output Score Dist for train/val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_train_val.json', 'r') as f:\n",
    "    IN_perf_dict = json.load(f)\n",
    "\n",
    "labels_arr = ['train - fold ', 'val - fold ']\n",
    "val_weights_arr = []\n",
    "for fold_idx, (train_IN_dict, val_IN_dict, (train_index, val_index)) in enumerate(zip(IN_perf_dict['train'], IN_perf_dict['val'], skf.split(data_hlf, label))):\n",
    "    plot_roc(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison_fold{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)]\n",
    "    )\n",
    "    rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "    rectified_train_index[val_index] = False\n",
    "    sig_train_mask = rectified_train_index & (label == 1)\n",
    "    sig_val_mask = np.logical_not(rectified_train_index) & (label == 1)\n",
    "    bkg_train_mask = rectified_train_index & (label == 0)\n",
    "    bkg_val_mask = np.logical_not(rectified_train_index) & (label == 0)\n",
    "    weights = [\n",
    "        {'sig': data_aux.loc[sig_train_mask, \"eventWeight\"], 'bkg': data_aux.loc[bkg_train_mask, \"eventWeight\"]},\n",
    "        {'sig': data_aux.loc[sig_val_mask, \"eventWeight\"], 'bkg': data_aux.loc[bkg_val_mask, \"eventWeight\"]}\n",
    "    ]\n",
    "    val_weights_arr.append(copy.deepcopy(weights[1]))\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_weighted_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights\n",
    "    )\n",
    "    plot_output_score(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_density_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=[{'sig': None, 'bkg': None}]*len(labels_arr)\n",
    "    )\n",
    "    s_over_root_b(\n",
    "        [train_IN_dict, val_IN_dict], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_train_val_comparison{fold_idx}', \n",
    "        method='IN_arr', labels=[labels_arr[0]+str(fold_idx), labels_arr[1]+str(fold_idx)], weights=weights\n",
    "    )\n",
    "labels_arr = ['val - fold 0', 'val - fold 1', 'val - fold 2', 'val - fold 3', 'val - fold 4']\n",
    "s_over_root_b(\n",
    "    IN_perf_dict['val'], plot_destdir+'/'+CURRENT_TIME, plot_postfix=f'_val_comparison', \n",
    "    method='IN_arr', labels=labels_arr, weights=val_weights_arr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input Vars (pre-standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_pre_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/pre_std/\"\n",
    "if not os.path.exists(output_dir_pre_std):\n",
    "    os.makedirs(output_dir_pre_std)\n",
    "\n",
    "pre_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    sig_mask = (label == 1)\n",
    "    sig_test_mask = (label_test == 1)\n",
    "    bkg_mask = (label == 0)\n",
    "    bkg_test_mask = (label_test == 0)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        \n",
    "        sig_train_mask = rectified_train_index & sig_mask\n",
    "        sig_val_mask = np.logical_not(rectified_train_index) & sig_mask\n",
    "        bkg_train_mask = rectified_train_index & bkg_mask\n",
    "        bkg_val_mask = np.logical_not(rectified_train_index) & bkg_mask\n",
    "\n",
    "        sig_train_np = data_df.loc[sig_train_mask, var_name].to_numpy()\n",
    "        sig_val_np = data_df.loc[sig_val_mask, var_name].to_numpy()\n",
    "        sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "        bkg_train_np = data_df.loc[bkg_train_mask, var_name].to_numpy()\n",
    "        bkg_val_np = data_df.loc[bkg_val_mask, var_name].to_numpy()\n",
    "        bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_pre_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np = data_df.loc[sig_mask, var_name].to_numpy()\n",
    "    sig_test_np = data_test_df.loc[sig_test_mask, var_name].to_numpy()\n",
    "    bkg_train_np = data_df.loc[bkg_mask, var_name].to_numpy()\n",
    "    bkg_test_np = data_test_df.loc[bkg_test_mask, var_name].to_numpy()\n",
    "    sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    pre_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_pre_std, var_name, pre_std_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Input Vars (post-standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_post_std = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/post_std/\"\n",
    "if not os.path.exists(output_dir_post_std):\n",
    "    os.makedirs(output_dir_post_std)\n",
    "\n",
    "post_std_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = data_list, data_list_test\n",
    "    else:\n",
    "        data, data_test = data_hlf, data_hlf_test\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name,\n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_post_std, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    post_std_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_post_std, var_name, post_std_hists[var_name], labels=label_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian smearing on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smear_particle_list(var_name, particle_list_to_smear, method='multiply', seed=SEED):\n",
    "    index2, index3 = data_list_index_map(var_name)\n",
    "\n",
    "    # Performs the smearing and returns the result\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_list_to_smear[:, index2, index3] *= rng.normal(size=len(particle_list_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_list_to_smear[:, index2, index3] += rng.normal(size=len(particle_list_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "\n",
    "    return particle_list_to_smear\n",
    "\n",
    "\n",
    "def smear_particle_hlf(var_name, particle_hlf_to_smear, method='multiply', seed=SEED):\n",
    "    index2 = hlf_vars_columns[var_name]\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    if method == 'multiply':\n",
    "        particle_hlf_to_smear[:, index2] *= rng.normal(size=len(particle_hlf_to_smear))\n",
    "    elif method == 'add':\n",
    "        particle_hlf_to_smear[:, index2] += rng.normal(size=len(particle_hlf_to_smear))\n",
    "    else:\n",
    "        raise Exception(f\"Only 'multiply' and 'add' are allowed as methods. You passed {method}.\")\n",
    "    \n",
    "    return particle_hlf_to_smear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate smeared variable test-data on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_perf_smear_dict = {}\n",
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'eventWeight'}:\n",
    "        continue\n",
    "    gauss_data_list, gauss_data_hlf = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        gauss_data_list = smear_particle_list(var_name, copy.deepcopy(data_list_test))\n",
    "        gauss_data_hlf = data_hlf_test\n",
    "    else:\n",
    "        gauss_data_list = data_list_test\n",
    "        gauss_data_hlf = smear_particle_hlf(var_name, copy.deepcopy(data_hlf_test))\n",
    "\n",
    "    IN_perf_smear_dict[var_name] = evaluate(\n",
    "        gauss_data_list, gauss_data_hlf, label_test, OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'w') as f:\n",
    "    json.dump(IN_perf_smear_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROC for gaussian smear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_gauss_smear.json', 'r') as f:\n",
    "    IN_perf_smear_dict = json.load(f)\n",
    "IN_perf_smear_list = []\n",
    "label_arr = []\n",
    "for var_name, IN_perf_smear in IN_perf_smear_dict.items():\n",
    "    IN_perf_smear_list.append(IN_perf_smear)\n",
    "    label_arr.append(var_name)\n",
    "sort = np.argsort([IN_perf_smear['mean_area'] for IN_perf_smear in IN_perf_smear_list])\n",
    "plot_roc(\n",
    "    list(IN_perf_smear_dict.values()), plot_destdir+'/'+CURRENT_TIME, plot_postfix='_gauss_smear', \n",
    "    method='IN_arr', labels=label_arr, yscale='log', run2=False, sort=sort\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gaussian Smeared input Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_gauss_smear = CURRENT_DIRPATH + f\"/input_comparison{'_v2' if V2_MERGED else ''}/{VERSION}/gauss_smear/\"\n",
    "if not os.path.exists(output_dir_gauss_smear):\n",
    "    os.makedirs(output_dir_gauss_smear)\n",
    "\n",
    "gauss_hists = {}\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "label_arr_fold = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" val\", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test\",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" val\", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test\"\n",
    "]\n",
    "for var_name in high_level_fields:\n",
    "    if var_name in {'event', 'puppiMET_eta'}:\n",
    "        continue\n",
    "    data, data_test = None, None\n",
    "    if var_name in (high_level_fields - set(input_hlf_vars)):\n",
    "        data, data_test = smear_particle_list(var_name, data_list), smear_particle_list(var_name, data_list_test)\n",
    "    else:\n",
    "        data, data_test = smear_particle_hlf(var_name, data_hlf), smear_particle_hlf(var_name, data_hlf_test)\n",
    "    for fold_idx, (train_index, val_index) in enumerate(skf.split(data_hlf, label)):\n",
    "        rectified_train_index = np.ones(len(label), dtype=bool)\n",
    "        rectified_train_index[val_index] = False\n",
    "        (\n",
    "            sig_train_np, sig_val_np, sig_test_np, \n",
    "            bkg_train_np, bkg_val_np, bkg_test_np \n",
    "        ) = post_std_np_arrays(\n",
    "            data, data_test, var_name, \n",
    "            train_index=rectified_train_index, val_index=np.logical_not(rectified_train_index)\n",
    "        )\n",
    "        sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "        sig_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_val_np[sig_val_np != 0])\n",
    "        sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "        bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "        bkg_val_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_val_np[bkg_val_np != 0])\n",
    "        bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "\n",
    "        make_input_plot(\n",
    "            output_dir_gauss_smear, var_name, \n",
    "            [sig_train_hist, sig_val_hist, sig_test_hist, bkg_train_hist, bkg_val_hist, bkg_test_hist], \n",
    "            fold_idx=fold_idx, labels=label_arr_fold\n",
    "        )\n",
    "    sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = post_std_np_arrays(data, data_test, var_name, index_map)\n",
    "\n",
    "    sig_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_train_np[sig_train_np != 0])\n",
    "    sig_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=sig_test_np[sig_test_np != 0])\n",
    "    bkg_train_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_train_np[bkg_train_np != 0])\n",
    "    bkg_test_hist = hist.Hist(VARIABLES_STD[var_name]).fill(var=bkg_test_np[bkg_test_np != 0])\n",
    "    gauss_hists[var_name] = [\n",
    "        copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "        copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "    ]\n",
    "    make_input_plot(output_dir_gauss_smear, var_name, gauss_hists[var_name], labels=label_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mass sculpting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model_outputs/v0/BestConfigReallyTopclass.json', 'r') as f:\n",
    "# with open(OUTPUT_DIRPATH + CURRENT_TIME + '_BestConfigReallyTopclass.json') as f:\n",
    "with open('/uscms/home/tsievert/nobackup/XHYbbgg/HHtobbyy/model_outputs/v4/extra_vars/2024-08-20_23-02-48_BestConfigReallyTopclass.json') as f:\n",
    "    best_conf = json.load(f)\n",
    "IN_full_eval_dict = {}\n",
    "for data_type, p_list, hlf, y in [('train', data_list, data_hlf, label), ('test', data_list_test, data_hlf_test, label_test)]:\n",
    "    IN_full_eval_dict[data_type] = evaluate(\n",
    "        p_list, hlf, y, \n",
    "        OUTPUT_DIRPATH, CURRENT_TIME, skf, best_conf,\n",
    "    )\n",
    "\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'w') as f:\n",
    "    json.dump(IN_full_eval_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mass dists with successive score cuts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_destdir = OUTPUT_DIRPATH + 'plots'\n",
    "if not os.path.exists(plot_destdir):\n",
    "    os.makedirs(plot_destdir)\n",
    "with open(OUTPUT_DIRPATH + f'{CURRENT_TIME}_IN_perf_full_eval.json', 'r') as f:\n",
    "    IN_full_eval_dict = json.load(f)\n",
    "\n",
    "score_cuts = [0, 0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99]\n",
    "label_arr = [\n",
    "    MC_NAMES_PRETTY[\"GluGluToHH\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"GluGluToHH\"]+\" test, score cut = \",\n",
    "    MC_NAMES_PRETTY[\"ttHToGG\"]+\" train, score cut = \", MC_NAMES_PRETTY[\"ttHToGG\"]+\" test, score cut = \"\n",
    "] * len(score_cuts)\n",
    "label_arr = [label_arr[label_idx]+str(score_cuts[score_idx // (len(label_arr)//len(score_cuts))]) for score_idx, label_idx in enumerate(range(len(label_arr)))]\n",
    "hist_dict = {'mass': [], 'dijet_mass': []}\n",
    "for var_name in hist_dict.keys():\n",
    "    for i, score_cut in enumerate(score_cuts):\n",
    "        sig_train_np, sig_test_np, bkg_train_np, bkg_test_np = aux_np_arrays(var_name, score_cut, IN_full_eval_dict)\n",
    "        sig_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_train_np)\n",
    "        sig_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=sig_test_np)\n",
    "        bkg_train_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_train_np)\n",
    "        bkg_test_hist = hist.Hist(VARIABLES[var_name]).fill(var=bkg_test_np)\n",
    "        hist_dict[var_name].extend(\n",
    "            [\n",
    "                copy.deepcopy(sig_train_hist), copy.deepcopy(sig_test_hist), \n",
    "                copy.deepcopy(bkg_train_hist), copy.deepcopy(bkg_test_hist)\n",
    "            ]\n",
    "        )\n",
    "    for mod_factor, label_mod in enumerate(['sig_train', 'sig_test', 'bkg_train', 'bkg_test']):\n",
    "        plot_list = []\n",
    "        label_list = []\n",
    "        for i in range(len(hist_dict[var_name])):\n",
    "            if (i - mod_factor) % 4 == 0:\n",
    "                plot_list.append(hist_dict[var_name][i])\n",
    "                label_list.append(label_arr[i])\n",
    "        make_input_plot(\n",
    "            plot_destdir, var_name, plot_list, labels=label_list, density=True, \n",
    "            plot_prefix=CURRENT_TIME+'_', plot_postfix='_'+label_mod, alpha=0.5,\n",
    "            linestyle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "higgs-dna-hhbbgg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
